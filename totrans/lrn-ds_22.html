<html><head></head><body><section data-pdf-bookmark="Chapter 17. Theory for Inference and Prediction" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-inf-pred-theory">&#13;
<h1><span class="label">Chapter 17. </span>Theory for Inference and Prediction</h1>&#13;
&#13;
<p>When you want to generalize<a contenteditable="false" data-primary="theory for inference and prediction" data-type="indexterm" id="ix_theory_ch17"/> your findings beyond descriptions for your collection of data to a larger setting, the data needs to be representative of that larger world. For example, you may want to predict air quality at a future time based on a sensor reading (<a class="reference internal" data-type="xref" href="ch12.html#ch-pa">Chapter 12</a>), test whether an incentive improves the productivity of contributors based on experimental findings (<a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>), or construct an interval estimate for the amount of time you might spend waiting for a bus (<a class="reference internal" data-type="xref" href="ch05.html#ch-bus">Chapter 5</a>). We touched on all of these scenarios in earlier chapters. In this chapter, we’ll formalize the framework for making predictions and inferences.</p>&#13;
&#13;
<p>At the core of this framework is the notion of a distribution<a contenteditable="false" data-primary="data distributions" data-secondary="in testing, inference, prediction" data-secondary-sortas="testing, inference, prediction" data-type="indexterm" id="id1692"/>, be it a population, empirical (aka sample), or probability distribution. Understanding the connections between these distributions is central to the basics of hypothesis testing, confidence intervals, prediction bands, and risk. We begin with a brief review of the urn model, introduced in <a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>, then we introduce formal definitions of hypothesis tests, confidence intervals, and prediction bands. We use simulation in our examples, including the bootstrap as a special case. We wrap up the chapter with formal definitions of expectation, variance, and standard error—essential concepts in the theory of testing, inference, and prediction.</p>&#13;
&#13;
<section data-pdf-bookmark="Distributions: Population, Empirical, Sampling" data-type="sect1"><div class="sect1" id="distributions-population-empirical-sampling">&#13;
<h1>Distributions: Population, Empirical, Sampling</h1>&#13;
&#13;
<p>The population<a contenteditable="false" data-primary="urn model" data-secondary="theory for inference and prediction" data-type="indexterm" id="ix_urn_mod_theory_inf"/><a contenteditable="false" data-primary="theory for inference and prediction" data-secondary="distributions" data-type="indexterm" id="ix_theory_dist"/><a contenteditable="false" data-primary="predictions and predicting" data-secondary="data distribution" data-type="indexterm" id="ix_predict_data_dist"/><a contenteditable="false" data-primary="sampling distribution" data-type="indexterm" id="ix_samp_dist_ch17"/><a contenteditable="false" data-primary="empirical distribution" data-type="indexterm" id="ix_emp_dist_ch17"/><a contenteditable="false" data-primary="populations" data-secondary="distribution" data-type="indexterm" id="ix_pop_dist_ch17"/><a contenteditable="false" data-primary="data distributions" data-type="indexterm" id="ix_data_dist_ch17"/>, sampling, and empirical distributions are important concepts that guide us when we make inferences about a model or predictions for new observations. <a class="reference internal" data-type="xref" href="#triptych">Figure 17-1</a> provides a diagram that can help distinguish between them. The diagram uses the notions of population and access frame from <a class="reference internal" data-type="xref" href="ch02.html#ch-data-scope">Chapter 2</a> and the urn model from <a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>. On the left is the population that we are studying, represented as marbles in an urn with one marble for each unit. We have simplified the situation to where the access frame and the population are the same; that is, we can access every unit in the population. (The problems that arise when this is not the case are covered in Chapters <a class="reference internal" data-type="xref" data-xrefstyle="select:labelnumber" href="ch02.html#ch-data-scope">2</a> and <a class="reference internal" data-type="xref" data-xrefstyle="select:labelnumber" href="ch03.html#ch-theory-datadesign">3</a>.) The arrow from the urn to the sample represents the design, meaning the protocol for selecting the sample from the frame. The diagram shows this selection process<a contenteditable="false" data-primary="chance mechanism" data-secondary="in data generation process" data-secondary-sortas="data generation process" data-type="indexterm" id="id1693"/> as a chance mechanism, represented by draws from an urn filled with indistinguishable marbles. On the right side of the diagram, the collection of marbles constitutes our sample (the data we got).</p>&#13;
&#13;
<figure><div class="figure" id="triptych"><img src="assets/leds_1701.png"/>&#13;
<h6><span class="label">Figure 17-1. </span>Diagram of the data generation process</h6>&#13;
</div></figure>&#13;
&#13;
<p>We have kept the diagram simple by considering measurements for just one feature. Below the urn in the diagram is the <em>population histogram</em> for that feature. The population histogram represents the distribution of values across the entire population. On the far right, the <em>empirical histogram</em> shows the distribution of values for our actual sample. Notice that these two distributions are similar in shape. This happens when our sampling mechanism produces representative samples.</p>&#13;
&#13;
<p>We are often interested in a summary of the sample measurements, such as the mean, median, slope from a simple linear model, and so on. Typically, this summary statistic is an estimate for a population parameter, such as the population mean or median. The population parameter is shown as <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span> on the left of the diagram; on the right, the summary statistic, calculated from the sample, is <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>.</p>&#13;
&#13;
<p>The chance mechanism<a contenteditable="false" data-primary="probability distribution" data-type="indexterm" id="id1694"/> that generates our sample might well produce a different set of data if we were to conduct our investigation over again. But if the protocols are well designed, we expect the sample to still resemble the population. In other words, we can infer the population parameter from the summary statistic calculated from the sample. The <em>sampling distribution</em> in the middle of the diagram is a <em>probability distribution</em> for the statistic. It shows the possible values that the statistic might take for different samples and their chances. In <a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>, we used simulation to estimate the sampling distribution in several examples. In this chapter, we revisit these and other examples from earlier chapters to formalize the analyses.</p>&#13;
&#13;
<p>One last point about these three histograms: as introduced in <a class="reference internal" data-type="xref" href="ch10.html#ch-eda">Chapter 10</a>, the rectangles provide the fraction of observations in any bin. In the case of the population histogram, this is the fraction of the entire population; for the empirical histogram, the area represents the fraction in the sample; and for the sampling distribution, the area represents the chance the data generation mechanism would produce a sample statistic in this bin.</p>&#13;
&#13;
<p>Finally, we typically don’t know the population distribution or parameter, and we try to infer the parameter or predict values for unseen units in the population. At other times, a conjecture about the population can be tested using the sample. Testing is the topic of the next section<a contenteditable="false" data-primary="" data-startref="ix_theory_dist" data-type="indexterm" id="id1695"/><a contenteditable="false" data-primary="" data-startref="ix_predict_data_dist" data-type="indexterm" id="id1696"/><a contenteditable="false" data-primary="" data-startref="ix_test_theory_dist" data-type="indexterm" id="id1697"/><a contenteditable="false" data-primary="" data-startref="ix_samp_dist_ch17" data-type="indexterm" id="id1698"/><a contenteditable="false" data-primary="" data-startref="ix_emp_dist_ch17" data-type="indexterm" id="id1699"/><a contenteditable="false" data-primary="" data-startref="ix_pop_dist_ch17" data-type="indexterm" id="id1700"/><a contenteditable="false" data-primary="" data-startref="ix_data_dist_ch17" data-type="indexterm" id="id1701"/><a contenteditable="false" data-primary="" data-startref="ix_urn_mod_theory_inf" data-type="indexterm" id="id1702"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Basics of Hypothesis Testing" data-type="sect1"><div class="sect1" id="basics-of-hypothesis-testing">&#13;
<h1>Basics of Hypothesis Testing</h1>&#13;
&#13;
<p>In our experience, hypothesis<a contenteditable="false" data-primary="theory for inference and prediction" data-secondary="hypothesis testing" data-type="indexterm" id="ix_theory_inf_hyp_test"/><a contenteditable="false" data-primary="hypothesis testing" data-type="indexterm" id="ix_hyp_test_ch17"/> testing is one of the more challenging areas of data science—challenging to learn and challenging to apply. This is not necessarily because hypothesis testing is deeply technical; rather, hypothesis testing can be counterintuitive because it makes use of contradictions. As the name suggests, we often start hypothesis testing with a <em>hypothesis</em>: a statement about the world that we would like to verify.</p>&#13;
&#13;
<p>In an ideal world, we would directly prove our hypothesis is true. Unfortunately, we often don’t have access to all the information needed to determine the truth. For example, we might hypothesize that a new vaccine is effective, but contemporary medicine doesn’t yet understand all the details of the biology that govern vaccine efficacy. Instead, we turn to the tools of probability, random sampling, and data design.</p>&#13;
&#13;
<p>One reason hypothesis testing can be confusing is that it’s a lot like “proof by contradiction,” where we assume the opposite of our hypothesis is true and try to show that the data we observe is inconsistent with that assumption. We approach<a contenteditable="false" data-primary="null hypothesis" data-type="indexterm" id="id1703"/><a contenteditable="false" data-primary="alternative hypothesis " data-type="indexterm" id="id1704"/> the problem this way because often, something can be true for many reasons, but we only need a single example to contradict an assumption. We call this “opposite hypothesis” the <em>null hypothesis</em> and our original hypothesis the <em>alternative hypothesis</em>.</p>&#13;
&#13;
<p>To make matters a bit more confusing, the tools of probability don’t directly prove or disprove things. Instead, they tell us how likely or unlikely something we observe is under assumptions, like the assumptions of the null hypothesis. That’s why it’s so important to design the data collection well.</p>&#13;
&#13;
<p>Recall the randomized<a contenteditable="false" data-primary="COVID-19 vaccine efficacy" data-type="indexterm" id="id1705"/> clinical trial of the J&amp;J vaccine (<a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>), where 43,738 people enrolled in the trial were randomly split into two equal groups. The treatment group was given the vaccine and the control was given a fake vaccine, called a <span class="keep-together">placebo</span>. This random assignment created two groups that were similar in every way except for the vaccine.</p>&#13;
&#13;
<p>In this trial, 117 people in the treatment<a contenteditable="false" data-primary="simulation studies" data-secondary="vaccine randomized trial" data-type="indexterm" id="id1706"/><a contenteditable="false" data-primary="vaccine randomized trial simulation" data-type="indexterm" id="id1707"/><a contenteditable="false" data-primary="randomized controlled experiments" data-type="indexterm" id="id1708"/> group fell ill and 351 in the control group got sick. Since we want to provide convincing evidence that the vaccine works, we start with a null hypothesis that it doesn’t work, meaning it was just by chance that the random assignment led to so few illnesses in the treatment group. We can then use probability to calculate the chance of observing so few sick people in the treatment group. The probability calculations are based on the urn that has 43,738 marbles in it, with 468 marked 1 to denote a sick person. We then found that the probability of at most 117 marbles being drawn in 21,869 draws with replacement from the urn was nearly zero. We take this as evidence to reject the null hypothesis in favor of the alternative hypothesis that the vaccine works. Because the J&amp;J experiment was well designed, a rejection of the null leads us to conclude that the vaccine works. In other words, the truth of the hypothesis is left to us and how willing we are to be potentially wrong.</p>&#13;
&#13;
<p>In the rest of this section, we go over the four basic steps of a hypothesis test. We then provide two examples that continue two of the examples from <a data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>, and delve deeper into the formalities for testing.</p>&#13;
&#13;
<p>There are four basic steps to hypothesis testing:</p>&#13;
&#13;
<dl class="simple myst">&#13;
	<dt>Step 1: Set up</dt>&#13;
	<dd>&#13;
	<p>You have your data, and you want to test whether a particular model is reasonably consistent with the data. So you specify a statistic, <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>, such as the sample average, fraction of zeros in a sample, or fitted regression coefficient, with the goal of comparing your data’s statistic to what might have been produced under the model.</p>&#13;
	</dd>&#13;
	<dt>Step 2: Model</dt>&#13;
	<dd>&#13;
	<p>You spell out the model that you want to test in the form of a data generation mechanism, along with any specific assumptions about the population. This model typically includes specifying <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>, which may be the population mean, the proportion of zeros, or a regression coefficient. The sampling distribution of the statistic under this model is referred to as the <em>null distribution</em>, and the model itself is called the <em>null hypothesis</em>.</p>&#13;
	</dd>&#13;
	<dt>Step 3: Compute</dt>&#13;
	<dd>&#13;
	<p>How likely, according to the null model in step 2, is it to get data (and the resulting statistic) at least as extreme as what you actually got in step 1? In formal inference, this probability is called the <span><math> <mi>p</mi> </math></span>-<em>value</em>. To approximate the <span><math> <mi>p</mi> </math></span>-value, we often use the computer to generate a large number of repeated random trials using the assumptions in the model and find the fraction of samples that give a value of the statistic at least as extreme as our observed value. Other times, we can instead use mathematical theory to find the <span><math> <mi>p</mi> </math></span>-value.</p>&#13;
	</dd>&#13;
	<dt>Step 4: Interpret</dt>&#13;
	<dd>&#13;
	<p>The <span><math> <mi>p</mi> </math></span>-value<a contenteditable="false" data-primary="p-values" data-type="indexterm" id="id1709"/> is used as a measure of surprise. If the model that you spelled out in step 2 is believable, how surprised should you be to get the data (and summary statistic) that you actually got? A moderately sized <span><math> <mi>p</mi> </math></span>-value means that the observed statistic is pretty much what you would expect to get for data generated by the null model. A tiny <span><math> <mi>p</mi> </math></span>-value raises doubts about the null model. In other words, if the model is correct (or approximately correct), then it would be very unusual to get such an extreme value of the test statistic from data generated by the model. In this case, either the null model is wrong or a very unlikely outcome has occurred. Statistical logic says to conclude that the pattern is real, that it is more than just coincidence. Then it’s up to you to explain why the data generation process led to such an unusual value. This is when a careful consideration of the scope is important.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Let’s demonstrate these steps in the testing process with a couple of examples.</p>&#13;
&#13;
<section data-pdf-bookmark="Example: A Rank Test to Compare Productivity of Wikipedia Contributors" data-type="sect2"><div class="sect2" id="example-a-rank-test-to-compare-productivity-of-wikipedia-contributors">&#13;
<h2>Example: A Rank Test to Compare Productivity of <span class="keep-together">Wikipedia Contributors</span></h2>&#13;
&#13;
<p>Recall the Wikipedia example from <a class="reference internal" data-type="xref" href="ch02.html#ch-data-scope">Chapter 2</a>, where a randomly selected set of 200 contributors were chosen from among the top 1% of contributors who were active in the past 30 days on the English-language Wikipedia and who had never received an award. These 200 contributors were divided at random into two groups of 100. The contributors in one group, the treatment group, were each given an informal award, while no one in the other group was given one. All 200 contributors were followed for 90 days and their activity on Wikipedia recorded.</p>&#13;
&#13;
<p>It has been conjectured that informal awards have a reinforcing effect on volunteer work, and this experiment was designed to formally study this conjecture. We carry out a hypothesis test based on the rankings of the data.</p>&#13;
&#13;
<p>First, we read the data into a dataframe:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">wiki</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">read_csv</code></span><span><code class="p">(</code></span><span><code class="s2">"</code><code class="s2">data/Wikipedia.csv</code><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">wiki</code></span><span><code class="o">.</code></span><span><code class="n">shape</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
(200, 2)&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">wiki</code></span><span><code class="o">.</code></span><span><code class="n">describe</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">[</code></span><span><code class="mi">3</code></span><span><code class="p">:</code><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>experiment</th>&#13;
			<th>postproductivity</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>min</strong></td>&#13;
			<td>0.0</td>&#13;
			<td class="right">0.0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>25%</strong></td>&#13;
			<td>0.0</td>&#13;
			<td class="right">57.5</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>50%</strong></td>&#13;
			<td>0.5</td>&#13;
			<td class="right">250.5</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>75%</strong></td>&#13;
			<td>1.0</td>&#13;
			<td class="right">608.0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>max</strong></td>&#13;
			<td>1.0</td>&#13;
			<td class="right">2344.0</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>The dataframe has 200 rows, one for each contributor. The feature <code>experiment</code> is either 0 or 1, depending on whether the contributor was in the control or treatment group, respectively, and <code>postproductivity</code> is a count of the edits made by the contributor in the 90 days after the awards were made. The gap between the quartiles (lower, middle, and upper) suggests the distribution of productivity is skewed. We make a histogram to confirm:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">px</code><code class="o">.</code><code class="n">histogram</code><code class="p">(</code>&#13;
	<code class="n">wiki</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s1">'postproductivity'</code><code class="p">,</code> <code class="n">nbins</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code>&#13;
	<code class="n">labels</code><code class="o">=</code><code class="p">{</code><code class="s1">'postproductivity'</code><code class="p">:</code> <code class="s1">'Number of actions in 90 days post award'</code><code class="p">},</code>&#13;
	<code class="n">width</code><code class="o">=</code><code class="mi">350</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mi">250</code><code class="p">)</code>&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal width-55"><div class="figure"><img src="assets/leds_17in01.png"/></div></figure>&#13;
&#13;
<p>Indeed, the histogram of post-award productivity is highly skewed, with a spike near zero. The skewness suggests a statistic based on the ordering of the values from the two samples.</p>&#13;
&#13;
<p>To compute our statistic, we order all productivity values (from both groups) from smallest to largest. The smallest value has rank 1, the second smallest rank 2, and so on, up to the largest value, which has a rank of 200. We use these ranks to compute our statistic, <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>, which is the average rank of the treatment group. We chose this statistic because it is insensitive to highly skewed distributions. For example, whether the largest value is 700 or 700,000, it still receives the same rank, namely 200. If the informal award incentivizes contributors, then we would expect the average rank of the treatment group to be typically higher than the control.</p>&#13;
&#13;
<p>The null model assumes that an informal award has <em>no</em> effect on productivity, and any difference observed between the treatment and control groups is due to the chance process in assigning contributors to groups. The null hypothesis is set up for the status quo to be rejected; that is, we hope to find a surprise in assuming no effect.</p>&#13;
&#13;
<p>The null hypothesis can be represented by 100 draws from an urn with 200 marbles, marked 1, 2, 3, …, 200. In this case, the average rank would be <span><math> <mo stretchy="false">(</mo> <mn>1</mn> <mo>+</mo> <mn>200</mn> <mo stretchy="false">)</mo> <mrow> <mo>/</mo> </mrow> <mn>2</mn> <mo>=</mo> <mn>100.5</mn> </math></span>.</p>&#13;
&#13;
<p>We use<a contenteditable="false" data-primary="rankdata() method" data-type="indexterm" id="id1710"/> the <code>rankdata</code> method in <code>scipy.stats</code> to rank the 200 values and compute the sum of ranks in the treatment group:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">scipy</code><code class="nn">.</code><code class="nn">stats</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">rankdata</code></span><code>&#13;
</code><span><code class="n">ranks</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">rankdata</code></span><span><code class="p">(</code></span><span><code class="n">wiki</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">postproductivity</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">average</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Let’s confirm that the average rank of the 200 values is 100.5:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">average</code></span><span><code class="p">(</code></span><span><code class="n">ranks</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
100.5&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>And find the average rank of the 100 productivity scores in the treatment group:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">observed</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">average</code></span><span><code class="p">(</code></span><span><code class="n">ranks</code></span><span><code class="p">[</code></span><span><code class="mi">100</code></span><span><code class="p">:</code><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><span><code class="n">observed</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
113.68&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>The average rank in the treatment group is higher than expected, but we want to figure out if it is an unusually high value. We can use simulation to find the sampling distribution for this statistic to see if 113 is a routine value or a surprising one.</p>&#13;
&#13;
<p>To carry out this simulation, we set up the urn as the <code>ranks</code> array from the data. Shuffling the 200 values in the array and taking the first 100 represents a randomly sampled treatment group. We write a function to shuffle the array of ranks and find the average of the first 100.</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">rng</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">random</code></span><span><code class="o">.</code></span><span><code class="n">default_rng</code></span><span><code class="p">(</code></span><span><code class="mi">42</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">rank_avg</code></span><span><code class="p">(</code></span><span><code class="n">ranks</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">n</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="n">rng</code></span><span><code class="o">.</code></span><span><code class="n">shuffle</code></span><span><code class="p">(</code></span><span><code class="n">ranks</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">average</code></span><span><code class="p">(</code></span><span><code class="n">ranks</code></span><span><code class="p">[</code></span><span><code class="n">n</code></span><span><code class="p">:</code><code class="p">]</code><code class="p">)</code></span><code>      </code><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Our simulation mixes the marbles in the urn, draws 100 times, computes the average rank for the 100 draws, and repeats this 100,000 times.</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">rank_avg_simulation</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">[</code></span><span><code class="n">rank_avg</code></span><span><code class="p">(</code></span><span><code class="n">ranks</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">100</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">_</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="nb">range</code></span><span><code class="p">(</code></span><span><code class="mi">100_000</code></span><span><code class="p">)</code><code class="p">]</code></span><code> </code><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Here is a histogram of the simulated averages:</p>&#13;
&#13;
<figure class="informal width-60"><div class="figure"><img src="assets/leds_17in02.png"/></div></figure>&#13;
&#13;
<p>As we expected, the sampling distribution of the average rank is centered on 100 (100.5 actually) and is bell-shaped. The center of this distribution reflects the assumptions of the treatment having no effect. Our observed statistic is well outside the typical range of simulated average ranks, and we use this simulated sampling distribution to find the approximate <span><math> <mi>p</mi> </math></span>-value for observing a statistic at least as big as ours:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">mean</code></span><span><code class="p">(</code></span><span><code class="n">rank_avg_simulation</code></span><code> </code><span><code class="o">&gt;</code></span><code> </code><span><code class="n">observed</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
0.00058&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>This is a big surprise. Under the null, the chance of seeing an average rank at least as large as ours is about 5 in 10,000.</p>&#13;
&#13;
<p>This test raises doubt about the null model. Statistical logic has us conclude that the pattern is real. How do we interpret this? The experiment was well designed. The 200 contributors were selected at random from the top 1%, and then they were divided at random into two groups. These chance processes say that we can rely on the sample of 200 being representative of top contributors, and on the treatment and control groups being similar to each other in every way except for the application of the treatment (the award). Given the careful design, we conclude that informal awards have a positive effect on productivity for top contributors.</p>&#13;
&#13;
<p>Earlier, we implemented a simulation to find the <span><math> <mi>p</mi> </math></span>-value for our observed statistic. In practice, rank tests are commonly used and made available in most statistical <span class="keep-together">software</span>:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">scipy</code><code class="nn">.</code><code class="nn">stats</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">ranksums</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">ranksums</code></span><span><code class="p">(</code></span><span><code class="n">x</code></span><span><code class="o">=</code></span><span><code class="n">wiki</code></span><span><code class="o">.</code></span><span><code class="n">loc</code></span><span><code class="p">[</code></span><span><code class="n">wiki</code></span><span><code class="o">.</code></span><span><code class="n">experiment</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="mi">1</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">postproductivity</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>         </code><span><code class="n">y</code></span><span><code class="o">=</code></span><span><code class="n">wiki</code></span><span><code class="o">.</code></span><span><code class="n">loc</code></span><span><code class="p">[</code></span><span><code class="n">wiki</code></span><span><code class="o">.</code></span><span><code class="n">experiment</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="mi">0</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">postproductivity</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
RanksumsResult(statistic=3.220386553232206, pvalue=0.0012801785007519996)&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>The <span><math> <mi>p</mi> </math></span>-value<a contenteditable="false" data-primary="ranksums test" data-type="indexterm" id="id1711"/><a contenteditable="false" data-primary="p-values" data-type="indexterm" id="id1712"/> here is twice the <span><math> <mi>p</mi> </math></span>-value we computed because we considered only values greater than the observed, whereas the <code>ranksums</code> test computed the the <span><math> <mi>p</mi> </math></span>-value for both sides of the distribution. In our example, we are only interested in an increase in productivity, and so use a one-sided <span><math> <mi>p</mi> </math></span>-value, which is half the reported value (0.0006) and close to our simulated value.</p>&#13;
&#13;
<p>This somewhat unusual test statistic that uses ranks rather than the actual data values was developed in the 1950s and 1960s, before today’s era of powerful laptop computers. The mathematical properties of rank statistics is well developed and the sampling distribution is well behaved (it is symmetric and shaped like the bell curve even for small datasets). Rank<a contenteditable="false" data-primary="data distributions" data-secondary="normal distribution" data-type="indexterm" id="id1713"/><a contenteditable="false" data-primary="normal distribution" data-type="indexterm" id="id1714"/> tests remain popular for A/B testing where samples tend to be highly skewed, and it is common to carry out many, many tests where <span><math> <mi>p</mi> </math></span>-values can be computed rapidly from the normal distribution.</p>&#13;
&#13;
<p>The next example revisits the vaccine efficacy example from <a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>. There, we encountered a hypothesis test without actually calling it that.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Example: A Test of Proportions for Vaccine Efficacy" data-type="sect2"><div class="sect2" id="example-a-test-of-proportions-for-vaccine-efficacy">&#13;
<h2>Example: A Test of Proportions for Vaccine Efficacy</h2>&#13;
&#13;
<p>The approval of a vaccine<a contenteditable="false" data-primary="vaccine randomized trial simulation" data-type="indexterm" id="ix_vacc_eff_sim"/><a contenteditable="false" data-primary="simulation studies" data-secondary="vaccine randomized trial" data-type="indexterm" id="ix_sim_stud_vacc_eff"/><a contenteditable="false" data-primary="COVID-19 vaccine efficacy" data-type="indexterm" id="ix_covid_vacc_eff3"/> is subject to stricter requirements than the simple test we performed earlier where we compared the disease counts in the treatment group to those of the control group. The CDC requires stronger evidence of success based on a comparison of the proportion of sick individuals in each group. To explain, we express the sample proportion of sick people in the control and treatment groups as <span><math> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mrow> <mi>C</mi> </mrow> </msub> </math></span> and <span><math> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>T</mi> </msub> </math></span>, respectively, and use these proportions to compute vaccine efficacy:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>=</mo> <mfrac> <mrow> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mrow> <mi>C</mi> </mrow> </msub> <mo>−</mo> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>T</mi> </msub> </mrow> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>C</mi> </msub> </mfrac> <mo>=</mo> <mn>1</mn> <mo>−</mo> <mfrac> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>T</mi> </msub> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>C</mi> </msub> </mfrac> </math></div>&#13;
&#13;
<p>The observed value of vaccine efficacy in the J&amp;J trial is:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mn>1</mn> <mo>−</mo> <mfrac> <mrow> <mn>117</mn> <mrow> <mo>/</mo> </mrow> <mn>21869</mn> </mrow> <mrow> <mn>351</mn> <mrow> <mo>/</mo> </mrow> <mn>21869</mn> </mrow> </mfrac> <mo>=</mo> <mn>1</mn> <mo>−</mo> <mfrac> <mn>117</mn> <mn>351</mn> </mfrac> <mo>=</mo> <mn>0.667</mn> </math></div>&#13;
&#13;
<p>If the treatment doesn’t work, the efficacy would be near 0. The CDC sets a standard of 50% for vaccine efficacy, meaning that the efficacy has to exceed 50% to be approved for distribution. In this situation, the null model assumes that vaccine efficacy is 50% (<span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> <mo>=</mo> <mn>0.5</mn> </math></span>), and any difference of the observed value from the expected is due to the chance process in assigning people to groups. Again, we set the null hypothesis to be the status quo that the vaccine isn’t effective enough to warrant approval, and we hope to find a surprise and reject the null.</p>&#13;
&#13;
<p>With a little algebra, the null model <span><math> <mn>0.5</mn> <mo>=</mo> <mn>1</mn> <mo>−</mo> <msub> <mi>p</mi> <mi>T</mi> </msub> <mrow> <mo>/</mo> </mrow> <msub> <mi>p</mi> <mi>C</mi> </msub> </math></span> reduces to <span><math> <msub> <mi>p</mi> <mi>T</mi> </msub> <mo>=</mo> <mn>0.5</mn> <msub> <mi>p</mi> <mi>C</mi> </msub> </math></span>. That is, the null hypothesis implies that the proportion of ill people among those receiving the treatment is at most half that of the control. Notice that the actual values for the two risks (<span><math> <msub> <mi>p</mi> <mi>T</mi> </msub> </math></span> and <span><math> <msub> <mi>p</mi> <mi>C</mi> </msub> </math></span>) are not assumed in the null. That is, the model doesn’t assume the treatment doesn’t work, but rather, that its efficacy is no larger than 0.5.</p>&#13;
&#13;
<p>Our urn model<a contenteditable="false" data-primary="urn model" data-secondary="theory for inference and prediction" data-type="indexterm" id="ix_urn_mod_theory_inf2"/> in this situation is a bit different from what we set up in <a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>. The urn still has 43,738 marbles in it, corresponding to the enrollees in the experiment. But now each marble has two numbers on it, which for simplicity appear in a pair, such as <span><math> <mo stretchy="false">(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo stretchy="false">)</mo> </math></span>. The number on the left is the response if the person receives the treatment, and the number on the right corresponds to the response to no treatment (the control). As usual, 1 means they become ill and 0 means they stay healthy.</p>&#13;
&#13;
<p>The null model assumes that the proportion of ones on the left of the pair is half the proportion on the right. Since we don’t know these two proportions, we can use the data to estimate them. There are three types of marbles in the urn <span><math> <mo stretchy="false">(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo stretchy="false">)</mo> </math></span>, <span><math> <mo stretchy="false">(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo stretchy="false">)</mo> </math></span>, and <span><math> <mo stretchy="false">(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo stretchy="false">)</mo> </math></span>. We assume that <span><math> <mo stretchy="false">(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn> <mo stretchy="false">)</mo> </math></span>, which corresponds to a person getting ill under treatment and not under control, is not possible. We observed 351 people getting sick in control and 117 in treatment. With the assumption that the treatment rate of illness is half that of the control, we can tray a scenario for the makeup of the urn. For example, we can study the case where 117 people in treatment didn’t get sick but would have if they were in the control group, so combined, all 585 people (<span><math> <mn>351</mn> <mo>+</mo> <mn>117</mn> <mo>+</mo> <mn>117</mn> </math></span>) would get the virus if they didn’t receive the vaccine and half of them would not get the virus if they received treatment. <a class="reference internal" data-type="xref" href="#vacc-urn">Table 17-1</a> shows these counts.</p>&#13;
&#13;
<table id="vacc-urn">&#13;
	<caption><span class="label">Table 17-1. </span><span>Vaccine trial urn</span></caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th class="head">Label</th>&#13;
			<th class="head">Count</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>(0, 0)</td>&#13;
			<td class="right">&#13;
			<p>43,152</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>(0, 1)</td>&#13;
			<td class="right">&#13;
			<p>293</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>(1, 0)</td>&#13;
			<td class="right">&#13;
			<p>0</p>&#13;
			</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>(1, 1)</td>&#13;
			<td class="right">&#13;
			<p>293</p>&#13;
			</td>&#13;
		</tr>&#13;
	</tbody>&#13;
	<tfoot>&#13;
		<tr>&#13;
			<td>Total</td>&#13;
			<td class="right">&#13;
			<p>43,738</p>&#13;
			</td>&#13;
		</tr>&#13;
	</tfoot>&#13;
</table>&#13;
&#13;
<p>We can use these counts to carry out a simulation of the clinical trial and compute vaccine efficacy. As shown in <a class="reference internal" data-type="xref" href="ch03.html#ch-theory-datadesign">Chapter 3</a>, the multivariate hypergeometric function simulates draws from an urn when there are more than two kinds of marbles. We set up this urn and sampling process:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">N</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="mi">43738</code></span><code>&#13;
</code><span><code class="n">n_samp</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="mi">21869</code></span><code>&#13;
</code><span><code class="n">N_groups</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">array</code></span><span><code class="p">(</code><code class="p">[</code></span><span><code class="mi">293</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">293</code></span><span><code class="p">,</code></span><code> </code><span><code class="p">(</code></span><span><code class="n">N</code></span><code> </code><span><code class="o">-</code></span><code> </code><span><code class="mi">586</code></span><span><code class="p">)</code><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="kn">from</code></span><code> </code><span><code class="nn">scipy</code><code class="nn">.</code><code class="nn">stats</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">multivariate_hypergeom</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">vacc_eff</code></span><span><code class="p">(</code></span><span><code class="n">N_groups</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">n_samp</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="n">treat</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">multivariate_hypergeom</code></span><span><code class="o">.</code></span><span><code class="n">rvs</code></span><span><code class="p">(</code></span><span><code class="n">N_groups</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">n_samp</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="n">ill_t</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">treat</code></span><span><code class="p">[</code></span><span><code class="mi">1</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>    </code><span><code class="n">ill_c</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">N_groups</code></span><span><code class="p">[</code></span><span><code class="mi">0</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">-</code></span><code> </code><span><code class="n">treat</code></span><span><code class="p">[</code></span><span><code class="mi">0</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="n">N_groups</code></span><span><code class="p">[</code></span><span><code class="mi">1</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">-</code></span><code> </code><span><code class="n">treat</code></span><span><code class="p">[</code></span><span><code class="mi">1</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="p">(</code></span><span><code class="n">ill_c</code></span><code> </code><span><code class="o">-</code></span><code> </code><span><code class="n">ill_t</code></span><span><code class="p">)</code></span><code> </code><span><code class="o">/</code></span><code> </code><span><code class="n">ill_c</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Now we can simulate the clinical trial 100,000 times and calculate the vaccine efficacy for each trial:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">random</code></span><span><code class="o">.</code></span><span><code class="n">seed</code></span><span><code class="p">(</code></span><span><code class="mi">42</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">sim_vacc_eff</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">array</code></span><span><code class="p">(</code><code class="p">[</code></span><span><code class="n">vacc_eff</code></span><span><code class="p">(</code></span><span><code class="n">N_groups</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">n_samp</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">_</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="nb">range</code></span><span><code class="p">(</code></span><span><code class="mi">100_000</code></span><span><code class="p">)</code><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">px</code></span><span><code class="o">.</code></span><span><code class="n">histogram</code></span><span><code class="p">(</code></span><span><code class="n">x</code></span><span><code class="o">=</code></span><span><code class="n">sim_vacc_eff</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">nbins</code></span><span><code class="o">=</code></span><span><code class="mi">50</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>            </code><span><code class="n">labels</code></span><span><code class="o">=</code></span><span><code class="nb">dict</code></span><span><code class="p">(</code></span><span><code class="n">x</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">Simulated vaccine efficacy</code><code class="s1">'</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>            </code><span><code class="n">width</code></span><span><code class="o">=</code></span><span><code class="mi">350</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">height</code></span><span><code class="o">=</code></span><span><code class="mi">250</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal width-60"><div class="figure"><img src="assets/leds_17in03.png"/></div></figure>&#13;
&#13;
<p>The sampling distribution is centered at 0.5, which agrees with our model assumptions. We see that 0.667 is far out in the tail of this distribution:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">mean</code></span><span><code class="p">(</code></span><span><code class="n">sim_vacc_eff</code></span><code> </code><span><code class="o">&gt;</code></span><code> </code><span><code class="mf">0.667</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
1e-05&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Only a tiny handful of the 100,000 simulations have a vaccine efficacy as large as the observed 0.667. This is a rare event, and that’s why the CDC approved the Johnson &amp; Johnson vaccine for distribution.</p>&#13;
&#13;
<p>In this example of hypothesis testing, we were not able to completely specify the model, and we had to provide approximate values for <span><math> <msub> <mi>p</mi> <mi>C</mi> </msub> </math></span> and <span><math> <msub> <mi>p</mi> <mi>T</mi> </msub> </math></span> based on our observed values of <span><math> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>C</mi> </msub> </math></span> and <span><math> <msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>T</mi> </msub> </math></span>. At times, the null model isn’t entirely specified, and we must rely on the data to set up the model. The next section introduces a general approach, called the bootstrap, to approximate the model using the data<a contenteditable="false" data-primary="" data-startref="ix_vacc_eff_sim" data-type="indexterm" id="id1715"/><a contenteditable="false" data-primary="" data-startref="ix_covid_vacc_eff3" data-type="indexterm" id="id1716"/><a contenteditable="false" data-primary="" data-startref="ix_sim_stud_vacc_eff" data-type="indexterm" id="id1717"/><a contenteditable="false" data-primary="" data-startref="ix_hyp_test_ch17" data-type="indexterm" id="id1718"/><a contenteditable="false" data-primary="" data-startref="ix_theory_inf_hyp_test" data-type="indexterm" id="id1719"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Bootstrapping for Inference" data-type="sect1"><div class="sect1" id="bootstrapping-for-inference">&#13;
<h1>Bootstrapping for Inference</h1>&#13;
&#13;
<p>In many hypothesis<a contenteditable="false" data-primary="urn model" data-secondary="bootstrapping" data-type="indexterm" id="ix_urn_mod_boot"/><a contenteditable="false" data-primary="theory for inference and prediction" data-secondary="bootstrapping for inference" data-type="indexterm" id="ix_theory_inf_boot"/><a contenteditable="false" data-primary="bootstrapping for inference" data-type="indexterm" id="ix_boot_inf"/> tests the assumptions of the null hypothesis lead to a complete specification of a hypothetical population and data design (see <a class="reference internal" data-type="xref" href="#triptych">Figure 17-1</a>), and we use this specification to simulate the sampling distribution of a statistic. For example, the rank test for the Wikipedia experiment led us to sample the integers 1, …, 200, which we easily simulated. Unfortunately, we can’t always specify the population and model completely. To remedy the situation, we substitute the data for the population. This substitution is at the heart of the notion of the bootstrap. <a class="reference internal" data-type="xref" href="#boot-triptych">Figure 17-2</a> updates <a class="reference internal" data-type="xref" href="#triptych">Figure 17-1</a> to reflect this idea; here the population distribution is replaced by the empirical distribution to create what is called the <em>bootstrap population</em>.</p>&#13;
&#13;
<figure><div class="figure" id="boot-triptych"><img src="assets/leds_1702.png"/>&#13;
<h6><span class="label">Figure 17-2. </span>Diagram of bootstrapping the data generation process</h6>&#13;
</div></figure>&#13;
&#13;
<p>The rationale for the bootstrap goes like this:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Your sample looks like the population because it is a representative sample, so we replace the population with the sample and call it the bootstrap population.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Use the same data generation process that produced the original sample to get a new sample, which is called a <em>bootstrap sample</em>, to reflect the change in the population. Calculate the statistic on the bootstrap sample in the same manner as before and call it the <em>bootstrap statistic</em>. The <em>bootstrap sampling distribution</em> of the bootstrap statistic should be similar in shape and spread to the true sampling distribution of the statistic.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Simulate the data generation process many times, using the bootstrap population, to get bootstrap samples and their bootstrap statistics. The distribution of the simulated bootstrap statistics approximates the bootstrap sampling distribution of the bootstrap statistic, which itself approximates the original sampling <span class="keep-together">distribution</span>.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Take a close look at <a class="reference internal" data-type="xref" href="#boot-triptych">Figure 17-2</a> and compare it to <a class="reference internal" data-type="xref" href="#triptych">Figure 17-1</a>. Essentially, the bootstrap simulation involves two approximations: the original sample approximates the population, and the simulation approximates the sampling distribution. We have been using the second approximation in our examples so far; the approximation of the population by the sample is the core notion behind bootstrapping. Notice that in <a class="reference internal" data-type="xref" href="#boot-triptych">Figure 17-2</a>, the distribution of the bootstrap population (on the left) looks like the original sample histogram; the sampling distribution (in the middle) is still a probability distribution based on the same data generation process as in the original study, but it now uses the bootstrap population; and the sample distribution (on the right) is a histogram of one sample taken from the bootstrap population.</p>&#13;
&#13;
<p>You might be wondering how to take a simple random sample from your bootstrap population and not wind up with the exact same sample each time. After all, if your sample has 100 units in it and you use it as your bootstrap population, then 100 draws from the bootstrap population without replacement will take all of the units and give you the same bootstrap sample every time. There are two approaches to solving this problem:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>When sampling from the bootstrap population, draw units from the bootstrap population with replacement. Essentially, if the original population is very large, then there is little difference between sampling with and without replacement. This is the more common approach by far.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>“Blow up the sample” to be the same size as the original population. That is, tally the fraction of each unique value in the sample, and add units to the bootstrap population so that it is the same size as the original population, while maintaining the proportions. For example, if the sample is size 30 and 1/3 of the sample values are 0, then a bootstrap population of 750 should include 250 zeros. Once you have this bootstrap population, use the original data generation procedure to take the bootstrap samples.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The example of vaccine efficacy<a contenteditable="false" data-primary="parameterized bootstrap" data-type="indexterm" id="id1720"/> used a bootstrap-like process, called the <em>parameterized bootstrap</em>. Our null model specified 0-1 urns, but we didn’t know how many 0s and 1s to put in the urn. We used the sample to determine the proportions of 0s and 1s; that is, the sample specified the parameters of the multivariate hypergeometric. Next, we use the example of calibrating air quality monitors to show how bootstrapping could be used to test a hypothesis.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>It’s a common mistake to think that the center of the bootstrap sampling distribution is the same as the center of the true sampling distribution. If the mean of the sample is not 0, then the mean of the bootstrap population is also not 0. That’s why we use the spread of the bootstrap distribution, and not its center, in hypothesis testing. The next example shows how we might use the bootstrap to test a hypothesis.</p>&#13;
</div>&#13;
&#13;
<p>The case study<a contenteditable="false" data-primary="air quality sensors study" data-secondary="bootstrapping for inference" data-type="indexterm" id="ix_aqs_boot_humid"/> on calibrating air quality monitors (see <a class="reference internal" data-type="xref" href="ch12.html#ch-pa">Chapter 12</a>) fit a model to adjust the measurements from an inexpensive monitor to more accurately reflect true air quality. This adjustment included a term in the model related to humidity. The fitted coefficient was about <span><math> <mn>0.2</mn> </math></span> so that on days of high humidity the measurement is adjusted upward more than on days of low humidity. However, this coefficient is close to 0, and we might wonder whether including humidity in the model is really needed. In other words, we want to test the hypothesis that the coefficient for humidity in the linear model is 0. Unfortunately, we can’t fully specify the model, because it is based on measurements taken over a particular time period from a set of air monitors (both PurpleAir and those maintained by the EPA). This is where the bootstrap can help.</p>&#13;
&#13;
<p>Our model makes the assumption that the air quality measurements taken resemble the population of measurements. Note that weather conditions, the time of year, and the location of the monitors make this statement a bit hand-wavy; what we mean here is that the measurements are similar to others taken under the same conditions as those when the original measurements were taken. Also, since we can imagine a virtually infinite supply of air quality measurements, we think of the procedure for generating measurements as draws with replacement from the urn. Recall that in <a class="reference internal" data-type="xref" href="ch02.html#ch-data-scope">Chapter 2</a> we modeled the urn as repeated draws with replacement from an urn of measurement errors. This situation is a bit different because we are also including the other factors mentioned already (weather, season, location).</p>&#13;
&#13;
<p>Our model is focused on the coefficient for humidity in the linear model:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtext>PA</mtext> <mo>≈</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mtext>AQ</mtext> <mo>+</mo> <msub> <mi>θ</mi> <mn>2</mn> </msub> <mtext>RH</mtext> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Here, <span><math> <mtext>PA</mtext> </math></span> refers to the PurpleAir PM2.5 measurement, <span><math> <mtext>RH</mtext> </math></span> is the relative humidity, and <span><math> <mtext>AQ</mtext> </math></span> stands for the more exact measurement of PM2.5 made by the more accurate AQS monitors. The null hypothesis is <span><math> <msub> <mi>θ</mi> <mn>2</mn> </msub> <mo>=</mo> <mn>0</mn> </math></span>; that is, the null model is the simpler model:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtext>PA</mtext> <mo>≈</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mtext>AQ</mtext> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>To estimate <span><math> <msub> <mi>θ</mi> <mn>2</mn> </msub> </math></span>, we use the linear model fitting procedure from <a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>.</p>&#13;
&#13;
<p>Our bootstrap population<a contenteditable="false" data-primary="boot_stat() function" data-type="indexterm" id="id1721"/><a contenteditable="false" data-primary="chance mechanism" data-secondary="randint" data-type="indexterm" id="id1722"/> consists of the measurements from Georgia that we used in <a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>. Now we sample rows from the dataframe (which is equivalent to our urn) with replacement using the chance mechanism <code>randint</code>. This function takes random samples with replacement from a set of integers. We use the random sample of indices to create the bootstrap sample from the dataframe. Then we fit the linear model and get the coefficient for humidity (our bootstrap statistic). The following <code>boot_stat</code> function performs this simulation process:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">scipy</code><code class="nn">.</code><code class="nn">stats</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">randint</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">boot_stat</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="n">n</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="nb">len</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="n">bootstrap_indexes</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">randint</code></span><span><code class="o">.</code></span><span><code class="n">rvs</code></span><span><code class="p">(</code></span><span><code class="n">low</code></span><span><code class="o">=</code></span><span><code class="mi">0</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">high</code></span><span><code class="o">=</code></span><span><code class="p">(</code></span><span><code class="n">n</code></span><code> </code><span><code class="o">-</code></span><code> </code><span><code class="mi">1</code></span><span><code class="p">)</code><code class="p">,</code></span><code> </code><span><code class="n">size</code></span><span><code class="o">=</code></span><span><code class="n">n</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="n">theta2</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><code>&#13;
</code><code>        </code><span><code class="n">LinearRegression</code></span><span><code class="p">(</code><code class="p">)</code></span><code>&#13;
</code><code>        </code><span><code class="o">.</code></span><span><code class="n">fit</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="o">.</code></span><span><code class="n">iloc</code></span><span><code class="p">[</code></span><span><code class="n">bootstrap_indexes</code></span><span><code class="p">,</code></span><code> </code><span><code class="p">:</code><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="o">.</code></span><span><code class="n">iloc</code></span><span><code class="p">[</code></span><span><code class="n">bootstrap_indexes</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><code>        </code><span><code class="o">.</code></span><span><code class="n">coef_</code></span><span><code class="p">[</code></span><span><code class="mi">1</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>    </code><span><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">theta2</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We set up the design matrix and the outcome variable and check our <code>boot_stat</code> function once to test it:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">X</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">GA</code></span><span><code class="p">[</code><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">pm25aqs</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">rh</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">]</code></span><code>&#13;
</code><span><code class="n">y</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">GA</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">pm25pa</code><code class="s1">'</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">boot_stat</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
0.21572251745549495&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>When we repeat this process 10,000 times, we get an approximation to the bootstrap sampling distribution of the bootstrap statistic (the fitted humidity coefficient):</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">random</code></span><span><code class="o">.</code></span><span><code class="n">seed</code></span><span><code class="p">(</code></span><span><code class="mi">42</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">boot_theta_hat</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">array</code></span><span><code class="p">(</code><code class="p">[</code></span><span><code class="n">boot_stat</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">_</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="nb">range</code></span><span><code class="p">(</code></span><span><code class="mi">10_000</code></span><span><code class="p">)</code><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We are interested in the shape and spread of this bootstrap sampling distribution (we know that the center will be close to the original coefficient of <span><math> <mn>0.21</mn> </math></span>):</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">px</code></span><span><code class="o">.</code></span><span><code class="n">histogram</code></span><span><code class="p">(</code></span><span><code class="n">x</code></span><span><code class="o">=</code></span><span><code class="n">boot_theta_hat</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">nbins</code></span><span><code class="o">=</code></span><span><code class="mi">50</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>             </code><span><code class="n">labels</code></span><span><code class="o">=</code></span><span><code class="nb">dict</code></span><span><code class="p">(</code></span><span><code class="n">x</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">Bootstrapped humidity coefficient</code><code class="s1">'</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>             </code><span><code class="n">width</code></span><span><code class="o">=</code></span><span><code class="mi">350</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">height</code></span><span><code class="o">=</code></span><span><code class="mi">250</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal width-60"><div class="figure"><img src="assets/leds_17in04.png"/></div></figure>&#13;
&#13;
<p>By design, the center of the bootstrap sampling distribution will be near <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span> because the bootstrap population consists of the observed data. So, rather than compute the chance of a value at least as large as the observed statistic, we find the chance of a value at least as small as 0. The hypothesized value of 0 is far from the sampling <span class="keep-together">distribution</span>.</p>&#13;
&#13;
<p>None of the 10,000 simulated regression coefficients are as small as the hypothesized coefficient. Statistical logic leads us to reject the null hypothesis that we do not need to adjust the model for humidity.</p>&#13;
&#13;
<p>The form of the hypothesis test we performed here looks different than the earlier tests because the sampling distribution of the statistic is not centered on the null. That is because we are using the bootstrap to create the sampling distribution. We are, in effect, using a confidence interval for the coefficient to test the hypothesis. In the next section we introduce interval estimates more generally, including those based on the bootstrap, and we connect the concepts of hypothesis testing and confidence <span class="keep-together">intervals<a contenteditable="false" data-primary="" data-startref="ix_urn_mod_theory_inf2" data-type="indexterm" id="id1723"/></span><a contenteditable="false" data-primary="" data-startref="ix_theory_inf_boot" data-type="indexterm" id="id1724"/><a contenteditable="false" data-primary="" data-startref="ix_boot_inf" data-type="indexterm" id="id1725"/><a contenteditable="false" data-primary="" data-startref="ix_aqs_boot_humid" data-type="indexterm" id="id1726"/><a contenteditable="false" data-primary="" data-startref="ix_urn_mod_boot" data-type="indexterm" id="id1727"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Basics of Confidence Intervals" data-type="sect1"><div class="sect1" id="basics-of-confidence-intervals">&#13;
<h1>Basics of Confidence Intervals</h1>&#13;
&#13;
<p>We have seen that modeling<a contenteditable="false" data-primary="theory for inference and prediction" data-secondary="confidence intervals" data-type="indexterm" id="ix_theory_conf_interval"/><a contenteditable="false" data-primary="confidence intervals" data-type="indexterm" id="ix_conf_interval"/> leads to estimates, such as the typical time that a bus is late (<a class="reference internal" data-type="xref" href="ch04.html#ch-modeling">Chapter 4</a>), a humidity adjustment to an air quality measurement (<a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>), and an estimate of vaccine efficacy (<a class="reference internal" data-type="xref" href="ch02.html#ch-data-scope">Chapter 2</a>). These examples are point estimates for unknown values, called <em>parameters</em>: the median lateness of the bus is 0.74 minutes; the humidity adjustment to air quality is 0.21 PM2.5 per humidity percentage point; and the ratio of COVID infection rates in vaccine efficacy is 0.67. However, a different sample would have produced a different estimate. Simply providing a point estimate doesn’t give a sense of the estimate’s precision. Alternatively, an interval estimate can reflect the estimate’s accuracy. These intervals typically take one of two forms<a contenteditable="false" data-primary="bootstrap confidence interval" data-type="indexterm" id="id1728"/>:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>A <em>bootstrap confidence interval</em> created from the percentiles of the bootstrap sampling distribution</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>A <em>normal confidence interval</em> constructed using the standard error (SE) of the sampling distribution and additional assumptions about the distribution having the shape of a normal curve</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>We describe these two types of intervals and then give an example. Recall that the sampling distribution (see <a class="reference internal" data-type="xref" href="#triptych">Figure 17-1</a>) is a probability distribution that reflects the chance of observing different values of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>. Confidence intervals are constructed from the spread of the sampling distribution of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>, so the endpoints of the interval are random because they are based on <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>. These intervals are designed so that 95% of the time the interval covers <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>.</p>&#13;
&#13;
<p>As its name suggests, the percentile-based bootstrap confidence interval is created from the percentiles of the bootstrap sampling distribution. Specifically, we compute the quantiles of the sampling distribution of <span><math> <msub> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>B</mi> </msub> </math></span>, where <span><math> <msub> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>B</mi> </msub> </math></span> is the bootstrapped statistic. For a 95th percentile interval, we identify the 2.5 and 97.5 quantiles, called <span><math> <msub> <mi>q</mi> <mrow> <mn>2.5</mn> <mo>,</mo> <mi>B</mi> </mrow> </msub> </math></span> and <span><math> <msub> <mi>q</mi> <mrow> <mn>97.5</mn> <mo>,</mo> <mi>B</mi> </mrow> </msub> </math></span>, respectively, where 95% of the time the bootstrapped statistic is in the interval:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <msub> <mi>q</mi> <mrow> <mn>2.5</mn> <mo>,</mo> <mi>B</mi> </mrow> </msub> <mo>≤</mo> <msub> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mi>B</mi> </msub> <mtext> </mtext> <mo>≤</mo> <mtext> </mtext> <msub> <mi>q</mi> <mrow> <mn>97.5</mn> <mo>,</mo> <mi>B</mi> </mrow> </msub> </math></div>&#13;
</div>&#13;
&#13;
<p>This bootstrap percentile  confidence interval is considered a quick-and-dirty interval. There are many alternatives that adjust for bias, take into consideration the shape of the distribution, and are better suited for small samples.</p>&#13;
&#13;
<p>The percentile confidence interval does not rely on the sampling distribution having a particular shape or the center of the distribution being <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>. In contrast, the normal confidence interval often doesn’t require bootstrapping to compute, but it does make additional assumptions about the shape of the sampling distribution of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>.</p>&#13;
&#13;
<p>We use the normal confidence interval when the sampling distribution is well approximated by a normal curve. For a normal probability distribution, with center <span><math> <mi>μ</mi> </math></span> and spread <span><math> <mi>σ</mi> </math></span>, there is a 95% chance that a random value from this distribution is in the interval <span><math> <mi>μ</mi> <mtext> </mtext> <mo>±</mo> <mtext> </mtext> <mn>1.96</mn> <mi>σ</mi> </math></span>. Since the center of the sampling distribution is typically <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>, the chance is 95% that for a randomly generated <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mo stretchy="false">|</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>−</mo> <msup> <mi>θ</mi> <mo>∗</mo> </msup> <mrow> <mo stretchy="false">|</mo> </mrow> <mo>≤</mo> <mn>1.96</mn> <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </math></div>&#13;
</div>&#13;
&#13;
<p>where <span><math> <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </math></span> is the spread of the sampling distribution of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>. We use this inequality to make a 95% confidence interval for <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mo stretchy="false">[</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mtext> </mtext> <mo>−</mo> <mtext> </mtext> <mn>1.96</mn> <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo>,</mo> <mtext> </mtext> <mtext> </mtext> <mtext> </mtext> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mtext> </mtext> <mo>+</mo> <mtext> </mtext> <mn>1.96</mn> <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> </math></div>&#13;
</div>&#13;
&#13;
<p>Confidence intervals of other sizes can be formed with different multiples of <span><math> <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </math></span>, all based on the normal curve. For example, a 99% confidence interval is <span><math> <mo>±</mo> <mn>2.58</mn> <mi>S</mi> <mi>E</mi> </math></span>, and a one-sided upper 95% confidence interval is <span><math> <mo stretchy="false">[</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mtext> </mtext> <mo>−</mo> <mtext> </mtext> <mn>1.64</mn> <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo>,</mo> <mtext> </mtext> <mtext> </mtext> <mi mathvariant="normal">∞</mi> <mo stretchy="false">]</mo> </math></span>.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The SD of a parameter<a contenteditable="false" data-primary="standard error (SE)" data-type="indexterm" id="id1729"/><a contenteditable="false" data-primary="SE (standard error)" data-type="indexterm" id="id1730"/> estimate is often called the <em>standard error</em>, or SE, to distinguish it from the SD of a sample, population, or one draw from an urn. In this book, we don’t differentiate between them. We call them SDs.</p>&#13;
</div>&#13;
&#13;
<p>We provide an example of each type of interval next.</p>&#13;
&#13;
<p>Earlier in this chapter we tested the hypothesis that the coefficient for humidity in a linear model for air quality is 0. The fitted coefficient for these data was <span><math> <mn>0.21</mn> </math></span>. Since the null model did not completely specify the data generation mechanism, we resorted to bootstrapping. That is, we used the data as the population, took a sample of 11,226 records with replacement from the bootstrap population, and fitted the model to find the bootstrap sample coefficient for humidity. Our simulation repeated this process 10,000 times to get an approximate bootstrap sampling distribution.</p>&#13;
&#13;
<p>We can use the percentiles of this bootstrap sampling distribution to create a 99% confidence interval for <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>. To do this, we find the quantiles, <span><math> <msub> <mi>q</mi> <mrow> <mn>0.5</mn> </mrow> </msub> </math></span> and <span><math> <msub> <mi>q</mi> <mrow> <mn>99.5</mn> </mrow> </msub> </math></span>, of the bootstrap sampling distribution:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">q_995</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">percentile</code></span><span><code class="p">(</code></span><span><code class="n">boot_theta_hat</code></span><span><code class="p">,</code></span><code> </code><span><code class="mf">99.5</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">method</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">lower</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">q_005</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">percentile</code></span><span><code class="p">(</code></span><span><code class="n">boot_theta_hat</code></span><span><code class="p">,</code></span><code> </code><span><code class="mf">0.05</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">method</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">lower</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Lower 0.05th percentile: </code></span><span><code class="si">{</code></span><span><code class="n">q_005</code></span><span><code class="si">:</code></span><span><code class="s2">.3f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Upper 99.5th percentile: </code></span><span><code class="si">{</code></span><span><code class="n">q_995</code></span><span><code class="si">:</code></span><span><code class="s2">.3f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Lower 0.05th percentile: 0.099&#13;
Upper 99.5th percentile: 0.260&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Alternatively, since the histogram of the sampling distribution looks roughly normal in shape, we can create a 99% confidence interval based on the normal distribution. First, we find the standard error of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>, which is just the standard deviation of the sampling distribution of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">standard_error</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">std</code></span><span><code class="p">(</code></span><span><code class="n">boot_theta_hat</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">standard_error</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
0.02653498609330345&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Then, a 99% confidence interval for <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span> is <span><math> <mn>2.58</mn> </math></span> SEs away from the observed <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span> in either direction:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Lower 0.05th endpoint: </code></span><span><code class="si">{</code></span><span><code class="n">theta2_hat</code></span><code> </code><span><code class="o">-</code></span><code> </code><span><code class="p">(</code></span><span><code class="mf">2.58</code></span><code> </code><span><code class="o">*</code></span><code> </code><span><code class="n">standard_error</code></span><span><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s2">.3f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Upper 99.5th endpoint: </code></span><span><code class="si">{</code></span><span><code class="n">theta2_hat</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="p">(</code></span><span><code class="mf">2.58</code></span><code> </code><span><code class="o">*</code></span><code> </code><span><code class="n">standard_error</code></span><span><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s2">.3f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Lower 0.05th endpoint: 0.138&#13;
Upper 99.5th endpoint: 0.275&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>These two intervals (bootstrap percentile and normal) are close but clearly not identical. We might expect this given the slight asymmetry in the bootstrapped sampling distribution.</p>&#13;
&#13;
<p>There are other versions of the normal-based confidence interval that reflect the variability in estimating the standard error of the sampling distribution using the SD of the data. And there are still other confidence intervals for statistics that are percentiles, rather than averages. (Also note that for permutation tests, the bootstrap tends not to be as accurate as normal approximations.)</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Confidence intervals can be easily misinterpreted as the chance that the parameter <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span> is in the interval. However, the confidence interval is created from one realization of the sampling distribution. The sampling distribution gives us a different probability statement; 95% of the time, an interval constructed in this way will contain <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>. Unfortunately, we don’t know whether this particular time is one of those that happens 95 times in 100 or not. That is why the term <em>confidence</em> is used rather than <em>probability</em> or <em>chance</em>, and we say that we are 95% confident that the parameter is in our interval.</p>&#13;
</div>&#13;
&#13;
<p>Confidence intervals and hypothesis tests are related in the following way. If, say, a 95% confidence interval contains the hypothesized value <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>, then the <span><math> <mi>p</mi> </math></span>-value for the test is less than 5%. That is, we can invert a confidence interval to create a hypothesis test. We used this technique in the previous section when we carried out the test that the coefficient for humidity in the air quality model is 0. In this section, we have created a 99% confidence interval for the coefficient (based on the bootstrap percentiles), and since 0 does not belong to the interval, the <span><math> <mi>p</mi> </math></span>-value is less than 1% and statistical logic would lead us to conclude that the coefficient is not 0.</p>&#13;
&#13;
<p>Another kind of interval estimate<a contenteditable="false" data-primary="variation" data-secondary="and prediction interval" data-secondary-sortas="prediction interval" data-type="indexterm" id="id1731"/> is the prediction interval. Prediction intervals focus on the variation in observations rather than the variation in an estimator. We explore these next<a contenteditable="false" data-primary="" data-startref="ix_conf_interval" data-type="indexterm" id="id1732"/><a contenteditable="false" data-primary="" data-startref="ix_theory_conf_interval" data-type="indexterm" id="id1733"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Basics of Prediction Intervals" data-type="sect1"><div class="sect1" id="basics-of-prediction-intervals">&#13;
<h1>Basics of Prediction Intervals</h1>&#13;
&#13;
<p>Confidence<a contenteditable="false" data-primary="theory for inference and prediction" data-secondary="prediction intervals" data-type="indexterm" id="ix_theory_predict_interval"/><a contenteditable="false" data-primary="prediction intervals" data-type="indexterm" id="ix_predict_interval"/> intervals convey the accuracy of an estimator, but sometimes we want the accuracy of a prediction for a future observation. For example, someone might say: half the time my bus arrives three-quarters of a minute late at most, but how late might it get? As another example, the California Department of Fish and Wildlife sets the minimum catch size for Dungeness crabs at 146 mm, and a recreational fishing company might wonder how much bigger than 146 mm their customer’s catch might be when they bring them fishing. And for another example, a vet estimates the weight of a donkey to be 169 kg based on its length and girth and uses this estimate to administer medication. For the donkey’s safety, the vet is keen to know how different the donkey’s real weight might be from this estimate.</p>&#13;
&#13;
<p>What these examples have in common is an interest in the prediction of a future observation and the desire to quantify how far that future observation might be from this prediction. Just like with confidence intervals, we compute the statistic (the estimator) and use it in making the prediction, but now we’re interested in typical deviations of future observations from the prediction. In the following sections, we work through examples of prediction intervals based on quantiles, standard deviations, and those conditional on covariates. Along the way, we provide additional information about the typical variation of observations about a prediction.</p>&#13;
&#13;
<section data-pdf-bookmark="Example: Predicting Bus Lateness" data-type="sect2"><div class="sect2" id="example-predicting-bus-lateness">&#13;
<h2>Example: Predicting Bus Lateness</h2>&#13;
&#13;
<p><a class="reference internal" data-type="xref" href="ch04.html#ch-modeling">Chapter 4</a> models<a contenteditable="false" data-primary="bus arrival times dataset" data-secondary="prediction intervals" data-type="indexterm" id="id1734"/> the lateness of a Seattle bus in arriving at a particular stop. We observed that the distribution was highly skewed and chose to estimate the typical lateness by the median, which was 0.74 minutes. We reproduce the sample histogram from that chapter here:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">times</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">read_csv</code></span><span><code class="p">(</code></span><span><code class="s2">"</code><code class="s2">data/seattle_bus_times_NC.csv</code><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">fig</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">px</code></span><span><code class="o">.</code></span><span><code class="n">histogram</code></span><span><code class="p">(</code></span><span><code class="n">times</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">x</code></span><span><code class="o">=</code></span><span><code class="s2">"</code><code class="s2">minutes_late</code><code class="s2">"</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">width</code></span><span><code class="o">=</code></span><span><code class="mi">350</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">height</code></span><span><code class="o">=</code></span><span><code class="mi">250</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">fig</code></span><span><code class="o">.</code></span><span><code class="n">update_xaxes</code></span><span><code class="p">(</code></span><span><code class="nb">range</code></span><span><code class="o">=</code></span><span><code class="p">[</code></span><span><code class="o">-</code></span><span><code class="mi">12</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">60</code></span><span><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="n">title_text</code></span><span><code class="o">=</code></span><span><code class="s2">"</code><code class="s2">Minutes late</code><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">fig</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal width-60"><div class="figure"><img src="assets/leds_17in05.png"/></div></figure>&#13;
&#13;
<p>The prediction problem addresses how late a bus might be. While the median is informative, it doesn’t provide information about the skewness of the distribution. That is, we don’t know how late the bus might be. The 75th percentile, or even the 95th percentile, would add useful information to consider. We compute those percentiles here:</p>&#13;
&#13;
<div class="cell tag_hide-input docutils container">&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
median:          0.74 mins late&#13;
75th percentile: 3.78 mins late&#13;
95th percentile: 13.02 mins late&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>From these statistics, we learn that while more than half the time the bus is not even a minute late, one-quarter of the time it’s almost four minutes late, and with some regularity it can happen that the bus is nearly 15 minutes late. These three values together help us make plans.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Example: Predicting Crab Size" data-type="sect2"><div class="sect2" id="example-predicting-crab-size">&#13;
<h2>Example: Predicting Crab Size</h2>&#13;
&#13;
<p>Fishing for Dungeness crabs is highly regulated, including limiting the shell size to 146 mm in width for crabs caught for recreation. To better understand the distribution of shell size of Dungeness crabs, the California Department of Fish and Wildlife worked with commercial crab fishers from Northern California and Southern Oregon to capture, measure, and release crabs. Here is a histogram of crab shell sizes for the approximately 450 crabs caught:</p>&#13;
&#13;
<figure class="informal width-60"><div class="figure"><img src="assets/leds_17in06.png"/></div></figure>&#13;
&#13;
<p>The distribution is somewhat skewed left, but the average and standard deviations are reasonable summary statistics of the distribution:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">crabs</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">shell</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">describe</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">[</code><code class="p">:</code></span><span><code class="mi">3</code></span><span><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
count    452.00&#13;
mean     131.53&#13;
std       11.07&#13;
Name: shell, dtype: float64&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>The average, 132 mm, is a good prediction for the typical size of a crab. However, it lacks information about how far an individual crab may vary from the average. The standard deviation can fill in this gap.</p>&#13;
&#13;
<p>In addition to the variability of individual observations about the center of the distribution, we also take into account the variability in our estimate of the mean shell size. We can use the bootstrap to estimate this variability, or we can use probability theory (we do this in the next section) to show that the standard deviation of the estimator is <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo> <mrow> <mo>/</mo> </mrow> <msqrt> <mi>n</mi> </msqrt> </math></span>. We also show, in the next section, that these two sources of variation combine as follows:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <msqrt> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi> <mi>p</mi> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> <mo>+</mo> <mfrac> <mrow> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi> <mi>p</mi> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </mrow> <mi>n</mi> </mfrac> </msqrt> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo> <msqrt> <mn>1</mn> <mo>+</mo> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> </msqrt> </math></div>&#13;
</div>&#13;
&#13;
<p>We substitute <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>s</mi> <mi>a</mi> <mi>m</mi> <mi>p</mi> <mi>l</mi> <mi>e</mi> <mo stretchy="false">)</mo> </math></span> for <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo> </math></span> and apply this formula to our crabs:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">std</code></span><span><code class="p">(</code></span><span><code class="n">crabs</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">shell</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">)</code></span><code> </code><span><code class="o">*</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">sqrt</code></span><span><code class="p">(</code></span><span><code class="mi">1</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="mi">1</code></span><span><code class="o">/</code></span><span><code class="nb">len</code></span><span><code class="p">(</code></span><span><code class="n">crabs</code></span><span><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
11.073329460297957&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We see that including the SE of the sample average essentially doesn’t change the prediction error because the sample is so large. We conclude that crabs routinely differ from the typical size of 132 mm by 11 to 22 mm. This information is helpful in <span class="keep-together">developing</span> policies around crab fishing to maintain the health of the crab population and to set expectations for the recreational fisher.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Example: Predicting the Incremental Growth of a Crab" data-type="sect2"><div class="sect2" id="example-predicting-the-incremental-growth-of-a-crab">&#13;
<h2>Example: Predicting the Incremental Growth of a Crab</h2>&#13;
&#13;
<p>After Dungeness<a contenteditable="false" data-primary="crab incremental growth predictions" data-type="indexterm" id="ix_crab_incr_growth"/> crabs mature, they continue to grow by casting off their shell and building a new, larger one to grow into each year; this process is called <em>molting</em>. The California Department of Fish and Wildlife wanted a better understanding of crab growth so that it could set better limits on fishing that would protect the crab population. The crabs caught in the study mentioned in the previous example were about to molt, and in addition to their size, the change in shell size from before to after molting was also recorded:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">crabs</code></span><span><code class="o">.</code></span><span><code class="n">corr</code></span><span><code class="p">(</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>shell</th>&#13;
			<th>inc</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>shell</strong></td>&#13;
			<td class="right">1.0</td>&#13;
			<td class="right">-0.6</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>inc</strong></td>&#13;
			<td class="right">-0.6</td>&#13;
			<td class="right">1.0</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>These two measurements are negatively correlated, meaning that the larger the crab, the less they grow when they molt. We plot the growth increment against the shell size to determine whether the relationship between these variables is roughly linear:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">px</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">crabs</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'inc'</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code> <code class="s1">'shell'</code><code class="p">,</code> <code class="n">width</code><code class="o">=</code><code class="mi">350</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mi">250</code><code class="p">,</code>&#13;
	<code class="n">labels</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">shell</code><code class="o">=</code><code class="s1">'Dungeness crab shell width (mm)'</code><code class="p">,</code>&#13;
				<code class="n">inc</code><code class="o">=</code><code class="s1">'Growth (mm)'</code><code class="p">))</code>&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_17in07.png"/></div></figure>&#13;
&#13;
<p>The relationship<a contenteditable="false" data-primary="statsmodels library" data-type="indexterm" id="id1735"/> appears linear, and we can fit a simple linear model to explain the growth increment by the pre-molt size of the shell. For this example, we use the <code>statsmodels</code> library, which provides prediction intervals with <code>get_prediction</code>. We first set up the design matrix and response variable, and then we use least squares to fit the model:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">import</code></span><code> </code><span><code class="nn">statsmodels</code><code class="nn">.</code><code class="nn">api</code></span><code> </code><span><code class="k">as</code></span><code> </code><span><code class="nn">sm</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">X</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">sm</code></span><span><code class="o">.</code></span><span><code class="n">add_constant</code></span><span><code class="p">(</code></span><span><code class="n">crabs</code></span><span><code class="p">[</code><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">shell</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><span><code class="n">y</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">crabs</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">inc</code><code class="s1">'</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">inc_model</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">sm</code></span><span><code class="o">.</code></span><span><code class="n">OLS</code></span><span><code class="p">(</code></span><span><code class="n">y</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">X</code></span><span><code class="p">)</code></span><span><code class="o">.</code></span><span><code class="n">fit</code></span><span><code class="p">(</code><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Increment estimate = </code></span><span><code class="si">{</code></span><span><code class="n">inc_model</code></span><span><code class="o">.</code></span><span><code class="n">params</code></span><span><code class="p">[</code></span><span><code class="mi">0</code></span><span><code class="p">]</code></span><span><code class="si">:</code></span><span><code class="s2">0.2f</code></span><span><code class="si">}</code></span><span><code class="s2"> + </code><code class="s2">"</code></span><span><code class="p">,</code></span><code> </code><code>&#13;
</code><code>      </code><span><code class="sa">f</code></span><span><code class="s2">"</code></span><span><code class="si">{</code></span><span><code class="n">inc_model</code></span><span><code class="o">.</code></span><span><code class="n">params</code></span><span><code class="p">[</code></span><span><code class="mi">1</code></span><span><code class="p">]</code></span><span><code class="si">:</code></span><span><code class="s2">0.2f</code></span><span><code class="si">}</code></span><span><code class="s2"> x Shell Width</code><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Increment estimate = 29.80 +  -0.12 x Shell Width&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>When modeling, we create prediction intervals for given values of the explanatory variable. For example, if a newly caught crab is 120 mm across, then we use our fitted model to predict its shell’s growth.</p>&#13;
&#13;
<p>As in the previous example, the variability of our prediction for an individual observation includes the variability in our estimate of the crab’s growth and the crab-to-crab variation in shell size. Again, we can use the bootstrap to estimate this variation, or we can use probability theory to show that these two sources of variation combine as follows:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mi mathvariant="bold">e</mi> </mrow> <mo stretchy="false">)</mo> <msqrt> <mn>1</mn> <mo>+</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo stretchy="false">(</mo> <msup> <mtext mathvariant="bold">X</mtext> <mi mathvariant="normal">⊤</mi> </msup> <mtext mathvariant="bold">X</mtext> <msup> <mo stretchy="false">)</mo> <mrow> <mo>−</mo> <mn>1</mn> </mrow> </msup> <msubsup> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> <mi mathvariant="normal">⊤</mi> </msubsup> </msqrt> </math></div>&#13;
</div>&#13;
&#13;
<p>Here <span><math> <mtext mathvariant="bold">X</mtext> </math></span> is the design matrix that consists of the original data, <span><math> <mrow> <mi mathvariant="bold">e</mi> </mrow> </math></span> is the <span><math> <mi>n</mi> <mo>×</mo> <mn>1</mn> </math></span> column vector of residuals from the regression, and <span><math> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> </math></span> is the <span><math> <mn>1</mn> <mo>×</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo> </math></span> row vector of features for the new observation (in this example, these are <span><math> <mrow> <mo>[</mo> <mn>1</mn> <mo>,</mo> <mn>120</mn> <mo>]</mo> </mrow> </math></span>):</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">new_data</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="nb">dict</code></span><span><code class="p">(</code></span><span><code class="n">const</code></span><span><code class="o">=</code></span><span><code class="mi">1</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">shell</code></span><span><code class="o">=</code></span><span><code class="mi">120</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">new_X</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">DataFrame</code></span><span><code class="p">(</code></span><span><code class="n">new_data</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">index</code></span><span><code class="o">=</code></span><span><code class="p">[</code></span><span><code class="mi">0</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><span><code class="n">new_X</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>const</th>&#13;
			<th>shell</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>0</strong></td>&#13;
			<td>1</td>&#13;
			<td>120</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We use the <code>get_prediction</code> method in <code>statsmodels</code> to find a 95% prediction interval for a crab with a 120 mm shell:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">pred</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">inc_model</code></span><span><code class="o">.</code></span><span><code class="n">get_prediction</code></span><span><code class="p">(</code></span><span><code class="n">new_X</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">pred</code></span><span><code class="o">.</code></span><span><code class="n">summary_frame</code></span><span><code class="p">(</code></span><span><code class="n">alpha</code></span><span><code class="o">=</code></span><span><code class="mf">0.05</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>mean</th>&#13;
			<th>mean_se</th>&#13;
			<th>mean_ci_lower</th>&#13;
			<th>mean_ci_upper</th>&#13;
			<th>obs_ci_lower</th>&#13;
			<th>obs_ci_upper</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>0</strong></td>&#13;
			<td>15.86</td>&#13;
			<td>0.12</td>&#13;
			<td>15.63</td>&#13;
			<td>16.08</td>&#13;
			<td>12.48</td>&#13;
			<td>19.24</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Here we have both a confidence interval for the average growth increment for a crab with a 120 mm shell, [15.6, 16.1] and a prediction interval for the growth increment, [12.5, 19.2]. The prediction interval is quite a bit wider because it takes into account the variation in individual crabs. This variation is seen in the spread of the points about the regression line, which we approximate by the SD of the residuals. The correlation between shell size and growth increment means that the variation in a growth increment prediction for a particular shell size is smaller than the overall SD of the growth increment:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Residual SD:    </code></span><span><code class="si">{</code></span><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">std</code></span><span><code class="p">(</code></span><span><code class="n">inc_model</code></span><span><code class="o">.</code></span><span><code class="n">resid</code></span><span><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s2">0.2f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Crab growth SD: </code></span><span><code class="si">{</code></span><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">std</code></span><span><code class="p">(</code></span><span><code class="n">crabs</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">inc</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s2">0.2f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Residual SD:    1.71&#13;
Crab growth SD: 2.14&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>The intervals provided by <code>get_prediction</code> rely on the normal approximation to the distribution of growth increment. That’s why the 95% prediction interval endpoints are roughly twice the residual SD away from the prediction. In the next section, we dive deeper into these calculations of standard deviations, estimators, and predictions. We also discuss some of the assumptions that we make in calculating them<a contenteditable="false" data-primary="" data-startref="ix_crab_incr_growth" data-type="indexterm" id="id1736"/><a contenteditable="false" data-primary="" data-startref="ix_predict_interval" data-type="indexterm" id="id1737"/><a contenteditable="false" data-primary="" data-startref="ix_theory_predict_interval" data-type="indexterm" id="id1738"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Probability for Inference and Prediction" data-type="sect1"><div class="sect1" id="sec-theory-probintro">&#13;
<h1>Probability for Inference and Prediction</h1>&#13;
&#13;
<p>Hypothesis<a contenteditable="false" data-primary="probability" data-secondary="for inference and prediction" data-secondary-sortas="inference and prediction" data-type="indexterm" id="ix_prob_predict"/><a contenteditable="false" data-primary="theory for inference and prediction" data-secondary="probability's role" data-type="indexterm" id="ix_theory_prob"/><a contenteditable="false" data-primary="predictions and predicting" data-secondary="probability for" data-type="indexterm" id="ix_predict_prob"/> testing, confidence intervals, and prediction intervals rely on probability calculations computed from the sampling distribution and the data generation process. These probability frameworks also enable us to run simulation and bootstrap studies for a hypothetical survey, an experiment, or some other chance process in order to study its random behavior. For example, we found the sampling distribution for an average of ranks under the assumption that the treatment in a Wikipedia experiment was not effective. Using simulation, we quantified the typical deviations from the expected outcome and the distribution of the possible values for the summary statistic. The triptych in <a class="reference internal" data-type="xref" href="#triptych">Figure 17-1</a> provided a diagram to guide us in the process; it helped keep straight the differences between the population, probability, and sample and also showed their connections. In this section, we bring more mathematical rigor to these concepts.</p>&#13;
&#13;
<p>We formally introduce the notions of expected value, standard deviation, and random variable, and we connect them to the concepts we have been using in this chapter for testing hypotheses and making confidence and prediction intervals. We begin with the specific example from the Wikipedia experiment, and then we generalize. Along the way, we connect this formalism to the triptych that we have used as our guide throughout the chapter.</p>&#13;
&#13;
<section data-pdf-bookmark="Formalizing the Theory for Average Rank Statistics" data-type="sect2"><div class="sect2" id="formalizing-the-theory-for-average-rank-statistics">&#13;
<h2>Formalizing the Theory for Average Rank Statistics</h2>&#13;
&#13;
<p>Recall<a contenteditable="false" data-primary="urn model" data-secondary="theory for inference and prediction" data-type="indexterm" id="ix_urn_mod_theory_inf3"/><a contenteditable="false" data-primary="average rank statistics theory" data-type="indexterm" id="ix_av_rank_stat"/> in the Wikipedia experiment that we pooled the post-award productivity values from the treatment and control groups and converted them into ranks, <span><math> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>3</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>200</mn> </math></span>, so the population is simply made up of the integers from 1 to 200. <a class="reference internal" data-type="xref" href="#triptychrank">Figure 17-3</a> is a diagram that represents this specific situation. Notice that the population distribution is flat and ranges from 1 to 200 (left side of <a class="reference internal" data-type="xref" href="#triptychrank">Figure 17-3</a>). Also, the population summary (called <em>population parameter</em>) we use is the average rank:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <msup> <mi>θ</mi> <mo>∗</mo> </msup> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mtext>Avg</mtext> <mo stretchy="false">(</mo> <mtext>pop</mtext> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>k</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>200</mn> </mrow> </msubsup> <mi>k</mi> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mn>100.5</mn> </math></div>&#13;
</div>&#13;
&#13;
<p>Another relevant summary is the spread about <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>, defined as the population standard deviation:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtext>SD</mtext> <mo stretchy="false">(</mo> <mtext>pop</mtext> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <msqrt> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>k</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>200</mn> </mrow> </msubsup> <mo stretchy="false">(</mo> <mi>k</mi> <mo>−</mo> <msup> <mi>θ</mi> <mo>∗</mo> </msup> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </msqrt> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <msqrt> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>k</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>200</mn> </mrow> </msubsup> <mo stretchy="false">(</mo> <mi>k</mi> <mo>−</mo> <mn>100.5</mn> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </msqrt> <mtext> </mtext> <mo>≈</mo> <mtext> </mtext> <mn>57.7</mn> </math></div>&#13;
</div>&#13;
&#13;
<p>The SD(pop) represents the typical deviation of a rank from the population average. To calculate SD(pop) for this example takes some mathematical handiwork:</p>&#13;
&#13;
<figure><div class="figure" id="triptychrank"><img src="assets/leds_1703.png"/>&#13;
<h6><span class="label">Figure 17-3. </span>Diagram of the data generation process for the Wikipedia experiment; this is a special case where we know the population</h6>&#13;
</div></figure>&#13;
&#13;
<p>The observed sample consists of the integer ranks of the treatment group; we refer to these values as <span><math> <msub> <mi>k</mi> <mn>1</mn> </msub> <mo>,</mo> <msub> <mi>k</mi> <mn>2</mn> </msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub> <mi>k</mi> <mrow> <mn>100</mn></mrow></msub><mo>.</mo> </math></span> The sample distribution appears on the right in <a class="reference internal" data-type="xref" href="#triptychrank">Figure 17-3</a> (each of the 100 integers appears once).</p>&#13;
&#13;
<p>The parallel to the population average is the sample average, which is our statistic of interest:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtext>Avg</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mfrac> <mn>1</mn> <mn>100</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>100</mn> </mrow> </msubsup> <msub> <mi>k</mi> <mi>i</mi> </msub> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mrow> <mover> <mi>k</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mn>113.7</mn> </math></div>&#13;
</div>&#13;
&#13;
<p>The <span><math> <mtext>Avg</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext> <mo stretchy="false">)</mo> </math></span> is the observed value for <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>. Similarly, the spread about <span><math> <mtext>Avg</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext> <mo stretchy="false">)</mo> </math></span>, called the standard deviation of the sample, represents the typical deviation of a rank in the sample from the sample average:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtext>SD</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <msqrt> <mfrac> <mn>1</mn> <mn>100</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>100</mn> </mrow> </msubsup> <mo stretchy="false">(</mo> <msub> <mi>k</mi> <mi>i</mi> </msub> <mo>−</mo> <mrow> <mover> <mi>k</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </msqrt> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mn>553.</mn> </math></div>&#13;
</div>&#13;
&#13;
<p>Notice the parallel between the definitions of the sample statistic and the population parameter in the case where they are averages. The parallel between the two SDs is also noteworthy.</p>&#13;
&#13;
<p>Next we turn to the data<a contenteditable="false" data-primary="random variables" data-type="indexterm" id="id1739"/> generation process: draw 100 marbles from the urn (with values <span><math> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>200</mn> </math></span>), without replacement, to create the treatment ranks. We represent the action of drawing the first marble from the urn, and the integer that we get, by the capital letter <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span>. This <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> is called a <em>random variable</em>. It has a probability distribution determined by the urn model. That is, we can list all of the values that <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> might take and the probability associated with each:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mi>k</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> <mtext> </mtext> <mtext> </mtext> <mtext> </mtext> <mtext> </mtext> <mtext> for </mtext> <mi>k</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>200</mn> </math></div>&#13;
</div>&#13;
&#13;
<p>In this example, the probability distribution of <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> is determined by a simple formula because all of the integers are equally likely to be drawn from the urn.</p>&#13;
&#13;
<p>We often summarize<a contenteditable="false" data-primary="standard deviation" data-type="indexterm" id="id1740"/><a contenteditable="false" data-primary="expected value" data-type="indexterm" id="id1741"/> the distribution of a random variable by its <em>expected value</em> and <em>standard deviation</em>. Like with the population and sample, these two quantities give us a sense of what to expect as an outcome and how far the actual value might be from what is expected.</p>&#13;
&#13;
<p>For our example, the expected value of <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> is simply:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">]</mo> </mtd> <mtd> <mi/> <mo>=</mo> <mn>1</mn> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>+</mo> <mn>2</mn> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mn>2</mn> <mo stretchy="false">)</mo> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mn>200</mn> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mn>200</mn> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mn>1</mn> <mo>×</mo> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> <mo>+</mo> <mn>2</mn> <mo>×</mo> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mn>200</mn> <mo>×</mo> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mn>100.5</mn> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Notice that <span><math> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">]</mo> <mo>=</mo> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>, the population average from the urn. The average value in a population and the expected value of a random variable that represents one draw at random from an urn that contains the population are always the same. This is more easily seen by expressing the population average as an average of the unique values in the population, weighted by the fraction of units that have that value. The expected value of a random variable of a draw at random from the population urn uses the exact same weights because they match the chance of selecting the particular value.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The term <em>expected value</em> can be a bit confusing because it need not be a possible value of the random variable. For example, <span><math> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">]</mo> <mo>=</mo> <mn>100.5</mn> </math></span>, but only integers are possible values for <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span>.</p>&#13;
</div>&#13;
&#13;
<p>Next, the variance<a contenteditable="false" data-primary="urn model" data-secondary="variance in" data-type="indexterm" id="ix_urn_mod_var"/><a contenteditable="false" data-primary="variance" data-secondary="in urn model" data-type="indexterm" id="ix_var_urn_mod"/> of <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> is defined as follows:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> </mtd> <mtd> <mi/> <mo>=</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mo stretchy="false">[</mo> <mn>1</mn> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo stretchy="false">[</mo> <mn>200</mn> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mn>200</mn> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mn>100.5</mn> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> <mo>×</mo> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo stretchy="false">(</mo> <mn>200</mn> <mo>−</mo> <mn>100.5</mn> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> <mo>×</mo> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mn>3333.25</mn> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
&#13;
<p>Additionally, we define the standard deviation of <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> as follows:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mtext>SD</mtext> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> <mo>=</mo> <msqrt> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> </msqrt> <mo>=</mo> <mn>57.7</mn> </math></div>&#13;
&#13;
<p>We again point out that the standard deviation of <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> matches the <span><math> <mtext>SD</mtext> </math></span>(pop).</p>&#13;
&#13;
<p>To describe the entire data generation process in <a class="reference internal" data-type="xref" href="#triptychrank">Figure 17-3</a>, we also define <span><math> <msub> <mi>Z</mi> <mn>2</mn> </msub> <mo>,</mo> <msub> <mi>Z</mi> <mn>3</mn> </msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub> <mi>Z</mi> <mrow> <mn>100</mn> </mrow> </msub> </math></span> as the result of the remaining 99 draws from the urn. By symmetry, these random variables should all have the same probability distribution. That is, for any <span><math> <mi>k</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>200</mn> </math></span>:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mi>k</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>2</mn> </msub> <mo>=</mo> <mi>k</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mo>⋯</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mrow> <mn>100</mn> </mrow> </msub> <mo>=</mo> <mi>k</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mfrac> <mn>1</mn> <mn>200</mn> </mfrac> </math></div>&#13;
&#13;
<p>This implies that each <span><math> <msub> <mi>Z</mi> <mi>i</mi> </msub> </math></span> has the same expected value, 100.5, and standard deviation, 57.7. However, these random variables are not independent. For example, if you know that <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo>=</mo> <mn>17</mn> </math></span>, then it is not possible for <span><math> <msub> <mi>Z</mi> <mn>2</mn> </msub> <mo>=</mo> <mn>17</mn> </math></span>.</p>&#13;
&#13;
<p>To complete the middle portion of <a class="reference internal" data-type="xref" href="#triptychrank">Figure 17-3</a>, which involves the sampling distribution of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>, we express the average rank statistic as follows:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>=</mo> <mfrac> <mn>1</mn> <mn>100</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>100</mn> </mrow> </msubsup> <msub> <mi>Z</mi> <mi>i</mi> </msub> </math></div>&#13;
&#13;
<p>We can use the expected value and SD of <span><math> <msub> <mi>Z</mi> <mn>1</mn> </msub> </math></span> and our knowledge of the data generation process to find the expected value and SD of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>. We first find the expected value of <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mrow> <mo>[</mo> <mfrac> <mn>1</mn> <mn>100</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>100</mn> </mrow> </msubsup> <msub> <mi>Z</mi> <mi>i</mi> </msub> <mo>]</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mfrac> <mn>1</mn> <mn>100</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>100</mn> </mrow> </msubsup> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <msub> <mi>Z</mi> <mi>i</mi> </msub> <mo stretchy="false">]</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mn>100.5</mn> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
&#13;
<p>In other words, the expected value of the average of random draws from the population equals the population average. Here we provide formulas for the variance of the average in terms of the population variance, as well as the SD:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mrow> <mo>[</mo> <mfrac> <mn>1</mn> <mn>100</mn> </mfrac> <msubsup> <mi mathvariant="normal">Σ</mi> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mn>100</mn> </mrow> </msubsup> <msub> <mi>Z</mi> <mi>i</mi> </msub> <mo>]</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mfrac> <mrow> <mn>200</mn> <mo>−</mo> <mn>100</mn> </mrow> <mrow> <mn>100</mn> <mo>−</mo> <mn>1</mn> </mrow> </mfrac> <mo>×</mo> <mfrac> <mrow> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mi>i</mi> </msub> <mo stretchy="false">)</mo> </mrow> <mn>100</mn> </mfrac> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mn>16.75</mn> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mtext> </mtext> </mtd> </mtr> <mtr> <mtd> <mtext>SD</mtext> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <msqrt> <mfrac> <mn>100</mn> <mn>199</mn> </mfrac> </msqrt> <mfrac> <mrow> <mtext>SD</mtext> <mo stretchy="false">(</mo> <msub> <mi>Z</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> </mrow> <mn>10</mn> </mfrac> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mn>4.1</mn> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
&#13;
<p>These computations relied on several properties of expected value and variance of a random variable and sums of random variables. Next, we provide properties of sums and averages of random variables that can be used to derive the formulas we just <span class="keep-together">presented</span><a contenteditable="false" data-primary="" data-startref="ix_av_rank_stat" data-type="indexterm" id="id1742"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="General Properties of Random Variables" data-type="sect2"><div class="sect2" id="general-properties-of-random-variables">&#13;
<h2>General Properties of Random Variables</h2>&#13;
&#13;
<p>In general<a contenteditable="false" data-primary="random variables" data-type="indexterm" id="ix_rand_var"/><a contenteditable="false" data-primary="variables and variable types" data-secondary="random variables" data-type="indexterm" id="ix_var_rand"/>, a <em>random variable</em> represents a numeric<a contenteditable="false" data-primary="numeric data" data-secondary="and random variables" data-secondary-sortas="random variables" data-type="indexterm" id="id1743"/> outcome of a chance event. In this book, we use capital letters like <span><math> <mi>X</mi> </math></span> or <span><math> <mi>Y</mi> </math></span> or <span><math> <mi>Z</mi> </math></span> to denote a random variable. The <span class="keep-together">probability</span> distribution for <span><math> <mi>X</mi> </math></span> is the specification <span><math> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo stretchy="false">)</mo> <mo>=</mo> <msub> <mi>p</mi> <mi>x</mi> </msub> </math></span> for all values <span><math> <mi>x</mi> </math></span> that the random variable takes on.</p>&#13;
&#13;
<p>Then, the expected value of <span><math> <mi>X</mi> </math></span> is defined as:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mi>X</mi> <mo stretchy="false">]</mo> <mo>=</mo> <munder> <mo>∑</mo> <mrow> <mi>x</mi> </mrow> </munder> <mi>x</mi> <msub> <mi>p</mi> <mi>x</mi> </msub> </math></div>&#13;
&#13;
<p>The variance <span><math> <mi>X</mi> </math></span> is defined as:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <mi>X</mi> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mi>X</mi> <mo stretchy="false">]</mo> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> <mo stretchy="false">]</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <munder> <mo>∑</mo> <mrow> <mi>x</mi> </mrow> </munder> <mo stretchy="false">[</mo> <mi>x</mi> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> <msub> <mi>p</mi> <mi>x</mi> </msub> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
&#13;
<p>And the <span><math> <mtext>SD</mtext> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> </math></span> is the square root of <span><math> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> </math></span>.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Although random variables can represent quantities that are either discrete (such as the number of children in a family drawn at random from a population) or continuous (such as the air quality measured by an air monitor), we address only random variables with discrete outcomes in this book. Since most measurements are made to a certain degree of precision, this simplification doesn’t limit us too much.</p>&#13;
</div>&#13;
&#13;
<p>Simple formulas provide the expected value, variance, and standard deviation when we make scale and shift changes to random variables, such as <span><math> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> </math></span> for constants <span><math> <mi>a</mi> </math></span> and <span><math> <mi>b</mi> </math></span>:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <msup> <mi>b</mi> <mn>2</mn> </msup> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mo stretchy="false">|</mo> </mrow> <mi>b</mi> <mrow> <mo stretchy="false">|</mo> </mrow> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
&#13;
<p>To convince yourself that these formulas make sense, think about how a distribution changes if you add a constant <span><math> <mi>a</mi> </math></span> to each value or scale each value by <span><math> <mi>b</mi> </math></span>. Adding <span><math> <mi>a</mi> </math></span> to each value would simply shift the distribution, which in turn would shift the expected value but not change the size of the deviations about the expected value. On the other hand, scaling the values by, say, 2 would spread the distribution out and essentially double both the expected value and the deviations from the expected value.</p>&#13;
&#13;
<p>We are also interested in the properties of the sum of two or more random variables. Let’s consider two random variables, <span><math> <mi>X</mi> </math></span> and <span><math> <mi>Y</mi> </math></span>. Then:</p>&#13;
&#13;
<div data-type="equation"><math display="block"> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo> <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mi>c</mi> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo> </math></div>&#13;
&#13;
<p>But to find the variance<a contenteditable="false" data-primary="joint distribution" data-type="indexterm" id="id1744"/> of <span><math> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo> <mi>c</mi> <mi>Y</mi> </math></span>, we need to know how <span><math> <mi>X</mi> </math></span> and <span><math> <mi>Y</mi> </math></span> vary together, which is called the <em>joint distribution</em> of <span><math> <mi>X</mi> </math></span> and <span><math> <mi>Y</mi> </math></span>. The joint distribution of <span><math> <mi>X</mi> </math></span> and <span><math> <mi>Y</mi> </math></span> assigns probabilities to combinations of their outcomes:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo>,</mo> <mi>Y</mi> <mo>=</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <msub> <mi>p</mi> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </msub> </math></div>&#13;
</div>&#13;
&#13;
<p>A summary of how <span><math> <mi>X</mi> </math></span> and <span><math> <mi>Y</mi> </math></span> vary together, called the <em>covariance</em>, is defined as:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mi>C</mi> <mi>o</mi> <mi>v</mi> <mo stretchy="false">(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <mi>X</mi> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mi>X</mi> <mo stretchy="false">]</mo> <mo stretchy="false">)</mo> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mi>Y</mi> <mo stretchy="false">]</mo> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <mi>X</mi> <mi>Y</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <msub> <mi mathvariant="normal">Σ</mi> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </msub> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <mi>x</mi> <mi>y</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <msub> <mi>p</mi> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </msub> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>The covariance enters into the calculation of <span><math> <mrow> <mo mathvariant="bold" stretchy="false">(</mo> </mrow> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo> <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo> </math></span>, as shown here:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo> <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <msup> <mi>b</mi> <mn>2</mn> </msup> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mn>2</mn> <mi>b</mi> <mi>c</mi> <mi>C</mi> <mi>o</mi> <mi>v</mi> <mo stretchy="false">(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mo>+</mo> <msup> <mi>c</mi> <mn>2</mn> </msup> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo> </math></div>&#13;
</div>&#13;
&#13;
<p>In the special case where <span><math> <mi>X</mi> </math></span> and <span><math> <mi>Y</mi> </math></span> are independent, their joint distribution is simplified to <span><math> <msub> <mi>p</mi> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </msub> <mo>=</mo> <msub> <mi>p</mi> <mi>x</mi> </msub> <msub> <mi>p</mi> <mi>y</mi> </msub> </math></span>. And in this case, <span><math> <mi>C</mi> <mi>o</mi> <mi>v</mi> <mo stretchy="false">(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mn>0</mn> </math></span>, so:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo> <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <msup> <mi>b</mi> <mn>2</mn> </msup> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mo>+</mo> <msup> <mi>c</mi> <mn>2</mn> </msup> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo> </math></div>&#13;
</div>&#13;
&#13;
<p>These properties can be used to show that for random variables <span><math> <msub> <mi>X</mi> <mn>1</mn> </msub> <mo>,</mo> <msub> <mi>X</mi> <mn>2</mn> </msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub> <mi>X</mi> <mn>n</mn> </msub> </math></span> that are independent with expected value <span><math> <mi>μ</mi> </math></span> and standard deviation <span><math> <mi>σ</mi> </math></span>, the average, <span><math> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> </math></span>, has the following expected value, variance, and standard deviation:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mi>μ</mi> </mtd> </mtr> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <msup> <mi>σ</mi> <mn>2</mn> </msup> <mrow> <mo>/</mo> </mrow> <mi>n</mi> </mtd> </mtr> <mtr> <mtd> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mi>σ</mi> <mrow> <mo>/</mo> </mrow> <msqrt> <mi>n</mi> </msqrt> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>This situation arises with the urn model where <span><math> <msub> <mi>X</mi> <mn>1</mn> </msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub> <mi>X</mi> <mi>n</mi> </msub> </math></span> are the result of random draws with replacement. In this case, <span><math> <mi>μ</mi> </math></span> represents the average of the urn and <span><math> <mi>σ</mi> </math></span> the standard deviation.</p>&#13;
&#13;
<p>However, when we make random draws from the urn without replacement, the <span><math> <msub> <mi>X</mi> <mi>i</mi> </msub> </math></span> are not independent. In this situation, <span><math> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> </math></span> has the following expected value and variance:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mi>μ</mi> </mtd> </mtr> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mfrac> <mrow> <mi>N</mi> <mo>−</mo> <mi>n</mi> </mrow> <mrow> <mi>N</mi> <mo>−</mo> <mn>1</mn> </mrow> </mfrac> <mo>×</mo> <mfrac> <msup> <mi>σ</mi> <mn>2</mn> </msup> <mi>n</mi> </mfrac> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Notice<a contenteditable="false" data-primary="finite population corrective factor" data-type="indexterm" id="id1745"/> that while the expected value is the same as when the draws are without replacement, the variance and SD are smaller. These quantities are adjusted by <span><math> <mo stretchy="false">(</mo> <mi>N</mi> <mo>−</mo> <mi>n</mi> <mo stretchy="false">)</mo> <mrow> <mo>/</mo> </mrow> <mo stretchy="false">(</mo> <mi>N</mi> <mo>−</mo> <mn>1</mn> <mo stretchy="false">)</mo> </math></span>, which is called the <em>finite population correction factor</em>. We used this formula earlier to compute the <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </math></span> in our Wikipedia example.</p>&#13;
&#13;
<p>Returning to <a class="reference internal" data-type="xref" href="#triptychrank">Figure 17-3</a>, we see that the sampling distribution for <span><math> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> </math></span> in the center of the diagram has an expectation that matches the population average; the SD decreases like <span><math> <mn>1</mn> <mrow> <mo>/</mo> </mrow> <msqrt> <mi>n</mi> </msqrt> </math></span> but even more quickly because we are drawing without replacement; and the distribution is shaped like a normal curve. We saw these properties earlier in our simulation study.</p>&#13;
&#13;
<p>Now that we have outlined the general properties of random variables and their sums, we connect these ideas to testing, confidence, and prediction intervals<a contenteditable="false" data-primary="" data-startref="ix_urn_mod_theory_inf3" data-type="indexterm" id="id1746"/><a contenteditable="false" data-primary="" data-startref="ix_var_rand" data-type="indexterm" id="id1747"/><a contenteditable="false" data-primary="" data-startref="ix_rand_var" data-type="indexterm" id="id1748"/><a contenteditable="false" data-primary="" data-startref="ix_var_urn_mod" data-type="indexterm" id="id1749"/><a contenteditable="false" data-startref="ix_urn_mod_var" data-type="indexterm" id="id1750"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Probability Behind Testing and Intervals" data-type="sect2"><div class="sect2" id="probability-behind-testing-and-intervals">&#13;
<h2>Probability Behind Testing and Intervals</h2>&#13;
&#13;
<p>As mentioned<a contenteditable="false" data-primary="probability" data-secondary="in testing and intervals" data-secondary-sortas="testing and intervals" data-type="indexterm" id="ix_prob_test_inter"/> at the beginning of this chapter, probability is the underpinning of conducting a hypothesis test, providing a confidence interval for an estimator and a prediction interval for a future observation.</p>&#13;
&#13;
<p>We now have the technical machinery to explain these concepts, which we have carefully defined in this chapter without the use of formal technicalities. This time we present the results in terms of random variables and their distributions.</p>&#13;
&#13;
<p>Recall that a hypothesis test relies on a null model that provides the probability distribution for the statistic, <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>. The tests we carried out were essentially computing (sometimes approximately) the following probability. Given the assumptions of the null distribution:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>≥</mo> <mtext>observed statistic</mtext> <mo stretchy="false">)</mo> </math></div>&#13;
</div>&#13;
&#13;
<p>Oftentimes, the random variable is normalized to make these computations easier and standard:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mrow> <mo>(</mo> <mfrac> <mrow> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>−</mo> <msup> <mrow> <mi>θ</mi> </mrow> <mo>∗</mo> </msup> </mrow> <mrow> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </mrow> </mfrac> <mo>≥</mo> <mfrac> <mrow> <mtext>observed stat</mtext> <mo>−</mo> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </mrow> <mrow> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </mrow> </mfrac> <mo>)</mo> </mrow> </math></div>&#13;
</div>&#13;
&#13;
<p>When <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </math></span> is not known<a contenteditable="false" data-primary="normalization" data-secondary="of random variables in testing" data-secondary-sortas="random variables in testing" data-type="indexterm" id="id1751"/><a contenteditable="false" data-primary="data distributions" data-secondary="normal distribution" data-type="indexterm" id="id1752"/><a contenteditable="false" data-primary="normal distribution" data-type="indexterm" id="id1753"/>, we have approximated it via simulation, and when we have a formula for <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </math></span> in terms of <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo> </math></span>, we substitute <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>s</mi> <mi>a</mi> <mi>m</mi> <mi>p</mi> <mo stretchy="false">)</mo> </math></span> for <span><math> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo> </math></span>. This normalization is popular because it simplifies the null distribution. For example, if <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span> has an approximate normal distribution, then the normalized version will have a standard normal distribution with center 0 and SD 1. These approximations are useful when a lot of hypothesis tests are being carried out, such as with A/B testing, since there is no need to simulate for every statistic because we can just use the normal curve probabilities.</p>&#13;
&#13;
<p>The probability statement behind a confidence interval is quite similar to the probability calculations used in testing. In particular, to create a 95% confidence interval where the sampling distribution of the estimator is roughly normal, we standardize and use the probability:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mrow> <mo>(</mo> <mfrac> <mrow> <mo stretchy="false">|</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>−</mo> <msup> <mi>θ</mi> <mo>∗</mo> </msup> <mo stretchy="false">|</mo> </mrow> <mrow> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </mrow> </mfrac> <mo>≤</mo> <mn>1.96</mn> <mo>)</mo> </mrow> </mtd> <mtd> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">P</mi> </mrow> <mrow> <mo>(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>−</mo> <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo>≤</mo> <msup> <mi>θ</mi> <mo>∗</mo> </msup> <mo>≤</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo>+</mo> <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd/> <mtd> <mtext> </mtext> <mo>≈</mo> <mtext> </mtext> <mn>0.95</mn> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Note that <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span> is a random variable in the preceding probability statement and <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span> is considered a fixed unknown parameter value. The confidence interval is created by substituting the observed statistic for <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span> and calling it a 95% confidence interval:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mo>[</mo> <mtext>observed stat</mtext> <mo>−</mo> <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo>,</mo> <mtext> </mtext> <mtext>observed stat</mtext> <mo>+</mo> <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mo>]</mo> </mrow> </math></div>&#13;
</div>&#13;
&#13;
<p>Once the observed statistic is substituted for the random variable, then we say that we are 95% confident that the interval we have created contains the true value <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span>. In other words, in 100 cases where we compute an interval in this way, we expect 95 of them to cover the population parameter that we are estimating.</p>&#13;
&#13;
<p>Next, we consider prediction<a contenteditable="false" data-primary="variation" data-secondary="testing and intervals" data-type="indexterm" id="id1754"/> intervals. The basic notion is to provide an interval that denotes the expected variation of a future observation about the estimator. In the simple case where the statistic is <span><math> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> </math></span> and we have a hypothetical new observation <span><math> <msub> <mi>X</mi> <mn>0</mn> </msub> </math></span> that has the same expected value, say <span><math> <mi>μ</mi> </math></span>, and standard deviation, say <span><math> <mi>σ</mi> </math></span>, of the <span><math> <msub> <mi>X</mi> <mi>i</mi> </msub> </math></span>, then we find the expected variation of the squared loss:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub> <mi>X</mi> <mn>0</mn> </msub> <mo>−</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> <mo stretchy="false">]</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo fence="false" stretchy="false">{</mo> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub> <mi>X</mi> <mn>0</mn> </msub> <mo>−</mo> <mi>μ</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mo stretchy="false">(</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mo>−</mo> <mi>μ</mi> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> <mo fence="false" stretchy="false">}</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>X</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>+</mo> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <msup> <mi>σ</mi> <mn>2</mn> </msup> <mo>+</mo> <msup> <mi>σ</mi> <mn>2</mn> </msup> <mrow> <mo>/</mo> </mrow> <mi>n</mi> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mi>σ</mi> <msqrt> <mn>1</mn> <mo>+</mo> <mn>1</mn> <mrow> <mo>/</mo> </mrow> <mi>n</mi> </msqrt> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Notice there are two parts to the variation: one due to the variation of <span><math> <msub> <mi>X</mi> <mn>0</mn> </msub> </math></span> and the other due to the approximation of <span><math> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>X</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> </math></span> by <span><math> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> </math></span>.</p>&#13;
&#13;
<p>In the case of more complex models, the variation in prediction also breaks down into two components: the inherent variation in the data about the model plus the variation in the sampling distribution due to the estimation of the model. Assuming the model is roughly correct, we can express it as follows:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mi mathvariant="bold">Y</mi> </mrow> <mtext> </mtext> <mo>=</mo> <mtext> </mtext> <mtext mathvariant="bold">X</mtext> <msup> <mi mathvariant="bold-italic">θ</mi> <mrow> <mo>∗</mo> </mrow> </msup> <mo>+</mo> <mi mathvariant="bold-italic">ϵ</mi> </math></div>&#13;
</div>&#13;
&#13;
<p>where <span><math> <msup> <mi mathvariant="bold-italic">θ</mi> <mo>∗</mo> </msup> </math></span> is a <span><math> <mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>×</mo> <mn>1</mn> </math></span> column vector, <span><math> <mtext mathvariant="bold">X</mtext> </math></span> is an <span><math> <mi>n</mi> <mo>×</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo> </math></span> design matrix, and <span><math> <mi mathvariant="bold-italic">ϵ</mi> </math></span> consists of <span><math> <mi>n</mi> </math></span> independent random variables that each have expected value 0 and variance <span><math> <msup> <mi>σ</mi> <mn>2</mn> </msup> </math></span>. In this equation, <span><math> <mrow> <mi mathvariant="bold">Y</mi> </mrow> </math></span> is a vector of random variables, where the expected value of each variable is determined by the design matrix and the variance is <span><math> <msup> <mi>σ</mi> <mn>2</mn> </msup> </math></span>. That is, the variation about the line is constant in that it does not change with <span><math> <mrow> <mi mathvariant="bold">x</mi> </mrow> </math></span>.</p>&#13;
&#13;
<p>When we create prediction intervals in regression, they are given a <span><math> <mn>1</mn> <mo>×</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo> </math></span> row vector of covariates, called <span><math> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> </math></span>. Then the prediction is <span><math> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mrow> <mover> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo> </mover> </mrow> </math></span>, where <span><math> <mrow> <mover> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo> </mover> </mrow> </math></span> is the estimated parameter vector based on the original <span><math> <mrow> <mi mathvariant="bold">y</mi> </mrow> </math></span> and design matrix <span><math> <mtext mathvariant="bold">X</mtext> </math></span>. The expected squared error in this prediction is:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub> <mi>Y</mi> <mn>0</mn> </msub> <mo>−</mo> <mrow> <msub> <mi mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn> </msub> </mrow> <mrow> <mover> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo> </mover> </mrow> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> <mo stretchy="false">]</mo> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo fence="false" stretchy="false">{</mo> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub> <mi>Y</mi> <mn>0</mn> </msub> <mo>−</mo> <mrow> <msub> <mi mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn> </msub> <msup> <mi mathvariant="bold-italic">θ</mi> <mrow> <mo mathvariant="bold">∗</mo> </mrow> </msup> </mrow> <mo stretchy="false">)</mo> <mo>−</mo> <mo stretchy="false">(</mo> <mrow> <msub> <mi mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn> </msub> </mrow> <mrow> <mover> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo> </mover> </mrow> <mo>−</mo> <mrow> <msub> <mi mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn> </msub> </mrow> <msup> <mi mathvariant="bold-italic">θ</mi> <mrow> <mo>∗</mo> </mrow> </msup> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> <mo fence="false" stretchy="false">}</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <msub> <mi>ϵ</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>+</mo> <mrow> <mi mathvariant="double-struck">V</mi> </mrow> <mo stretchy="false">(</mo> <mrow> <msub> <mi mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn> </msub> </mrow> <mrow> <mover> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd> <mtext> </mtext> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <msup> <mi>σ</mi> <mn>2</mn> </msup> <mo stretchy="false">[</mo> <mn>1</mn> <mo>+</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo stretchy="false">(</mo> <msup> <mtext mathvariant="bold">X</mtext> <mi mathvariant="normal">⊤</mi> </msup> <mtext mathvariant="bold">X</mtext> <msup> <mo stretchy="false">)</mo> <mrow> <mo>−</mo> <mn>1</mn> </mrow> </msup> <msubsup> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> <mi mathvariant="normal">⊤</mi> </msubsup> <mo stretchy="false">]</mo> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>We approximate the variance of <span><math> <mi>ϵ</mi> </math></span> with the variance of the residuals from the least squares fit.</p>&#13;
&#13;
<p>The prediction<a contenteditable="false" data-primary="data distributions" data-secondary="normal distribution" data-type="indexterm" id="id1755"/><a contenteditable="false" data-primary="normal distribution" data-type="indexterm" id="id1756"/> intervals we create using the normal curve rely on the additional assumption that the distribution of the errors is approximately normal. This is a stronger assumption than we make for the confidence intervals. With confidence intervals, the probability distribution of <span><math> <msub> <mi>X</mi> <mi>i</mi> </msub> </math></span> need not look normal for <span><math> <mrow> <mover> <mi>X</mi> <mo stretchy="false">¯</mo> </mover> </mrow> </math></span> to have an approximate normal distribution. Similarly, the probability distribution of <span><math> <mi mathvariant="bold-italic">ϵ</mi> </math></span> in the linear model need not look normal for the estimator <span><math> <mrow> <mover> <mi>θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span> to have an approximate normal distribution.</p>&#13;
&#13;
<p>We also assume that the linear model is approximately correct when making these prediction intervals. In <a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a>, we considered the case where the fitted model doesn’t match the model that has produced the data. We now have the technical machinery to derive the model bias-variance trade-off introduced in that chapter. It’s very similar to the prediction interval derivation with a couple of small twists<a contenteditable="false" data-primary="" data-startref="ix_prob_test_inter" data-type="indexterm" id="id1757"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Probability Behind Model Selection" data-type="sect2"><div class="sect2" id="probability-behind-model-selection">&#13;
<h2>Probability Behind Model Selection</h2>&#13;
&#13;
<p>In <a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a>, we introduced model<a contenteditable="false" data-primary="probability" data-secondary="in model selection" data-secondary-sortas="model selection" data-type="indexterm" id="ix_prob_mod_sel"/> under- and overfitting with mean squared error (MSE). We described a general setup where the data might be expressed as follows:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mi>y</mi> <mo>=</mo> <mi>g</mi> <mo stretchy="false">(</mo> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mo stretchy="false">)</mo> <mo>+</mo> <mrow> <mi>ϵ</mi> </mrow> </math></div>&#13;
</div>&#13;
&#13;
<p>The <span><math> <mi>ϵ</mi> </math></span> are assumed to behave like random errors that have no trends or patterns, have constant variance, and are independent of one another. The <em>signal</em> in the model is the function <span><math> <mi>g</mi> <mo stretchy="false">(</mo> <mo stretchy="false">)</mo> </math></span>. The data are the <span><math> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mi>i</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo stretchy="false">)</mo> </math></span> pairs, and we fit models by minimizing the MSE:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <munder> <mo movablelimits="true">min</mo> <mrow> <mi>f</mi> <mo>∈</mo> <mrow> <mi mathvariant="script">F</mi> </mrow> </mrow> </munder> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mi>n</mi> </mrow> </munderover> <mo stretchy="false">(</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mi>i</mi> </msub> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </math></div>&#13;
</div>&#13;
&#13;
<p>Here <span><math> <mrow> <mi mathvariant="script">F</mi> </mrow> </math></span> is the collection of models over which we are minimizing. This collection might be all polynomials of degree <span><math> <mi>m</mi> </math></span> or less, bent lines with a bend at point <span><math> <mi>k</mi> </math></span>, and so on. Note that <span><math> <mi>g</mi> </math></span> doesn’t have to be in the collection of functions that we are using to fit a model.</p>&#13;
&#13;
<p>Our goal in model selection is to land on a model that predicts a new observation well. For a new observation, we would like the expected loss to be small:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <msub> <mi>y</mi> <mn>0</mn> </msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> </math></div>&#13;
</div>&#13;
&#13;
<p>This expectation is with respect to the distribution of possible <span><math> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo>,</mo> <msub> <mi>y</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> </math></span> and is called <em>risk</em>. Since we don’t know the population distribution of <span><math> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo>,</mo> <msub> <mi>y</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> </math></span>, we can’t calculate the risk, but we can approximate it by the average loss over the data we have collected:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <msub> <mi>y</mi> <mn>0</mn> </msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> </mtd> <mtd> <mi/> <mo>≈</mo> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mi>n</mi> </mrow> </munderover> <mo stretchy="false">(</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mi>i</mi> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>This approximation goes by the name of <em>empirical risk</em>. But hopefully you recognize it as the MSE:</p>&#13;
&#13;
<p>We fit models by minimizing the empirical risk (or MSE) over all possible models, <span><math> <mrow> <mi mathvariant="script">F</mi> </mrow> <mo mathvariant="script">=</mo> <mo fence="false" stretchy="false">{</mo> <mi mathvariant="script">f</mi> <mo fence="false" stretchy="false">}</mo> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <munder> <mo movablelimits="true">min</mo> <mrow> <mi>f</mi> <mo>∈</mo> <mrow> <mi mathvariant="script">F</mi> </mrow> </mrow> </munder> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mrow> <mi>n</mi> </mrow> </munderover> <mo stretchy="false">(</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mi>i</mi> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> </math></div>&#13;
</div>&#13;
&#13;
<p>The fitted<a contenteditable="false" data-primary="empirical risk minimization" data-type="indexterm" id="id1758"/> model is called <span><math> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>, a slightly more general representation of the linear model <span><math> <mtext mathvariant="bold">X</mtext> <mrow> <mover> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>. This technique is aptly called <em>empirical risk minimization</em>.</p>&#13;
&#13;
<p>In <a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a>, we saw problems arise when we used the empirical risk to both fit a model and evaluate the risk for a new observation. Ideally, we want to estimate the risk (expected loss):</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub> <mi>y</mi> <mn>0</mn> </msub> <mo>−</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">)</mo> <mn>2</mn> </msup> <mo stretchy="false">]</mo> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>where the expected value is over the new observation <span><math> <mo stretchy="false">(</mo> <msub> <mrow> <mi mathvariant="bold">x</mi> </mrow> <mn>0</mn> </msub> <mo>,</mo> <msub> <mi>y</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> </math></span> and over <span><math> <mrow> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> </mrow> </math></span> (which involves the original data <span><math> <mo stretchy="false">(</mo> <msub> <mtext mathvariant="bold">x</mtext> <mi>i</mi> </msub> <mo>,</mo> <msub> <mrow> <mi>y</mi> </mrow> <mi>i</mi> </msub> <mo stretchy="false">)</mo> </math></span>, <span><math> <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mi>n</mi> </math></span>).</p>&#13;
&#13;
<p>To understand the problem, we decompose this risk into three parts representing the model bias, the model variance, and the irreducible error from <span><math> <mi>ϵ</mi> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left right" columnspacing="0em 2em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> </mtd> <mtd> <mi/> <mo stretchy="false">[</mo> <msub> <mi>y</mi> <mn>0</mn> </msub> <mo>−</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>+</mo> <msub> <mi>ϵ</mi> <mn>0</mn> </msub> <mo>−</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> </mtd> <mtd> <mtext>definition</mtext> <mtext> </mtext> <mtext>of</mtext> <mtext> </mtext> <msub> <mi>y</mi> <mn>0</mn> </msub> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>+</mo> <msub> <mi>ϵ</mi> <mn>0</mn> </msub> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <mo>+</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <mo>−</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> </mtd> <mtd> <mtext>adding</mtext> <mtext> </mtext> <mo>±</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <mo>−</mo> <mo stretchy="false">(</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <mo stretchy="false">)</mo> <mo>+</mo> <msub> <mi>ϵ</mi> <mn>0</mn> </msub> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> </mtd> <mtd> <mtext>rearranging terms</mtext> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mo stretchy="false">[</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> <mo>+</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <msup> <mo stretchy="false">]</mo> <mn>2</mn> </msup> <mo>+</mo> <msup> <mi>σ</mi> <mn>2</mn> </msup> </mtd> <mtd> <mtext>expanding the square</mtext> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mtext> </mtext> <mtext> </mtext> <msup> <mtext>model bias</mtext> <mn>2</mn> </msup> <mtext> </mtext> <mtext> </mtext> <mtext> </mtext> <mo>+</mo> <mtext> </mtext> <mtext> </mtext> <mtext> </mtext> <mtext>model variance</mtext> <mtext> </mtext> <mtext> </mtext> <mo>+</mo> <mtext> </mtext> <mtext> </mtext> <mtext>error</mtext> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>To derive the equality labeled “expanding the square,” we need to formally prove that the cross-product terms in the expansion are all 0. This takes a bit of algebra and we don’t present it here. But the main idea is that the terms <span><math> <msub> <mi>ϵ</mi> <mn>0</mn> </msub> </math></span> and <span><math> <mo stretchy="false">(</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">)</mo> <mo>−</mo> <mrow> <mi mathvariant="double-struck">E</mi> </mrow> <mo stretchy="false">[</mo> <mrow> <mover> <mi>f</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <msub> <mi>x</mi> <mn>0</mn> </msub> <mo stretchy="false">]</mo> <mo stretchy="false">)</mo> </math></span> are independent and both have the expected value 0. The remaining three terms in the final equation—model bias, model variance, and irreducible error—are described as follows:</p>&#13;
&#13;
<dl class="simple myst">&#13;
	<dt>Model bias</dt>&#13;
	<dd>&#13;
	<p>The first<a contenteditable="false" data-primary="model bias" data-type="indexterm" id="id1759"/><a contenteditable="false" data-primary="bias" data-secondary="model" data-type="indexterm" id="id1760"/> of the three terms in the final equation is model bias (squared). When the signal, <span><math> <mi>g</mi> </math></span>, does not belong to the model space, we have model bias. If the model space can approximate <span><math> <mi>g</mi> </math></span> well, then the bias is small. Note that this term is not present in our prediction intervals because we assumed that there is no (or minimal) model bias.</p>&#13;
	</dd>&#13;
	<dt>Model variance</dt>&#13;
	<dd>&#13;
	<p>The second<a contenteditable="false" data-primary="model variance" data-type="indexterm" id="id1761"/><a contenteditable="false" data-primary="variance" data-secondary="model" data-type="indexterm" id="id1762"/> term represents the variability in the fitted model that comes from the data. We have seen in earlier examples that high-degree polynomials can overfit, and so vary a lot from one set of data to the next. The more complex the model space, the greater the variability in the fitted model.</p>&#13;
	</dd>&#13;
	<dt>Irreducible error</dt>&#13;
	<dd>&#13;
	<p>Finally, the last term<a contenteditable="false" data-primary="errors" data-secondary="irreducible" data-type="indexterm" id="id1763"/><a contenteditable="false" data-primary="irreducible error" data-type="indexterm" id="id1764"/> is the variability in the error, the <span><math> <msub> <mi>ϵ</mi> <mn>0</mn> </msub> </math></span>, which is dubbed the “irreducible error.” This error sticks around whether we have underfit with a simple model (high bias) or overfit with a complex model (high variance).</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>This representation of the expected loss shows the bias-variance decomposition of a fitted model. Model selection aims to balance these two competing sources of error. The train-test split, cross-validation, and regularization introduced in <a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a> are techniques to either mimic the expected loss for a new observation or penalize a model from overfitting.</p>&#13;
&#13;
<p>While we have covered a lot of theory in this chapter, we have attempted to tie it to the basics of the urn model and the three distributions: population, sample, and sampling. We wrap up the chapter with a few cautions to keep in mind when performing hypothesis tests and when making confidence or prediction intervals<a contenteditable="false" data-primary="" data-startref="ix_prob_mod_sel" data-type="indexterm" id="id1765"/><a contenteditable="false" data-primary="" data-startref="ix_theory_prob" data-type="indexterm" id="id1766"/><a contenteditable="false" data-primary="" data-startref="ix_predict_prob" data-type="indexterm" id="id1767"/><a contenteditable="false" data-primary="" data-startref="ix_prob_predict" data-type="indexterm" id="id1768"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="sec-inf-pred-gen-summary">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Throughout this chapter, we based our development of the theory behind inference and prediction on the urn model<a contenteditable="false" data-primary="urn model" data-secondary="theory for inference and prediction" data-type="indexterm" id="id1769"/>. The urn induced a probability distribution on the estimator, such as the sample mean and the least squares regression coefficients. We end this chapter with some cautions about these statistical procedures.</p>&#13;
&#13;
<p>We saw how the SD of an estimator has a factor of the square root of the sample size in the denominator. When samples are large, the SD can be quite small and can lead to rejecting a hypothesis or very narrow confidence intervals. When this happens it’s good to consider the following:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Is the difference<a contenteditable="false" data-primary="practical versus statistical significance" data-type="indexterm" id="id1770"/><a contenteditable="false" data-primary="statistical versus practical significance" data-type="indexterm" id="id1771"/> that you have detected an important difference? That is, a <span><math> <mi>p</mi> </math></span>-value may be quite small, indicating a surprising result, but the actual effect observed may be unimportant. <em>Statistical significance</em> does not imply <em>practical significance</em>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Keep in mind that these calculations do not incorporate bias, such as non-response bias and measurement bias. The bias might well be larger than any difference due to chance variation in the sampling distribution.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before less_space">At times, we know the sample is not from a chance<a contenteditable="false" data-primary="predictions and predicting" data-secondary="and chance mechanism" data-secondary-sortas="chance mechanism" data-type="indexterm" id="id1772"/><a contenteditable="false" data-primary="chance mechanism" data-secondary="in inference and prediction" data-secondary-sortas="inference and prediction" data-type="indexterm" id="id1773"/> mechanism, but it can still be useful to carry out a hypothesis test. In this case, the null model would test whether the sample (and estimator) are as if they were at random. When this test is rejected, we confirm that something nonrandom has led to the observed data. This can be a useful conclusion: that the difference between what we expect and what we observed is not explained by chance.</p>&#13;
&#13;
<p>At other times, the sample consists of the complete population. When this happens, we might not need to make confidence intervals or hypothesis tests because we have observed all values in the population. That is, inference is not required. However, we can instead place a different interpretation on hypothesis tests: we can suppose that any relation observed between two features was randomly distributed without relation to each other.</p>&#13;
&#13;
<p>We also saw how the bootstrap can be used when we don’t have enough information about the population. The bootstrap is a powerful technique, but it has limitations:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Make sure that the original sample is large and random so that the sample resembles the population.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Repeat the bootstrap process many times. Typically 10,000 replications is a reasonable number.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The bootstrap tends to have difficulties when:</p>&#13;
&#13;
	<ul>&#13;
		<li>&#13;
		<p>The estimator is influenced by outliers.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>The parameter is based on extreme values of the distribution.</p>&#13;
		</li>&#13;
		<li>&#13;
		<p>The sampling distribution of the statistic is far from bell shaped.</p>&#13;
		</li>&#13;
	</ul>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>Alternatively, we rely on the sampling distribution being approximately normal in shape. At times, the sampling distribution looks roughly normal but has thicker tails. In these situations, the family of <span><math> <mi>t</mi> </math></span>-distributions might be appropriate to use instead of the normal.</p>&#13;
&#13;
<p>A model is usually only an approximation of underlying reality, and the precision of the statement that <span><math> <msup> <mi>θ</mi> <mo>∗</mo> </msup> </math></span> exactly equals 0 is at odds with this notion of a model. The inference depends on the correctness of our model. We can partially check the model assumptions, but some amount of doubt goes with any model. In fact, it often happens that the data suggest more than one possible model, and these models may even be contradictory<a contenteditable="false" data-primary="" data-startref="ix_theory_ch17" data-type="indexterm" id="id1774"/>.</p>&#13;
&#13;
<p>Lastly, at times, the number of hypothesis tests or confidence intervals can be quite large, and we need to exercise caution to avoid spurious results. This problem is called <span><math> <mi>p</mi> </math></span>-hacking and is another example of the reproducibility crisis in science described in <a class="reference internal" data-type="xref" href="ch10.html#ch-eda">Chapter 10</a>. <span><math> <mi>P</mi> </math></span>-hacking is based on the notion that if we test, say, 100 hypotheses, all of which are true, then we would expect to get a few surprise results and reject a few of these hypotheses. This phenomenon can happen in multiple linear regression when we have a large number of features in a model, and techniques have been developed to limit the dangers of these false discoveries.</p>&#13;
&#13;
<p>We next recap the modeling process with a case study.</p>&#13;
</div></section>&#13;
</div></section></body></html>