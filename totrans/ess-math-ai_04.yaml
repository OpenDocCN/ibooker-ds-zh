- en: Chapter 4\. Optimization For Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章. 神经网络的优化
- en: '*I have lived each and every day of my life optimizing… My first aha moment
    was when I learned that our brain too, learns a model of the world.*'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我已经活了我生命中的每一天进行优化...我的第一个顿悟是当我发现我们的大脑也会学习世界的模型。*'
- en: Various artificial neural networks have fully connected layers in their architecture.
    In this chapter, we explain how the mathematics of a fully connected neural network
    works and walk through an end to end example with a real data set. We design and
    experiment with various training and loss functions. We also explain that the
    optimization and back-propagation steps used when training neural networks are
    similar to how learning happens in our brain. The brain learns by reinforcing
    neuron connections when faced with a concept it has seen before and weakening
    connections if it learns new information that undoes or contradicts previously
    learned concepts. Machines only understand numbers. Mathematically, stronger connections
    correspond to larger numbers, and weaker connections correspond to smaller numbers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 各种人工神经网络在其架构中具有全连接层。在本章中，我们解释全连接神经网络的数学原理，并通过一个真实数据集的端到端示例进行讲解。我们设计并尝试各种训练和损失函数。我们还解释了训练神经网络时使用的优化和反向传播步骤与我们大脑中学习的方式相似。大脑通过在面对之前见过的概念时加强神经元之间的连接来学习，并在学习到破坏或与之前学到的概念相矛盾的新信息时削弱连接。机器只能理解数字。从数学上讲，更强的连接对应于更大的数字，而更弱的连接对应于更小的数字。
- en: We finally walk through various regularization techniques, explaining their
    advantages, disadvantages, and use cases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将介绍各种正则化技术，解释它们的优点、缺点和用例。
- en: The Brain Cortex And Artificial Neural Networks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大脑皮层和人工神经网络
- en: Neural networks are modeled after the brain cortex, which involves billions
    of neurons arranged in a layered structure. [Figure 4-1](#Fig_Cajal_cortex_drawings)
    shows an image of three vertical cross sections of the brain neocortex and [Figure 4-2](#Fig_neural_network)
    shows a diagram of a fully connected artificial neural network.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是模仿大脑皮层的结构，其中涉及数十亿个神经元排列成分层结构。[图4-1](#Fig_Cajal_cortex_drawings)显示了大脑新皮层的三个垂直横截面图像，[图4-2](#Fig_neural_network)显示了一个完全连接的人工神经网络的图表。
- en: '![300](assets/emai_0401.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0401.png)'
- en: 'Figure 4-1\. Three drawings of cortical lamination by Santiago Ramon y Cajal,
    taken from the book Comparative study of the sensory areas of the human cortex.
    Each drawing shows a vertical cross-section of the cortex, with the surface (outermost
    side which is the closest to the skull) of the cortex at the top. Left: Nissl
    stained visual cortex of a human adult. Middle: Nissl stained motor cortex of
    a human adult. Right: Golgi-stained cortex of a month and a half old infant. The
    Nissl stain shows the cell bodies of neurons. The Golgi stain shows the dendrites
    and axons of a random subset of neurons.Image source: [Wikipedia](https://commons.wikimedia.org/wiki/File:Cajal_cortex_drawings.png).
    The layered structure of the neurons in the cortex is evident in all three cross-sections.'
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1. Santiago Ramon y Cajal绘制的三幅皮层层析图，摘自书籍《人类皮层感觉区域的比较研究》。每幅图显示了皮层的垂直横截面，皮层的表面（最靠近头骨的最外侧）位于顶部。左侧：染色的成年人视觉皮层。中间：染色的成年人运动皮层。右侧：一个半月大婴儿的Golgi染色皮层。Nissl染色显示神经元的细胞体。Golgi染色显示了一组随机神经元的树突和轴突。图片来源：[维基百科](https://commons.wikimedia.org/wiki/File:Cajal_cortex_drawings.png)。在所有三个横截面中，皮层中神经元的分层结构是显而易见的。
- en: '![300](assets/emai_0402.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0402.png)'
- en: Figure 4-2\. A fully connected or dense artificial neural network with four
    layers.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2. 一个具有四层的全连接或密集的人工神经网络。
- en: 'Even though different regions of the cortex are responsible for different functions,
    such as vision, auditory perception, logical thinking, language, speech, *etc.*,
    what actually determines the function of a specific region are its *connections*:
    Which sensory and motor skills input and output regions it connects to. This means
    that if a cortical region is connected to a different sensory input/output region,
    for example, a vision instead of an auditory locality, then it will perform vision
    tasks (computations), not auditory tasks. In a very simplified sense, the cortex
    performs one basic function at the neuron level. In an artificial neural network,
    the basic computation unit is *the perceptron*, and it functions in the same way
    across the whole network. The various connections, layers, and architechture of
    the neural network (both the brain cortex and artificial neural networks) are
    what allows these computational structures to do very impressive things.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大脑皮层的不同区域负责不同的功能，比如视觉、听觉感知、逻辑思维、语言、言语等，但实际上决定特定区域功能的是它的连接：它连接到哪些感觉和运动技能输入输出区域。这意味着，如果一个皮层区域连接到不同的感觉输入/输出区域，例如，一个视觉而不是听觉的地方，那么它将执行视觉任务（计算），而不是听觉任务。在非常简化的意义上，皮层在神经元水平上执行一个基本功能。在人工神经网络中，基本的计算单元是*感知器*，它在整个网络中的功能方式相同。神经网络（无论是大脑皮层还是人工神经网络）的各种连接、层和架构是使这些计算结构能够执行非常令人印象深刻的任务的原因。
- en: 'Training Function: Fully Connected, Or Dense, Feed Forward Neural Networks'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练功能：全连接或密集的前馈神经网络
- en: In a *fully connected* or *dense* artificial neural network (see [Figure 4-2](#Fig_neural_network)),
    every neuron, represented by a node (the circles) in every layer is connected
    to all the neurons in the next layer. The first layer is the input layer, the
    last layer is the output layer, and the intermediate layers are called *hidden
    layers*. The neural network itself, whether fully connected or not (the networks
    that we will encounter in the new few chapters are *convolutional* and are not
    fully connected), is a computational graph representing the formula of the training
    function. Recall that it is this function that we use to make predictions after
    training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在*全连接*或*密集*的人工神经网络中（参见[图4-2](#Fig_neural_network)），每个神经元，由每一层中的节点（圆圈）表示，都连接到下一层中的所有神经元。第一层是输入层，最后一层是输出层，中间层称为*隐藏层*。神经网络本身，无论是全连接还是不全连接（我们将在接下来的几章中遇到的网络是*卷积*的，不是全连接的），都是表示训练函数公式的计算图。请记住，训练后我们使用这个函数进行预测。
- en: 'Training in the neural networks context means finding the parameter values,
    or weights, that enter into the formula of the training function, via minimizing
    a loss function. This is similar to training linear regression, logistic regression,
    softmax regression and support vector machine models that we discussed in [Chapter 3](ch03.xhtml#ch03).
    The mathematical structure here remains the same:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的背景下，训练意味着通过最小化损失函数找到进入训练函数公式的参数值或权重。这类似于我们在[第3章](ch03.xhtml#ch03)中讨论的线性回归、逻辑回归、softmax回归和支持向量机模型的训练。数学结构保持不变：
- en: Training function
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练函数
- en: Loss function
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数
- en: Optimization
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化
- en: 'The only difference is that for the simple models of [Chapter 3](ch03.xhtml#ch03),
    the formulas of the training functions are very uncomplicated. They linearly combine
    the data features, add a bias term ( <math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math> ), and pass the result into at most one nonlinear function
    (for example, the logistic function in logistic regression). As a consequence,
    the results of these models are also simple: a linear (flat) function for linear
    regression, a linear division boundary between different classes in logistic regression,
    softmax regression and support vector machines. Even when we use these simple
    models to represent nonlinear data, such as in the cases of polynomial regression
    (fitting the data into polynomial functions of the features) or support vector
    machines with the kernel trick, we still end up with linear functions or division
    boundaries, but in higher dimensions (for polynomial regression, the dimensions
    will be the feature and its powers) or in transformed dimensions (such as when
    we use the kernel trick with support vector machines).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别是，在[第3章](ch03.xhtml#ch03)中简单模型的训练函数公式非常简单。它们线性组合数据特征，添加偏置项（<math alttext="omega
    0"><msub><mi>ω</mi> <mn>0</mn></msub></math>），并将结果传递至最多一个非线性函数（例如，在逻辑回归中的逻辑函数）。因此，这些模型的结果也很简单：线性（平坦）函数用于线性回归，逻辑回归中不同类别之间的线性分界线，softmax回归和支持向量机。即使我们使用这些简单模型来表示非线性数据，例如多项式回归（将数据拟合到特征的多项式函数中）或带有核技巧的支持向量机，我们仍然得到线性函数或分界线，但在更高的维度（对于多项式回归，维度将是特征及其幂）或转换后的维度（例如当我们使用支持向量机的核技巧时）。
- en: 'For neural network models, on the other hand, the process of linearly combining
    the features, adding a bias term, then passing the result through a nonlinear
    function (now called *activation function*), is the computation that happens *only
    in one neuron*. This simple process happens over and over again in dozens, hundreds,
    thousands, or sometimes millions of neurons, arranged in layers, where the output
    of one layer acts as the input of the next layer. Similar to the brain cortex,
    the aggregation of simple and similar processes over many neurons and layers produces,
    or allows for the representation, of much more complex functionalities. This is
    sort of miraculous. Thankfully, we are able to understand much more about artificial
    neural networks than our brain’s neural networks, mainly because we design them,
    and after all, an artificial neural network is just one mathematical function.
    No *black box* remains dark once we dissect it under the lens of mathematics.
    That said, the mathematical analysis of artificial neural networks is a relatively
    new field: There are still many questions to be answered and a lot to be discovered.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于神经网络模型，线性组合特征、添加偏置项，然后通过非线性函数（现在称为*激活函数*）传递结果的过程是发生在*一个神经元*中的计算。这个简单的过程在成百上千、甚至数百万个神经元中一遍又一遍地发生，这些神经元排列在层中，其中一层的输出作为下一层的输入。类似于大脑皮层，许多神经元和层上的简单和相似过程的聚合产生了或允许了更复杂功能的表示。这有点神奇。幸运的是，我们能够比我们的大脑神经网络更多地了解人工神经网络，主要是因为我们设计它们，毕竟，人工神经网络只是一个数学函数。一旦我们在数学的镜头下剖析它，没有*黑匣子*会保持黑暗。也就是说，对人工神经网络的数学分析是一个相对较新的领域：仍有许多问题有待解答，还有很多待发现的东西。
- en: A Neural Network Is A Computational Graph Representation Of The Training Function
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络是训练函数的计算图表示
- en: 'Even for a network with only five neurons, such as the one in [Figure 4-3](#Fig_five_neurons),
    it is pretty messy to write the formula of the training function. This justifies
    the use of computational graphs to represent neural networks in an organized and
    easy way. Graphs are characterized by two things: nodes and edges (congratulations,
    this was *lesson one in graph theory*). In a neural network, an edge connecting
    node i in layer m to node j in layer n is assigned a weight <math alttext="omega
    Subscript m n comma i j"><msub><mi>ω</mi> <mrow><mi>m</mi><mi>n</mi><mo>,</mo><mi>i</mi><mi>j</mi></mrow></msub></math>
    . That is four indices for only one edge! At the risk of drowning in a deep ocean
    of indices, we must organize a neural network’s weights in matrices.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是只有五个神经元的网络，比如[图4-3](#Fig_five_neurons)中的网络，编写训练函数的公式也会非常混乱。这证明了使用计算图来有组织地、简单地表示神经网络的合理性。图表的特点是两个方面：节点和边（恭喜，这是图论中的第一课）。在神经网络中，连接第m层的节点i和第n层的节点j的边被赋予权重<math
    alttext="omega Subscript m n comma i j"><msub><mi>ω</mi> <mrow><mi>m</mi><mi>n</mi><mo>,</mo><mi>i</mi><mi>j</mi></mrow></msub></math>。这是一个边上的四个指标！为了避免淹没在指标的深海中，我们必须将神经网络的权重组织成矩阵。
- en: '![250](assets/emai_0403.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0403.png)'
- en: Figure 4-3\. A fully connected (or dense) feed forward neural network with only
    five neurons arranged in three layers. The first layer (the three black dots on
    the very left) is the input layer, the second layer is the only hidden layer with
    three neurons, and the last layer is the output layer with two neurons.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3. 一个只有五个神经元的全连接（或密集）前馈神经网络，排列在三层中。第一层（最左边的三个黑点）是输入层，第二层是唯一的隐藏层，有三个神经元，最后一层是有两个神经元的输出层。
- en: Let’s model the training function of a *feed forward* fully connected neural
    network. Feed forward means that the information flows forward through the computational
    graph representing the network’s training function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们建模一个*前馈*全连接神经网络的训练函数。前馈意味着信息通过代表网络训练函数的计算图向前传递。
- en: Linearly Combine, Add Bias, Then Activate
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性组合，添加偏差，然后激活
- en: 'What kind computations happen as the information flows through one neuron:
    Linearly combine the input information using different weights, add a bias term,
    then use a nonlinear function to *activate* the neuron. We will go through this
    process one step at a time.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当信息通过一个神经元流动时会发生什么样的计算：使用不同的权重线性组合输入信息，添加偏差项，然后使用非线性函数*激活*神经元。我们将逐步进行这个过程。
- en: The weights
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重
- en: Let the matrix <math alttext="upper W Superscript 1"><msup><mi>W</mi> <mn>1</mn></msup></math>
    contain the weights of the edges *incident to* hidden layer 1, <math alttext="upper
    W squared"><msup><mi>W</mi> <mn>2</mn></msup></math> contain the weights of the
    edges incident to hidden layer 2, and so on, until we reach the output layer.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让矩阵<math alttext="upper W Superscript 1"><msup><mi>W</mi> <mn>1</mn></msup></math>包含*incident
    to*隐藏层1的边的权重，<math alttext="upper W squared"><msup><mi>W</mi> <mn>2</mn></msup></math>包含*incident
    to*隐藏层2的边的权重，依此类推，直到达到输出层。
- en: 'So for the small neural network represented in [Figure 4-3](#Fig_five_neurons),
    we only have *h=1* hidden layer, obtaining two matrices of weights:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于[图4-3](#Fig_five_neurons)中表示的小型神经网络，我们只有*h=1*个隐藏层，获得两个权重矩阵：
- en: <math alttext="dollar-sign upper W Superscript 1 Baseline equals Start 3 By
    3 Matrix 1st Row 1st Column omega 11 Superscript 1 Baseline 2nd Column omega 12
    Superscript 1 Baseline 3rd Column omega 13 Superscript 1 Baseline 2nd Row 1st
    Column omega 21 Superscript 1 Baseline 2nd Column omega 22 Superscript 1 Baseline
    3rd Column omega 23 Superscript 1 Baseline 3rd Row 1st Column omega 31 Superscript
    1 Baseline 2nd Column omega 32 Superscript 1 Baseline 3rd Column omega 33 Superscript
    1 Baseline EndMatrix and upper W Superscript h plus 1 Baseline equals upper W
    squared equals upper W Superscript o u t p u t Baseline equals Start 2 By 3 Matrix
    1st Row 1st Column omega 11 squared 2nd Column omega 12 squared 3rd Column omega
    13 squared 2nd Row 1st Column omega 21 squared 2nd Column omega 22 squared 3rd
    Column omega 23 squared EndMatrix comma dollar-sign"><mrow><msup><mi>W</mi> <mn>1</mn></msup>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>23</mn></mrow> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi>
    <mrow><mn>31</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>32</mn></mrow>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>33</mn></mrow> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced>
    <mtext>and</mtext> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>=</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>=</mo> <msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>2</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>23</mn></mrow> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced> <mo>,</mo></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper W Superscript 1 Baseline equals Start 3 By
    3 Matrix 1st Row 1st Column omega 11 Superscript 1 Baseline 2nd Column omega 12
    Superscript 1 Baseline 3rd Column omega 13 Superscript 1 Baseline 2nd Row 1st
    Column omega 21 Superscript 1 Baseline 2nd Column omega 22 Superscript 1 Baseline
    3rd Column omega 23 Superscript 1 Baseline 3rd Row 1st Column omega 31 Superscript
    1 Baseline 2nd Column omega 32 Superscript 1 Baseline 3rd Column omega 33 Superscript
    1 Baseline EndMatrix and upper W Superscript h plus 1 Baseline equals upper W
    squared equals upper W Superscript o u t p u t Baseline equals Start 2 By 3 Matrix
    1st Row 1st Column omega 11 squared 2nd Column omega 12 squared 3rd Column omega
    13 squared 2nd Row 1st Column omega 21 squared 2nd Column omega 22 squared 3rd
    Column omega 23 squared EndMatrix comma dollar-sign"><mrow><msup><mi>W</mi> <mn>1</mn></msup>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>23</mn></mrow> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi>
    <mrow><mn>31</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>32</mn></mrow>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>33</mn></mrow> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced>
    <mtext>and</mtext> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>=</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>=</mo> <msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>2</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>23</mn></mrow> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced> <mo>,</mo></mrow></math>
- en: 'where the superscripts indicate the layer which the edges point to. Note that
    if we only had one node at the output layer instead of two, then the last matrix
    of weights <math alttext="upper W Superscript h plus 1 Baseline equals upper W
    Superscript o u t p u t"><mrow><msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>=</mo> <msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup></mrow></math>
    will only be a row vector:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上标表示边指向的层。请注意，如果输出层只有一个节点而不是两个，那么权重矩阵的最后一个矩阵将只是一个行向量：
- en: <math alttext="dollar-sign upper W Superscript h plus 1 Baseline equals upper
    W squared equals upper W Superscript o u t p u t Baseline equals Start 1 By 3
    Matrix 1st Row 1st Column omega 11 squared 2nd Column omega 12 squared 3rd Column
    omega 13 squared EndMatrix dollar-sign"><mrow><msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>=</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>=</mo> <msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper W Superscript h plus 1 Baseline equals upper
    W squared equals upper W Superscript o u t p u t Baseline equals Start 1 By 3
    Matrix 1st Row 1st Column omega 11 squared 2nd Column omega 12 squared 3rd Column
    omega 13 squared EndMatrix dollar-sign"><mrow><msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>=</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>=</mo> <msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Now at one node of this neural network two computations take place:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在这个神经网络的一个节点上进行两次计算：
- en: A linear combination plus bias.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个线性组合加上偏差。
- en: Passing the result through a nonlinear activation function (the composition
    operation from calculus).
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过非线性激活函数（微积分中的复合运算）传递结果。
- en: We elaborate on these two then ultimately construct the training function of
    the fully connected feed forward neural network represented in [Figure 4-3](#Fig_five_neurons).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细说明这两个，然后最终构建完全连接前馈神经网络的训练函数，该网络在[图4-3](#Fig_five_neurons)中表示。
- en: A linear combination plus bias
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性组合加偏差
- en: 'At the first node in the first hidden layer (the only hidden layer for this
    small network), we linearly combine the inputs:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个隐藏层的第一个节点处（这个小网络的唯一隐藏层），我们线性组合输入：
- en: <math alttext="dollar-sign z 1 Superscript 1 Baseline equals omega 11 Superscript
    1 Baseline x 1 plus omega 12 Superscript 1 Baseline x 2 plus omega 13 Superscript
    1 Baseline x 3 plus omega 01 Superscript 1 dollar-sign"><mrow><msubsup><mi>z</mi>
    <mn>1</mn> <mn>1</mn></msubsup> <mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>1</mn></msubsup></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign z 1 Superscript 1 Baseline equals omega 11 Superscript
    1 Baseline x 1 plus omega 12 Superscript 1 Baseline x 2 plus omega 13 Superscript
    1 Baseline x 3 plus omega 01 Superscript 1 dollar-sign"><mrow><msubsup><mi>z</mi>
    <mn>1</mn> <mn>1</mn></msubsup> <mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>1</mn></msubsup></mrow></math>
- en: 'At the second node in the first hidden layer, we linearly combine the inputs
    using different weights than the previous linear combination:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个隐藏层的第二个节点处，我们使用不同的权重线性组合输入：
- en: <math alttext="dollar-sign z 2 Superscript 1 Baseline equals omega 21 Superscript
    1 Baseline x 1 plus omega 22 Superscript 1 Baseline x 2 plus omega 23 Superscript
    1 Baseline x 3 plus omega 02 Superscript 1 dollar-sign"><mrow><msubsup><mi>z</mi>
    <mn>2</mn> <mn>1</mn></msubsup> <mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>23</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup></mrow></math>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign z 2 Superscript 1 Baseline equals omega 21 Superscript
    1 Baseline x 1 plus omega 22 Superscript 1 Baseline x 2 plus omega 23 Superscript
    1 Baseline x 3 plus omega 02 Superscript 1 dollar-sign"><mrow><msubsup><mi>z</mi>
    <mn>2</mn> <mn>1</mn></msubsup> <mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>23</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup></mrow></math>
- en: 'At the third node in the first hidden layer, we inearly combine the inputs
    using different weights than the previous two linear combinations:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个隐藏层的第三个节点处，我们使用不同的权重线性组合输入：
- en: <math alttext="dollar-sign z 3 Superscript 1 Baseline equals omega 31 Superscript
    1 Baseline x 1 plus omega 32 Superscript 1 Baseline x 2 plus omega 33 Superscript
    1 Baseline x 3 plus omega 03 Superscript 1 dollar-sign"><mrow><msubsup><mi>z</mi>
    <mn>3</mn> <mn>1</mn></msubsup> <mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>31</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>32</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>33</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>03</mn></mrow> <mn>1</mn></msubsup></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign z 3 Superscript 1 Baseline equals omega 31 Superscript
    1 Baseline x 1 plus omega 32 Superscript 1 Baseline x 2 plus omega 33 Superscript
    1 Baseline x 3 plus omega 03 Superscript 1 dollar-sign"><mrow><msubsup><mi>z</mi>
    <mn>3</mn> <mn>1</mn></msubsup> <mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>31</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>32</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>33</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>03</mn></mrow> <mn>1</mn></msubsup></mrow></math>
- en: 'Let’s express the three equations above using vector and matrix notation. This
    will be extremely convenient for our optimization task later, and of course it
    will preserve our sanity:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用向量和矩阵表示上述三个方程。这将极大方便我们后面的优化任务，当然也会保持我们的理智：
- en: <math alttext="dollar-sign Start 3 By 1 Matrix 1st Row  z 1 Superscript 1 Baseline
    2nd Row  z 2 Superscript 1 Baseline 3rd Row  z 3 Superscript 1 Baseline EndMatrix
    equals Start 3 By 1 Matrix 1st Row  omega 11 Superscript 1 Baseline 2nd Row  omega
    21 Superscript 1 Baseline 3rd Row  omega 31 Superscript 1 Baseline EndMatrix x
    1 plus Start 3 By 1 Matrix 1st Row  omega 12 Superscript 1 Baseline 2nd Row  omega
    22 Superscript 1 Baseline 3rd Row  omega 32 Superscript 1 Baseline EndMatrix x
    2 plus Start 3 By 1 Matrix 1st Row  omega 13 Superscript 1 Baseline 2nd Row  omega
    23 Superscript 1 Baseline 3rd Row  omega 33 Superscript 1 Baseline EndMatrix x
    3 plus Start 3 By 1 Matrix 1st Row  omega 01 Superscript 1 Baseline 2nd Row  omega
    02 Superscript 1 Baseline 3rd Row  omega 03 Superscript 1 Baseline EndMatrix equals
    Start 3 By 3 Matrix 1st Row 1st Column omega 11 Superscript 1 Baseline 2nd Column
    omega 12 Superscript 1 Baseline 3rd Column omega 13 Superscript 1 Baseline 2nd
    Row 1st Column omega 21 Superscript 1 Baseline 2nd Column omega 22 Superscript
    1 Baseline 3rd Column omega 23 Superscript 1 Baseline 3rd Row 1st Column omega
    31 Superscript 1 Baseline 2nd Column omega 32 Superscript 1 Baseline 3rd Column
    omega 33 Superscript 1 Baseline EndMatrix Start 3 By 1 Matrix 1st Row  x 1 2nd
    Row  x 2 3rd Row  x 3 EndMatrix plus Start 3 By 1 Matrix 1st Row  omega 01 Superscript
    1 Baseline 2nd Row  omega 02 Superscript 1 Baseline 3rd Row  omega 03 Superscript
    1 Baseline EndMatrix period dollar-sign"><mrow><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>z</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>z</mi> <mn>2</mn>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>z</mi> <mn>3</mn> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>31</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>32</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>23</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>33</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>03</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>13</mn></mrow> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi>
    <mrow><mn>21</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>23</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>31</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>32</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>33</mn></mrow> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <mfenced
    close=")" open="("><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>+</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>03</mn></mrow> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced>
    <mo>.</mo></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign Start 3 By 1 Matrix 1st Row  z 1 Superscript 1 Baseline
    2nd Row  z 2 Superscript 1 Baseline 3rd Row  z 3 Superscript 1 Baseline EndMatrix
    equals Start 3 By 1 Matrix 1st Row  omega 11 Superscript 1 Baseline 2nd Row  omega
    21 Superscript 1 Baseline 3rd Row  omega 31 Superscript 1 Baseline EndMatrix x
    1 plus Start 3 By 1 Matrix 1st Row  omega 12 Superscript 1 Baseline 2nd Row  omega
    22 Superscript 1 Baseline 3rd Row  omega 32 Superscript 1 Baseline EndMatrix x
    2 plus Start 3 By 1 Matrix 1st Row  omega 13 Superscript 1 Baseline 2nd Row  omega
    23 Superscript 1 Baseline 3rd Row  omega 33 Superscript 1 Baseline EndMatrix x
    3 plus Start 3 By 1 Matrix 1st Row  omega 01 Superscript 1 Baseline 2nd Row  omega
    02 Superscript 1 Baseline 3rd Row  omega 03 Superscript 1 Baseline EndMatrix equals
    Start 3 By 3 Matrix 1st Row 1st Column omega 11 Superscript 1 Baseline 2nd Column
    omega 12 Superscript 1 Baseline 3rd Column omega 13 Superscript 1 Baseline 2nd
    Row 1st Column omega 21 Superscript 1 Baseline 2nd Column omega 22 Superscript
    1 Baseline 3rd Column omega 23 Superscript 1 Baseline 3rd Row 1st Column omega
    31 Superscript 1 Baseline 2nd Column omega 32 Superscript 1 Baseline 3rd Column
    omega 33 Superscript 1 Baseline EndMatrix Start 3 By 1 Matrix 1st Row  x 1 2nd
    Row  x 2 3rd Row  x 3 EndMatrix plus Start 3 By 1 Matrix 1st Row  omega 01 Superscript
    1 Baseline 2nd Row  omega 02 Superscript 1 Baseline 3rd Row  omega 03 Superscript
    1 Baseline EndMatrix period dollar-sign"><mrow><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>z</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>z</mi> <mn>2</mn>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>z</mi> <mn>3</mn> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>31</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>32</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>13</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>23</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>33</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>03</mn></mrow>
    <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>13</mn></mrow> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>ω</mi>
    <mrow><mn>21</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi> <mrow><mn>23</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>31</mn></mrow> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>ω</mi> <mrow><mn>32</mn></mrow> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>ω</mi>
    <mrow><mn>33</mn></mrow> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced> <mfenced
    close=")" open="("><mtable><mtr><mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>x</mi>
    <mn>3</mn></msub></mtd></mtr></mtable></mfenced> <mo>+</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><msubsup><mi>ω</mi> <mrow><mn>03</mn></mrow> <mn>1</mn></msubsup></mtd></mtr></mtable></mfenced>
    <mo>.</mo></mrow></math>
- en: 'We can now summarize the above expression compactly as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将上述表达式简洁地总结为：
- en: <math alttext="dollar-sign ModifyingAbove z With right-arrow Superscript 1 Baseline
    equals upper W Superscript 1 Baseline ModifyingAbove x With right-arrow plus ModifyingAbove
    omega With right-arrow Subscript 0 Superscript 1 dollar-sign"><mrow><msup><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover> <mn>1</mn></msup> <mo>=</mo> <msup><mi>W</mi>
    <mn>1</mn></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove z With right-arrow Superscript 1 Baseline
    equals upper W Superscript 1 Baseline ModifyingAbove x With right-arrow plus ModifyingAbove
    omega With right-arrow Subscript 0 Superscript 1 dollar-sign"><mrow><msup><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover> <mn>1</mn></msup> <mo>=</mo> <msup><mi>W</mi>
    <mn>1</mn></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup></mrow></math>
- en: Pass the result through a nonlinear activation function
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过非线性激活函数传递结果
- en: 'Linearly combining the features and adding bias are not enough to pick up on
    more complex information in the data, and neural networks would have never been
    successful without this crucial but very simple step: Compose with a *nonlinear*
    function at each node of the hidden layers.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 线性组合特征并添加偏差不足以捕捉数据中更复杂的信息，神经网络如果没有这一关键但非常简单的步骤就永远不会成功：在隐藏层的每个节点处与*非线性*函数组合。
- en: Linear combination of a linear combination is still a linear combination
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性组合的线性组合仍然是线性组合
- en: 'If we skip the step of composing with a nonlinear function, and pass the information
    from the first layer to the next layer using only linear combinations, then our
    network will not learn anything new in the next layer. It will not be able to
    pick up on more complex features from one layer to the next. The math of why this
    is the case is straightforward. Suppose for simplicity that we only have two input
    features, the first hidden layer has only two nodes, and the second one has two
    nodes as well. Then the outputs of the first hidden layer without a nonlinear
    activation function would be:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们跳过与非线性函数组合的步骤，仅使用线性组合将信息从第一层传递到下一层，那么我们的网络将无法在下一层学到任何新知识。它将无法从一层到下一层捕捉到更复杂的特征。为什么会这样的数学原因很简单。为了简单起见，假设我们只有两个输入特征，第一个隐藏层只有两个节点，第二个隐藏层也有两个节点。那么没有非线性激活函数的第一个隐藏层的输出将是：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column z 1 Superscript 1
    2nd Column equals omega 11 Superscript 1 Baseline x 1 plus omega 12 Superscript
    1 Baseline x 2 plus omega 01 Superscript 1 Baseline 2nd Row 1st Column z 2 Superscript
    1 2nd Column equals omega 21 Superscript 1 Baseline x 1 plus omega 22 Superscript
    1 Baseline x 2 plus omega 02 Superscript 1 EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>z</mi> <mn>1</mn> <mn>1</mn></msubsup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msubsup><mi>z</mi> <mn>2</mn> <mn>1</mn></msubsup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup></mrow></mtd></mtr></mtable></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column z 1 Superscript 1
    2nd Column equals omega 11 Superscript 1 Baseline x 1 plus omega 12 Superscript
    1 Baseline x 2 plus omega 01 Superscript 1 Baseline 2nd Row 1st Column z 2 Superscript
    1 2nd Column equals omega 21 Superscript 1 Baseline x 1 plus omega 22 Superscript
    1 Baseline x 2 plus omega 02 Superscript 1 EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>z</mi> <mn>1</mn> <mn>1</mn></msubsup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msubsup><mi>z</mi> <mn>2</mn> <mn>1</mn></msubsup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup></mrow></mtd></mtr></mtable></math>
- en: 'At the second hidden layer, these will be linearly combined again, so the output
    of the first node of this layer would be:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个隐藏层，这些将再次进行线性组合，因此这一层的第一个节点的输出将是：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column z 1 squared 2nd Column
    equals omega 11 squared z 1 Superscript 1 Baseline plus omega 12 squared z 2 Superscript
    1 Baseline plus omega 01 squared 2nd Row 1st Column Blank 2nd Column equals omega
    11 squared left-parenthesis omega 11 Superscript 1 Baseline x 1 plus omega 12
    Superscript 1 Baseline x 2 plus omega 01 Superscript 1 Baseline right-parenthesis
    plus omega 21 squared left-parenthesis omega 21 Superscript 1 Baseline x 1 plus
    omega 22 Superscript 1 Baseline x 2 plus omega 02 Superscript 1 Baseline right-parenthesis
    plus omega 01 squared 3rd Row 1st Column Blank 2nd Column equals left-parenthesis
    omega 11 squared omega 11 Superscript 1 Baseline plus omega 21 squared omega 21
    Superscript 1 Baseline right-parenthesis x 1 plus left-parenthesis omega 11 squared
    omega 12 Superscript 1 Baseline plus omega 21 squared omega 22 Superscript 1 Baseline
    right-parenthesis x 2 plus left-parenthesis omega 11 squared omega 01 Superscript
    1 Baseline plus omega 21 squared omega 02 Superscript 1 Baseline plus omega 01
    squared right-parenthesis 4th Row 1st Column Blank 2nd Column equals omega 1 x
    1 plus omega 2 x 2 plus omega 3 period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>z</mi> <mn>1</mn> <mn>2</mn></msubsup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>2</mn></msubsup> <msubsup><mi>z</mi> <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>z</mi>
    <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>2</mn></msubsup></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>1</mn></msubsup> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>2</mn></msubsup> <mrow><mo>(</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup> <mo>)</mo></mrow>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>2</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mrow><mo>(</mo> <msubsup><mi>ω</mi>
    <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup>
    <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>1</mn></msubsup> <mo>)</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mrow><mo>(</mo> <msubsup><mi>ω</mi>
    <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow>
    <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup>
    <msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <mo>)</mo></mrow>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mrow><mo>(</mo> <msubsup><mi>ω</mi>
    <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup>
    <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>01</mn></mrow> <mn>2</mn></msubsup> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <mo>.</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column z 1 squared 2nd Column
    equals omega 11 squared z 1 Superscript 1 Baseline plus omega 12 squared z 2 Superscript
    1 Baseline plus omega 01 squared 2nd Row 1st Column Blank 2nd Column equals omega
    11 squared left-parenthesis omega 11 Superscript 1 Baseline x 1 plus omega 12
    Superscript 1 Baseline x 2 plus omega 01 Superscript 1 Baseline right-parenthesis
    plus omega 21 squared left-parenthesis omega 21 Superscript 1 Baseline x 1 plus
    omega 22 Superscript 1 Baseline x 2 plus omega 02 Superscript 1 Baseline right-parenthesis
    plus omega 01 squared 3rd Row 1st Column Blank 2nd Column equals left-parenthesis
    omega 11 squared omega 11 Superscript 1 Baseline plus omega 21 squared omega 21
    Superscript 1 Baseline right-parenthesis x 1 plus left-parenthesis omega 11 squared
    omega 12 Superscript 1 Baseline plus omega 21 squared omega 22 Superscript 1 Baseline
    right-parenthesis x 2 plus left-parenthesis omega 11 squared omega 01 Superscript
    1 Baseline plus omega 21 squared omega 02 Superscript 1 Baseline plus omega 01
    squared right-parenthesis 4th Row 1st Column Blank 2nd Column equals omega 1 x
    1 plus omega 2 x 2 plus omega 3 period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>z</mi> <mn>1</mn> <mn>2</mn></msubsup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>2</mn></msubsup> <msubsup><mi>z</mi> <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>z</mi>
    <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>2</mn></msubsup></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow> <mn>1</mn></msubsup>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>1</mn></msubsup> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>2</mn></msubsup> <mrow><mo>(</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow>
    <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup> <mo>)</mo></mrow>
    <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow> <mn>2</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mrow><mo>(</mo> <msubsup><mi>ω</mi>
    <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>ω</mi> <mrow><mn>11</mn></mrow>
    <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup>
    <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>1</mn></msubsup> <mo>)</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mrow><mo>(</mo> <msubsup><mi>ω</mi>
    <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>ω</mi> <mrow><mn>12</mn></mrow>
    <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup>
    <msubsup><mi>ω</mi> <mrow><mn>22</mn></mrow> <mn>1</mn></msubsup> <mo>)</mo></mrow>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mrow><mo>(</mo> <msubsup><mi>ω</mi>
    <mrow><mn>11</mn></mrow> <mn>2</mn></msubsup> <msubsup><mi>ω</mi> <mrow><mn>01</mn></mrow>
    <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mrow><mn>21</mn></mrow> <mn>2</mn></msubsup>
    <msubsup><mi>ω</mi> <mrow><mn>02</mn></mrow> <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mrow><mn>01</mn></mrow> <mn>2</mn></msubsup> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <mo>.</mo></mrow></mtd></mtr></mtable></math>
- en: This output is nothing but a simple linear combination of the original features
    plus bias. Hence, adding a layer without any nonlinear activation contributes
    nothing new. In other words, the training function would remain linear and would
    lack the ability to pick up on any nonlinear relationships in the data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出只是原始特征加偏差的简单线性组合。因此，添加一个没有任何非线性激活的层不会带来任何新的东西。换句话说，训练函数将保持线性，并且缺乏捕捉数据中任何非线性关系的能力。
- en: 'We are the ones who decide on the formula for the nonlinear activation function,
    and different nodes can have different activation functions, even though it is
    rare to do this in practice. Let f be this activation function, then the output
    of the first hidden layer will be:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是决定非线性激活函数公式的人，不同的节点可以有不同的激活函数，尽管在实践中很少这样做。设f为这个激活函数，那么第一个隐藏层的输出将是：
- en: <math alttext="dollar-sign ModifyingAbove s With right-arrow Superscript 1 Baseline
    equals ModifyingAbove f With right-arrow left-parenthesis ModifyingAbove z With
    right-arrow Superscript 1 Baseline right-parenthesis equals ModifyingAbove f With
    right-arrow left-parenthesis upper W Superscript 1 Baseline ModifyingAbove x With
    right-arrow plus ModifyingAbove omega With right-arrow Subscript 0 Superscript
    1 Baseline right-parenthesis period dollar-sign"><mrow><msup><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover> <mn>1</mn></msup> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mover accent="true"><mi>z</mi> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>)</mo></mrow> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>1</mn></msup> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove s With right-arrow Superscript 1 Baseline
    equals ModifyingAbove f With right-arrow left-parenthesis ModifyingAbove z With
    right-arrow Superscript 1 Baseline right-parenthesis equals ModifyingAbove f With
    right-arrow left-parenthesis upper W Superscript 1 Baseline ModifyingAbove x With
    right-arrow plus ModifyingAbove omega With right-arrow Subscript 0 Superscript
    1 Baseline right-parenthesis period dollar-sign"><mrow><msup><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover> <mn>1</mn></msup> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mover accent="true"><mi>z</mi> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>)</mo></mrow> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>1</mn></msup> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
- en: 'It is now straightforward to see that if we had more hidden layers, their outputs
    will be *chained* with those of previous layers, making writing the training function
    a bit tedious:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很容易看到，如果我们有更多的隐藏层，它们的输出将与前一层的输出*链接*在一起，使得编写训练函数有点繁琐：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column ModifyingAbove s With
    right-arrow squared 2nd Column ModifyingAbove f With right-arrow left-parenthesis
    ModifyingAbove z With right-arrow squared right-parenthesis equals ModifyingAbove
    f With right-arrow left-parenthesis upper W squared ModifyingAbove s With right-arrow
    Superscript 1 Baseline plus ModifyingAbove omega With right-arrow Subscript 0
    Superscript 2 Baseline right-parenthesis equals ModifyingAbove f With right-arrow
    left-parenthesis upper W squared left-parenthesis ModifyingAbove f With right-arrow
    left-parenthesis upper W Superscript 1 Baseline ModifyingAbove x With right-arrow
    plus ModifyingAbove omega With right-arrow Subscript 0 Superscript 1 Baseline
    right-parenthesis right-parenthesis plus ModifyingAbove omega With right-arrow
    Subscript 0 Superscript 2 Baseline right-parenthesis comma 2nd Row 1st Column
    ModifyingAbove s With right-arrow cubed 2nd Column ModifyingAbove f With right-arrow
    left-parenthesis ModifyingAbove z With right-arrow cubed right-parenthesis equals
    ModifyingAbove f With right-arrow left-parenthesis upper W cubed ModifyingAbove
    s With right-arrow squared plus ModifyingAbove omega With right-arrow Subscript
    0 Superscript 3 Baseline right-parenthesis equals ModifyingAbove f With right-arrow
    left-parenthesis upper W cubed left-parenthesis ModifyingAbove f With right-arrow
    left-parenthesis upper W squared left-parenthesis ModifyingAbove f With right-arrow
    left-parenthesis upper W Superscript 1 Baseline ModifyingAbove x With right-arrow
    plus ModifyingAbove omega With right-arrow Subscript 0 Superscript 1 Baseline
    right-parenthesis right-parenthesis plus ModifyingAbove omega With right-arrow
    Subscript 0 Superscript 2 Baseline right-parenthesis right-parenthesis plus ModifyingAbove
    omega With right-arrow Subscript 0 Superscript 3 Baseline right-parenthesis period
    EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mn>2</mn></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover> <mn>2</mn></msup> <mo>)</mo></mrow>
    <mo>=</mo> <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo>
    <msup><mi>W</mi> <mn>2</mn></msup> <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn> <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mrow><mo>(</mo>
    <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi>
    <mn>1</mn></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn> <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>,</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mn>3</mn></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mover accent="true"><mi>z</mi> <mo>→</mo></mover>
    <mn>3</mn></msup> <mo>)</mo></mrow> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>3</mn></msup> <msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mn>2</mn></msup> <mo>+</mo> <msubsup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>3</mn></msubsup> <mo>)</mo></mrow>
    <mo>=</mo> <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo>
    <msup><mi>W</mi> <mn>3</mn></msup> <mrow><mo>(</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mrow><mo>(</mo>
    <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi>
    <mn>1</mn></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn> <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn>
    <mn>3</mn></msubsup> <mo>)</mo></mrow> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column ModifyingAbove s With
    right-arrow squared 2nd Column ModifyingAbove f With right-arrow left-parenthesis
    ModifyingAbove z With right-arrow squared right-parenthesis equals ModifyingAbove
    f With right-arrow left-parenthesis upper W squared ModifyingAbove s With right-arrow
    Superscript 1 Baseline plus ModifyingAbove omega With right-arrow Subscript 0
    Superscript 2 Baseline right-parenthesis equals ModifyingAbove f With right-arrow
    left-parenthesis upper W squared left-parenthesis ModifyingAbove f With right-arrow
    left-parenthesis upper W Superscript 1 Baseline ModifyingAbove x With right-arrow
    plus ModifyingAbove omega With right-arrow Subscript 0 Superscript 1 Baseline
    right-parenthesis right-parenthesis plus ModifyingAbove omega With right-arrow
    Subscript 0 Superscript 2 Baseline right-parenthesis comma 2nd Row 1st Column
    ModifyingAbove s With right-arrow cubed 2nd Column ModifyingAbove f With right-arrow
    left-parenthesis ModifyingAbove z With right-arrow cubed right-parenthesis equals
    ModifyingAbove f With right-arrow left-parenthesis upper W cubed ModifyingAbove
    s With right-arrow squared plus ModifyingAbove omega With right-arrow Subscript
    0 Superscript 3 Baseline right-parenthesis equals ModifyingAbove f With right-arrow
    left-parenthesis upper W cubed left-parenthesis ModifyingAbove f With right-arrow
    left-parenthesis upper W squared left-parenthesis ModifyingAbove f With right-arrow
    left-parenthesis upper W Superscript 1 Baseline ModifyingAbove x With right-arrow
    plus ModifyingAbove omega With right-arrow Subscript 0 Superscript 1 Baseline
    right-parenthesis right-parenthesis plus ModifyingAbove omega With right-arrow
    Subscript 0 Superscript 2 Baseline right-parenthesis right-parenthesis plus ModifyingAbove
    omega With right-arrow Subscript 0 Superscript 3 Baseline right-parenthesis period
    EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mn>2</mn></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover> <mn>2</mn></msup> <mo>)</mo></mrow>
    <mo>=</mo> <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo>
    <msup><mi>W</mi> <mn>2</mn></msup> <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn> <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mrow><mo>(</mo>
    <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi>
    <mn>1</mn></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn> <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>,</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mn>3</mn></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mover accent="true"><mi>z</mi> <mo>→</mo></mover>
    <mn>3</mn></msup> <mo>)</mo></mrow> <mo>=</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>3</mn></msup> <msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mn>2</mn></msup> <mo>+</mo> <msubsup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>3</mn></msubsup> <mo>)</mo></mrow>
    <mo>=</mo> <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo>
    <msup><mi>W</mi> <mn>3</mn></msup> <mrow><mo>(</mo> <mover accent="true"><mi>f</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mrow><mo>(</mo>
    <mover accent="true"><mi>f</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi>
    <mn>1</mn></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>1</mn></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn> <mn>2</mn></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn>
    <mn>3</mn></msubsup> <mo>)</mo></mrow> <mo>.</mo></mrow></mtd></mtr></mtable></math>
- en: This chaining goes on, until we reach the output layer. What happens at this
    very last layer depends on the task of the network. If the goal is regression
    (predict one numerical value) or binary classification (classify into two classes)
    then we only have one output node (see [Figure 4-4](#Fig_network_regression)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种链接继续下去，直到我们到达输出层。在这个最后一层发生的情况取决于网络的任务。如果目标是回归（预测一个数值）或二元分类（分类为两类），那么我们只有一个输出节点（参见[图4-4](#Fig_network_regression)）。
- en: '![250](assets/emai_0404.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0404.png)'
- en: Figure 4-4\. A fully connected (or dense) feed forward neural network with only
    nine neurons arranged in four layers. The first layer on the very left is the
    input layer, the second and third layers are the two hidden layers with four neurons
    each, and the last layer is the output layer with only one neuron (this network
    performs either a regression task or a binary classification task).
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4. 一个仅有九个神经元排列在四层中的全连接（或密集）前馈神经网络。最左边的第一层是输入层，第二层和第三层是每个四个神经元的两个隐藏层，最后一层是仅有一个神经元的输出层（此网络执行回归任务或二元分类任务）。
- en: 'If the task is regression, we linearly combine the outputs of the previous
    layer at the final output node, add bias, and go home (we *do not* pass the result
    through a nonlinear function in this case). Since the output layer only has one
    node, the output matrix is just a row vector <math alttext="upper W Superscript
    o u t p u t Baseline equals upper W Superscript h plus 1"><mrow><msup><mi>W</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <mo>=</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></math>
    , and one bias <math alttext="omega 0 Superscript h plus 1"><msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math> . The
    prediction of the network will now be:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任务是回归，我们在最终输出节点处线性组合前一层的输出，添加偏差，然后结束（在这种情况下*不*通过非线性函数传递结果）。由于输出层只有一个节点，输出矩阵只是一个行向量<math
    alttext="upper W Superscript o u t p u t Baseline equals upper W Superscript h
    plus 1"><mrow><msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <mo>=</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></math>，和一个偏差<math
    alttext="omega 0 Superscript h plus 1"><msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>。网络的预测现在将是：
- en: <math alttext="dollar-sign y Subscript p r e d i c t Baseline equals upper W
    Superscript h plus 1 Baseline ModifyingAbove s With right-arrow Superscript h
    Baseline plus omega 0 Superscript h plus 1 Baseline comma dollar-sign"><mrow><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup>
    <mo>,</mo></mrow></math>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y Subscript p r e d i c t Baseline equals upper W
    Superscript h plus 1 Baseline ModifyingAbove s With right-arrow Superscript h
    Baseline plus omega 0 Superscript h plus 1 Baseline comma dollar-sign"><mrow><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup>
    <mo>,</mo></mrow></math>
- en: where h is the total number of hidden layers in the network (this does not include
    the input and output layers).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中h是网络中隐藏层的总数（不包括输入和输出层）。
- en: 'If on the other hand the task is binary classification, then again we have
    only one output node, where we linearly combine the outputs of the previous layer,
    add bias, then pass the result through the logistic function <math alttext="sigma
    left-parenthesis s right-parenthesis equals StartFraction 1 Over 1 plus e Superscript
    negative s Baseline EndFraction"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac></mrow></math> , resulting
    in the network’s prediction:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，如果任务是二元分类，那么我们再次只有一个输出节点，在这里我们线性组合前一层的输出，添加偏差，然后通过逻辑函数传递结果<math alttext="sigma
    left-parenthesis s right-parenthesis equals StartFraction 1 Over 1 plus e Superscript
    negative s Baseline EndFraction"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac></mrow></math>，导致网络的预测：
- en: <math alttext="dollar-sign y Subscript p r e d i c t Baseline equals sigma left-parenthesis
    upper W Superscript h plus 1 Baseline ModifyingAbove s With right-arrow Superscript
    h Baseline plus omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y Subscript p r e d i c t Baseline equals sigma left-parenthesis
    upper W Superscript h plus 1 Baseline ModifyingAbove s With right-arrow Superscript
    h Baseline plus omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup>
    <mo>)</mo></mrow></mrow></math>
- en: 'If the task is to classify into multiple classes, say five classes, then the
    output layer would include five nodes. At each of these nodes, we linearly combine
    the outputs of the previous layer, add bias, then pass the result through the
    softmax function:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任务是分类到多个类别，比如五个类别，那么输出层将包括五个节点。在这些节点中，我们线性组合前一层的输出，添加偏差，然后通过softmax函数传递结果：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column sigma left-parenthesis
    z Superscript 1 Baseline right-parenthesis 2nd Column equals StartFraction e Superscript
    z Super Superscript 1 Superscript Baseline Over e Superscript z Super Superscript
    1 Superscript Baseline plus e Superscript z squared Baseline plus e Superscript
    z cubed Baseline plus e Superscript z Super Superscript 4 Superscript Baseline
    plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction comma
    2nd Row 1st Column sigma left-parenthesis z squared right-parenthesis 2nd Column
    equals StartFraction e Superscript z squared Baseline Over e Superscript z Super
    Superscript 1 Superscript Baseline plus e Superscript z squared Baseline plus
    e Superscript z cubed Baseline plus e Superscript z Super Superscript 4 Superscript
    Baseline plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction
    comma 3rd Row 1st Column sigma left-parenthesis z cubed right-parenthesis 2nd
    Column equals StartFraction e Superscript z cubed Baseline Over e Superscript
    z Super Superscript 1 Superscript Baseline plus e Superscript z squared Baseline
    plus e Superscript z cubed Baseline plus e Superscript z Super Superscript 4 Superscript
    Baseline plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction
    comma 4th Row 1st Column sigma left-parenthesis z Superscript 4 Baseline right-parenthesis
    2nd Column equals StartFraction e Superscript z Super Superscript 4 Superscript
    Baseline Over e Superscript z Super Superscript 1 Superscript Baseline plus e
    Superscript z squared Baseline plus e Superscript z cubed Baseline plus e Superscript
    z Super Superscript 4 Superscript Baseline plus e Superscript z Super Superscript
    5 Superscript Baseline EndFraction comma 5th Row 1st Column sigma left-parenthesis
    z Superscript 5 Baseline right-parenthesis 2nd Column equals StartFraction e Superscript
    z Super Superscript 5 Superscript Baseline Over e Superscript z Super Superscript
    1 Superscript Baseline plus e Superscript z squared Baseline plus e Superscript
    z cubed Baseline plus e Superscript z Super Superscript 4 Superscript Baseline
    plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction period
    EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>σ</mi>
    <mo>(</mo> <msup><mi>z</mi> <mn>1</mn></msup> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><msup><mi>e</mi> <msup><mi>z</mi> <mn>1</mn></msup></msup> <mrow><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>2</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>2</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>3</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>3</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>4</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>5</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>5</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column sigma left-parenthesis
    z Superscript 1 Baseline right-parenthesis 2nd Column equals StartFraction e Superscript
    z Super Superscript 1 Superscript Baseline Over e Superscript z Super Superscript
    1 Superscript Baseline plus e Superscript z squared Baseline plus e Superscript
    z cubed Baseline plus e Superscript z Super Superscript 4 Superscript Baseline
    plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction comma
    2nd Row 1st Column sigma left-parenthesis z squared right-parenthesis 2nd Column
    equals StartFraction e Superscript z squared Baseline Over e Superscript z Super
    Superscript 1 Superscript Baseline plus e Superscript z squared Baseline plus
    e Superscript z cubed Baseline plus e Superscript z Super Superscript 4 Superscript
    Baseline plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction
    comma 3rd Row 1st Column sigma left-parenthesis z cubed right-parenthesis 2nd
    Column equals StartFraction e Superscript z cubed Baseline Over e Superscript
    z Super Superscript 1 Superscript Baseline plus e Superscript z squared Baseline
    plus e Superscript z cubed Baseline plus e Superscript z Super Superscript 4 Superscript
    Baseline plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction
    comma 4th Row 1st Column sigma left-parenthesis z Superscript 4 Baseline right-parenthesis
    2nd Column equals StartFraction e Superscript z Super Superscript 4 Superscript
    Baseline Over e Superscript z Super Superscript 1 Superscript Baseline plus e
    Superscript z squared Baseline plus e Superscript z cubed Baseline plus e Superscript
    z Super Superscript 4 Superscript Baseline plus e Superscript z Super Superscript
    5 Superscript Baseline EndFraction comma 5th Row 1st Column sigma left-parenthesis
    z Superscript 5 Baseline right-parenthesis 2nd Column equals StartFraction e Superscript
    z Super Superscript 5 Superscript Baseline Over e Superscript z Super Superscript
    1 Superscript Baseline plus e Superscript z squared Baseline plus e Superscript
    z cubed Baseline plus e Superscript z Super Superscript 4 Superscript Baseline
    plus e Superscript z Super Superscript 5 Superscript Baseline EndFraction period
    EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>σ</mi>
    <mo>(</mo> <msup><mi>z</mi> <mn>1</mn></msup> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><msup><mi>e</mi> <msup><mi>z</mi> <mn>1</mn></msup></msup> <mrow><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>2</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>2</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>3</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>3</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>4</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>σ</mi> <mo>(</mo> <msup><mi>z</mi> <mn>5</mn></msup>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>5</mn></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>1</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>2</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi> <mn>3</mn></msup></msup> <mo>+</mo><msup><mi>e</mi>
    <msup><mi>z</mi> <mn>4</mn></msup></msup> <mo>+</mo><msup><mi>e</mi> <msup><mi>z</mi>
    <mn>5</mn></msup></msup></mrow></mfrac> <mo>.</mo></mrow></mtd></mtr></mtable></math>
- en: 'Group the above into a vector function <math alttext="ModifyingAbove sigma
    With right-arrow"><mover accent="true"><mi>σ</mi> <mo>→</mo></mover></math> that
    also takes vectors as input: <math alttext="ModifyingAbove sigma With right-arrow
    left-parenthesis ModifyingAbove z With right-arrow right-parenthesis"><mrow><mover
    accent="true"><mi>σ</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> , then the final prediction
    of the neural network is a vector of five probability scores that a data instance
    belongs to each of the five classes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述分组为一个矢量函数<math alttext="ModifyingAbove sigma With right-arrow"><mover accent="true"><mi>σ</mi>
    <mo>→</mo></mover></math>，该函数还将矢量作为输入：<math alttext="ModifyingAbove sigma With
    right-arrow left-parenthesis ModifyingAbove z With right-arrow right-parenthesis"><mrow><mover
    accent="true"><mi>σ</mi> <mo>→</mo></mover> <mrow><mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>，那么神经网络的最终预测是一个包含五个概率分数的向量，表示数据实例属于五个类别中的每一个的概率：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column ModifyingAbove y With
    right-arrow Subscript p r e d i c t 2nd Column equals ModifyingAbove sigma With
    right-arrow left-parenthesis ModifyingAbove z With right-arrow right-parenthesis
    2nd Row 1st Column Blank 2nd Column equals ModifyingAbove sigma With right-arrow
    left-parenthesis upper W Superscript o u t p u t Baseline ModifyingAbove s With
    right-arrow Superscript h Baseline plus ModifyingAbove omega 0 With right-arrow
    Superscript h plus 1 Baseline right-parenthesis EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mi>σ</mi> <mo>→</mo></mover>
    <mrow><mo>(</mo> <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mi>σ</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column ModifyingAbove y With
    right-arrow Subscript p r e d i c t 2nd Column equals ModifyingAbove sigma With
    right-arrow left-parenthesis ModifyingAbove z With right-arrow right-parenthesis
    2nd Row 1st Column Blank 2nd Column equals ModifyingAbove sigma With right-arrow
    left-parenthesis upper W Superscript o u t p u t Baseline ModifyingAbove s With
    right-arrow Superscript h Baseline plus ModifyingAbove omega 0 With right-arrow
    Superscript h plus 1 Baseline right-parenthesis EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mi>σ</mi> <mo>→</mo></mover>
    <mrow><mo>(</mo> <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mi>σ</mi>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: Notation overview
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 符号概述
- en: 'We will try to remain consistent with notation throughout our discussion of
    neural networks: The x’s are the input features, the W’s are the matrices or column
    vectors containing the weights that we use for linear combinations, the <math
    alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math> ’s are the biases
    which are sometimes grouped into a vector, the z’s are the results of linear combinations
    plus biases, and the *s*’s are the results of passing those into the nonlinear
    activation functions.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论神经网络时，我们将尽量保持符号的一致性：x是输入特征，W是包含用于线性组合的权重的矩阵或列向量，<math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math>是有时分组为矢量的偏差，z是线性组合加偏差的结果，*s*是将这些结果传递到非线性激活函数中的结果。
- en: Common Activation Functions
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的激活函数
- en: In theory, we can use any nonlinear function to *activate our nodes* (think
    of all calculus functions we’ve ever encountered). In practice, there are some
    popular ones, listed below and graphed in [Figure 4-5](#Fig_activation_functions).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们可以使用任何非线性函数来*激活我们的节点*（想象我们曾经遇到的所有微积分函数）。在实践中，有一些流行的函数，如下所列，并在[图4-5](#Fig_activation_functions)中绘制。
- en: '![275](assets/emai_0405.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![275](assets/emai_0405.png)'
- en: Figure 4-5\. Various activation functions for neural networks. The first row
    consists of sigmoidal-type activation functions, shaped like the letter S. These
    saturate (become flat and output the same values) for inputs large in magnitude.
    The second row consists of ReLU- type activation functions, which do not saturate.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。神经网络的各种激活函数。第一行由类似字母S形状的Sigmoid型激活函数组成。对于输入的绝对值较大，这些函数会饱和（变得平坦并输出相同的值）。第二行由不会饱和的ReLU型激活函数组成。
- en: By far the Rectified Linear Unit function (abbreviated ReLU) is the most commonly
    used in nowaday’s networks, and [AlexNet](https://en.wikipedia.org/wiki/AlexNet)’s
    success in 2012 is partially attributed to the use of this activation function,
    as opposed to the hyperbolic tangent and logistic functions (sigmoid) that were
    commonly used in neural networks at the time (and are still in use). The first
    four functions in the list below and in [Figure 4-5](#Fig_activation_functions)
    are all inspired from computational neuroscience, where they attempt to model
    a threshold for the activation (firing) of one neuron cell. Their graphs look
    similar to each other, some are smoother variants of others, some output only
    positive numbers, others output more balanced numbers between -1 and 1, or between
    <math alttext="minus StartFraction pi Over 2 EndFraction"><mrow><mo>-</mo> <mfrac><mi>π</mi>
    <mn>2</mn></mfrac></mrow></math> and <math alttext="StartFraction pi Over 2 EndFraction"><mfrac><mi>π</mi>
    <mn>2</mn></mfrac></math> . They all *saturate* for small or large inputs, meaning
    their graphs become flat for inputs large in mangnitude. This creates a problem
    for *learning*, since if these functions output the same numbers over and over
    again, there will not be much learning happening. Mathematically, this manifests
    itself as what is known as *the vanishing gradient problem*. The second set of
    activation functions attempts to rectify this saturation problem, which it does,
    as we see in the graphs of the second row in [Figure 4-5](#Fig_activation_functions).
    This however, introduces another problem, called *the exploding gradient problem*,
    since these activation functions are unbounded and can now output big numbers,
    and if these numbers grow over multiple layers, we have a problem. Every new set
    of problems that gets introduced comes with its own set of techniques attempting
    to fix it, such as *gradient clipping*, normalizing the outputs after each layer,
    *etc.*. The take home lesson is that none of this is magic. A lot of it is trial
    and error, and new methods emerge in order to fix problems that other new methods
    introduced. We only need to understand the principles, the why and the how, get
    a descent exposure to what is popular in the field, while keeping an open mind
    for improving things, or doing things entirely differently.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，修正线性单元函数（简称ReLU）是当今网络中最常用的激活函数，2012年[AlexNet](https://en.wikipedia.org/wiki/AlexNet)的成功部分归因于使用这种激活函数，而不是当时神经网络中常用的双曲正切和逻辑函数（Sigmoid）。下面列表中的前四个函数以及[图4-5](#Fig_activation_functions)中的函数都受到计算神经科学的启发，它们试图模拟一个神经元细胞的激活（发射）阈值。它们的图形看起来相似，有些是其他函数的平滑变体，有些只输出正数，其他输出在-1和1之间或在<math
    alttext="minus StartFraction pi Over 2 EndFraction"><mrow><mo>-</mo> <mfrac><mi>π</mi>
    <mn>2</mn></mfrac></mrow></math>和<math alttext="StartFraction pi Over 2 EndFraction"><mfrac><mi>π</mi>
    <mn>2</mn></mfrac></math>之间的更平衡的数字。它们对于小或大的输入都会饱和，意味着它们的图形对于绝对值较大的输入会变得平坦。这会导致*学习*问题，因为如果这些函数一遍又一遍地输出相同的数字，那么学习就不会发生太多。从数学上讲，这表现为所谓的*梯度消失问题*。第二组激活函数试图纠正这种饱和问题，它确实做到了，正如我们在[图4-5](#Fig_activation_functions)的第二行图中所看到的。然而，这引入了另一个问题，称为*梯度爆炸问题*，因为这些激活函数是无界的，现在可以输出大数字，如果这些数字在多个层上增长，我们就会遇到问题。引入的每个新问题都伴随着一套尝试解决它的技术，例如*梯度裁剪*，在每一层后对输出进行归一化，*等等*。重要的一点是，这一切都不是魔法。其中很多是试错，新方法出现是为了解决其他新方法引入的问题。我们只需要理解原则，为什么和如何，对领域中流行的内容有一定了解，同时保持开放的思维来改进事物，或者完全不同的方式来做事。
- en: 'Let’s state the formulas of common activation functions, as well as their derivatives.
    We need to calculate one derivative of the training function when we optimize
    the loss function, in our search for the best weights of the neural network:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出常见激活函数的公式，以及它们的导数。在我们寻找神经网络最佳权重时，我们需要计算训练函数的一个导数，以优化损失函数：
- en: 'Step function: <math alttext="f left-parenthesis z right-parenthesis equals
    StartLayout Enlarged left-brace 1st Row 1st Column Blank 2nd Column 0 if z less-than
    0 2nd Row 1st Column Blank 2nd Column 1 if z greater-than-or-equal-to 0 EndLayout"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced close="" open="{"
    separators=""><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mn>0</mn>
    <mtext>if</mtext> <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mn>1</mn> <mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阶跃函数：<math alttext="f left-parenthesis z right-parenthesis equals StartLayout
    Enlarged left-brace 1st Row 1st Column Blank 2nd Column 0 if z less-than 0 2nd
    Row 1st Column Blank 2nd Column 1 if z greater-than-or-equal-to 0 EndLayout"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced close="" open="{"
    separators=""><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mn>0</mn>
    <mtext>if</mtext> <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mn>1</mn> <mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Its derivative: <math alttext="f prime left-parenthesis z right-parenthesis
    equals StartLayout Enlarged left-brace 1st Row 1st Column Blank 2nd Column 0 if
    z not-equals 0 2nd Row 1st Column Blank 2nd Column u n d e f i n e d if z equals
    0 EndLayout"><mrow><msup><mi>f</mi> <mo>''</mo></msup> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable
    displaystyle="true"><mtr><mtd columnalign="left"><mrow><mn>0</mn> <mtext>if</mtext>
    <mi>z</mi> <mo>≠</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>u</mi>
    <mi>n</mi> <mi>d</mi> <mi>e</mi> <mi>f</mi> <mi>i</mi> <mi>n</mi> <mi>e</mi> <mi>d</mi>
    <mtext>if</mtext> <mi>z</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数：
- en: 'Logistic function: <math alttext="sigma left-parenthesis z right-parenthesis
    equals StartFraction 1 Over 1 plus e Superscript negative z Baseline EndFraction"><mrow><mi>σ</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math> .'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑函数：
- en: 'Its derivative: <math alttext="sigma prime left-parenthesis z right-parenthesis
    equals StartFraction e Superscript negative z Baseline Over left-parenthesis 1
    plus e Superscript negative z Baseline right-parenthesis squared EndFraction equals
    sigma left-parenthesis z right-parenthesis left-parenthesis 1 minus sigma left-parenthesis
    z right-parenthesis right-parenthesis"><mrow><msup><mi>σ</mi> <mo>''</mo></msup>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mfrac>
    <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mrow></math> .'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数：
- en: 'Hyperbolic tangent function: <math alttext="hyperbolic tangent left-parenthesis
    z right-parenthesis equals StartFraction e Superscript z Baseline minus e Superscript
    negative z Baseline Over e Superscript z Baseline plus e Superscript negative
    z Baseline EndFraction equals StartFraction 2 Over 1 plus e Superscript minus
    2 z Baseline EndFraction minus 1"><mrow><mo form="prefix">tanh</mo> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><msup><mi>e</mi> <mi>z</mi></msup>
    <mo>-</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow> <mrow><msup><mi>e</mi>
    <mi>z</mi></msup> <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>2</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mn>2</mn><mi>z</mi></mrow></msup></mrow></mfrac>
    <mo>-</mo> <mn>1</mn></mrow></math>'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双曲正切函数：
- en: 'Its derivative: <math alttext="hyperbolic tangent prime left-parenthesis z
    right-parenthesis equals StartFraction 4 Over left-parenthesis e Superscript z
    Baseline plus e Superscript negative z Baseline right-parenthesis squared EndFraction
    equals 1 minus f left-parenthesis z right-parenthesis squared"><mrow><msup><mo
    form="prefix">tanh</mo> <mo>''</mo></msup> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>4</mn> <msup><mrow><mo>(</mo><msup><mi>e</mi> <mi>z</mi></msup>
    <mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow></msup> <mo>)</mo></mrow>
    <mn>2</mn></msup></mfrac> <mo>=</mo> <mn>1</mn> <mo>-</mo> <mi>f</mi> <msup><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它的导数：
- en: 'Inverse tangent function: <math alttext="f left-parenthesis z right-parenthesis
    equals arc tangent left-parenthesis z right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>z</mi> <mo>)</mo> <mo>=</mo> <mo form="prefix">arctan</mo> <mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow></math> .'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反正切函数：
- en: 'Its derivative: <math alttext="f prime left-parenthesis z right-parenthesis
    equals StartFraction 1 Over 1 plus z squared EndFraction"><mrow><msup><mi>f</mi>
    <mo>''</mo></msup> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>z</mi> <mn>2</mn></msup></mrow></mfrac></mrow></math>
    .'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数：
- en: 'Rectified Linear Unit function or ReLU(z): <math alttext="f left-parenthesis
    z right-parenthesis equals StartLayout Enlarged left-brace 1st Row 1st Column
    Blank 2nd Column 0 if z less-than 0 2nd Row 1st Column Blank 2nd Column z if z
    greater-than-or-equal-to 0 EndLayout"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable
    displaystyle="true"><mtr><mtd columnalign="left"><mrow><mn>0</mn> <mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>z</mi>
    <mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整流线性单元函数或ReLU(z)：
- en: 'Its derivative: <math alttext="f prime left-parenthesis z right-parenthesis
    equals StartLayout Enlarged left-brace 1st Row 1st Column Blank 2nd Column 0 if
    z less-than 0 2nd Row 1st Column Blank 2nd Column u n d e f i n e d if z equals
    0 3rd Row 1st Column Blank 2nd Column 1 if z greater-than 0 EndLayout"><mrow><msup><mi>f</mi>
    <mo>''</mo></msup> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced
    close="" open="{" separators=""><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mn>0</mn>
    <mtext>if</mtext> <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>u</mi> <mi>n</mi> <mi>d</mi> <mi>e</mi> <mi>f</mi>
    <mi>i</mi> <mi>n</mi> <mi>e</mi> <mi>d</mi> <mtext>if</mtext> <mi>z</mi> <mo>=</mo>
    <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>1</mn> <mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数：
- en: 'Leaky Rectified Linear Unit function (or parametric linear unit): <math alttext="f
    left-parenthesis z right-parenthesis equals StartLayout Enlarged left-brace 1st
    Row 1st Column Blank 2nd Column alpha z if z less-than 0 2nd Row 1st Column Blank
    2nd Column z if z greater-than-or-equal-to 0 EndLayout"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable
    displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>α</mi> <mi>z</mi> <mtext>if</mtext>
    <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>z</mi>
    <mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泄漏整流线性单元函数（或参数化线性单元）：
- en: 'Its derivative: <math alttext="f prime left-parenthesis z right-parenthesis
    equals StartLayout Enlarged left-brace 1st Row 1st Column Blank 2nd Column alpha
    if z less-than 0 2nd Row 1st Column Blank 2nd Column u n d e f i n e d if z equals
    0 3rd Row 1st Column Blank 2nd Column 1 if z greater-than 0 EndLayout"><mrow><msup><mi>f</mi>
    <mo>''</mo></msup> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced
    close="" open="{" separators=""><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>α</mi>
    <mtext>if</mtext> <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>u</mi> <mi>n</mi> <mi>d</mi> <mi>e</mi> <mi>f</mi>
    <mi>i</mi> <mi>n</mi> <mi>e</mi> <mi>d</mi> <mtext>if</mtext> <mi>z</mi> <mo>=</mo>
    <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>1</mn> <mtext>if</mtext>
    <mi>z</mi> <mo>></mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其导数：
- en: 'Exponential Linear Unit function: <math alttext="f left-parenthesis z right-parenthesis
    equals StartLayout Enlarged left-brace 1st Row 1st Column Blank 2nd Column alpha
    left-parenthesis e Superscript z Baseline minus 1 right-parenthesis if z less-than
    0 2nd Row 1st Column Blank 2nd Column z if z greater-than-or-equal-to 0 EndLayout"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced close="" open="{"
    separators=""><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>α</mi>
    <mo>(</mo> <msup><mi>e</mi> <mi>z</mi></msup> <mo>-</mo> <mn>1</mn> <mo>)</mo>
    <mtext>if</mtext> <mi>z</mi> <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>z</mi> <mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数线性单元函数：f（z）= {α（e^z - 1）如果z < 0；z如果z ≥ 0}。
- en: 'Its derivative: <math alttext="f prime left-parenthesis z right-parenthesis
    equals StartLayout Enlarged left-brace 1st Row 1st Column Blank 2nd Column f left-parenthesis
    z right-parenthesis plus alpha if z less-than 0 2nd Row 1st Column Blank 2nd Column
    1 if z greater-than-or-equal-to 0 EndLayout"><mrow><msup><mi>f</mi> <mo>''</mo></msup>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfenced close="" open="{"
    separators=""><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>f</mi>
    <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>+</mo> <mi>α</mi> <mtext>if</mtext> <mi>z</mi>
    <mo><</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>1</mn>
    <mtext>if</mtext> <mi>z</mi> <mo>≥</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 它的导数：f'（z）= {f（z）+ α如果z < 0；1如果z ≥ 0}。
- en: 'Softplus function: <math alttext="f left-parenthesis z right-parenthesis equals
    ln left-parenthesis 1 plus e Superscript z Baseline right-parenthesis"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mo form="prefix">ln</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mi>z</mi></msup> <mo>)</mo></mrow></mrow></math>
    .'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softplus函数：f（z）= ln（1 + e^z）。
- en: 'Its derivative: <math alttext="f prime left-parenthesis z right-parenthesis
    equals StartFraction 1 Over 1 plus e Superscript negative z Baseline EndFraction
    equals sigma left-parenthesis z right-parenthesis"><mrow><msup><mi>f</mi> <mo>''</mo></msup>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 它的导数：f'（z）= 1 /（1 + e^-z）= σ（z）。
- en: 'Note that all of these activations are rather elementary functions. This is
    a good thing, since both they and their derivatives are usually involved in massive
    computations with thousands of parameters (weights) and data instances during
    training, testing, and deployment of neural networks, so better keep them elementary.
    The other reason is that in theory, is doesn’t really matter what activation function
    we end up choosing, because of the *universal function approximation theorems*,
    discussed next. Careful here: Operationally, it definitely matters what activation
    function we choose for our neural network nodes. As we mentioned earlier in this
    section, AlexNet’s success in image classification tasks is partly due to its
    use of the Rectified Linear Unit function ReLU(z). Theory and practice do not
    contradict each other in this case, even though it seems so on the surface. We
    explain this in the next subsection.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有这些激活函数都是相当基本的函数。这是一件好事，因为它们及其导数通常参与在训练、测试和部署神经网络过程中涉及数千个参数（权重）和数据实例的大规模计算，因此最好保持它们基本。另一个原因是，在理论上，我们最终选择的激活函数并不真的重要，因为接下来将讨论的*通用函数逼近定理*。在操作上，我们选择神经网络节点的激活函数确实很重要。正如我们在本节前面提到的，AlexNet在图像分类任务中取得成功部分归功于其使用的修正线性单元函数ReLU（z）。在这种情况下，理论和实践并不矛盾，尽管表面上看起来是这样。我们将在下一小节中解释这一点。
- en: Universal Function Approximation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用函数逼近
- en: Approximation theorems, when available, are awesome, because they tell us, with
    mathematical confidence and authority, that if we have a function that we do not
    know, or that we know but is difficult to include in our computations, then we
    do not have to deal with this unknown or difficult function altogether. We can,
    instead, approximate it using known functions that are much easier to compute,
    to a great degree of precision. This means that under certain conditions on both
    the unknown or complicated function, and the known and simple (sometimes elemetary)
    functions, we can use the simple functions and be confident that our computations
    are doing the right thing. These types of approximation theorems quantify how
    far off the true function is from its approximation, so we know exactly how much
    error we are committing when substituting the true function with this approximation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 逼近定理一旦可用，就会很棒，因为它们以数学的信心和权威告诉我们，如果我们有一个我们不知道的函数，或者我们知道但难以包含在我们的计算中的函数，那么我们根本不必处理这个未知或困难的函数。相反，我们可以使用更容易计算的已知函数来近似它，以很高的精度。这意味着在未知或复杂函数以及已知和简单（有时是基本的）函数的一定条件下，我们可以使用简单函数，并确信我们的计算是正确的。这些类型的逼近定理量化了真实函数与其近似之间的差距，因此我们确切地知道在用这个近似替代真实函数时我们所犯的错误有多少。
- en: 'The fact that neural networks, even sometimes *nondeep* neural networks with
    only one hidden layer, have proved so successful for accomplishing various tasks
    in vision, speech recognition, classification, regression, and others, means that
    they have some universal approximation property going on for them: The training
    function that a neural network represents (built from elementary linear combinations,
    biases, and very simple activation functions) approximates the underlying unknown
    function that truly represents or generates the data rather well.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，甚至有时只有一个隐藏层的*非深度*神经网络，在视觉、语音识别、分类、回归等各种任务中取得了如此成功的成就，这意味着它们具有某种通用逼近性质：神经网络表示的训练函数（由基本线性组合、偏差和非常简单的激活函数构建）相当好地逼近了真正表示或生成数据的未知函数。
- en: 'The natural questions that mathematicians must now answer with a theorem, or
    a bunch of theorems are:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数学家现在必须用一个定理或一堆定理来回答的自然问题是：
- en: '*Given some function that we don’t know but we really care for (because we
    think it is the true function underlying or generating our data), is there a neural
    network that can approximate it to a good degree of precision (without ever having
    to know this true function)?* Practice using neural networks successfully suggests
    that the answer is yes, and universal approximation theorems for neural networks
    *prove* that the answer is yes, for a certain class of functions and networks.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于我们不知道但非常关心的某个函数（因为我们认为它是潜在或生成我们数据的真实函数），是否有一个神经网络可以以很高的精度近似它（而不必知道这个真实函数）？成功使用神经网络的实践表明答案是肯定的，并且神经网络的通用逼近定理证明了对于某一类函数和网络，答案是肯定的。
- en: '*If there is a neural network that approximates this true and elusive data
    generating function, how do we construct it? How many layers should it have? How
    many nodes in each layer? What type of activation function should it include?*
    In other words, what is the *architecture* of this network? Sadly, as of now,
    little is known on how to construct these networks, and experimentation with various
    architectures and activations is the only way forward, until more mathematicians
    get on this.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*如果有一个神经网络可以逼近这个真实而难以捉摸的数据生成函数，我们如何构建它？它应该有多少层？每层有多少节点？应该包含什么类型的激活函数？*换句话说，这个网络的*架构*是什么？遗憾的是，到目前为止，我们对如何构建这些网络知之甚少，尝试各种架构和激活是唯一的前进之路，直到更多的数学家参与其中。'
- en: '*Are there multiple neural network architecture that work well? Are there some
    that are better than others?* Experiments suggest that the answer is yes, given
    the comparable performance of various architectures on the same tasks and data
    sets.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*有多种神经网络架构表现良好吗？有些比其他的更好吗？*实验证明答案是肯定的，因为各种架构在相同任务和数据集上的可比性表现。'
- en: 'Note that having definite answers for the above questions is very useful. Affirmative
    answer to the first question tells us: Hey, there is no magic here, neural networks
    do approximate a *wide class of functions* rather well! This *wide coverage*,
    or universality, is crucial, because recall that we do not know the underlying
    generating function of the data, but if the approximation theorem covers a wide
    class of functions, our unknown and elusive function might as well be included,
    hence the success of the neural network. Answering the second and third questions
    is even more useful for practical applications, because if we know which architecture
    works best for each task type and data set, then we would be saved from so much
    experimentation, and we’d immediately choose a well performing architecture.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对上述问题有明确答案非常有用。对第一个问题的肯定回答告诉我们：嘿，这里没有魔法，神经网络确实相当好地逼近了*广泛类别的函数*！这种*广泛覆盖*或普适性至关重要，因为请记住，我们不知道数据生成函数，但如果逼近定理涵盖了广泛类别的函数，我们未知和难以捉摸的函数很可能也被包括在内，因此神经网络的成功。回答第二和第三个问题对实际应用更有用，因为如果我们知道哪种架构最适合每种任务类型和数据集，那么我们将免于进行大量实验，并立即选择一个表现良好的架构。
- en: 'Before stating the universal approximation theorems for neural networks and
    discussing their proofs, let’s go over two examples where we had already encountered
    approximation type theorems, even when we were as young as middle school. The
    same principle applies for all examples: We have an unruly quantity that for whatever
    reason is difficult to deal with or is unknown, and we want to approximate it
    using another quantity that is easier to deal with. If we want universal results,
    we need to specify three things:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在陈述神经网络的通用逼近定理并讨论它们的证明之前，让我们回顾两个例子，我们已经在中学时代遇到过逼近类型的定理。所有例子都适用同一原则：我们有一个难以处理或未知的难以处理的量，我们想用另一个更容易处理的量来近似它。如果我们想要通用结果，我们需要指定三件事：
- en: What class or what kind of *space* does the unruly quantity or function belong
    to? Is it the set of real numbers <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    ? The set of irrational numbers? The space of continuous functions on an interval?
    The space of compactly supported functions on <math alttext="double-struck upper
    R"><mi>ℝ</mi></math> ? The space of Lebesgue measurable functions (I did slide
    in some *measure theory* stuff in here, hoping that no one notices or runs away)?
    *Etc.*
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 难以处理的量或函数属于什么类别或什么*空间*？它是实数集<math alttext="双击上标R"><mi>ℝ</mi></math>吗？无理数集？区间上连续函数的空间？紧支持函数在<math
    alttext="双击上标R"><mi>ℝ</mi></math>上的空间？勒贝格可测函数的空间（我在这里插入了一些*测度论*的内容，希望没有人注意到或逃跑）？*等等*
- en: What kind of easier quantities or functions are we using to approximate the
    unruly entities, and how does using these quantities instead of the true function
    benefit us? How do these approximations fair against *other approximations*, if
    there are already some other popular approximations?
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用什么样的更简单的量或函数来近似难以处理的实体，使用这些量而不是真实函数对我们有什么好处？如果已经有一些其他流行的近似方法，这些近似方法与*其他近似方法*相比如何？
- en: '*In what sense* is the approximation happening, meaning that when we say we
    can approximate <math alttext="f Subscript t r u e"><msub><mi>f</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    using <math alttext="f Subscript a p p r o x i m a t e"><msub><mi>f</mi> <mrow><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub></math>
    , how exactly are we measuring the distance between <math alttext="f Subscript
    t r u e"><msub><mi>f</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    and <math alttext="f Subscript a p p r o x i m a t e"><msub><mi>f</mi> <mrow><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub></math>
    ? Recall that in math we can measure sizes of objects, including distances, in
    many ways. So exactly which way are we using for our particular approximations?
    This is where we hear about the Euclidean norm, uniform norm, *supremum* norm,
    <math alttext="upper L squared"><msup><mi>L</mi> <mn>2</mn></msup></math> norm,
    *etc.* What do norms (sizes) have to do with distances? A norm induces a distance.
    This is intuitive: If our space allows us to talk about sizes of objects, then
    it better allow us talk about distances as well. We’ll formalize this later in
    the probability chapter.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 近似发生在什么意义上，也就是说，当我们说我们可以用<math alttext="f Subscript t r u e"><msub><mi>f</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>来近似<math alttext="f
    Subscript a p p r o x i m a t e"><msub><mi>f</mi> <mrow><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub></math>时，我们究竟是如何测量<math
    alttext="f Subscript t r u e"><msub><mi>f</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>和<math
    alttext="f Subscript a p p r o x i m a t e"><msub><mi>f</mi> <mrow><mi>a</mi><mi>p</mi><mi>p</mi><mi>r</mi><mi>o</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow></msub></math>之间的距离？回想一下，在数学中我们可以用许多方式来测量物体的大小，包括距离。那么我们究竟在我们特定的近似中使用哪种方式？这就是我们听说欧几里得范数、均匀范数、*上确界*范数、<math
    alttext="上L平方"><msup><mi>L</mi> <mn>2</mn></msup></math>范数等的地方。范数（大小）与距离有什么关系？范数引出距离。这是直观的：如果我们的空间允许我们谈论物体的大小，那么它最好也允许我们谈论距离。我们将在概率章节中稍后形式化这一点。
- en: 'Example 1: Approximating irrational numbers with rational numbers'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例1：用有理数近似无理数
- en: Any irrational number can be approximated by a rational number, up to any precision
    that we desire. Rational numbers are so well behaved and useful, since they are
    just pairs of whole numbers. Our minds can easily intuit about whole numbers and
    fractions. Irrational numbers are quite the opposite. Have you ever been asked,
    in grade 6, to calculate <math alttext="StartRoot 47 EndRoot equals 6.8556546
    period period period"><mrow><msqrt><mn>47</mn></msqrt> <mo>=</mo> <mn>6</mn> <mo>.</mo>
    <mn>8556546</mn> <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math> , without a calculator,
    and stay at it until you had a definite answer? I have. Pretty mean! Even calculators
    and computers approximate irrational numbers with rationals. But I had to sit
    there thinking I could keep writing digits until I either found a pattern or the
    computation terminated. Of course neither happened, and around 30 digits later,
    I learned that some numbers are just irrational.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何无理数都可以用有理数近似，直到我们想要的任意精度。有理数表现得很好且有用，因为它们只是整数对。我们的思维可以轻松地理解整数和分数。无理数则完全相反。你有没有在六年级被要求计算<math
    alttext="StartRoot 47 EndRoot equals 6.8556546 period period period"><mrow><msqrt><mn>47</mn></msqrt>
    <mo>=</mo> <mn>6</mn> <mo>.</mo> <mn>8556546</mn> <mo>.</mo> <mo>.</mo> <mo>.</mo></mrow></math>，没有计算器，一直计算直到得到一个确定的答案？我有。相当刻薄！即使计算器和计算机也用有理数近似无理数。但我必须坐在那里想着我可以继续写数字，直到我找到一个模式或计算终止。当然，两者都没有发生，大约30位数字后，我才知道有些数字就是无理数。
- en: 'There is more than one way to write a mathematical statement quantifying this
    approximation. They are all equivalent and useful:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以编写数学陈述来量化这种近似。它们都是等效且有用的：
- en: '*The approximating entity can be made arbitrarily close to the true quantity*:
    This is the most intuitive way.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*近似实体可以无限接近真实数量*：这是最直观的方式。'
- en: Given an irrational number *s* and any precision <math alttext="epsilon"><mi>ϵ</mi></math>
    , no matter how small, we can find a rational number *q* within a distance <math
    alttext="epsilon"><mi>ϵ</mi></math> from *s*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个无理数*s*和任意精度<math alttext="epsilon"><mi>ϵ</mi></math>，无论多么小，我们都可以找到一个距离*s*小于<math
    alttext="epsilon"><mi>ϵ</mi></math>的有理数*q*。
- en: <math alttext="dollar-sign StartAbsoluteValue s minus q EndAbsoluteValue less-than
    epsilon period dollar-sign"><mrow><mo>|</mo> <mi>s</mi> <mo>-</mo> <mi>q</mi>
    <mo>|</mo> <mo><</mo> <mi>ϵ</mi> <mo>.</mo></mrow></math>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartAbsoluteValue s minus q EndAbsoluteValue less-than
    epsilon period dollar-sign"><mrow><mo>|</mo> <mi>s</mi> <mo>-</mo> <mi>q</mi>
    <mo>|</mo> <mo><</mo> <mi>ϵ</mi> <mo>.</mo></mrow></math>
- en: This means that rational and irrational numbers live arbitrarily close to each
    other on the real line <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    . This introduces the idea of denseness.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着有理数和无理数在实数线<math alttext="double-struck upper R"><mi>ℝ</mi></math>上彼此之间生活得非常接近。这引入了密度的概念。
- en: '*Densness and closure*: Approximating entities are *dense* in the space where
    the true quantities live.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*密度和闭包*：逼近实体在真实数量所在的空间中是*密集*的。'
- en: This means that if we focus only on the space of approximating members, then
    add in all the limits of all their sequences, we get the whole space of the true
    members. Adding in all the limiting points of a certain space S is called *closing*
    the space, or taking its *closure*, <math alttext="upper S overbar"><mover accent="true"><mi>S</mi>
    <mo>¯</mo></mover></math> . For example, when we add to the open interval <math
    alttext="left-parenthesis a comma b right-parenthesis"><mrow><mo>(</mo> <mi>a</mi>
    <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow></math> its limit points *a* and *b*, we
    get the *closed* interval [a,b]. Thus the *closure* of <math alttext="left-parenthesis
    a comma b right-parenthesis"><mrow><mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>b</mi>
    <mo>)</mo></mrow></math> is [a,b]. We write <math alttext="ModifyingAbove left-parenthesis
    a comma b right-parenthesis With bar equals"><mrow><mover accent="true"><mrow><mo>(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo>)</mo></mrow>
    <mo>¯</mo></mover> <mo>=</mo></mrow></math> [a,b].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们只关注逼近成员的空间，然后加入所有它们序列的所有极限，我们就得到了真实成员的整个空间。在某个空间S中加入所有极限点被称为*闭合*空间，或者取其*闭包*，<math
    alttext="upper S overbar"><mover accent="true"><mi>S</mi> <mo>¯</mo></mover></math>。例如，当我们在开区间<math
    alttext="left-parenthesis a comma b right-parenthesis"><mrow><mo>(</mo> <mi>a</mi>
    <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow></math>中加入其极限点*a*和*b*时，我们得到了*闭合*区间[a,b]。因此，<math
    alttext="left-parenthesis a comma b right-parenthesis"><mrow><mo>(</mo> <mi>a</mi>
    <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow></math>的*闭包*是[a,b]。我们写成<math alttext="ModifyingAbove
    left-parenthesis a comma b right-parenthesis With bar equals"><mrow><mover accent="true"><mrow><mo>(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo>)</mo></mrow>
    <mo>¯</mo></mover> <mo>=</mo></mrow></math>[a,b]。
- en: The set of rational numbers <math alttext="double-struck upper Q"><mi>ℚ</mi></math>
    is *dense* in the real line <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    . In other words, the *closure* of <math alttext="double-struck upper Q"><mi>ℚ</mi></math>
    is <math alttext="double-struck upper R"><mi>ℝ</mi></math> . We write <math alttext="double-struck
    upper Q overbar equals double-struck upper R"><mrow><mover accent="true"><mi>ℚ</mi>
    <mo>¯</mo></mover> <mo>=</mo> <mi>ℝ</mi></mrow></math> .
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有理数集<math alttext="double-struck upper Q"><mi>ℚ</mi></math>在实数线<math alttext="double-struck
    upper R"><mi>ℝ</mi></math>中是*密集*的。换句话说，<math alttext="double-struck upper Q"><mi>ℚ</mi></math>的*闭包*是<math
    alttext="double-struck upper R"><mi>ℝ</mi></math>。我们写成<math alttext="double-struck
    upper Q overbar equals double-struck upper R"><mrow><mover accent="true"><mi>ℚ</mi>
    <mo>¯</mo></mover> <mo>=</mo> <mi>ℝ</mi></mrow></math>。
- en: '*Limits of sequences*: The true quantity is the limit of a sequence of the
    approximating entities.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*序列的极限*：真实数量是逼近实体序列的极限。'
- en: The idea of *adding in* the *limit points* in the previous bullet introduces
    approximation using the terminology of sequences and their limits.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的点中*添加* *极限点*的概念引入了使用序列及其极限的术语进行逼近。
- en: 'In the context of rational numbers approximating irrational numbers, we can
    therefore write: For any irrational number *s*, there is a sequence <math alttext="q
    Subscript n"><msub><mi>q</mi> <mi>n</mi></msub></math> of rational numbers such
    that <math alttext="limit Underscript n right-arrow normal infinity Endscripts
    q Subscript n Baseline equals s"><mrow><msub><mo form="prefix" movablelimits="true">lim</mo>
    <mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></msub> <msub><mi>q</mi> <mi>n</mi></msub>
    <mo>=</mo> <mi>s</mi></mrow></math> . This gives us the chance to write as an
    example one of the favorite definitions of the most famous irrational number:
    *e=2.71828182…*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在有理数逼近无理数的背景下，因此我们可以写成：对于任意无理数*s*，存在一个有理数序列<math alttext="q Subscript n"><msub><mi>q</mi>
    <mi>n</mi></msub></math>，使得<math alttext="limit Underscript n right-arrow normal
    infinity Endscripts q Subscript n Baseline equals s"><mrow><msub><mo form="prefix"
    movablelimits="true">lim</mo> <mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></msub>
    <msub><mi>q</mi> <mi>n</mi></msub> <mo>=</mo> <mi>s</mi></mrow></math>。这使我们有机会举例说明最著名的无理数之一的最喜欢的定义：*e=2.71828182…*
- en: <math alttext="dollar-sign limit Underscript n right-arrow normal infinity Endscripts
    left-parenthesis 1 plus StartFraction 1 Over n EndFraction right-parenthesis Superscript
    n Baseline equals e dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">lim</mo>
    <mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></msub> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn>
    <mi>n</mi></mfrac><mo>)</mo></mrow> <mi>n</mi></msup> <mo>=</mo> <mi>e</mi></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign limit Underscript n right-arrow normal infinity Endscripts
    left-parenthesis 1 plus StartFraction 1 Over n EndFraction right-parenthesis Superscript
    n Baseline equals e dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">lim</mo>
    <mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow></msub> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn>
    <mi>n</mi></mfrac><mo>)</mo></mrow> <mi>n</mi></msup> <mo>=</mo> <mi>e</mi></mrow></math>
- en: That is, the *irrational* number *e* is the limit of the sequence of *rational*
    numbers <math alttext="left-parenthesis 1 plus one-first right-parenthesis Superscript
    1 Baseline comma left-parenthesis 1 plus one-half right-parenthesis squared comma
    left-parenthesis 1 plus one-third right-parenthesis cubed comma ellipsis"><mrow><msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn>
    <mn>1</mn></mfrac><mo>)</mo></mrow> <mn>1</mn></msup> <mo>,</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn>
    <mn>2</mn></mfrac><mo>)</mo></mrow> <mn>2</mn></msup> <mo>,</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn>
    <mn>3</mn></mfrac><mo>)</mo></mrow> <mn>3</mn></msup> <mo>,</mo> <mo>⋯</mo></mrow></math>
    which is equivalent to <math alttext="2 comma 2.25 comma 2.370370 period period
    comma ellipsis"><mrow><mn>2</mn> <mo>,</mo> <mn>2</mn> <mo>.</mo> <mn>25</mn>
    <mo>,</mo> <mn>2</mn> <mo>.</mo> <mn>370370</mn> <mo>.</mo> <mo>.</mo> <mo>,</mo>
    <mo>⋯</mo></mrow></math> .
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，*无理数*e是有理数序列<math alttext="left-parenthesis 1 plus one-first right-parenthesis
    Superscript 1 Baseline comma left-parenthesis 1 plus one-half right-parenthesis
    squared comma left-parenthesis 1 plus one-third right-parenthesis cubed comma
    ellipsis"><mrow><msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn> <mn>1</mn></mfrac><mo>)</mo></mrow>
    <mn>1</mn></msup> <mo>,</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn>
    <mn>2</mn></mfrac><mo>)</mo></mrow> <mn>2</mn></msup> <mo>,</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn>
    <mn>3</mn></mfrac><mo>)</mo></mrow> <mn>3</mn></msup> <mo>,</mo> <mo>⋯</mo></mrow></math>的极限，等价于<math
    alttext="2 comma 2.25 comma 2.370370 period period comma ellipsis"><mrow><mn>2</mn>
    <mo>,</mo> <mn>2</mn> <mo>.</mo> <mn>25</mn> <mo>,</mo> <mn>2</mn> <mo>.</mo>
    <mn>370370</mn> <mo>.</mo> <mo>.</mo> <mo>,</mo> <mo>⋯</mo></mrow></math>。
- en: 'Whether we approximate an irrational number with a rational number using the
    *arbitrarily close* concept, the *denseness and closure* concepts, or the *limits
    of sequences* concept, any distance involved in the mathematical statements is
    measured using the usual Euclidean norm: <math alttext="d left-parenthesis s comma
    q right-parenthesis equals StartAbsoluteValue s minus q EndAbsoluteValue"><mrow><mi>d</mi>
    <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo> <mo>=</mo> <mo>|</mo> <mi>s</mi>
    <mo>-</mo> <mi>q</mi> <mo>|</mo></mrow></math> , which is the normal distance
    between two numbers.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们使用*任意接近*概念、*密集性和闭包*概念还是*序列极限*概念来用有理数逼近无理数，数学陈述中涉及的任何距离都是使用通常的欧几里德范数来衡量的：<math
    alttext="d左括号s，q右括号等于StartAbsoluteValue s减q EndAbsoluteValue"><mrow><mi>d</mi>
    <mo>(</mo> <mi>s</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo> <mo>=</mo> <mo>|</mo> <mi>s</mi>
    <mo>-</mo> <mi>q</mi> <mo>|</mo></mrow></math>，这是两个数之间的正常距离。
- en: 'Note: Closeness Statements Need to Be Accompanied By a Specific Norm'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意：接近性质需要伴随特定的范数。
- en: 'We might wonder: What if we change the norm? Would the approximation property
    still hold? Can we still approximate irrationals using rationals if we measure
    the distance between them using some other definition of distance than the usual
    Eclidean norm? Welcome to *mathematical analysis*. In general, the answer is *no*.
    Quantities can be close to each other using some norm and very far using another
    norm. So in mathematics, when we say that quantities are close to each other,
    approximate others, or converge somewhere, we need to mention the accompanying
    norm, in order to pin point *in what sense these closeness statements are happening*.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想：如果我们改变范数会怎样？逼近性质是否仍然成立？如果我们使用某种与通常的欧几里德范数不同的距离定义来衡量它们之间的距离，我们是否仍然可以用有理数逼近无理数？欢迎来到*数学分析*。一般来说，答案是否定的。使用某种范数可以使量之间彼此接近，而使用另一种范数可以使它们之间相距很远。因此，在数学中，当我们说量之间彼此接近、逼近其他量或收敛到某处时，我们需要提及伴随的范数，以便准确定义*这些接近性质是如何发生*。
- en: 'Example 2: Approximating continuous functions with polynomials'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例2：用多项式逼近连续函数
- en: 'Continuous functions can be anything. A child can draw a wiggly line on a piece
    of paper and that would be a continuous function that no one knows the formula
    of. Polynomials, on the other hand, are a special type of continuous functions
    that are extremely easy to evaluate, differentiate, integrate, explain, and do
    computations with. The only operations involved in polynomial functions are powers,
    scalar multiplication, addition and subtraction. A polynomial of degree *n* has
    a simple formula:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 连续函数可以是任何东西。一个孩子可以在一张纸上画一条曲线，那就是一个没有人知道公式的连续函数。另一方面，多项式是一种特殊类型的连续函数，极易评估、求导、积分、解释和进行计算。多项式函数中唯一涉及的操作是幂、标量乘法、加法和减法。*n*次多项式有一个简单的公式：
- en: <math alttext="dollar-sign p Subscript n Baseline left-parenthesis x right-parenthesis
    equals a 0 plus a 1 x plus a 2 x squared plus a 3 x cubed plus ellipsis plus a
    Subscript n Baseline x Superscript n Baseline comma dollar-sign"><mrow><msub><mi>p</mi>
    <mi>n</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>a</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo>
    <msub><mi>a</mi> <mn>2</mn></msub> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo>
    <msub><mi>a</mi> <mn>3</mn></msub> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo>
    <mo>⋯</mo> <mo>+</mo> <msub><mi>a</mi> <mi>n</mi></msub> <msup><mi>x</mi> <mi>n</mi></msup>
    <mo>,</mo></mrow></math>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p Subscript n Baseline left-parenthesis x right-parenthesis
    equals a 0 plus a 1 x plus a 2 x squared plus a 3 x cubed plus ellipsis plus a
    Subscript n Baseline x Superscript n Baseline comma dollar-sign"><mrow><msub><mi>p</mi>
    <mi>n</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>a</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo>
    <msub><mi>a</mi> <mn>2</mn></msub> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo>
    <msub><mi>a</mi> <mn>3</mn></msub> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo>
    <mo>⋯</mo> <mo>+</mo> <msub><mi>a</mi> <mi>n</mi></msub> <msup><mi>x</mi> <mi>n</mi></msup>
    <mo>,</mo></mrow></math>
- en: where the <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math>
    ’s are scalar numbers. Naturally, it is extremely desirable to be able to approximate
    non-polynomial continuous functions using polynomial functions. The wonderful
    news is that we can, up to any precision <math alttext="epsilon"><mi>ϵ</mi></math>
    . This is a classical result in mathematical analysis, called Weierstrass Approximation
    Theorem.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="a下标i"><msub><mi>a</mi> <mi>i</mi></msub></math>是标量数。自然地，我们非常希望能够用多项式函数逼近非多项式连续函数。令人惊喜的消息是，我们可以，直到任意精度<math
    alttext="epsilon"><mi>ϵ</mi></math>。这是数学分析中的一个经典结果，称为魏尔斯特拉斯逼近定理。
- en: '**Weierstrass Approximation Theorem**: Suppose *f* is a continuous real-valued
    function defined on a real interval [a,b]. For any precision <math alttext="epsilon
    greater-than 0"><mrow><mi>ϵ</mi> <mo>></mo> <mn>0</mn></mrow></math> , there exists
    a polynomial <math alttext="p Subscript n"><msub><mi>p</mi> <mi>n</mi></msub></math>
    such that for all *x* in [a,b], we have <math alttext="StartAbsoluteValue f left-parenthesis
    x right-parenthesis minus p Subscript n Baseline left-parenthesis x right-parenthesis
    EndAbsoluteValue less-than epsilon"><mrow><mrow><mo>|</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo></mrow> <msub><mi>p</mi> <mi>n</mi></msub>
    <mrow><mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>|</mo> <mo><</mo> <mi>ϵ</mi></mrow></mrow></math>
    , or equivalently, the supremum norm <math alttext="parallel-to f minus p Subscript
    n Baseline parallel-to less-than epsilon"><mrow><mrow><mo>∥</mo> <mi>f</mi> <mo>-</mo></mrow>
    <msub><mi>p</mi> <mi>n</mi></msub> <mrow><mo>∥</mo> <mo><</mo> <mi>ϵ</mi></mrow></mrow></math>
    .'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 魏尔斯特拉斯逼近定理：假设*f*是定义在实区间[a，b]上的连续实值函数。对于任意精度<math alttext="epsilon大于0"><mrow><mi>ϵ</mi>
    <mo>></mo> <mn>0</mn></mrow></math>，存在一个多项式<math alttext="p下标n"><msub><mi>p</mi>
    <mi>n</mi></msub></math>，使得对于[a，b]中的所有*x*，我们有<math alttext="StartAbsoluteValue
    f左括号x右括号减p下标n Baseline左括号x右括号EndAbsoluteValue小于epsilon"><mrow><mrow><mo>|</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo></mrow> <msub><mi>p</mi>
    <mi>n</mi></msub> <mrow><mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>|</mo>
    <mo><</mo> <mi>ϵ</mi></mrow></mrow></math>，或者等价地，上确界范数<math alttext="parallel-to
    f减p下标n Baseline parallel-to小于epsilon"><mrow><mrow><mo>∥</mo> <mi>f</mi> <mo>-</mo></mrow>
    <msub><mi>p</mi> <mi>n</mi></msub> <mrow><mo>∥</mo> <mo><</mo> <mi>ϵ</mi></mrow></mrow></math>。
- en: 'Note that the same principle as the one that we disucssed for using rational
    numbers to approximate irrationals applies here: The theorem asserts that we can
    always find polynomials that are *arbitrarily close* to a continuous function;
    which means that the set of polynomials is *dense* in the space of continuous
    functions over the interval [a,b]; or equivalently, for any continuous function
    *f*, we can find a sequence of polynomial functions that converges to *f* (so
    *f* is the *limit of a sequence of polynomials*). In all of these variations of
    the same fact, the distances are measured with respect to the *supremum* norm.
    In [Figure 4-6](#Fig_polyn_approx), we verify that the continuous function <math
    alttext="sine x"><mrow><mo form="prefix">sin</mo> <mi>x</mi></mrow></math> is
    the limit of the sequence of the polynomial functions <math alttext="StartSet
    x comma x minus StartFraction x cubed Over 3 factorial EndFraction comma x minus
    StartFraction x cubed Over 3 factorial EndFraction plus StartFraction x Superscript
    5 Baseline Over 5 factorial EndFraction comma ellipsis EndSet"><mrow><mo>{</mo>
    <mi>x</mi> <mo>,</mo> <mi>x</mi> <mo>-</mo> <mfrac><msup><mi>x</mi> <mn>3</mn></msup>
    <mrow><mn>3</mn><mo>!</mo></mrow></mfrac> <mo>,</mo> <mi>x</mi> <mo>-</mo> <mfrac><msup><mi>x</mi>
    <mn>3</mn></msup> <mrow><mn>3</mn><mo>!</mo></mrow></mfrac> <mo>+</mo> <mfrac><msup><mi>x</mi>
    <mn>5</mn></msup> <mrow><mn>5</mn><mo>!</mo></mrow></mfrac> <mo>,</mo> <mo>⋯</mo>
    <mo>}</mo></mrow></math>'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与我们讨论使用有理数逼近无理数的原理相同：定理断言我们总是可以找到多项式，这些多项式与连续函数*任意接近*；这意味着多项式集合在区间[a，b]上的连续函数空间中是*密集*的；或者等价地，对于任何连续函数*f*，我们可以找到一系列多项式函数，它们收敛于*f*（因此*f*是一系列多项式的*极限*）。在这些相同事实的所有变化中，距离是根据*上确界*范数来衡量的。在[图4-6](#Fig_polyn_approx)中，我们验证连续函数<math
    alttext="sine x"><mrow><mo form="prefix">sin</mo> <mi>x</mi></mrow></math>是多项式函数序列的极限<math
    alttext="StartSet x comma x minus StartFraction x cubed Over 3 factorial EndFraction
    comma x minus StartFraction x cubed Over 3 factorial EndFraction plus StartFraction
    x Superscript 5 Baseline Over 5 factorial EndFraction comma ellipsis EndSet"><mrow><mo>{</mo>
    <mi>x</mi> <mo>,</mo> <mi>x</mi> <mo>-</mo> <mfrac><msup><mi>x</mi> <mn>3</mn></msup>
    <mrow><mn>3</mn><mo>!</mo></mrow></mfrac> <mo>,</mo> <mi>x</mi> <mo>-</mo> <mfrac><msup><mi>x</mi>
    <mn>3</mn></msup> <mrow><mn>3</mn><mo>!</mo></mrow></mfrac> <mo>+</mo> <mfrac><msup><mi>x</mi>
    <mn>5</mn></msup> <mrow><mn>5</mn><mo>!</mo></mrow></mfrac> <mo>,</mo> <mo>⋯</mo>
    <mo>}</mo></mrow></math>
- en: '![250](assets/emai_0406.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0406.png)'
- en: Figure 4-6\. Approximation of the continuous function <math alttext="sine x"><mrow><mo
    form="prefix">sin</mo> <mi>x</mi></mrow></math> by a sequence of polynomials.
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6。通过一系列多项式逼近连续函数<math alttext="sine x"><mrow><mo form="prefix">sin</mo> <mi>x</mi></mrow></math>。
- en: Statement of the Universal Approximation Theorem For Neural Networks
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的通用逼近定理陈述
- en: Now that we understand the principles of approximation, let’s state the most
    recent approximation theorems for neural networks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了逼近原理，让我们陈述神经网络的最新逼近定理。
- en: Recall that a neural network is the representation of the training function
    as a computational graph. We want this training function to approximate the unknown
    function that generates the data well. This allows us to use the training function,
    instead of the underlying true function which we do not know, and probably will
    never know, to make predictions. The following approximation theorems assert that
    neural networks can approximate the underlying functions up to any precision.
    When we compare the statements of these theorems to the two examples on irrational
    numbers and continuous functions above, we notice that they are the same kind
    of mathematical statements.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下神经网络是将训练函数表示为计算图。我们希望这个训练函数能够很好地逼近生成数据的未知函数。这使我们能够使用训练函数，而不是我们不知道的基础真实函数，也可能永远不会知道，来进行预测。以下逼近定理断言神经网络可以以任意精度逼近基础函数。当我们将这些定理的陈述与上述无理数和连续函数的两个示例进行比较时，我们注意到它们是相同类型的数学陈述。
- en: 'The following result is due to Hornik, Stinchombe, and White 1989: Let f be
    a continuous function on a compact set K (this is the true but unknown function
    underlying the data) whose outputs are in <math alttext="double-struck upper R
    Superscript d"><msup><mi>ℝ</mi> <mi>d</mi></msup></math> . Then:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果归功于Hornik，Stinchombe和White 1989：设f是紧集K上的连续函数（这是数据背后的真实但未知函数），其输出在<math alttext="double-struck
    upper R Superscript d"><msup><mi>ℝ</mi> <mi>d</mi></msup></math>中。那么：
- en: '*Arbitrarily close*: There exists a feedforward neural network, having only
    a single hidden layer, which uniformly approximates f to within an arbitrary <math
    alttext="epsilon greater-than 0"><mrow><mi>ϵ</mi> <mo>></mo> <mn>0</mn></mrow></math>
    on K.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*任意接近*：存在一个只有单隐藏层的前馈神经网络，可以在K上均匀逼近f，精度在K上任意<math alttext="epsilon greater-than
    0"><mrow><mi>ϵ</mi> <mo>></mo> <mn>0</mn></mrow></math>。'
- en: '*Denseness*: The set of neural networks, with prescribed nonlinear activations
    and bounds on the number of neurons and layers depending on d, is dense in the
    uniform topology of <math alttext="upper C left-parenthesis upper K comma double-struck
    upper R Superscript d Baseline right-parenthesis"><mrow><mi>C</mi> <mo>(</mo>
    <mi>K</mi> <mo>,</mo> <msup><mi>ℝ</mi> <mi>d</mi></msup> <mo>)</mo></mrow></math>
    .'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*密集性*：具有预定非线性激活和神经元数量和层数限制的神经网络集合在<math alttext="upper C left-parenthesis upper
    K comma double-struck upper R Superscript d Baseline right-parenthesis"><mrow><mi>C</mi>
    <mo>(</mo> <mi>K</mi> <mo>,</mo> <msup><mi>ℝ</mi> <mi>d</mi></msup> <mo>)</mo></mrow></math>的均匀拓扑中是密集的。'
- en: In both variations of the same fact, the distances are measured with respect
    to the *supremum* norm on continuous functions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种相同事实的变化中，距离是根据连续函数上的*上确界*范数来衡量的。
- en: 'The proof needs mathematical concepts from *measure theory* and *functional
    analysis*. We will introduce measure theory in [Chapter 11](ch11.xhtml#ch11) on
    *Probability*. For now we only list what is needed for the proof without any details:
    Borel and Radon measures; Hahn Banach Theorem; and Reiz Representation Theorem.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 证明需要来自*测度论*和*泛函分析*的数学概念。我们将在[第11章](ch11.xhtml#ch11)中介绍测度论，即*概率*。目前我们只列出证明所需的内容，没有任何细节：Borel和Radon测度；Hahn
    Banach定理；以及Reiz表示定理。
- en: Approximation Theory For Deep Learning
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的逼近理论
- en: After motivating approximation theory and stating one of its main results for
    deep learning, we point to the state of the art results such as [the ability of
    neural networks to learn probability distributions](https://arxiv.org/pdf/1702.07028.pdf),
    Barron’s theorem, the neural tangent kernel, [and others](https://arxiv.org/pdf/1901.02220.pdf).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在激励逼近理论并陈述其在深度学习中的一个主要结果后，我们指向了诸如[神经网络学习概率分布的能力](https://arxiv.org/pdf/1702.07028.pdf)、Barron定理、神经切线核、[以及其他内容](https://arxiv.org/pdf/1901.02220.pdf)等最新成果。
- en: Loss Functions
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'Even though in this chapter we transitioned from [Chapter 3](ch03.xhtml#ch03)’s
    traditional machine learning to era of deep learning, the structure of training
    function, loss function, and optimization is still exactly the same. The loss
    functions used for neural networks are not different than those discussed in [Chapter 3](ch03.xhtml#ch03),
    since the goal of a loss function has not changed: To capture the error between
    the ground truth and prediction made by the training function. In deep learning,
    the neural network represents the training function, and for feed forward neural
    networks, we saw that this is nothing more than a sequence of linear combinations
    followed by compositions with nonlinear activation functions.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在本章中我们从[第3章](ch03.xhtml#ch03)的传统机器学习过渡到了深度学习时代，但训练函数、损失函数和优化的结构仍然完全相同。用于神经网络的损失函数与[第3章](ch03.xhtml#ch03)中讨论的并无不同，因为损失函数的目标并未改变：捕捉地面真相与训练函数所做预测之间的误差。在深度学习中，神经网络代表训练函数，对于前馈神经网络，我们看到这只不过是一系列线性组合后跟随非线性激活函数的组合。
- en: The most popular loss functions used in deep learning are still the mean squared
    error for regression tasks and the cross entropy function for classification tasks.
    Go back to [Chapter 3](ch03.xhtml#ch03) for a thorough explanation of these functions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中使用最广泛的损失函数仍然是用于回归任务的均方误差和用于分类任务的交叉熵函数。回到[第3章](ch03.xhtml#ch03)详细解释这些函数。
- en: There are other loss functions that we sometimes come across in the field. When
    we encounter a new loss function, usually the designers of the model have a certain
    reason to prefer it over the other more popular ones, so make sure you go through
    their rationale for using that specific loss function for their particular set
    up. Ideally, a good loss function penalizes bad predictions, is not expensive
    to compute, and has one derivative that is easy to compute. We need this derivative
    to exist so that our optimization method behaves well. As we discussed in [Chapter 3](ch03.xhtml#ch03),
    functions with one good derivative have smoother terrains than functions with
    discontinuous derivatives, and hence easier to navigate during the optimization
    process, when searching for minimizers of the loss function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在领域中我们有时会遇到其他损失函数。当我们遇到一个新的损失函数时，通常模型的设计者会有某种理由更喜欢它而不是其他更流行的损失函数，所以确保你仔细阅读他们使用特定损失函数的理由。理想情况下，一个好的损失函数会惩罚糟糕的预测，计算成本不高，并且有一个易于计算的导数。我们需要这个导数存在，以便我们的优化方法表现良好。正如我们在[第3章](ch03.xhtml#ch03)中讨论的，具有一个良好导数的函数比具有不连续导数的函数具有更平滑的地形，因此在搜索损失函数的最小值时，在优化过程中更容易导航。
- en: 'Note: Minimizing the cross entropy loss function is the same as maximizing
    the log likelihood function. KL divergence for probability distributions is closely
    related.'
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意：最小化交叉熵损失函数等同于最大化对数似然函数。概率分布的KL散度与之密切相关。
- en: 'Recall that the cross entropy function is borrowed from information theory
    and statistical mechanics, and it quantifies the cross entropy between the true
    (empirical) distribution of the data and the distribution (of predictions) produced
    by the neural network’s training function. The cross entropy function has a negative
    sign and a <math alttext="log"><mo form="prefix">log</mo></math> function in its
    formula. Minimizing the minus of a function is the same as maximizing the same
    function without the minus sign, so sometimes you encounter the following statement
    in the field: *Maximizing the log likelihood function*, which for us is equivalent
    to *minimizing the cross entropy loss function*. A closely related concept is
    the *Kullback-Leibler divergence, also called KL divergence*. Sometimes, as in
    the cases where we generate images or machine audio, we need to learn a probability
    distribution, not a deterministic function. Our loss function in this case should
    capture the *difference* (I will not say a distance since its mathematical formula
    is not a distance metric) between the true probability distribution of the data
    and the learned probability distribution. KL divergence is an example of such
    a loss function, which quantifies the amount of information lost when the learned
    distribution is used to approximate the true distribution, or the relative entropy
    of the true distribution with respect to the learned distribution.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，交叉熵函数是从信息论和统计力学中借来的，它量化了数据的真实（经验）分布与神经网络训练函数产生的分布（预测）之间的交叉熵。交叉熵函数的公式中有一个负号和一个<math
    alttext="log"><mo form="prefix">log</mo></math>函数。最小化一个函数的负号等同于最大化没有负号的相同函数，因此有时你会在领域中遇到以下陈述：*最大化对数似然函数*，对我们来说等同于*最小化交叉熵损失函数*。一个密切相关的概念是*Kullback-Leibler散度，也称为KL散度*。有时，例如在生成图像或机器音频的情况下，我们需要学习一个概率分布，而不是一个确定性函数。在这种情况下，我们的损失函数应该捕捉数据的真实概率分布与学习到的概率分布之间的*差异*（我不会说距离，因为它的数学公式不是一个距离度量）。KL散度就是这样一种损失函数的例子，它量化了当学习到的分布用于近似真实分布时丢失的信息量，或者真实分布相对于学习到的分布的相对熵。
- en: Optimization
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: 'Faithful to our *training function*, *loss function* and *optimization* mathematical
    structure, we now discuss the optimization step. Our goal is to perform an efficient
    search of the landscape of the loss function <math alttext="upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math> in
    order to find the minimizing <math alttext="omega"><mi>ω</mi></math> ’s. Note
    that when we explicitly wrote formulas for the training functions of the neural
    network in the early sections, we bundled up the <math alttext="omega"><mi>ω</mi></math>
    weights in matrices *W* and the biases in vectors <math alttext="ModifyingAbove
    omega With right-arrow Subscript 0"><msub><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn></msub></math> . In this section, for the sake of simplifying notation
    and in order to keep the focus on the mathematics, we put all the weights and
    biases in one very long vector <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> . That is, we write the loss
    function as <math alttext="upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> , while in reality, for a fully connected
    neural network with *h* hidden layers, it is:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 忠实于我们的*训练函数*、*损失函数*和*优化*数学结构，我们现在讨论优化步骤。我们的目标是对损失函数<math alttext="upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>的景观进行高效搜索，以找到最小化的<math
    alttext="omega"><mi>ω</mi></math>。请注意，当我们在早期章节明确写出神经网络的训练函数的公式时，我们将<math alttext="omega"><mi>ω</mi></math>权重捆绑在矩阵*W*中，将偏差捆绑在向量<math
    alttext="ModifyingAbove omega With right-arrow Subscript 0"><msub><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn></msub></math>中。在本节中，为了简化符号并保持对数学的关注，我们将所有权重和偏差放在一个非常长的向量<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>中。也就是说，我们将损失函数写为<math alttext="upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>，而实际上，对于具有*h*隐藏层的全连接神经网络，它是：
- en: <math alttext="dollar-sign Loss function equals upper L left-parenthesis upper
    W Superscript 1 Baseline comma ModifyingAbove omega With right-arrow Subscript
    0 Superscript 1 Baseline comma upper W squared comma ModifyingAbove omega With
    right-arrow Subscript 0 Superscript 2 Baseline comma ellipsis comma upper W Superscript
    h plus 1 Baseline comma ModifyingAbove omega With right-arrow Subscript 0 Superscript
    h plus 1 Baseline right-parenthesis period dollar-sign"><mrow><mtext>Loss</mtext>
    <mtext>function</mtext> <mo>=</mo> <mi>L</mi> <mo>(</mo> <msup><mi>W</mi> <mn>1</mn></msup>
    <mo>,</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn>
    <mn>1</mn></msubsup> <mo>,</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>,</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>,</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo> <mo>.</mo></mrow></math>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign Loss function equals upper L left-parenthesis upper
    W Superscript 1 Baseline comma ModifyingAbove omega With right-arrow Subscript
    0 Superscript 1 Baseline comma upper W squared comma ModifyingAbove omega With
    right-arrow Subscript 0 Superscript 2 Baseline comma ellipsis comma upper W Superscript
    h plus 1 Baseline comma ModifyingAbove omega With right-arrow Subscript 0 Superscript
    h plus 1 Baseline right-parenthesis period dollar-sign"><mrow><mtext>Loss</mtext>
    <mtext>function</mtext> <mo>=</mo> <mi>L</mi> <mo>(</mo> <msup><mi>W</mi> <mn>1</mn></msup>
    <mo>,</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn>
    <mn>1</mn></msubsup> <mo>,</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>,</mo>
    <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>,</mo> <msubsup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo> <mo>.</mo></mrow></math>
- en: We only need the above representation when we explicitly compute the derivative
    of the loss function using *backpropagation*, which we postpone till later in
    this chapter.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要上述表示当我们明确使用*反向传播*计算损失函数的导数时，这将在本章后面讨论。
- en: 'For deep learning, the number of <math alttext="omega"><mi>ω</mi></math> ’s
    in the vector <math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math> can be extremely high, as in tens of thousands, millions,
    or even billions: [OpenAI’s GPT-2 for natural language](https://en.wikipedia.org/wiki/GPT-2)
    has 1.5 billion parameters, and was trained on a dataset of 8 million web pages.
    We need to solve for this many unknowns!'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习，向量<math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>中的<math alttext="omega"><mi>ω</mi></math>的数量可能非常高，可以达到数万、百万甚至十亿：[OpenAI的GPT-2自然语言](https://en.wikipedia.org/wiki/GPT-2)拥有15亿个参数，并且是在800万个网页数据集上训练的。我们需要解决这么多未知数！
- en: Using optimization methods, such as Newton-type methods, that require computing
    matrices of second derivatives of the loss function in that many unknowns is simply
    unfeasible even with our current powerful computational abilities. This is a great
    example where the mathematical theory of a numerical method works perfectly fine
    but is impractical for computational and real life implementation. The sad part
    here is that numerical optimization methods that use the second derivative usually
    converge faster than those that use only the first derivative, because they take
    advantage of the extra knowledge about the concavity of the function (the shape
    of its bowl), as opposed to only using the information on whether the function
    is increasing or decreasing that the first derivative provides. Until we invent
    even more powerful computers, we have to satisfy ourselves with first order methods
    that use only one derivative of the loss function with respect to the unknown
    <math alttext="omega"><mi>ω</mi></math> ’s. These are the *gradient-descent-type*
    methods, and luckily, they perform extremely well for many real life AI systems
    that are currently deployed for use in our everyday life, such as Amazon’s Alexa.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用优化方法，如牛顿类型方法，需要计算这么多未知数的损失函数的二阶导数矩阵，即使在我们当前强大的计算能力下也是不可行的。这是一个很好的例子，数值方法的数学理论完全有效，但在计算和实际应用中是不切实际的。这里令人沮丧的是，使用二阶导数的数值优化方法通常比仅使用一阶导数的方法收敛更快，因为它们利用了关于函数凹凸性（其碗的形状）的额外知识，而不仅仅使用一阶导数提供的关于函数增减的信息。在我们发明更强大的计算机之前，我们必须满足于只使用损失函数对未知<math
    alttext="omega"><mi>ω</mi></math>的一阶导数的一阶方法。这些是*梯度下降类型*的方法，幸运的是，它们在许多当前部署在我们日常生活中使用的真实AI系统中表现非常出色，例如亚马逊的Alexa。
- en: Mathematics And The Mysterious Success of Neural Networks
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学与神秘的神经网络成功
- en: 'It is worth pausing here to reflect on the success of neural networks, which
    in this section’s context translates to: Our ability to locate a minimizer for
    the loss function <math alttext="upper L left-parenthesis ModifyingAbove omega
    With right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> that makes the training function generalize
    to new and unseen data really well. I do not have a North American accent and
    Amazon’s Alexa understands me perfectly fine. Mathematically, this success of
    neural networks is still puzzling for various reasons:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 值得在这里停下来反思神经网络的成功，这在本节的背景下可以理解为：我们能够找到使训练函数很好地泛化到新的和未见过的数据的损失函数的最小化器。我没有北美口音，亚马逊的Alexa能够完全理解我。从数学上讲，神经网络的这种成功仍然令人困惑，原因有很多。
- en: The loss function’s <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> -domain <math alttext="upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    where the search for the minimum is happening is very high dimensional (can reach
    billions of dimensions). We have billions or even trillions of options. How are
    we finding the right one?
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数的ω域，即发生最小化的地方，是非常高维的（可以达到数十亿维）。我们有数十亿甚至数万亿的选择。我们如何找到正确的解决方案？
- en: The landscape of the loss function itself is non-convex, so it has a bunch of
    local minima and saddle points where optimization methods can get stuck or converge
    to the wrong local minimum. Again, how are we finding the right one?
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数本身的景观是非凸的，因此有很多局部最小值和鞍点，优化方法可能会陷入困境或收敛到错误的局部最小值。同样，我们如何找到正确的解决方案？
- en: In some AI applications, such as computer vision, there are much more <math
    alttext="omega"><mi>ω</mi></math> ’s than data points (images). Recall that for
    images each pixel is a feature, so that is already a lot of <math alttext="omega"><mi>ω</mi></math>
    ’s only at the input level. For such applications, there are much more unknowns
    (the <math alttext="omega"><mi>ω</mi></math> ’s) than information required to
    determine them (the data points). Mathematically, this is an *under-determined*
    system, and such systems have *infinitely many possible solutions*! So exaclty
    how is the optimization method for our network picking up on the good solutions?
    The ones that generalize well?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一些人工智能应用中，比如计算机视觉，比数据点（图像）要多得多的ω。回想一下，对于图像，每个像素都是一个特征，因此在输入级别就已经有很多ω。对于这样的应用，未知数（ω）比确定它们所需的信息（数据点）要多得多。从数学上讲，这是一个“欠定”系统，这样的系统有“无限多个可能的解”！那么我们的网络的优化方法究竟是如何选择好的解决方案的呢？哪些能很好地泛化？
- en: 'Some of this mysterious success is attributed to techniques that have become
    a staple during the training process, such as regularization (discussed in a later
    section of this chapter), validation, testing, *etc*. However, deep learning still
    lacks a solid theoretical foundation. This is why a lot of mathematicians have
    recently converged to answer such questions. The National Science Foundation (NSF)
    efforts in this direction, and the quotes (*in italics*) that we copy next from
    its announcements are quite informative and give a great insight on how mathematics
    is interwined with advancing AI:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一些神秘的成功归因于在训练过程中已经成为基本技术的技巧，比如正则化（在本章的后面部分讨论）、验证、测试等。然而，深度学习仍然缺乏坚实的理论基础。这就是为什么很多数学家最近聚集在一起回答这些问题。国家科学基金会（NSF）在这方面的努力，以及我们从其公告中摘录的引用（用斜体表示）非常具有信息性，并深入了解数学如何与推进人工智能相互交织：
- en: '[The NSF has recently established 11 new artificial intelligence research institutes](https://www.nsf.gov/cise/ai.jsp)
    *to advance AI in various fields, such as human-AI interaction and collaboration,
    AI for advances in optimization, AI and advanced cyberinfrastructure, AI in computer
    and network systems, AI in dynamic systems, AI-augmented learning and AI-driven
    innovation in agriculture and the food system. The NSF’s ability to bring together
    numerous fields of scientific inquiry, including computer and information science
    and engineering, along with cognitive science and psychology, economics and game
    theory, engineering and control theory, ethics, linguistics, mathematics and philosophy,
    uniquely positions the agency to lead the nation in expanding the frontiers of
    AI. NSF-funding will help the U.S. capitalize on the full potential of AI to strengthen
    the economy, advance job growth, and bring benefits to society for decades to
    come*.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[NSF最近成立了11个新的人工智能研究所](https://www.nsf.gov/cise/ai.jsp)来推动各个领域的人工智能发展，比如人机交互与协作、优化进展的人工智能、高级网络基础设施的人工智能、计算机和网络系统中的人工智能、动态系统中的人工智能、增强学习和农业和食品系统中的人工智能驱动创新。NSF能够汇集包括计算机和信息科学工程、认知科学和心理学、经济学和博弈论、工程和控制理论、伦理学、语言学、数学和哲学在内的多个科学领域，使该机构在拓展人工智能前沿方面独具优势。NSF的资助将帮助美国充分利用人工智能的全部潜力，以加强经济、促进就业增长，并为未来几十年为社会带来利益。'
- en: 'The following is quoted from NSF’s [mathematical and scientific foundations
    of deep learning SCALE MoDl](https://www.nsf.gov/events/event_summ.jsp?cntn_id=302168):
    *Deep learning has met with impressive empirical success that has fueled fundamental
    scientific discoveries and transformed numerous application domains of artificial
    intelligence. Our incomplete theoretical understanding of the field, however,
    impedes accessibility to deep learning technology by a wider range of participants.
    Confronting our incomplete understanding of the mechanisms underlying the success
    of deep learning should serve to overcome its limitations and expand its applicability.
    The SCALE MoDL program will sponsor new research collaborations consisting of
    mathematicians, statisticians, electrical engineers, and computer scientists.
    Research activities should be focused on explicit topics involving some of the
    most challenging theoretical questions in the general area of Mathematical and
    Scientific Foundations of Deep Learning. Each collaboration should conduct training
    through research involvement of recent doctoral degree recipients, graduate students,
    and/or undergraduate students from across this multi-disciplinary spectrum. A
    wide range of scientific themes on theoretical foundations of deep learning may
    be addressed in these proposals. Likely topics include but are not limited to
    geometric, topological, Bayesian, or game-theoretic formulations, to analysis
    approaches exploiting optimal transport theory, optimization theory, approximation
    theory, information theory, dynamical systems, partial differential equations,
    or mean field theory, to application-inspired viewpoints exploring efficient training
    with small data sets, adversarial learning, and closing the decision-action loop,
    not to mention foundational work on understanding success metrics, privacy safeguards,
    causal inference, and algorithmic fairness*.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容摘自NSF的[深度学习SCALE MoDl的数学和科学基础](https://www.nsf.gov/events/event_summ.jsp?cntn_id=302168)：*深度学习取得了令人印象深刻的经验成功，推动了基础科学发现，并改变了人工智能的许多应用领域。然而，我们对该领域的理论理解尚不完整，这阻碍了更广泛范围的参与者接触深度学习技术。面对我们对深度学习成功机制的不完整理解应该有助于克服其局限性并扩展其适用性。SCALE
    MoDL计划将赞助由数学家、统计学家、电气工程师和计算机科学家组成的新研究合作。研究活动应集中在涉及数学和科学基础领域中一些最具挑战性的理论问题的明确主题上。每个合作应通过涉及最近获得博士学位的研究人员、研究生和/或本科生在跨学科领域的研究中进行培训。这些提案可能涉及深度学习理论基础的各种科学主题。可能的主题包括但不限于几何、拓扑、贝叶斯或博弈论形式，利用最优输运理论、优化理论、逼近理论、信息理论、动力系统、偏微分方程或平均场理论的分析方法，以及应用启发式观点探索小数据集的有效训练、对抗性学习和关闭决策-行动循环，更不用说对成功指标、隐私保障、因果推断和算法公平性的基础性工作*。
- en: Gradient Descent <math alttext="ModifyingAbove omega With right-arrow Superscript
    i plus 1 Baseline equals ModifyingAbove omega With right-arrow Superscript i Baseline
    minus eta normal nabla upper L left-parenthesis ModifyingAbove omega With right-arrow
    Superscript i Baseline right-parenthesis"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup> <mo>=</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup> <mo>-</mo>
    <mi>η</mi> <mi>∇</mi> <mi>L</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></mrow></math>
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降<math alttext="ModifyingAbove omega With right-arrow Superscript i plus
    1 Baseline equals ModifyingAbove omega With right-arrow Superscript i Baseline
    minus eta normal nabla upper L left-parenthesis ModifyingAbove omega With right-arrow
    Superscript i Baseline right-parenthesis"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></row></msup> <mo>=</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup> <mo>-</mo>
    <mi>η</mi> <mi>∇</mi> <mi>L</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></mrow></math>
- en: 'The widely used gradient descent method for optimization in deep learning is
    so simple that we could fit its formula in this subsection’s title. This is how
    gradient descent searches the landscape of the loss function <math alttext="upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    for a local minimum:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中用于优化的梯度下降方法非常简单，以至于我们可以将其公式放在本小节的标题中。这就是梯度下降如何搜索损失函数<math alttext="upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>的局部最小值的方式：
- en: '**Initialize somewhere at <math alttext="ModifyingAbove omega With right-arrow
    Superscript 0"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math>**
    : Randomly pick starting numerical values for <math alttext="ModifyingAbove omega
    With right-arrow Superscript 0 Baseline equals left-parenthesis omega 0 comma
    omega 1 comma ellipsis comma omega Subscript n Baseline right-parenthesis"><mrow><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup> <mo>=</mo> <mrow><mo>(</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>ω</mi> <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math>
    . This choice places us somewhere in the search space and at the landscape of
    <math alttext="upper L left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> . One big warning here: *Where we
    start matters*! Do not initialize with all zeros or all equal numbers. This will
    dimish the network’s ability to learn different features, since different nodes
    will output exactly the same numbers. We will discuss initialization shorlty in
    an upcoming subsection.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在某处初始化 <math alttext="ModifyingAbove omega With right-arrow Superscript 0"><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math>**：随机选择起始数值
    <math alttext="ModifyingAbove omega With right-arrow Superscript 0 Baseline equals
    left-parenthesis omega 0 comma omega 1 comma ellipsis comma omega Subscript n
    Baseline right-parenthesis"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn></msup> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>,</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <mo>)</mo></mrow></mrow></math>。这个选择将我们放置在搜索空间的某处以及
    <math alttext="upper L left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> 的景观中。这里有一个重要的警告：*我们从哪里开始很重要*！不要用全零或全相等的数字初始化。这将减弱网络学习不同特征的能力，因为不同节点将输出完全相同的数字。我们将在即将到来的子节中讨论初始化。'
- en: '**Move to a new point <math alttext="ModifyingAbove omega With right-arrow
    Superscript 1"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>1</mn></msup></math>**
    : The gradient descent moves in the direction opposite to the gradient vector
    of the loss function <math alttext="minus normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow Superscript 0 Baseline right-parenthesis"><mrow><mo>-</mo>
    <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn></msup> <mo>)</mo></mrow></math> . This is guaranteed to decrease the
    gradient if the step size <math alttext="eta"><mi>η</mi></math> , also called
    the *learning rate* is not too large:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动到新点 <math alttext="ModifyingAbove omega With right-arrow Superscript 1"><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>1</mn></msup></math>**：梯度下降沿着损失函数的梯度向量相反的方向移动
    <math alttext="minus normal nabla upper L left-parenthesis ModifyingAbove omega
    With right-arrow Superscript 0 Baseline right-parenthesis"><mrow><mo>-</mo> <mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn></msup> <mo>)</mo></mrow></math>。如果步长 <math alttext="eta"><mi>η</mi></math>，也称为*学习率*不是太大，这将保证梯度下降。'
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow Superscript
    1 Baseline equals ModifyingAbove omega With right-arrow Superscript 0 Baseline
    minus eta normal nabla upper L left-parenthesis ModifyingAbove omega With right-arrow
    Superscript 0 Baseline right-parenthesis dollar-sign"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>1</mn></msup> <mo>=</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn></msup> <mo>-</mo> <mi>η</mi> <mi>∇</mi> <mi>L</mi>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove omega With right-arrow Superscript
    1 Baseline equals ModifyingAbove omega With right-arrow Superscript 0 Baseline
    minus eta normal nabla upper L left-parenthesis ModifyingAbove omega With right-arrow
    Superscript 0 Baseline right-parenthesis dollar-sign"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>1</mn></msup> <mo>=</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn></msup> <mo>-</mo> <mi>η</mi> <mi>∇</mi> <mi>L</mi>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup>
    <mo>)</mo></mrow></mrow></math>
- en: '**Move to a new point <math alttext="ModifyingAbove omega With right-arrow
    squared"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>2</mn></msup></math>**
    : Again, the gradient descent moves in the direction opposite to the gradient
    vector of the loss function <math alttext="minus normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow Superscript 1 Baseline right-parenthesis"><mrow><mo>-</mo>
    <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>)</mo></mrow></math> . This is again guaranteed to decrease
    the gradient if the learning <math alttext="eta"><mi>η</mi></math> is not too
    large:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动到新点 <math alttext="ModifyingAbove omega With right-arrow squared"><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>2</mn></msup></math>**：同样，梯度下降沿着损失函数的梯度向量相反的方向移动
    <math alttext="minus normal nabla upper L left-parenthesis ModifyingAbove omega
    With right-arrow Superscript 1 Baseline right-parenthesis"><mrow><mo>-</mo> <mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>)</mo></mrow></math>。如果学习率 <math alttext="eta"><mi>η</mi></math>
    不是太大，这将再次保证梯度下降。'
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow squared equals
    ModifyingAbove omega With right-arrow Superscript 1 Baseline minus eta normal
    nabla upper L left-parenthesis ModifyingAbove omega With right-arrow Superscript
    1 Baseline right-parenthesis dollar-sign"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>2</mn></msup> <mo>=</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>1</mn></msup> <mo>-</mo> <mi>η</mi> <mi>∇</mi> <mi>L</mi>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>1</mn></msup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove omega With right-arrow squared equals
    ModifyingAbove omega With right-arrow Superscript 1 Baseline minus eta normal
    nabla upper L left-parenthesis ModifyingAbove omega With right-arrow Superscript
    1 Baseline right-parenthesis dollar-sign"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>2</mn></msup> <mo>=</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>1</mn></msup> <mo>-</mo> <mi>η</mi> <mi>∇</mi> <mi>L</mi>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>1</mn></msup>
    <mo>)</mo></mrow></mrow></math>
- en: '**Keep going until the sequence of points <math alttext="StartSet ModifyingAbove
    omega With right-arrow Superscript 0 Baseline comma ModifyingAbove omega With
    right-arrow Superscript 1 Baseline comma ModifyingAbove omega With right-arrow
    squared comma ellipsis EndSet"><mrow><mo>{</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>0</mn></msup> <mo>,</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>1</mn></msup> <mo>,</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mn>2</mn></msup> <mo>,</mo> <mo>⋯</mo> <mo>}</mo></mrow></math>
    converges**.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**继续直到点序列 <math alttext="StartSet ModifyingAbove omega With right-arrow Superscript
    0 Baseline comma ModifyingAbove omega With right-arrow Superscript 1 Baseline
    comma ModifyingAbove omega With right-arrow squared comma ellipsis EndSet"><mrow><mo>{</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup> <mo>,</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>1</mn></msup> <mo>,</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>2</mn></msup> <mo>,</mo>
    <mo>⋯</mo> <mo>}</mo></mrow></math> 收敛**。'
- en: '[Figure 4-7](#Fig_gradient_descent_steps) shows a picture of minimizing a certain
    loss function <math alttext="upper L left-parenthesis omega 1 comma omega 2 right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> using gradient descent. We as humans are limited to the
    three dimensional space that we exist in so we cannot visualize beyond three dimensions.
    This is a severe limitation for us in terms of visualization since our loss functions
    usually act on very high dimensional spaces. They are functions of many <math
    alttext="omega"><mi>ω</mi></math> ’s, but we can only visualize them accurately
    if they depend on at most two <math alttext="omega"><mi>ω</mi></math> ’s. That
    is, we can visualize a loss function <math alttext="upper L left-parenthesis omega
    1 comma omega 2 right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>
    depending on two <math alttext="omega"><mi>ω</mi></math> ’s but not a loss function
    <math alttext="upper L left-parenthesis omega 1 comma omega 2 comma omega 3 right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <mo>)</mo></mrow></math> depending
    on three (or more) <math alttext="omega"><mi>ω</mi></math> ’s. Even with this
    severe limitation on our capacity to visualize loss functions acting on high dimensional
    spaces, [Figure 4-7](#Fig_gradient_descent_steps) gives an accurate picture of
    how the gradient descent method operates in general. In [Figure 4-7](#Fig_gradient_descent_steps),
    the search happens in the two dimensional <math alttext="left-parenthesis omega
    1 comma omega 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math> -plane
    (the flat ground in [Figure 4-7](#Fig_gradient_descent_steps)), and we track the
    progress on the landscape of the function <math alttext="upper L left-parenthesis
    omega 1 comma omega 2 right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>
    that is embedded in <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math> . The search space always has one dimension less than
    the dimension of the space in which the landscape of the loss function is embedded.
    This makes the optimization process harder, since we are looking for a minimizer
    of a busy landscape in a flattened or squooshed version of its terrain (the ground
    level in [Figure 4-7](#Fig_gradient_descent_steps)).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图4-7显示了使用梯度下降最小化某个损失函数L(ω1,ω2)的图片。我们作为人类受限于我们存在的三维空间，因此无法可视化超过三维。这对我们来说在可视化方面是一个严重的限制，因为我们的损失函数通常作用在非常高维的空间上。它们是许多ω的函数，但如果它们最多依赖于两个ω，我们只能准确地可视化它们。也就是说，我们可以可视化依赖于两个ω的损失函数L(ω1,ω2)，但不能可视化依赖于三个（或更多）ω的损失函数L(ω1,ω2,ω3)。即使我们在可视化高维空间中的损失函数方面受到严重限制，图4-7给出了梯度下降方法的一般操作的准确图像。在图4-7中，搜索发生在二维(ω1,ω2)-平面（图4-7中的平坦地面），我们跟踪嵌入在ℝ³中的函数L(ω1,ω2)的景观上的进展。搜索空间的维度始终比嵌入损失函数景观的空间的维度少一个。这使得优化过程更加困难，因为我们正在寻找一个繁忙景观的最小化者，而这个景观在其地形的扁平或压缩版本中（图4-7中的地面水平）进行了压缩。
- en: '![250](assets/emai_0407.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0407.png)'
- en: Figure 4-7\. Two gradient descent steps. Note that if we start on the other
    side of the mountain, we wouldn’t converge to the minimum. So when we are searching
    for the minimum of a non-convex function, where we start, or how we initiate the
    <math alttext="omega"><mi>ω</mi></math> ’s, matters a lot.
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7。两个梯度下降步骤。请注意，如果我们从山的另一侧开始，我们将无法收敛到最小值。因此，当我们搜索非凸函数的最小值时，我们从哪里开始，或者如何初始化ω很重要。
- en: Explaining The Role Of The Learning Rate Hyperparameter <math alttext="eta"><mi>η</mi></math>
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释学习率超参数η的作用
- en: At each iteration, the gradient descent method <math alttext="ModifyingAbove
    omega With right-arrow Superscript i plus 1 Baseline equals ModifyingAbove omega
    With right-arrow Superscript i Baseline minus eta normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>=</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>-</mo> <mi>η</mi> <mi>∇</mi> <mi>L</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></mrow></math> moves us
    from the point <math alttext="ModifyingAbove omega With right-arrow Superscript
    i"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup></math>
    in the search space to another point <math alttext="ModifyingAbove omega With
    right-arrow Superscript i plus 1"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></math> . The gradient descent
    adds <math alttext="minus eta normal nabla upper L left-parenthesis ModifyingAbove
    omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mo>-</mo>
    <mi>η</mi> <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></math> to the current <math
    alttext="ModifyingAbove omega With right-arrow Superscript i"><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup></math> in order to obtain <math alttext="ModifyingAbove
    omega With right-arrow Superscript i plus 1"><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></math> .
    The quantity <math alttext="minus eta normal nabla upper L left-parenthesis ModifyingAbove
    omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mo>-</mo>
    <mi>η</mi> <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></math> is made up of scalar
    number <math alttext="eta"><mi>η</mi></math> multiplied by the negative of the
    gradient vector <math alttext="minus normal nabla upper L left-parenthesis ModifyingAbove
    omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mo>-</mo>
    <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math> , which points in the direction of
    the steepest decrease of the loss function from the point <math alttext="ModifyingAbove
    omega With right-arrow Superscript i"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup></math> . Thus, the scaled <math alttext="minus eta normal nabla
    upper L left-parenthesis ModifyingAbove omega With right-arrow Superscript i Baseline
    right-parenthesis"><mrow><mo>-</mo> <mi>η</mi> <mi>∇</mi> <mi>L</mi> <mo>(</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></math>
    tells us how far in the search space we are going to go along the steepest descent
    direction in order to choose the next point <math alttext="ModifyingAbove omega
    With right-arrow Superscript i plus 1"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></math> . In other words, the
    vector <math alttext="minus normal nabla upper L left-parenthesis ModifyingAbove
    omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mo>-</mo>
    <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math> specifies in which direction we will
    move away from our current point, and the scalar number <math alttext="eta"><mi>η</mi></math>
    , called the *learning rate*, controls how far we are going to step along that
    direction. [Figure 4-8](#Fig_different_learning_rate) shows one step of gradient
    descent with two different learning rates <math alttext="eta"><mi>η</mi></math>
    . Too large of a learning rate might overshoot the minimum and cross to the other
    side of the valley. On the other hand, too small of a learning rate takes a while
    to get to the minimum. So the tradeoff is between choosing a large learning rate
    and risking overshooting the minimum, and choosing a small learning rate and increasing
    computational cost and time for convergence.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，梯度下降方法将我们从搜索空间中的点<math alttext="ModifyingAbove omega With right-arrow
    Superscript i"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup></math>移动到另一个点<math
    alttext="ModifyingAbove omega With right-arrow Superscript i plus 1"><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></math>。梯度下降将<math
    alttext="minus eta normal nabla upper L left-parenthesis ModifyingAbove omega
    With right-arrow Superscript i Baseline right-parenthesis"><mrow><mo>-</mo> <mi>η</mi>
    <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math>添加到当前<math alttext="ModifyingAbove omega
    With right-arrow Superscript i"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup></math>，以获得<math alttext="ModifyingAbove omega With right-arrow
    Superscript i plus 1"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></math>。数量<math alttext="minus
    eta normal nabla upper L left-parenthesis ModifyingAbove omega With right-arrow
    Superscript i Baseline right-parenthesis"><mrow><mo>-</mo> <mi>η</mi> <mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math>由标量<math alttext="eta"><mi>η</mi></math>乘以梯度向量的负值组成，该向量指向从点<math
    alttext="ModifyingAbove omega With right-arrow Superscript i"><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup></math>开始的损失函数最陡降方向。因此，缩放的<math alttext="minus
    eta normal nabla upper L left-parenthesis ModifyingAbove omega With right-arrow
    Superscript i Baseline right-parenthesis"><mrow><mo>-</mo> <mi>η</mi> <mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math>告诉我们在搜索空间中沿着最陡降方向要走多远，以选择下一个点<math alttext="ModifyingAbove
    omega With right-arrow Superscript i plus 1"><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msup></math>。换句话说，向量<math
    alttext="minus normal nabla upper L left-parenthesis ModifyingAbove omega With
    right-arrow Superscript i Baseline right-parenthesis"><mrow><mo>-</mo> <mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math>指定我们将远离当前点的方向，而标量<math alttext="eta"><mi>η</mi></math>，称为*学习率*，控制我们将沿着该方向迈出多远。太大的学习率可能会超过最小值并越过山谷的另一侧。另一方面，学习率太小需要一段时间才能到达最小值。因此，权衡之处在于选择较大的学习率并冒着超过最小值的风险，或选择较小的学习率并增加计算成本和收敛时间。
- en: '![250](assets/emai_0408.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0408.png)'
- en: Figure 4-8\. One step gradient descent with two different learning rates. On
    the left, the learning rate is too large so the gradient descent overshoots the
    minimum (the star point) and lands on the other side of the valley. On the right,
    the learning rate is small, however, it will take a while to get to the minimum
    (the star point). Note how the gradient vector at a point is perpendicular to
    the level set at that point.
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8。两种不同学习率的一步梯度下降。左侧，学习率过大，梯度下降超过了最小值（星点），落在山谷的另一侧。右侧，学习率较小，但是需要一段时间才能到达最小值（星点）。请注意，某点处的梯度向量与该点处的等高线垂直。
- en: The learning rate <math alttext="eta"><mi>η</mi></math> is another example of
    a hyperparameter of a machine learning model. It is not one of the weights that
    goes into the formula of the training function. It is a parameter that is intrinsic
    to the algorithm that we employ in order to estimate the weights of the training
    function.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率<mi>η</mi>是机器学习模型的另一个超参数的例子。它不是进入训练函数公式的权重之一。它是一种固有于我们所使用的算法的参数，用于估计训练函数的权重。
- en: The scale of the features affects the performance of the gradient descent
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征的比例影响梯度下降的性能
- en: This is one of the reasons to standarize the features ahead of time. Standarizing
    a feature means subtracting from each data instance the mean and dividing by the
    standard deviation. This forces all the data values to have the same scale, with
    mean zero and standard deviation one, as opposed to having vastly different scales,
    such as a feature measured in the millions and another measured in 0.001\. But
    why does this affect the performance of the gradient descent method? Read on.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提前标准化特征的原因之一。标准化特征意味着从每个数据实例中减去平均值，然后除以标准差。这样可以强制所有数据值具有相同的比例，均值为零，标准差为一，而不是具有迥异的比例，例如一个特征的测量值在百万级，另一个特征的测量值在0.001级。但为什么这会影响梯度下降方法的性能？继续阅读。
- en: Recall that the values of the input features get multiplied by the weights in
    the training function, and the training function in turn enters into the formula
    of the loss function. Very different scales of the input features change the shape
    of the bowl of the loss function, making minimization process harder. [Figure 4-9](#Fig_shapes_of_bowl)
    shows the level sets of the function <math alttext="upper L left-parenthesis omega
    1 comma omega 2 right-parenthesis equals omega 1 squared plus a omega 2 squared"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mi>ω</mi> <mn>1</mn>
    <mn>2</mn></msubsup> <mo>+</mo> <mi>a</mi> <msubsup><mi>ω</mi> <mn>2</mn> <mn>2</mn></msubsup></mrow></math>
    with different values of *a*, mimicking different scales of input features. Note
    how the level sets of loss function become much more narrow and elongated as the
    value of *a* increases. This means that the shape of the bowl of the loss function
    is a long narrow valley.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，输入特征的值会在训练函数中与权重相乘，而训练函数又会进入损失函数的公式中。输入特征的非常不同的比例会改变损失函数的碗的形状，使最小化过程变得更加困难。[图4-9](#Fig_shapes_of_bowl)显示了函数<mrow><mi>L</mi>
    <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mi>ω</mi> <mn>1</mn>
    <mn>2</mn></msubsup> <mo>+</mo> <mi>a</mi> <msubsup><mi>ω</mi> <mn>2</mn> <mn>2</mn></msubsup></mrow>的等高线，模拟不同输入特征的不同比例。请注意，随着*a*的值增加，损失函数的等高线变得更加狭窄和延长。这意味着损失函数的碗的形状是一个长而窄的山谷。
- en: '![250](assets/emai_0409.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0409.png)'
- en: Figure 4-9\. The level sets of the loss function <math alttext="upper L left-parenthesis
    omega 1 comma omega 2 right-parenthesis equals omega 1 squared plus a omega 2
    squared"><mrow><mi>L</mi> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <mi>a</mi> <msubsup><mi>ω</mi> <mn>2</mn>
    <mn>2</mn></msubsup></mrow></math> become much more narrow and elongated as the
    value of *a* increases from 1 to 20 to 40.
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9。损失函数<mrow><mi>L</mi> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <mi>a</mi> <msubsup><mi>ω</mi> <mn>2</mn>
    <mn>2</mn></msubsup></mrow>的等高线随着*a*的值从1增加到20再到40变得更加狭窄和延长。
- en: When the gradient descent method tries to operate in such a narrow valley, its
    points hop from one side of the valley to the other, zig-zagging as it tries to
    locate the minimum, and slowing down the convergence considerably. Imagine zig-zagging
    along all the streets of Rome before arriving at the Vatican, as opposed to taking
    a helicopter straight to the Vatican.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当梯度下降方法试图在这样一个狭窄的山谷中操作时，它的点会从山谷的一侧跳到另一侧，试图找到最小值时会蜿蜒曲折，大大减慢收敛速度。想象一下在抵达梵蒂冈之前在罗马的所有街道上蜿蜒曲折，而不是直接乘直升机到达梵蒂冈。
- en: But why does this zig-zagging behavior happen? One hallmark of the gradient
    vector of a function is that it is perpendicular to the level sets of that function.
    We do this computation in the appendix of this book. So if the valley of the loss
    function is so long and narrow, its level sets almost look like lines that are
    parallel to each other *and* with a large enough step size (learning rate), we
    can literary cross from one side of the valley to the other, since it is so narrow.
    Google *gradient descent zigzag* and you will see many images illustrating this
    behavior.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么会发生这种蜿蜒曲折的行为？函数的梯度向量的一个特点是它与该函数的等高线垂直。我们在本书的附录中进行了这种计算。因此，如果损失函数的山谷如此长而窄，其等高线几乎看起来像是彼此平行的线条，而且具有足够大的步长（学习率），我们可以从山谷的一侧穿越到另一侧，因为它是如此狭窄。搜索*梯度下降蜿蜒曲折*，你会看到许多插图展示这种行为。
- en: One fix against zig-zagging, even with a narrow long valley (assuming we did
    not scale the input feature values ahead of time), is to choose a very small learning
    rate, preventing the gradient descent method to step from one side of the valley
    to the other. That, however, slows down arrival to the minimum in its own way,
    since the method will step only incrementally at each iteration. This way, we
    will eventually arrive to the Vatican from Rome, but at a turtle’s pace.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止在狭窄的长谷底部来回摇摆（假设我们没有提前缩放输入特征值），一个修正方法是选择非常小的学习率，防止梯度下降方法从谷的一侧跨越到另一侧。然而，这会以自己的方式减慢到达最小值的速度，因为方法每次迭代只会逐步前进。这样，我们最终会从罗马到梵蒂冈，但速度很慢。
- en: Near the minima (local and/or global), flat regions, or saddle points of the
    loss function’s landscape, the gradient descent method crawls
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在损失函数景观的最小值（局部和/或全局）、平坦区域或鞍点附近，梯度下降方法爬行。
- en: The gradient descent method updates the current point <math alttext="ModifyingAbove
    omega With right-arrow Superscript i"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup></math> by adding the vector <math alttext="minus eta normal
    nabla upper L left-parenthesis ModifyingAbove omega With right-arrow Superscript
    i Baseline right-parenthesis"><mrow><mo>-</mo> <mi>η</mi> <mi>∇</mi> <mi>L</mi>
    <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>)</mo></mrow></math> . Therefore, the exact length of the step from the point
    <math alttext="ModifyingAbove omega With right-arrow Superscript i"><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup></math> in the direction
    of the negative gradient vector is <math alttext="eta"><mi>η</mi></math> multiplied
    by the length of the gradient vector <math alttext="normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math> . At a mininum, maximum, saddle point,
    or any flat region of the landscape of the loss function, the gradient vector
    is zero, hence, its length is zero as well. This means that near a minimum, maximum,
    saddle point, or any flat region, the step size of the gradient descent method
    becomes very small, and the method slows down significantly. If this happens near
    a minimum, then there is not much worry since this can be used as a stopping criterion,
    unless this minimum is a local minimum very far from the global minimum. If on
    the other hand in happens in a flat region or near a saddle point, then the method
    will get stuck there for a while, and that is undersirable. Some practitioners
    put the learning rate <math alttext="eta"><mi>η</mi></math> on a schedule, changing
    its value during the optimization process. When we look into these, we notice
    that the goals are to avoid crawling, save computational time, and speed up convergence.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降方法通过添加向量<math alttext="minus eta normal nabla upper L left-parenthesis ModifyingAbove
    omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mo>-</mo>
    <mi>η</mi> <mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></math> 更新当前点<math alttext="ModifyingAbove
    omega With right-arrow Superscript i"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup></math>。因此，从点<math alttext="ModifyingAbove omega With right-arrow
    Superscript i"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>i</mi></msup></math>沿着负梯度向量的方向的步长是<math
    alttext="eta"><mi>η</mi></math>乘以梯度向量<math alttext="normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math>的长度。在损失函数的景观中的最小值、最大值、鞍点或任何平坦区域附近，梯度向量为零，因此其长度也为零。这意味着在最小值、最大值、鞍点或任何平坦区域附近，梯度下降方法的步长变得非常小，方法会显著减慢。如果这发生在最小值附近，那么就没有太多担心，因为这可以作为停止准则，除非这个最小值是离全局最小值很远的局部最小值。另一方面，如果发生在平坦区域或鞍点附近，那么方法将会在那里停留一段时间，这是不可取的。一些从业者将学习率<math
    alttext="eta"><mi>η</mi></math>放在一个时间表上，在优化过程中改变其值。当我们研究这些时，我们注意到目标是避免爬行，节省计算时间，并加快收敛速度。
- en: We will discuss stochastic (random) gradient descent later in this chapter.
    Due to the random nature of this method, the points hop around a lot, as opposed
    to following a more consistent route towards the minimum. This works to our advantage
    in situations where we are stuck, such as saddle points or local minima, since
    we might get randomly propelled out of the local minimum or away from the saddle
    point into a part of the landscape with a better route towards the minimum.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面讨论随机梯度下降。由于这种方法的随机性质，点会频繁跳动，而不是沿着更一致的路径朝向最小值前进。这在我们被困的情况下（如鞍点或局部最小值）对我们有利，因为我们可能会被随机地推出局部最小值或远离鞍点，进入一个更好朝向最小值的部分景观。
- en: Convex *vs*. Non-Convex Landscapes
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 凸*vs*非凸景观
- en: We cannot have an optimization chapter without discussing *convexity*. In fact,
    entire mathematical fields are dedicated solely for [*convex optimization*](https://en.wikipedia.org/wiki/Convex_optimization).
    It is equally important to immediately note that optimization for neural networks
    is in general, *non-convex*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能没有讨论*凸性*的优化章节。事实上，整个数学领域专门致力于[*凸优化*](https://en.wikipedia.org/wiki/Convex_optimization)。立即注意到的是，神经网络的优化通常是*非凸*的。
- en: When we use nonconvex activation functions, such as the sigmoid-type functions
    in the first row of [Figure 4-5](#Fig_activation_functions), the landscapes of
    the loss functions involved in the resulting neural networks are not convex. This
    is why we spend a good amount of time talking about getting stuck at local minima,
    flat regions, and saddle points, which we wouldn’t worry about for convex landscapes.
    The contrast between convex and non-convex landscapes is obvious in [Figure 4-10](#Fig_contours_convex),
    which shows a convex loss function and its level sets, and [Figure 4-11](#Fig_contours_nonconvex3)
    and [Figure 4-12](#Fig_contours_nonconvex4), which show non-convex functions and
    their level sets.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用非凸激活函数，例如[图4-5](#Fig_activation_functions)中第一行中的S型函数时，导致的神经网络中涉及的损失函数的景观不是凸的。这就是为什么我们花了大量时间讨论在局部最小值、平坦区域和鞍点处卡住的问题，而对于凸景观，我们不会担心这些。凸和非凸景观之间的对比在[图4-10](#Fig_contours_convex)中是明显的，它展示了一个凸损失函数及其等高线图，以及[图4-11](#Fig_contours_nonconvex3)和[图4-12](#Fig_contours_nonconvex4)，展示了非凸函数及其等高线图。
- en: '![250](assets/emai_0410.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0410.png)'
- en: Figure 4-10\. Plot of three dimensional convex function and its level sets.
    Gradient vectors live in the same space ( <math alttext="double-struck upper R
    squared"><msup><mi>ℝ</mi> <mn>2</mn></msup></math> ) as the level sets, not in
    <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
    .
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10。三维凸函数及其等高线图。梯度向量存在于与等高线图相同的空间（<math alttext="双划线上标R平方"><msup><mi>ℝ</mi>
    <mn>2</mn></msup></math>），而不是<math alttext="双划线上标R立方"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>。
- en: '![250](assets/emai_0411.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0411.png)'
- en: Figure 4-11\. Plots of three dimensional non-convex functions and their level
    sets. Gradient vectors live in the same space ( <math alttext="double-struck upper
    R squared"><msup><mi>ℝ</mi> <mn>2</mn></msup></math> ) as the level sets, not
    in <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
    .
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11。三维非凸函数及其等高线图。梯度向量存在于与等高线图相同的空间（<math alttext="双划线上标R平方"><msup><mi>ℝ</mi>
    <mn>2</mn></msup></math>），而不是<math alttext="双划线上标R立方"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>。
- en: '![250](assets/emai_0412.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0412.png)'
- en: Figure 4-12\. Plots of three dimensional non-convex functions and their level
    sets. Gradient vectors live in the same space ( <math alttext="double-struck upper
    R squared"><msup><mi>ℝ</mi> <mn>2</mn></msup></math> ) as the level sets, not
    in <math alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
    .
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12。三维非凸函数及其等高线图。梯度向量存在于与等高线图相同的空间（<math alttext="双划线上标R平方"><msup><mi>ℝ</mi>
    <mn>2</mn></msup></math>），而不是<math alttext="双划线上标R立方"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>。
- en: When we use convex activation functions throughout the network, such as the
    ReLU-type functions in the second row of [Figure 4-5](#Fig_activation_functions),
    *and* convex loss functions, we can still end up with a *conconvex* optimization
    problem, because the composition of two convex functions is not necessarily convex.
    If the loss function happens to be non-decreasing and convex, then its composition
    with a convex function is convex. The loss functions which are popular for neural
    networks, such as the mean squared error, the cross entropy, and the hinge loss
    are all convex but not non-decreasing.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在整个网络中使用凸激活函数，例如[图4-5](#Fig_activation_functions)中第二行中的ReLU型函数，*和*凸损失函数时，我们仍然可能遇到一个*凸凸*优化问题，因为两个凸函数的组合不一定是凸的。如果损失函数恰好是非递减的和凸的，那么它与凸函数的组合就是凸的。神经网络中流行的损失函数，如均方误差、交叉熵和铰链损失都是凸的，但不是非递减的。
- en: 'It is important to become familiar with central concepts from convex optimization.
    If you do not know where to start, keep in mind that convexity replaces linearity
    when linearity is too simplistic or unavailable, then learn everything about the
    following (which will be tied to AI, deep learning, and reinforcement learning
    in our chapter on Operations Research):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉凸优化的中心概念是很重要的。如果你不知道从哪里开始，记住凸性取代了线性当线性过于简单或不可用时，然后学习关于以下内容的一切（这将与我们在运筹学章节中讨论的人工智能、深度学习和强化学习相关联）：
- en: Max of linear functions is convex
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性函数的最大值是凸的
- en: Max min and min max
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大最小和最小最大
- en: Saddle points
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鞍点
- en: Two player zero sum games
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两人零和博弈
- en: Duality
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对偶
- en: 'Since convex optimization is such a well developed and understood field (at
    least more than the mathematical foundations for neural networks) and neural networks
    still have long ways to go mathematically, it would be nice if we could exploit
    our knowledge about convexity inorder to gain a deeper understanding of neural
    networks. Research in this area is ongoing. For example, in a [recent paper (2020)](https://proceedings.mlr.press/v108/ergen20a.xhtml)
    titled *Convex Geometry of Two-Layer ReLU Networks: Implicit Autoencoding and
    Interpretable Models*, the authors frame the problem of training two layered ReLU
    networks as a convex analytic optimization problem. The following is the abstract
    of the paper:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于凸优化是一个发展成熟且被理解的领域（至少比神经网络的数学基础更为深入），而神经网络在数学上仍有很长的路要走，如果我们能利用我们对凸性的知识来更深入地理解神经网络将是很好的。这一领域的研究正在进行中。例如，在一篇[最近的论文（2020年）](https://proceedings.mlr.press/v108/ergen20a.xhtml)中，题为*两层ReLU网络的凸几何：隐式自编码和可解释模型*，作者将训练两层ReLU网络的问题框架为一个凸解析优化问题。以下是该论文的摘要：
- en: '*We develop a convex analytic framework for ReLU neural networks which elucidates
    the inner workings of hidden neurons and their function space characteristics.
    We show that rectified linear units in neural networks act as convex regularizers,
    where simple solutions are encouraged via extreme points of a certain convex set.
    For one dimensional regression and classification, we prove that finite two-layer
    ReLU networks with norm regularization yield linear spline interpolation. In the
    more general higher dimensional case, we show that the training problem for two-layer
    networks can be cast as a convex optimization problem with infinitely many constraints.
    We then provide a family of convex relaxations to approximate the solution, and
    a cutting-plane algorithm to improve the relaxations. We derive conditions for
    the exactness of the relaxations and provide simple closed form formulas for the
    optimal neural network weights in certain cases. Our results show that the hidden
    neurons of a ReLU network can be interpreted as convex autoencoders of the input
    layer. We also establish a connection to <math alttext="l 0 minus l 1"><mrow><msub><mi>l</mi>
    <mn>0</mn></msub> <mo>-</mo> <msub><mi>l</mi> <mn>1</mn></msub></mrow></math>
    equivalence for neural networks analogous to the minimal cardinality solutions
    in compressed sensing. Extensive experimental results show that the proposed approach
    yields interpretable and accurate models*.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们为ReLU神经网络开发了一个凸解析框架，阐明了隐藏神经元及其函数空间特征的内部工作。我们展示了神经网络中的修正线性单元作为凸正则化器，通过某个凸集的极端点鼓励简单解决方案。对于一维回归和分类，我们证明了带有范数正则化的有限两层ReLU网络产生线性样条插值。在更一般的高维情况下，我们展示了两层网络的训练问题可以被视为一个具有无限多个约束的凸优化问题。然后我们提供了一系列凸松弛来近似解决方案，并提供了一个切割平面算法来改进这些松弛。我们推导了松弛的准确性条件，并在某些情况下为最优神经网络权重提供了简单的闭合形式公式。我们的结果表明，ReLU网络的隐藏神经元可以被解释为输入层的凸自编码器。我们还建立了与压缩感知中最小基数解类似的神经网络的<math
    alttext="l 0 minus l 1"><mrow><msub><mi>l</mi> <mn>0</mn></msub> <mo>-</mo> <msub><mi>l</mi>
    <mn>1</mn></msub></mrow></math>等价性的联系。大量实验结果表明，所提出的方法产生了可解释且准确的模型*。'
- en: Stochastic Gradient Descent
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: 'So far, training a feed forward neural network has progressed as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，前馈神经网络的训练进展如下：
- en: Fix an initial set of weights <math alttext="ModifyingAbove omega With right-arrow
    Superscript 0"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math>
    for the training function.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练函数固定一组初始权重<math alttext="ModifyingAbove omega With right-arrow Superscript
    0"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math>。
- en: Evaluate this training function at *all* the data points in the training subset.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练子集中的所有数据点上评估这个训练函数。
- en: Calculate the individual losses at *all* the data points in the training subset
    by comparing their true labels to the predictions made by the training function.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将它们的真实标签与训练函数所做的预测进行比较，计算训练子集中所有数据点的个体损失。
- en: Do this for *all* the data in the training subset.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练子集中的所有数据执行此操作。
- en: Average *all* these individual losses. This average is the loss function.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有这些个体损失求平均。这个平均值就是损失函数。
- en: Evaluate the gradient of this loss function at this initial set of weights.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这组初始权重处评估这个损失函数的梯度。
- en: Choose the next set of weights according to the steepest descent rule.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据最陡下降规则选择下一组权重。
- en: Repeat until you converge somewhere, or stop after a certain number of iterations
    determined by the performance of the training function on the validation set.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到收敛到某个地方，或者根据验证集上训练函数的性能确定一定数量的迭代次数后停止。
- en: 'The problem with the above process is that when we have a large training subset
    with thousands of points, and a neural network with thousands of weights, it gets
    too expensive to evaluate the training function, the loss function, and the gradient
    of the loss function on *all* the data points in the training subset. The remedy
    is to randomize the process: Randomly choose a very small portion of the training
    subset to evaluate the training function, loss function, and gradient of this
    loss function at each step. This slashes the computational cost dramatically at
    each step.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以上过程的问题在于，当训练子集包含数千个点，神经网络包含数千个权重时，评估训练函数、损失函数以及在训练子集中的所有数据点上评估损失函数的梯度变得太昂贵。解决方法是随机化过程：随机选择训练子集的一个非常小的部分，在每一步评估训练函数、损失函数以及这个损失函数的梯度。这样可以大大减少每一步的计算成本。
- en: Keep repeating this random selection (in principle with replacement but in practice
    without replacement) of small portions of the training subset until you converge
    somewhere, or stop after a certain number of iterations determined by the performance
    of the training function on the validation set. One pass through the whole training
    subset is called *one epoch*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 重复这种随机选择（原则上是有替换的，但实际上是无替换的）训练子集的小部分，直到收敛到某个地方，或者根据验证集上训练函数的性能确定一定数量的迭代次数后停止。一次完整的训练子集称为*一个时代*。
- en: Stochastic gradient descent performs remarkably well, and it has become a staple
    in training neural networks.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降表现出色，已成为训练神经网络的重要方法。
- en: Initializing The Weights <math alttext="ModifyingAbove omega With right-arrow
    Superscript 0"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math>
    For The Optimization Process
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化权重<math alttext="ModifyingAbove omega With right-arrow Superscript 0"><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math>以进行优化过程
- en: We have already established that initializing with all zero weights or all the
    same weights is a really bad idea. The next logical step, and what had been the
    traditional practice (before 2010), would be to choose the weights in the initial
    <math alttext="ModifyingAbove omega With right-arrow Superscript 0"><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math> randomly,
    sampled either from the uniform distribution over small intervals such as [-1,1],
    [0,1], or [-0.3,0.3], or from the Gaussian distribution with a pre-selected mean
    and variance. Even though this has not been studied in depth, it seems from empirical
    evidence that it doesn’t matter whether the initial weights are sampled from the
    uniform distribution or Gaussian distribution, but it does seem that the scale
    of the initial weights matters when it comes to both the progress of the optimization
    process and the ability of the network to generalize well to unseen data. It turns
    out that some choices are better than others in this respect. Currently, the two
    state-of-the-art choices depend on the choice of the activation function, whether
    it is sigmoid-type or ReLu-type.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，使用全零权重或全相同权重进行初始化是一个非常糟糕的主意。下一个合乎逻辑的步骤，也是传统做法（2010年之前）将初始权重随机选择，可以从均匀分布中抽样，例如[-1,1]、[0,1]或[-0.3,0.3]，或者从高斯分布中抽样，具有预先选择的均值和方差。尽管这方面尚未深入研究，但根据经验证据，初始权重是从均匀分布还是高斯分布中抽样似乎并不重要，但初始权重的规模对于优化过程的进展以及网络泛化到未见数据的能力似乎很重要。事实证明，在这方面有一些选择比其他选择更好。目前，两种最先进的选择取决于激活函数的选择，无论是
    Sigmoid 类型还是 ReLu 类型。
- en: Xavier Glorot Initialization
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Xavier Glorot初始化
- en: Here, initial weights are sampled from uniform distribution over the interval
    [ <math alttext="minus StartFraction StartRoot 6 EndRoot Over StartRoot n plus
    m EndRoot EndFraction comma StartFraction StartRoot 6 EndRoot Over StartRoot n
    plus m EndRoot EndFraction"><mrow><mo>-</mo> <mfrac><msqrt><mn>6</mn></msqrt>
    <msqrt><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></msqrt></mfrac> <mo>,</mo>
    <mfrac><msqrt><mn>6</mn></msqrt> <msqrt><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></msqrt></mfrac></mrow></math>
    ], where n is the number of inputs to the node (e.g. number of nodes in the previous
    layer) and m is the number of outputs from the layer (e.g. number of nodes in
    the current layer).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，初始权重是从均匀分布中随机抽样，范围是[ <math alttext="minus StartFraction StartRoot 6 EndRoot
    Over StartRoot n plus m EndRoot EndFraction comma StartFraction StartRoot 6 EndRoot
    Over StartRoot n plus m EndRoot EndFraction"><mrow><mo>-</mo> <mfrac><msqrt><mn>6</mn></msqrt>
    <msqrt><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></msqrt></mfrac> <mo>,</mo>
    <mfrac><msqrt><mn>6</mn></msqrt> <msqrt><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></msqrt></mfrac></mrow></math>
    ]，其中n是节点的输入数量（例如，前一层中的节点数量），m是该层输出的数量（例如，当前层中的节点数量）。
- en: He Initialiation
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: He初始化
- en: Here, the initial weights are sampled from the Gaussian distribution with zero
    mean and variance 2/n, where n is the number of inputs to the node.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，初始权重是从均值为零、方差为2/n的高斯分布中随机抽样，其中n是节点的输入数量。
- en: Regularization Techniques
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化技术
- en: 'Regularization helps us arrive at a good choice for the weights of the training
    function while at the same time avoiding overfitting the data. We want our trained
    function to follow the signal in the data rather than the noise, so it can generalize
    well to unseen data. Here we include four simple yet popular regularization techniques
    that are used while training a neural network: Dropout, early stopping, batch
    normalization, and weight decay (Ridge, Lasso, and elastic net) regularizations.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化帮助我们找到训练函数权重的良好选择，同时避免过拟合数据。我们希望我们训练的函数跟随数据中的信号而不是噪音，这样它可以很好地泛化到未见的数据。在这里，我们包括四种简单但流行的正则化技术，这些技术在训练神经网络时使用：Dropout、提前停止、批量归一化和权重衰减（岭回归、Lasso和弹性网络）正则化。
- en: Dropout
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: Drop some randomly selected neurons from each layer during training. Usually,
    about twenty percent of the input layer’s nodes and about half of each of the
    hidden layers’ nodes are randomly dropped. No nodes from the output layer are
    dropped. Dropout is partially inspired by genetic reproduction, where half a parent’s
    genes are dropped and there is a small random mutation. This has the effect of
    training at once different networks (with different number of nodes at each layer)
    and averaging their results, which typically produces more reliable results.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，从每一层中随机删除一些神经元。通常，会随机删除输入层节点的约百分之二十，以及每个隐藏层节点的约一半。不会删除输出层的节点。Dropout
    在一定程度上受到遗传繁殖的启发，其中一半父母的基因被删除，然后有一个小的随机突变。这样做的效果是同时训练不同的网络（每层节点数量不同）并对它们的结果进行平均，通常会产生更可靠的结果。
- en: 'One way to implement dropout is by introducing a hyperparameter *p* for each
    layer that specifies the probability at which each node in that layer will be
    dropped. Recall the basic operations that take place at each node: Linearly combine
    the outputs of the nodes of the previous layer, then activate. With dropout, each
    of the outputs of the nodes of the previous layer (starting with the input layer),
    is multiplied by a random number *r*, which can be either zero or one with probability
    *p*. Thus when a node’s *r* takes the value zero, that node is essentially dropped
    from the network which now forces the other *retained* nodes to *pick up the slack*
    when adjusting the weights in one gradient descent step. We will explain this
    further in the backpropagation section, and this [link](https://www.tech-quantum.com/implementing-drop-out-regularization-in-neural-networks/)
    provides a step by step route to implement dropout.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 实现dropout的一种方法是为每个层引入一个超参数*p*，该参数指定该层中每个节点被丢弃的概率。回想一下每个节点发生的基本操作：线性组合前一层节点的输出，然后激活。使用dropout，前一层节点的每个输出（从输入层开始）都乘以一个随机数*r*，该随机数可以是零或一，概率为*p*。因此，当一个节点的*r*取零时，该节点实际上从网络中被丢弃，这样在调整权重时，其他*保留*节点就需要在一个梯度下降步骤中*接管*。我们将在反向传播部分进一步解释这一点，这个[链接](https://www.tech-quantum.com/implementing-drop-out-regularization-in-neural-networks/)提供了一个实现dropout的逐步路线。
- en: For a deeper mathematical exploration, the following [paper (2015)](https://arxiv.org/abs/1506.02142)
    connects dropout to Bayesian approximations of model uncertainty.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更深入的数学探索，以下[论文（2015）](https://arxiv.org/abs/1506.02142)将dropout与模型不确定性的贝叶斯近似联系起来。
- en: Early Stopping
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前停止
- en: As we update the weights during training, in particular during gradient descent,
    after each epoch, we evaluate the error made by the training function at the current
    weights on the validation subset of the data. This error should be decreasing
    as the model *learns* the training data, however, after a certain number of epochs,
    this error will start increasing, indicating that the training function has now
    started overfitting the training data and is failing to generalize well to the
    validation data. Once we observe this increase in the model’s prediction over
    the validation subset, we stop training, and go back to the set of weights where
    that error was lowest, right before we started observing the increase.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中更新权重时，特别是在梯度下降期间，每个时代之后，我们评估训练函数在当前权重下对数据的验证子集所产生的错误。这个错误应该随着模型*学习*训练数据而减少，然而，在一定数量的时代之后，这个错误将开始增加，表明训练函数现在开始过度拟合训练数据，并且无法很好地泛化到验证数据。一旦我们观察到模型在验证子集上的预测增加，我们就停止训练，并回到那个错误最低的权重集，就在我们开始观察到增加之前。
- en: Batch Normalization Of Each Layer
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每层的批量归一化
- en: The main idea here is to normalize the inputs to each layer of the network.
    This means that the inputs to each layer will have mean zero and variance one.
    This is usually accomplished by subtracting the mean and dividing by the variance
    for each of the layer’s inputs. We will detail this in a moment. The reason this
    is good to do at each hidden layer is similar to why it is good at the original
    input layer.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要思想是对网络的每一层的输入进行归一化。这意味着每一层的输入将具有零均值和单位方差。通常通过减去每个层输入的均值并除以方差来实现这一点。我们稍后会详细说明这一点。在每个隐藏层这样做的原因与在原始输入层这样做的原因类似。
- en: 'Applying *batch normalization* often eliminates the need for dropout, and allows
    us to be less particular about initialization. It makes the training faster and
    safer from vanishing and exploding gradients. It also has the added advantage
    of regularization. The cost for all of these gains is not too high, as it usually
    involves training only two additional parameters, one for scaling, and one for
    shifting, at each layer. The [paper by Ioffe and Szegedy(2015)](http://proceedings.mlr.press/v37/ioffe15.pdf)
    introduced the method. The abstract of their paper describes the batch normalization
    process and the problems it addresses (the brackets are my own comments):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 应用*批量归一化*通常消除了对dropout的需求，并使我们对初始化不那么挑剔。它使训练更快，更安全，避免了梯度消失和梯度爆炸。它还具有正则化的额外优势。所有这些收益的成本并不太高，因为通常只涉及训练每层的两个额外参数，一个用于缩放，一个用于移位。[Ioffe和Szegedy（2015）的论文](http://proceedings.mlr.press/v37/ioffe15.pdf)介绍了这种方法。他们的论文摘要描述了批量归一化过程以及它解决的问题（括号内是我的评论）：
- en: '*Training Deep Neural Networks is complicated by the fact that the distribution
    of each layer’s inputs changes during training, as the parameters of the previous
    layers change. This slows down the training by requiring lower learning rates
    and careful parameter initialization, and makes it notoriously hard to train models
    with saturating nonlinearities* [such as the sigmoid type activation functions,
    in [Figure 4-5](#Fig_activation_functions), which become almost constant, outputting
    the same value when the input is large in magnitude. This renders the nonlinearity
    useless in the training process, and the network stops learning at subsequent
    layers]. *We refer to this phenomenon* [the change in the distribution of the
    inputs to each layer] *as internal covariate shift, and address the problem by
    normalizing layer inputs. Our method draws its strength from making normalization
    a part of the model architecture and performing the normalization for each training
    mini-batch. Batch Normalization allows us to use much higher learning rates and
    be less careful about initialization, and in some cases eliminates the need for
    Dropout. Applied to a state-of-the-art image classification model, Batch Normalization
    achieves the same accuracy with 14 times fewer training steps, and beats the original
    model by a significant margin. Using an ensemble of batch-normalized networks,
    we improve upon the best published result on ImageNet classification: reaching
    4.82% top-5 test error, exceeding the accuracy of human raters.*'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度神经网络变得复杂，因为在训练过程中，每一层的输入分布会随着前面层的参数变化而改变。这需要更低的学习率和谨慎的参数初始化，从而减慢训练速度，并且使得训练具有饱和非线性的模型变得非常困难（例如在[图4-5](#Fig_activation_functions)中的Sigmoid类型激活函数，当输入的幅度很大时，输出几乎保持不变，这使得非线性在训练过程中变得无用，网络在后续层停止学习）。我们将这种现象（每一层输入分布的变化）称为内部协变量转移，通过对层输入进行归一化来解决这个问题。我们的方法之所以有效，是因为将归一化作为模型架构的一部分，并对每个训练小批量执行归一化。批量归一化使我们能够使用更高的学习率，对初始化要求不那么严格，在某些情况下甚至可以消除Dropout的需要。应用于最先进的图像分类模型时，批量归一化在14倍更少的训练步骤下达到相同的准确率，并且明显超过原始模型。通过使用批量归一化网络的集成，我们改进了ImageNet分类的最佳已发布结果：达到4.82%的前5测试错误率，超过了人类评分者的准确率。
- en: 'Batch normalization is often implemented in the architecture of a network either
    in its own layer before the activation step, or after activation. The process,
    *during training*, usually follows these steps:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化通常在网络的架构中实现，要么在激活步骤之前作为独立层，要么在激活之后。*在训练期间*，通常按照以下步骤进行：
- en: Choose a batch from the training data of size b. Each data point in this has
    feature vector <math alttext="ModifyingAbove x Subscript i Baseline With right-arrow"><mover
    accent="true"><msub><mi>x</mi> <mi>i</mi></msub> <mo>→</mo></mover></math> , so
    the whole batch has feature vectors <math alttext="ModifyingAbove x 1 With right-arrow
    comma ModifyingAbove x 2 With right-arrow comma ellipsis comma ModifyingAbove
    x Subscript b Baseline With right-arrow"><mrow><mover accent="true"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>→</mo></mover> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mover accent="true"><msub><mi>x</mi>
    <mi>b</mi></msub> <mo>→</mo></mover></mrow></math> .
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从大小为b的训练数据中选择一个批次。每个数据点都有特征向量<math alttext="ModifyingAbove x Subscript i Baseline
    With right-arrow"><mover accent="true"><msub><mi>x</mi> <mi>i</mi></msub> <mo>→</mo></mover></math>，因此整个批次具有特征向量<math
    alttext="ModifyingAbove x 1 With right-arrow comma ModifyingAbove x 2 With right-arrow
    comma ellipsis comma ModifyingAbove x Subscript b Baseline With right-arrow"><mrow><mover
    accent="true"><msub><mi>x</mi> <mn>1</mn></msub> <mo>→</mo></mover> <mo>,</mo>
    <mover accent="true"><msub><mi>x</mi> <mn>2</mn></msub> <mo>→</mo></mover> <mo>,</mo>
    <mo>⋯</mo> <mo>,</mo> <mover accent="true"><msub><mi>x</mi> <mi>b</mi></msub>
    <mo>→</mo></mover></mrow></math>。
- en: 'Calculate the vector whose entries are the means of each feature in this particular
    batch: <math alttext="ModifyingAbove mu With right-arrow equals StartFraction
    ModifyingAbove x 1 With right-arrow plus ModifyingAbove x 2 With right-arrow plus
    ellipsis plus ModifyingAbove x Subscript b Baseline With right-arrow Over b EndFraction"><mrow><mover
    accent="true"><mi>μ</mi> <mo>→</mo></mover> <mo>=</mo> <mfrac><mrow><mover accent="true"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>→</mo></mover><mo>+</mo><mover accent="true"><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>→</mo></mover><mo>+</mo><mo>⋯</mo><mo>+</mo><mover accent="true"><msub><mi>x</mi>
    <mi>b</mi></msub> <mo>→</mo></mover></mrow> <mi>b</mi></mfrac></mrow></math> .'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算向量，其条目是该特定批次中每个特征的均值：<math alttext="ModifyingAbove mu With right-arrow equals
    StartFraction ModifyingAbove x 1 With right-arrow plus ModifyingAbove x 2 With
    right-arrow plus ellipsis plus ModifyingAbove x Subscript b Baseline With right-arrow
    Over b EndFraction"><mrow><mover accent="true"><mi>μ</mi> <mo>→</mo></mover> <mo>=</mo>
    <mfrac><mrow><mover accent="true"><msub><mi>x</mi> <mn>1</mn></msub> <mo>→</mo></mover><mo>+</mo><mover
    accent="true"><msub><mi>x</mi> <mn>2</mn></msub> <mo>→</mo></mover><mo>+</mo><mo>⋯</mo><mo>+</mo><mover
    accent="true"><msub><mi>x</mi> <mi>b</mi></msub> <mo>→</mo></mover></mrow> <mi>b</mi></mfrac></mrow></math>。
- en: 'Calculate the variance across the batch: Subtract <math alttext="ModifyingAbove
    mu With right-arrow"><mover accent="true"><mi>μ</mi> <mo>→</mo></mover></math>
    from each <math alttext="ModifyingAbove x 1 With right-arrow comma ModifyingAbove
    x 2 With right-arrow comma ellipsis comma ModifyingAbove x Subscript b Baseline
    With right-arrow"><mrow><mover accent="true"><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mover accent="true"><msub><mi>x</mi>
    <mi>b</mi></msub> <mo>→</mo></mover></mrow></math> , calculate the result’s <math
    alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math> norm, add, and divide
    by *b*.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算批次的方差：从每个<math alttext="ModifyingAbove x 1 With right-arrow comma ModifyingAbove
    x 2 With right-arrow comma ellipsis comma ModifyingAbove x Subscript b Baseline
    With right-arrow"><mrow><mover accent="true"><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mover accent="true"><msub><mi>x</mi>
    <mi>b</mi></msub> <mo>→</mo></mover></mrow></math>中减去<math alttext="ModifyingAbove
    mu With right-arrow"><mover accent="true"><mi>μ</mi> <mo>→</mo></mover></math>，计算结果的<math
    alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>范数，相加，然后除以*b*。
- en: 'Normalize each of <math alttext="ModifyingAbove x 1 With right-arrow comma
    ModifyingAbove x 2 With right-arrow comma ellipsis comma ModifyingAbove x Subscript
    b Baseline With right-arrow"><mrow><mover accent="true"><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mover accent="true"><msub><mi>x</mi>
    <mi>b</mi></msub> <mo>→</mo></mover></mrow></math> by subtracting the mean and
    dividing by the square root of the variance:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去均值并除以方差的平方根来对<math alttext="ModifyingAbove x 1 With right-arrow comma ModifyingAbove
    x 2 With right-arrow comma ellipsis comma ModifyingAbove x Subscript b Baseline
    With right-arrow"><mrow><mover accent="true"><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>→</mo></mover> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mover accent="true"><msub><mi>x</mi>
    <mi>b</mi></msub> <mo>→</mo></mover></mrow></math>进行归一化：
- en: Scale and shift by trainable parameters that can be initialized and learned
    by gradient descent, the same way the weights of the training function are learned.
    This becomes the input to the first hidden layer.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过可由梯度下降初始化和学习的可训练参数进行缩放和移位，与训练函数的权重学习方式相同。这成为第一个隐藏层的输入。
- en: Do the same for the input of each of the subsequent layers.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个后续层的输入执行相同操作。
- en: Repeat for the next batch.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为下一批次重复。
- en: '*During testing and prediction*, there is no batch of data to train on, and
    the parameters at each layer are already learned. The batch normalization step,
    however, is already incorporated into the formula of the training function. During
    training, we were changing these *per batch* of training data. This in turn was
    changing the formula of the loss function slightly per batch. However, the point
    of normalization was partly *not to change the formula of the loss function too
    much*, because that in turn would change the locations of its minima, and that
    would cause us to *forever chase a moving target*. Alright, we fixed that with
    batch normalization during training, and now we want to validate, test, and predict.
    Which mean vector and variance do we then use for a particular data point that
    we are testing/predicting at? Do we use the means and variances of the features
    of the original data set? We have to make such decisions.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*在测试和预测过程中*，没有数据批次可供训练，每一层的参数已经学习。然而，批量归一化步骤已经融入到训练函数的公式中。在训练过程中，我们正在更改这些*每批次*的训练数据。这反过来会稍微改变每批次的损失函数公式。然而，规范化的目的部分是*不要过分改变损失函数的公式*，因为这会改变其最小值的位置，这将导致我们*永追逐一个移动的目标*。好了，我们在训练过程中通过批量归一化解决了这个问题，现在我们想要验证、测试和预测。那么我们在测试/预测时应该使用哪个均值和方差向量？我们应该使用原始数据集的特征的均值和方差吗？我们必须做出这样的决定。'
- en: Control The Size Of The Weights By Penalizing Their Norm
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过惩罚它们的范数来控制权重的大小
- en: 'Another way to regularize the training function, in order to avoid overfitting
    the data, is to introduce a *competing term* into the minimization problem. Instead
    of solving for the set of weights <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that minimizes *only* the loss
    function:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种规范化训练函数的方法，以避免过拟合数据，是在最小化问题中引入一个*竞争项*。而不是仅解决使损失函数最小化的权重集合<math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>：
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    comma dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mi>L</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>,</mo></mrow></math>
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    comma dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mi>L</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>,</mo></mrow></math>
- en: 'we introduce a new term <math alttext="alpha parallel-to ModifyingAbove omega
    With right-arrow parallel-to"><mrow><mrow><mi>α</mi> <mo>∥</mo></mrow> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mrow><mo>∥</mo></mrow></mrow></math>
    and solve for the set of weights <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that minimizes:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入一个新项<math alttext="alpha parallel-to ModifyingAbove omega With right-arrow
    parallel-to"><mrow><mrow><mi>α</mi> <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mo>∥</mo></mrow></mrow></math>并解决使权重集合<math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>最小化的问题：
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    plus alpha parallel-to ModifyingAbove omega With right-arrow parallel-to period
    dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mi>L</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>+</mo>
    <mi>α</mi> <mrow><mo>∥</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>∥</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    plus alpha parallel-to ModifyingAbove omega With right-arrow parallel-to period
    dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mi>L</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>+</mo>
    <mi>α</mi> <mrow><mo>∥</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>∥</mo></mrow> <mo>.</mo></mrow></math>
- en: 'For example, for the mean squared error loss function usually used for regression
    problems, the minimization problem looks like:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于通常用于回归问题的均方误差损失函数，最小化问题如下所示：
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts StartFraction 1 Over m EndFraction sigma-summation Underscript i equals
    1 Overscript m Endscripts StartAbsoluteValue y Subscript p r e d i c t Baseline
    left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis minus
    y Subscript t r u e Baseline EndAbsoluteValue squared plus alpha parallel-to ModifyingAbove
    omega With right-arrow parallel-to dollar-sign"><mrow><msub><mo form="prefix"
    movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mrow><mo>|</mo></mrow> <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>∥</mo></mrow></mrow></math>
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts StartFraction 1 Over m EndFraction sigma-summation Underscript i equals
    1 Overscript m Endscripts StartAbsoluteValue y Subscript p r e d i c t Baseline
    left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis minus
    y Subscript t r u e Baseline EndAbsoluteValue squared plus alpha parallel-to ModifyingAbove
    omega With right-arrow parallel-to dollar-sign"><mrow><msub><mo form="prefix"
    movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mrow><mo>|</mo></mrow> <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>∥</mo></mrow></mrow></math>
- en: 'Recall that so far we have established two ways to solve the above minimization
    problem:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们已经建立了两种解决上述最小化问题的方法：
- en: The minimum happens at points where the derivative (gradient) is equal to zero
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值出现在导数（梯度）为零的点处
- en: 'So the minimizing <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> must satisfy <math alttext="normal
    nabla upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    plus alpha normal nabla left-parenthesis parallel-to ModifyingAbove omega With
    right-arrow parallel-to right-parenthesis equals 0"><mrow><mi>∇</mi> <mi>L</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mi>α</mi> <mi>∇</mi> <mo>(</mo> <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mo>∥</mo> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>
    . Then we solve this equation for <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> if we have the luxury to get
    a *closed form* for the solution. In the case of linear regression (which we can
    think about as an extremely simplified neural network, with only one layer and
    zero nonlinear activation function), we do have this luxury, and for this *regularized*
    case the formula for the minimizing <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> is:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小化<math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>必须满足<math alttext="normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis plus alpha normal nabla
    left-parenthesis parallel-to ModifyingAbove omega With right-arrow parallel-to
    right-parenthesis equals 0"><mrow><mi>∇</mi> <mi>L</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>+</mo> <mrow><mi>α</mi>
    <mi>∇</mi> <mo>(</mo> <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mrow><mo>∥</mo> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>。然后，如果我们有幸获得解的*闭式形式*，我们解这个方程得到<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>。在线性回归的情况下（我们可以将其视为一个极其简化的神经网络，只有一层和零非线性激活函数），我们有这种奢侈，对于这种*正则化*情况，最小化<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>的公式是：
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X plus alpha upper B right-parenthesis Superscript
    negative 1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow
    Subscript t r u e Baseline comma dollar-sign"><mrow><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup>
    <mi>X</mi><mo>+</mo><mi>α</mi><mi>B</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>,</mo></mrow></math>
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X plus alpha upper B right-parenthesis Superscript
    negative 1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow
    Subscript t r u e Baseline comma dollar-sign"><mrow><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup>
    <mi>X</mi><mo>+</mo><mi>α</mi><mi>B</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>,</mo></mrow></math>
- en: 'where the columns of X are the feature columns of the data augmented with a
    vector of ones and B is the identity matrix (if we use ridge regression, discussed
    below). *The closed form solution for the extremely simple linear regression problem
    with regularization helps us appreciate weight decay type regularization and see
    the important role it plays*: Instead of inverting the matrix <math alttext="left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis"><mrow><mo>(</mo> <msup><mi>X</mi>
    <mi>t</mi></msup> <mi>X</mi> <mo>)</mo></mrow></math> in the *unregularized* solution
    and worrying about its ill-conditioning (for example, from highly correlated input
    features) and the resulting instabilities, we invert <math alttext="left-parenthesis
    upper X Superscript t Baseline upper X plus alpha upper B right-parenthesis"><mrow><mo>(</mo>
    <msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi> <mo>+</mo> <mi>α</mi> <mi>B</mi>
    <mo>)</mo></mrow></math> in the *regularized* solution. Adding this <math alttext="alpha
    upper B"><mrow><mi>α</mi> <mi>B</mi></mrow></math> term is equivalent to adding
    a small positive term to the denominator of a scalar number that helps us avoid
    division by zero: Instead of using <math alttext="1 slash x"><mrow><mn>1</mn>
    <mo>/</mo> <mi>x</mi></mrow></math> where *x* runs the risk of being zero, we
    use <math alttext="1 slash left-parenthesis x plus alpha right-parenthesis"><mrow><mn>1</mn>
    <mo>/</mo> <mo>(</mo> <mi>x</mi> <mo>+</mo> <mi>α</mi> <mo>)</mo></mrow></math>
    where <math alttext="alpha"><mi>α</mi></math> is a positive constant. Recall that
    matrix inversion is the analogue of scalar number division.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 其中X的列是数据的特征列，增加了一个包含1的向量，B是单位矩阵（如果我们使用下面讨论的岭回归）。*对于带有正则化的极其简单的线性回归问题的闭式解有助于我们理解权重衰减类型的正则化以及它所起的重要作用*：在*非正则化*解中，我们需要求逆矩阵<math
    alttext="left-parenthesis upper X Superscript t Baseline upper X right-parenthesis"><mrow><mo>(</mo>
    <msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi> <mo>)</mo></mrow></math>并担心其病态性（例如，来自高度相关的输入特征）和由此产生的不稳定性，而在*正则化*解中，我们需要求逆<math
    alttext="left-parenthesis upper X Superscript t Baseline upper X plus alpha upper
    B right-parenthesis"><mrow><mo>(</mo> <msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi>
    <mo>+</mo> <mi>α</mi> <mi>B</mi> <mo>)</mo></mrow></math>。添加这个<math alttext="alpha
    upper B"><mrow><mi>α</mi> <mi>B</mi></mrow></math>项等同于向标量数的分母添加一个小正数项，帮助我们避免除以零：我们使用<math
    alttext="1 slash left-parenthesis x plus alpha right-parenthesis"><mrow><mn>1</mn>
    <mo>/</mo> <mo>(</mo> <mi>x</mi> <mo>+</mo> <mi>α</mi> <mo>)</mo></mrow></math>而不是使用<math
    alttext="1 slash x"><mrow><mn>1</mn> <mo>/</mo> <mi>x</mi></mrow></math>，其中*x*可能为零，我们使用一个正常数<math
    alttext="alpha"><mi>α</mi></math>。请记住，矩阵求逆是标量数除法的类比。
- en: Gradient Descent
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降
- en: We use gradient descent or any of its variations, such as stochastic gradient
    descent, when we do not have the luxury of obtaining closed form solutions for
    the derivative equals zero equation, and when our problem is very large that computing
    second order derivatives is extremely expensive.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们没有幸运地获得导数为零的闭式解，以及当我们的问题非常庞大以至于计算二阶导数非常昂贵时，我们使用梯度下降或其任何变体，如随机梯度下降。
- en: Commonly used weight decay regularizations
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常用的权重衰减正则化
- en: 'There are three popular regularizations that control the size of the weights
    that we are forever searching for in this book:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，有三种流行的正则化方法来控制我们永远在寻找的权重的大小：
- en: '*Ridge regression*: Penalize the <math alttext="l squared"><msup><mi>l</mi>
    <mn>2</mn></msup></math> norm of <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> . In this case, we add the
    term <math alttext="alpha sigma-summation Underscript i equals 1 Overscript n
    Endscripts StartAbsoluteValue omega Subscript i Baseline EndAbsoluteValue squared"><mrow><mi>α</mi>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msup><mrow><mo>|</mo><msub><mi>ω</mi> <mi>i</mi></msub> <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math>
    to the loss function then we minimize.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*岭回归*：对<math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>的<math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>范数进行惩罚。在这种情况下，我们将项<math
    alttext="alpha sigma-summation Underscript i equals 1 Overscript n Endscripts
    StartAbsoluteValue omega Subscript i Baseline EndAbsoluteValue squared"><mrow><mi>α</mi>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msup><mrow><mo>|</mo><msub><mi>ω</mi> <mi>i</mi></msub> <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math>添加到损失函数中，然后我们最小化。'
- en: '*Lasso regression*: Penalize the <math alttext="l Superscript 1"><msup><mi>l</mi>
    <mn>1</mn></msup></math> norm of the <math alttext="omega"><mi>ω</mi></math> ’s.
    In this case, we add the term <math alttext="alpha sigma-summation Underscript
    i equals 1 Overscript n Endscripts StartAbsoluteValue omega Subscript i Baseline
    EndAbsoluteValue"><mrow><mi>α</mi> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>|</mo> <msub><mi>ω</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math>
    to the loss function then we minimize.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*套索回归*：对<math alttext="omega"><mi>ω</mi></math>的<math alttext="l Superscript
    1"><msup><mi>l</mi> <mn>1</mn></msup></math>范数进行惩罚。在这种情况下，我们将项<math alttext="alpha
    sigma-summation Underscript i equals 1 Overscript n Endscripts StartAbsoluteValue
    omega Subscript i Baseline EndAbsoluteValue"><mrow><mi>α</mi> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mrow><mo>|</mo>
    <msub><mi>ω</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math>添加到损失函数中，然后我们最小化。'
- en: '*Elastic net*: This is a middle ground case between Ridge and Lasso regressions.
    We introduce one additional hyper-parameter <math alttext="gamma"><mi>γ</mi></math>
    which can take any value between zero and one, and add a term to the loss function
    that combines both Ridge and Lasso regressions through <math alttext="gamma"><mi>γ</mi></math>
    : <math alttext="gamma alpha sigma-summation Underscript i equals 1 Overscript
    n Endscripts StartAbsoluteValue omega Subscript i Baseline EndAbsoluteValue squared
    plus left-parenthesis 1 minus gamma right-parenthesis alpha sigma-summation Underscript
    i equals 1 Overscript n Endscripts StartAbsoluteValue omega Subscript i Baseline
    EndAbsoluteValue"><mrow><mi>γ</mi> <mi>α</mi> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>|</mo></mrow> <msub><mi>ω</mi> <mi>i</mi></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>γ</mi> <mo>)</mo></mrow> <mi>α</mi> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>|</mo> <msub><mi>ω</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math>
    . When <math alttext="gamma equals 0"><mrow><mi>γ</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    this becomes Lasso regression, when it is equal to one it is Ridge regression,
    and when it is between zero and one it is some sort of a middle ground.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*弹性网络*：这是岭回归和套索回归之间的中间情况。我们引入一个额外的超参数<math alttext="gamma"><mi>γ</mi></math>，它可以取零到一之间的任何值，并通过<math
    alttext="gamma"><mi>γ</mi></math>将岭回归和套索回归结合起来添加到损失函数中的一个项：<math alttext="gamma
    alpha sigma-summation Underscript i equals 1 Overscript n Endscripts StartAbsoluteValue
    omega Subscript i Baseline EndAbsoluteValue squared plus left-parenthesis 1 minus
    gamma right-parenthesis alpha sigma-summation Underscript i equals 1 Overscript
    n Endscripts StartAbsoluteValue omega Subscript i Baseline EndAbsoluteValue"><mrow><mi>γ</mi>
    <mi>α</mi> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <mrow><mo>|</mo></mrow> <msub><mi>ω</mi> <mi>i</mi></msub> <msup><mrow><mo>|</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>γ</mi>
    <mo>)</mo></mrow> <mi>α</mi> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>|</mo> <msub><mi>ω</mi> <mi>i</mi></msub> <mo>|</mo></mrow></mrow></math>。当<math
    alttext="gamma equals 0"><mrow><mi>γ</mi> <mo>=</mo> <mn>0</mn></mrow></math>时，这变成套索回归，当它等于一时，它是岭回归，当它在零和一之间时，它是某种中间地带。'
- en: When do we use plain linear regression, Ridge, Lasso, or Elastic Net?
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用普通线性回归、岭回归、套索回归或弹性网络？
- en: 'If you are already confused and slightly overwhelmed by the multitude of choices
    that are available for building machine learning models, join the club, but do
    not get frustrated. Until the mathematical analysis that tells us exactly which
    choices are better than others and under what circumstances becomes available
    (or catches us with mathematical computation and experimentation), think about
    the enormity of available choices the same way you think about a home renovation:
    We have to choose from many available materials, designs, and architectures to
    produce a final product. This is a home renovation, not a home decoration, so
    our decisions are fateful and more consequential than a mere home decoration.
    They do affect the *quality* and the *function* of our final product, but they
    are choices nevertheless. Rest easy, there is more than one way to skin AI:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经对构建机器学习模型的多种选择感到困惑和有些不知所措，请加入我们，但不要感到沮丧。在数学分析告诉我们哪些选择比其他选择更好，并在什么情况下变得可用（或者通过数学计算和实验追赶我们）之前，想想可用选择的巨大数量，就像考虑家庭装修一样：我们必须从许多可用的材料、设计和架构中进行选择，以生产最终产品。这是一次家庭装修，而不是一次家庭装饰，因此我们的决定是决定性的，比单纯的家庭装饰更有意义。它们确实影响我们最终产品的*质量*和*功能*，但它们仍然是选择。放心，有多种方法可以实现人工智能：
- en: 'Some regularization is always good: Adding a term that controls the sizes of
    the weights and competes with minimizing the loss function is good in general.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些正则化总是有益的：添加一个控制权重大小并与最小化损失函数竞争的项通常是有益的。
- en: Ridge regression is usually a good choice because the <math alttext="l squared"><msup><mi>l</mi>
    <mn>2</mn></msup></math> norm is differentiable. Minimizing this is more stable
    than minimizing the <math alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>
    norm.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 岭回归通常是一个不错的选择，因为<math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>范数是可微的。最小化这个范数比最小化<math
    alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>范数更稳定。
- en: If we decide to go with the <math alttext="l Superscript 1"><msup><mi>l</mi>
    <mn>1</mn></msup></math> norm, even though it is not differentiable at 0, we can
    define its sub-differential or sub-gradient at zero to be zero (f(x)= <math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> is differentiable
    when <math alttext="x not-equals 0"><mrow><mi>x</mi> <mo>≠</mo> <mn>0</mn></mrow></math>
    . It has derivative 1 when x>0 and -1 when x<0).
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们决定选择<math alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>范数，即使在0处不可微，我们可以定义其在零点的次梯度或次梯度为零（f(x)=
    <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi>
    <mo>|</mo></mrow></math>在<math alttext="x not-equals 0"><mrow><mi>x</mi> <mo>≠</mo>
    <mn>0</mn></mrow></math>时是可微的。当x>0时，它的导数为1，当x<0时为-1）。
- en: If we suspect only few features are useful, then it is good to use either Lasso
    or Elastic Net as a data preprocessing step, to kill off the less important features.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们怀疑只有少数特征是有用的，那么最好使用套索或弹性网络作为数据预处理步骤，以消除不太重要的特征。
- en: Elastic Net is usually preferred over Lasso because Lasso might behave badly
    when the number of features is greater than the number of training instances or
    when several features are strongly correlated.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 弹性网络通常优于套索，因为当特征数量大于训练实例数量或多个特征强相关时，套索可能表现不佳。
- en: Penalizing The <math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>
    Norm *vs* Penalizing the <math alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>
    Norm
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 惩罚<math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>范数 *vs*
    惩罚<math alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>范数
- en: 'Our goal is to find <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that solves the minimization:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到解决最小化问题的<math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math>：
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow comma
    ModifyingAbove omega 0 With right-arrow right-parenthesis plus alpha parallel-to
    ModifyingAbove omega With right-arrow parallel-to period dollar-sign"><mrow><msub><mo
    form="prefix" movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mi>L</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>,</mo> <mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>∥</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow comma
    ModifyingAbove omega 0 With right-arrow right-parenthesis plus alpha parallel-to
    ModifyingAbove omega With right-arrow parallel-to period dollar-sign"><mrow><msub><mo
    form="prefix" movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mi>L</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>,</mo> <mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>∥</mo></mrow> <mo>.</mo></mrow></math>
- en: 'The first term wants to decrease the loss <math alttext="upper L left-parenthesis
    ModifyingAbove omega With right-arrow comma ModifyingAbove omega 0 With right-arrow
    right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> . The other term wants to decrease
    the values of the coordinates of <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> , all the way to zeros. The
    type of the norm that we choose for <math alttext="parallel-to ModifyingAbove
    omega With right-arrow parallel-to"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mo>∥</mo></mrow></mrow></math> determines the path <math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math> follows on its way to <math alttext="ModifyingAbove
    0 With right-arrow"><mover accent="true"><mn>0</mn> <mo>→</mo></mover></math>
    . * If we use the <math alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>
    norm, the coordinates of <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> will decrease, however, a lot
    of them might encounter premature death, hitting zero before others. That is,
    the <math alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>
    norm encourages sparsity: When a weight dies, it kills the contribution of the
    associated feature to the training function. The plot on the right in [Figure 4-13](#Fig_l1_vs_l2)
    shows the diamond shaped level sets of <math alttext="parallel-to ModifyingAbove
    omega With right-arrow parallel-to equals StartAbsoluteValue omega 1 EndAbsoluteValue
    plus StartAbsoluteValue omega 2 EndAbsoluteValue"><mrow><mrow><mo>∥</mo></mrow>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msub><mrow><mo>∥</mo></mrow>
    <msup><mi>l</mi> <mn>1</mn></msup></msub> <mrow><mo>=</mo> <mo>|</mo></mrow> <msub><mi>ω</mi>
    <mn>1</mn></msub> <mrow><mo>|</mo> <mo>+</mo> <mo>|</mo></mrow> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></math> in two dimensions (if
    we only had two features), namely, <math alttext="StartAbsoluteValue omega 1 EndAbsoluteValue
    plus StartAbsoluteValue omega 2 EndAbsoluteValue equals c"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>ω</mi> <mn>1</mn></msub> <mrow><mo>|</mo> <mo>+</mo> <mo>|</mo></mrow>
    <msub><mi>ω</mi> <mn>2</mn></msub> <mrow><mo>|</mo> <mo>=</mo> <mi>c</mi></mrow></mrow></math>
    for various values of *c*. If a minimization algorithm follows the path of steepest
    descent, such as the gradient descent, then we must travel in the direction perpendicular
    to the level sets, and as the arrow shows in the plot, <math alttext="omega 2"><msub><mi>ω</mi>
    <mn>2</mn></msub></math> becomes zero pretty fast since going perpendicular to
    the diamond shaped level sets is bound to hit one of the coordinate axes, effectively
    killing the respective feature. <math alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>
    then travels to zero along the horizontal axis.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个术语希望减少损失<math alttext="upper L left-parenthesis ModifyingAbove omega With
    right-arrow comma ModifyingAbove omega 0 With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>,</mo> <mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover> <mo>)</mo></mrow></math>。另一个术语希望减少<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>的坐标值，一直减少到零。我们选择<math alttext="parallel-to ModifyingAbove
    omega With right-arrow parallel-to"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mrow><mo>∥</mo></mrow></mrow></math>的范数类型决定了<math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>到<math
    alttext="ModifyingAbove 0 With right-arrow"><mover accent="true"><mn>0</mn> <mo>→</mo></mover></math>的路径。*如果我们使用<math
    alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>范数，<math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>的坐标将减少，然而，很多坐标可能会过早地变为零，比其他坐标更早。也就是说，<math
    alttext="l Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math>范数鼓励稀疏性：当一个权重消失时，它会消除相关特征对训练函数的贡献。在[图4-13](#Fig_l1_vs_l2)右侧的图中显示了<math
    alttext="parallel-to ModifyingAbove omega With right-arrow parallel-to equals
    StartAbsoluteValue omega 1 EndAbsoluteValue plus StartAbsoluteValue omega 2 EndAbsoluteValue"><mrow><mrow><mo>∥</mo></mrow>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msub><mrow><mo>∥</mo></mrow>
    <msup><mi>l</mi> <mn>1</mn></msup></msub> <mrow><mo>=</mo> <mo>|</mo></mrow> <msub><mi>ω</mi>
    <mn>1</mn></msub> <mrow><mo>|</mo> <mo>+</mo> <mo>|</mo></mrow> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></math>在二维空间中的菱形级别集（如果我们只有两个特征），即，<math
    alttext="StartAbsoluteValue omega 1 EndAbsoluteValue plus StartAbsoluteValue omega
    2 EndAbsoluteValue equals c"><mrow><mrow><mo>|</mo></mrow> <msub><mi>ω</mi> <mn>1</mn></msub>
    <mrow><mo>|</mo> <mo>+</mo> <mo>|</mo></mrow> <msub><mi>ω</mi> <mn>2</mn></msub>
    <mrow><mo>|</mo> <mo>=</mo> <mi>c</mi></mrow></mrow></math>对于不同的*c*值。如果一个最小化算法遵循最陡下降的路径，比如梯度下降，那么我们必须沿着垂直于级别集的方向前进，正如图中箭头所示，<math
    alttext="omega 2"><msub><mi>ω</mi> <mn>2</mn></msub></math>很快变为零，因为沿着垂直于菱形级别集的方向必然会碰到其中一个坐标轴，有效地消除了相应的特征。然后<math
    alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>沿着水平轴前进到零。
- en: '![250](assets/emai_0413.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0413.png)'
- en: Figure 4-13\. The plot on the left shows the circular level sets of the <math
    alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math> norm of <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    , along with the direction the gradient descent follows towards the minimum at
    <math alttext="left-parenthesis 0 comma 0 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> . The plot on the left
    shows the diamond shaped level sets of the <math alttext="l Superscript 1"><msup><mi>l</mi>
    <mn>1</mn></msup></math> norm of <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> , along with the direction
    the gradient descent follows towards the minimum at <math alttext="left-parenthesis
    0 comma 0 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>)</mo></mrow></math> .
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13。左侧的图显示了 <math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>
    范数的圆形级别集，以及梯度下降沿着朝向最小值 <math alttext="left-parenthesis 0 comma 0 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> 的方向。右侧的图显示了 <math alttext="l
    Superscript 1"><msup><mi>l</mi> <mn>1</mn></msup></math> 范数的菱形级别集，以及梯度下降沿着朝向最小值
    <math alttext="left-parenthesis 0 comma 0 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> 的方向。
- en: If we use the <math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>
    norm, the weights’ sizes get smaller without necessarily killing them. The plot
    on the left in [Figure 4-13](#Fig_l1_vs_l2) shows the circular shaped level sets
    of <math alttext="parallel-to ModifyingAbove omega With right-arrow parallel-to
    equals omega 1 squared plus omega 2 squared"><mrow><mrow><mo>∥</mo></mrow> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <msub><mrow><mo>∥</mo></mrow> <msup><mi>l</mi>
    <mn>2</mn></msup></msub> <mo>=</mo> <msubsup><mi>ω</mi> <mn>1</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msubsup><mi>ω</mi> <mn>2</mn> <mn>2</mn></msubsup></mrow></math> in
    two dimensions, namely, <math alttext="omega 1 squared plus omega 2 squared equals
    c"><mrow><msubsup><mi>ω</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>=</mo> <mi>c</mi></mrow></math> for various
    values of *c*. We see that following the path perpendicular to the circular level
    sets towards the minimum at <math alttext="left-parenthesis 0 comma 0 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> decreases the values
    of both <math alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math> and
    <math alttext="omega 2"><msub><mi>ω</mi> <mn>2</mn></msub></math> without either
    of them becoming zero before the other.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用 <math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math> 范数，权重的大小会变小，而不会将它们彻底消除。[图4-13](#Fig_l1_vs_l2)
    中左侧的图显示了二维空间中 <math alttext="parallel-to ModifyingAbove omega With right-arrow
    parallel-to equals omega 1 squared plus omega 2 squared"><mrow><mrow><mo>∥</mo></mrow>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msub><mrow><mo>∥</mo></mrow>
    <msup><mi>l</mi> <mn>2</mn></msup></msub> <mo>=</mo> <msubsup><mi>ω</mi> <mn>1</mn>
    <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi> <mn>2</mn> <mn>2</mn></msubsup></mrow></math>
    的圆形级别集，沿着垂直于圆形级别集的路径朝向最小值 <math alttext="left-parenthesis 0 comma 0 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> 减少了 <math alttext="omega
    1"><msub><mi>ω</mi> <mn>1</mn></msub></math> 和 <math alttext="omega 2"><msub><mi>ω</mi>
    <mn>2</mn></sub></math> 的值，而不会使它们中的任何一个在另一个之前变为零。
- en: Which norm to choose depends on our use cases. Note that in all cases, we do
    not regularize the bias weights <math alttext="ModifyingAbove omega 0 With right-arrow"><mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover></math> . This
    is why in this section we wrote them separately in the loss function <math alttext="upper
    L left-parenthesis ModifyingAbove omega With right-arrow comma ModifyingAbove
    omega 0 With right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>→</mo></mover> <mo>)</mo></mrow></math> .
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 选择哪种范数取决于我们的用例。请注意，在所有情况下，我们不对偏置权重 <math alttext="ModifyingAbove omega 0 With
    right-arrow"><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover></math>
    进行正则化。这就是为什么在本节中我们在损失函数中单独写出它们的原因 <math alttext="upper L left-parenthesis ModifyingAbove
    omega With right-arrow comma ModifyingAbove omega 0 With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>,</mo> <mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    。
- en: Explaining The Role Of The Regularization Hyper-parameter <math alttext="alpha"><mi>α</mi></math>
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释正则化超参数 <math alttext="alpha"><mi>α</mi></math> 的作用
- en: 'The minimization problem with weight decay regularization looks like:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 带有权重衰减正则化的最小化问题如下所示：
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    plus alpha parallel-to ModifyingAbove omega With right-arrow parallel-to dollar-sign"><mrow><msub><mo
    form="prefix" movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mi>L</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>∥</mo></mrow></mrow></math>
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    plus alpha parallel-to ModifyingAbove omega With right-arrow parallel-to dollar-sign"><mrow><msub><mo
    form="prefix" movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mi>L</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>+</mo> <mi>α</mi> <mrow><mo>∥</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>∥</mo></mrow></mrow></math>
- en: 'To understand the role of the regularization hyper-parameter <math alttext="alpha"><mi>α</mi></math>
    , we observe the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解正则化超参数 <math alttext="alpha"><mi>α</mi></math> 的作用，我们观察到以下内容：
- en: 'There is a competition between the first term, where the loss function <math
    alttext="upper L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    chooses <math alttext="omega"><mi>ω</mi></math> ’s that fit the training function
    to the training data, and the second term that just cares about making the <math
    alttext="omega"><mi>ω</mi></math> values small. These two objectives are not necessarily
    in sync: The values of <math alttext="omega"><mi>ω</mi></math> ’s that make the
    first term smaller might make the second term bigger and vice-versa.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个术语之间存在竞争，其中损失函数 <math alttext="upper L left-parenthesis ModifyingAbove omega
    With right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> 选择适合训练数据的训练函数的 <math alttext="omega"><mi>ω</mi></math>
    ，而第二个术语只关心使 <math alttext="omega"><mi>ω</mi></math> 的值变小。这两个目标不一定同步：使第一个术语变小的
    <math alttext="omega"><mi>ω</mi></math> 的值可能会使第二个术语变大，反之亦然。
- en: If <math alttext="alpha"><mi>α</mi></math> is big, then the minimization process
    will compensate by making values of <math alttext="omega"><mi>ω</mi></math> ’s
    very small, regardless of whether these small values of <math alttext="omega"><mi>ω</mi></math>
    ’s will make the first term small as well. So the more we increase <math alttext="alpha"><mi>α</mi></math>
    , the more important minimizing the second term becomes than the first term, so
    our ultimate model might end up not fitting the data perfectly (high bias), but
    this is sometimes desired (low variance) so that it generalizes well to unseen
    data.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 <math alttext="alpha"><mi>α</mi></math> 很大，那么最小化过程将通过使 <math alttext="omega"><mi>ω</mi></math>
    的值非常小来进行补偿，无论这些小值的 <math alttext="omega"><mi>ω</mi></math> 是否也会使第一项变小。因此，我们增加
    <math alttext="alpha"><mi>α</mi></math> 的值，第二项的最小化变得比第一项更重要，因此我们最终的模型可能最终不能完全拟合数据（高偏差），但有时这是期望的（低方差），以便它能很好地推广到未知数据。
- en: If on the other hand <math alttext="alpha"><mi>α</mi></math> is small (say close
    to zero), then we can choose larger <math alttext="omega"><mi>ω</mi></math> values,
    and minimizing the first term becomes more important. Here, the minimization process
    will result in <math alttext="omega"><mi>ω</mi></math> values that make the first
    term happy, so the data will fit into the model nicely (low bias) but the variance
    might be high. In this case, our model would work well on seen data (it is designed
    to fit it nicely through minimizing <math alttext="upper L left-parenthesis ModifyingAbove
    omega With right-arrow right-parenthesis"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> ), but might not generalize well to
    unseen data.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，如果 <math alttext="alpha"><mi>α</mi></math> 很小（接近零），那么我们可以选择更大的 <math alttext="omega"><mi>ω</mi></math>
    值，并且最小化第一项变得更加重要。在这种情况下，最小化过程将导致使第一项满意的 <math alttext="omega"><mi>ω</mi></math>
    值，因此数据将很好地适应模型（低偏差），但方差可能很高。在这种情况下，我们的模型在已知数据上表现良好（通过最小化 <math alttext="upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    来设计得很好），但可能不会很好地推广到未知数据。
- en: As <math alttext="alpha right-arrow 0"><mrow><mi>α</mi> <mo>→</mo> <mn>0</mn></mrow></math>
    , we can prove mathematically that the solution of the regularized problem converges
    to the solution of the un-regularized problem.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 <math alttext="alpha right-arrow 0"><mrow><mi>α</mi> <mo>→</mo> <mn>0</mn></mrow></math>
    时，我们可以数学上证明正则化问题的解收敛到未正则化问题的解。
- en: Hyper-parameter Examples That Appear In Machine Learning
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 出现在机器学习中的超参数示例
- en: We have now encountered many hyper-parameters that enter machine learning models.
    It is good practice to list the ones that enter our particular model along with
    their values. Let’s list the ones we have come across and recall that tuning these
    enhances the performance of our models. Most of the time, there are pre-recommended
    values for us to use. These are usually implemented as default values in machine
    learning libraries and software packages. However, it is always good to experiment
    with different values during the validation stage of our modeling process, given
    that we have the available time and resources.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经遇到了许多进入机器学习模型的超参数。列出进入我们特定模型的超参数及其值是一个好习惯。让我们列出我们遇到的超参数，并记住调整这些参数可以提高我们模型的性能。大多数情况下，我们可以使用预先推荐的值。这些通常作为机器学习库和软件包中的默认值实现。然而，在建模过程的验证阶段，尽管需要有可用的时间和资源，尝试不同的值总是一个好主意。
- en: The learning rate in gradient descent.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降中的学习率。
- en: Weight decay coefficients such as the ones that appear in Ridge, Lasso and Elastic
    Net regularizations.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重衰减系数，如 Ridge、Lasso 和 Elastic Net 正则化中出现的系数。
- en: The number of epochs before we stop training.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在停止训练之前的时代数量。
- en: The sizes of data split into training, validation, and testing subsets.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分割成训练、验证和测试子集的大小。
- en: The sizes of mini-batches during stochastic gradient descent and its variants.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降及其变体中的小批量大小。
- en: The acceleration coefficients in momentum methods.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量方法中的加速系数。
- en: 'The architecture of a neural network: number of layers, number of neurons in
    each layer, what happens at each layer (batch normalization, type of activation
    function), type of regularization (dropout, ridge, lasso), type of network (feedforward,
    dense, convolutional, adversarial, recurrent), type of loss functions, *etc.*'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的架构：层数、每层神经元数量、每层发生的情况（批量归一化、激活函数类型）、正则化类型（dropout、ridge、lasso）、网络类型（前馈、稠密、卷积、对抗、循环）、损失函数类型等。
- en: 'Chain Rule And Back-Propagation: Calculating <math alttext="normal nabla upper
    L left-parenthesis ModifyingAbove omega With right-arrow Superscript i Baseline
    right-parenthesis"><mrow><mi>∇</mi> <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>i</mi></msup> <mo>)</mo></mrow></math>'
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链式法则和反向传播：计算 <math alttext="normal nabla upper L left-parenthesis ModifyingAbove
    omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math>
- en: 'It is time to get our hands dirty and compute something important: The gradient
    of the loss function, namely, <math alttext="normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math> . Whether we decide to find our optimal
    weights using gradient descent, stochastic gradient descent, mini-batch gradient
    descent, or any other variant of gradient descent, there is no escape from calculating
    this quantity. Recall that the loss function includes in its formula the neural
    network’s training function, which in turn is made up of subsequent linear combinations
    and compositions with activation functions. This means that we have to use the
    chain rule. Cleverly. Back in Calculus, we only used the single variable chain
    rule for derivatives, but now, we somehow have to transition to a chain rule of
    several variables: Several, as in, sometimes billions.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候动手计算一些重要的东西了：损失函数的梯度，即，<math alttext="normal nabla upper L left-parenthesis
    ModifyingAbove omega With right-arrow Superscript i Baseline right-parenthesis"><mrow><mi>∇</mi>
    <mi>L</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>i</mi></msup> <mo>)</mo></mrow></math> 。无论我们决定使用梯度下降、随机梯度下降、小批量梯度下降或任何其他梯度下降的变体来找到最佳权重，都无法避免计算这个量。请记住，损失函数在其公式中包含了神经网络的训练函数，而训练函数又由后续的线性组合和激活函数组成。这意味着我们必须使用链式法则。聪明地使用。在微积分中，我们只使用了单变量的导数链式法则，但现在，我们不得不过渡到多变量的链式法则：有时是数十亿个变量。
- en: 'It is the layered architecture of a neural network that forces us to pause
    and think: How exaclty are we going to compute this one derivative of the loss
    function. The work horse here is the *back-propagation* algorithm (also called
    *backward mode automatic differentiation*), and it is a powerful one.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的分层架构迫使我们停下来思考：我们究竟如何计算损失函数的导数。这里的工作马是*反向传播*算法（也称为*反向模式自动微分*），它是一个强大的算法。
- en: 'Before writing formulas, let’s summarize the steps that we follow as we train
    a neural network:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在写出公式之前，让我们总结一下训练神经网络时遵循的步骤：
- en: 'The training function is a function of <math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> ,
    so the outcome of the neural network after a data point passes through it, which
    is the same as evaluating the training function at the data point, is: <math alttext="o
    u t c o m e equals f u n c t i o n left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis"><mrow><mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi>
    <mi>o</mi> <mi>m</mi> <mi>e</mi> <mo>=</mo> <mi>f</mi> <mi>u</mi> <mi>n</mi> <mi>c</mi>
    <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> . This is made up of linear combinations
    of node outputs followed by compositions with activation functions, repeated over
    all of the network’s layers. The output layer might or might not have an activation
    function and could have one node or multiple nodes, depending on the ultimate
    task of the network.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练函数是一个关于 <math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math> 的函数，因此数据点通过它后神经网络的结果，也就是在数据点上评估训练函数，是： <math alttext="o
    u t c o m e equals f u n c t i o n left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis"><mrow><mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi>
    <mi>o</mi> <mi>m</mi> <mi>e</mi> <mo>=</mo> <mi>f</mi> <mi>u</mi> <mi>n</mi> <mi>c</mi>
    <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> 。这由节点输出的线性组合和激活函数组成，重复在所有网络层上。输出层可能有或没有激活函数，可能有一个节点或多个节点，取决于网络的最终任务。
- en: The loss function provides a measure of how badly the outcome of the training
    function diverged from what is true.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数提供了训练函数的结果与真实值偏离的程度的度量。
- en: We initialize our learning function with a *random* set of weights <math alttext="ModifyingAbove
    omega With right-arrow Superscript 0"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn></msup></math> , according to preferred initialization rules prescribed
    in the previous sections. Then we compute the loss, or error that we committed
    because of using these particular weight values. *This is the forward pass of
    the data point through the net*.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用一个*随机*的权重集 <math alttext="ModifyingAbove omega With right-arrow Superscript
    0"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>0</mn></msup></math>
    来初始化我们的学习函数，根据前几节中规定的首选初始化规则。然后我们计算损失，或者因为使用这些特定权重值而导致的错误。*这是数据点通过网络的前向传递*。
- en: We want to move to the next set of weights <math alttext="ModifyingAbove omega
    With right-arrow Superscript 1"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>1</mn></msup></math> that gives a lower error. We move in the direction opposite
    to the gradient vector of the loss function.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望移动到下一个给出更低错误的权重集 <math alttext="ModifyingAbove omega With right-arrow Superscript
    1"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>1</mn></msup></math>
    。我们沿着损失函数的梯度向量的相反方向移动。
- en: 'BUT: The training function is built into the loss function, and given the layered
    structure of this function, which comes from the architechture of the neural network,
    along with its high dimensionality, how do we efficiently perform the multivariable
    chain rule in order to find the gradient and evaluate it at the current set of
    weights?'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是：训练函数内置于损失函数中，考虑到神经网络的架构以及其高维度，我们如何有效地执行多变量链式法则，以找到梯度并在当前权重集上评估它呢？
- en: The answer is that we send the data point back through the network, computing
    the gradient *backwards* from the output layer all the way to the input layer,
    evaluating along the way *how each node contributed to the error*. In essence,
    we compute <math alttext="StartFraction normal partial-differential upper L Over
    normal partial-differential node functions EndFraction"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><mtext>node</mtext><mtext>functions</mtext></mrow></mfrac></math>
    , *then we tweak the weights accordingly*, updating them from <math alttext="ModifyingAbove
    omega With right-arrow Superscript 0"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mn>0</mn></msup></math> to <math alttext="ModifyingAbove omega With right-arrow
    Superscript 1"><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mn>1</mn></msup></math>
    . The process continues as we pass more data points into the network, usually
    in batches. One *epoch* is then counted each time the network has *seen* the full
    training set.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是我们将数据点发送回网络，从输出层一直向输入层计算梯度*向后*，在途中评估*每个节点如何导致错误*。实质上，我们计算<math alttext="StartFraction
    normal partial-differential upper L Over normal partial-differential node functions
    EndFraction">∂L/∂node functions，*然后相应地调整权重*，从<math alttext="ModifyingAbove omega
    With right-arrow Superscript 0">ω^0</math>更新到<math alttext="ModifyingAbove omega
    With right-arrow Superscript 1">ω^1</math>。随着我们将更多数据点传递到网络中，这个过程会继续进行，通常是批量进行。每当网络*看到*完整的训练集时，就会计算一个*epoch*。
- en: Back-Propagation Is Not Too Different From How Our Brain Learns
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播与我们大脑学习的方式并没有太大不同
- en: 'When we encounter a new math concept, the neurons in our brain make certain
    connections. The next time we see the same concept, the same neurons connect better.
    The analogy for our neural network is that the value <math alttext="omega"><mi>ω</mi></math>
    of the edge connecting the neurons increases. When we see the same concept again
    and again, it becomes part of our brain’s model. This model will not change, unless
    we learn new information that undoes the previous information. In that case, the
    connection between the neurons weakens. For our neural network, the <math alttext="omega"><mi>ω</mi></math>
    value connecting the neurons decreases. Tweaking the <math alttext="omega"><mi>ω</mi></math>
    ’s via minimizing the loss function accomplishes exactly that: *Establishing the
    correct connections between the neurons*.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们遇到一个新的数学概念时，我们大脑中的神经元会建立某些连接。下次我们再次看到相同的概念时，相同的神经元连接得更好。对于我们的神经网络的类比是，连接神经元的边的值<math
    alttext="omega">ω</math>增加。当我们一遍又一遍地看到相同的概念时，它成为我们大脑模型的一部分。除非我们学到了撤销先前信息的新信息，否则这个模型不会改变。在这种情况下，神经元之间的连接会减弱。对于我们的神经网络，连接神经元的<math
    alttext="omega">ω</math>值会减少。通过最小化损失函数来微调<math alttext="omega">ω</math>，正是为了*建立神经元之间的正确连接*。
- en: 'The brain neuroscientist Donald Hebb mentions in his 1949 book (paraphrased):
    *When a biological neuron triggers another neuron often, the connection between
    these two neurons grows stronger. In other words, cells that fire together, wire
    together.*'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 神经科学家唐纳德·赫布在他1949年的著作中提到（paraphrased）：*当一个生物神经元经常触发另一个神经元时，这两个神经元之间的连接会变得更加牢固。换句话说，一起激活的细胞会一起连接。*
- en: 'Similarly, a neural network’s computational model takes into account the error
    made by the network when it produces an outcome: Since computers only understand
    numbers, the <math alttext="omega"><mi>ω</mi></math> of an edge increases if the
    node contributes to lowering the error, and decsreases if the node contributes
    to increasing the error function. So a neural network’s learning rule reinforces
    the connections that reduce the error by increasing the corresponding <math alttext="omega"><mi>ω</mi></math>
    ’s, and weakens the connections that increase the error by decreasing the corresponding
    <math alttext="omega"><mi>ω</mi></math> ’s.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，神经网络的计算模型考虑了网络产生结果时所产生的错误：由于计算机只能理解数字，如果节点有助于降低错误，则边的<math alttext="omega">ω</math>会增加，如果节点有助于增加错误函数，则<math
    alttext="omega">ω</math>会减少。因此，神经网络的学习规则通过增加相应的<math alttext="omega">ω</math>来加强减少错误的连接，并通过减少相应的<math
    alttext="omega">ω</math>来削弱增加错误的连接。
- en: Why It Is Better To Back-Propagate?
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么反向传播更好？
- en: Back-propagation computes the derivative of the training function with respect
    to each node, moving *backwards* through the network. This measures the contribution
    of each node to both the training function and the loss function <math alttext="upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    .
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播计算训练函数对每个节点的导数，通过网络向*后*移动。这衡量了每个节点对训练函数和损失函数的贡献。
- en: 'The most important formula to recall here is: *The chain rule from Calculus*.
    This calculates the derivatives of *chained* functions (or function compositions).
    Calculus’s chain rule mostly dealt with functions depending only on one variable
    <math alttext="omega"><mi>ω</mi></math> , for example, for three chained functions,
    the derivative with respect to <math alttext="omega"><mi>ω</mi></math> is:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要记住的最重要的公式是：*微积分中的链式法则*。这计算了*链式*函数（或函数组合）的导数。微积分的链式法则主要处理只依赖于一个变量<math alttext="omega">ω</math>的函数，例如，对于三个链式函数，关于<math
    alttext="omega">ω</math>的导数是：
- en: <math alttext="dollar-sign StartFraction d Over d omega EndFraction f 3 left-parenthesis
    f 2 left-parenthesis f 1 left-parenthesis omega right-parenthesis right-parenthesis
    right-parenthesis equals StartSet StartFraction d Over d omega EndFraction f 1
    left-parenthesis omega right-parenthesis EndSet StartSet StartFraction d Over
    d f 1 EndFraction f 2 left-parenthesis f 1 left-parenthesis omega right-parenthesis
    right-parenthesis EndSet StartSet StartFraction d Over f 2 EndFraction f 3 left-parenthesis
    f 2 left-parenthesis f 1 left-parenthesis omega right-parenthesis right-parenthesis
    right-parenthesis EndSet dollar-sign"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>ω</mi></mrow></mfrac>
    <msub><mi>f</mi> <mn>3</mn></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <mi>ω</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>{</mo>
    <mfrac><mi>d</mi> <mrow><mi>d</mi><mi>ω</mi></mrow></mfrac> <msub><mi>f</mi> <mn>1</mn></msub>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>}</mo></mrow> <mrow><mo>{</mo>
    <mfrac><mi>d</mi> <mrow><mi>d</mi><msub><mi>f</mi> <mn>1</mn></msub></mrow></mfrac>
    <msub><mi>f</mi> <mn>2</mn></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>}</mo></mrow>
    <mrow><mo>{</mo> <mfrac><mi>d</mi> <msub><mi>f</mi> <mn>2</mn></msub></mfrac>
    <msub><mi>f</mi> <mn>3</mn></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <mi>ω</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>}</mo></mrow></mrow></math>
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartFraction d Over d omega EndFraction f 3 left-parenthesis
    f 2 left-parenthesis f 1 left-parenthesis omega right-parenthesis right-parenthesis
    right-parenthesis equals StartSet StartFraction d Over d omega EndFraction f 1
    left-parenthesis omega right-parenthesis EndSet StartSet StartFraction d Over
    d f 1 EndFraction f 2 left-parenthesis f 1 left-parenthesis omega right-parenthesis
    right-parenthesis EndSet StartSet StartFraction d Over f 2 EndFraction f 3 left-parenthesis
    f 2 left-parenthesis f 1 left-parenthesis omega right-parenthesis right-parenthesis
    right-parenthesis EndSet dollar-sign"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>ω</mi></mrow></mfrac>
    <msub><mi>f</mi> <mn>3</mn></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <mi>ω</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>{</mo>
    <mfrac><mi>d</mi> <mrow><mi>d</mi><mi>ω</mi></mrow></mfrac> <msub><mi>f</mi> <mn>1</mn></msub>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>}</mo></mrow> <mrow><mo>{</mo>
    <mfrac><mi>d</mi> <mrow><mi>d</mi><msub><mi>f</mi> <mn>1</mn></msub></mrow></mfrac>
    <msub><mi>f</mi> <mn>2</mn></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>}</mo></mrow>
    <mrow><mo>{</mo> <mfrac><mi>d</mi> <msub><mi>f</mi> <mn>2</mn></msub></mfrac>
    <msub><mi>f</mi> <mn>3</mn></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mn>2</mn></msub>
    <mrow><mo>(</mo> <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <mi>ω</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>}</mo></mrow></mrow></math>
- en: For neural networks, we must apply the chain rule to the loss function that
    depends on matrices and vectors of variables *W* and <math alttext="ModifyingAbove
    omega 0 With right-arrow"><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover></math> . So we have to generalize the above rule to *a many
    variables chain rule*. The easiest way to do this is to follow the structure of
    the network computing the derivatives backwards, from the outcome layer all the
    way back to the input layer.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，我们必须将链式法则应用于依赖于变量*W*和<math alttext="ModifyingAbove omega 0 With right-arrow"><mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover></math> 的矩阵和向量的损失函数。因此，我们必须将上述规则推广到*许多变量的链式法则*。这样做的最简单方法是按照网络的结构计算导数，从输出层一直回溯到输入层。
- en: If instead we decide to compute the derivatives forward through the network,
    we would not know whether these derivatives with respect to each variable will
    ultimately contribute to our final outcome because we do not know if they will
    connect through the graph of the network. Even when the graph is fully connected,
    the weights for deeper layers are not present in earlier layers, so it is a big
    waste to compute for their derivatives in the early layers.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定通过网络向前计算导数，我们将不知道这些导数对每个变量的影响是否最终会对我们的最终结果产生影响，因为我们不知道它们是否会通过网络的图连接。即使图是完全连接的，深层的权重也不会出现在较早的层中，因此在早期层中计算它们的导数是一种巨大的浪费。
- en: When we compute the derivatives backward through the network, we start with
    the output and follow the edges of the graph of the network backwards, computing
    the derivatives at each node. Each node’s contribution is calculated only from
    the edges leading to it and edges going out of it. This is computationally much
    cheaper because now we are sure of how and when these nodes contribute to the
    network’s outcome.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过网络向后计算导数时，我们从输出开始，沿着网络的图向后计算导数，计算每个节点的导数。每个节点的贡献仅从指向它的边和从它出去的边计算。这在计算上要便宜得多，因为现在我们确切知道这些节点如何以及何时对网络的结果产生影响。
- en: 'In linear algebra, it is much cheaper to compute the multiplication of a matrix
    with a vector than computing the multiplication of two matrices together. We must
    always avoid multiplying two matrices with each other: Computing <math alttext="upper
    A left-parenthesis upper B bold v right-parenthesis"><mrow><mi>A</mi> <mo>(</mo>
    <mi>B</mi> <mi>𝐯</mi> <mo>)</mo></mrow></math> is cheaper than computing <math
    alttext="left-parenthesis upper A upper B right-parenthesis bold v"><mrow><mo>(</mo>
    <mi>A</mi> <mi>B</mi> <mo>)</mo> <mi>𝐯</mi></mrow></math> , even though in theory,
    these two are exactly the same. Over large matrices and vectors, this simple observation
    provides enormous cost savings.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，计算矩阵与向量的乘积要比计算两个矩阵的乘积便宜得多。我们必须始终避免将两个矩阵相乘：计算<math alttext="upper A left-parenthesis
    upper B bold v right-parenthesis"><mrow><mi>A</mi> <mo>(</mo> <mi>B</mi> <mi>𝐯</mi>
    <mo>)</mo></mrow></math>比计算<math alttext="left-parenthesis upper A upper B right-parenthesis
    bold v"><mrow><mo>(</mo> <mi>A</mi> <mi>B</mi> <mo>)</mo> <mi>𝐯</mi></mrow></math>要便宜，尽管从理论上讲，这两者完全相同。对于大型矩阵和向量，这个简单的观察提供了巨大的成本节约。
- en: Backpropagation In Detail
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播详细说明
- en: Let’s pause and be thankful that software packages exist so that we never have
    to implement the following computation ourselves. Let’s also not forget to be
    grateful to the creators of these software packages. Now we compute.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来感谢软件包的存在，这样我们就永远不必自己实现以下计算。我们也不要忘记感谢这些软件包的创造者。现在我们开始计算。
- en: 'For a neural network with *h* hidden layers, we can write the loss function
    as a function of the training function which in turn is a function of all the
    weights that appear in the network:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有*h*个隐藏层的神经网络，我们可以将损失函数写成训练函数的函数，而训练函数又是网络中出现的所有权重的函数：
- en: <math alttext="dollar-sign upper L equals upper L left-parenthesis g left-parenthesis
    upper W Superscript 1 Baseline comma ModifyingAbove omega 0 With right-arrow Superscript
    1 Baseline comma upper W squared comma ModifyingAbove omega 0 With right-arrow
    squared comma ellipsis comma upper W Superscript h Baseline comma ModifyingAbove
    omega 0 With right-arrow Superscript h Baseline comma upper W Superscript h plus
    1 Baseline comma ModifyingAbove omega 0 With right-arrow Superscript h plus 1
    Baseline right-parenthesis right-parenthesis dollar-sign"><mrow><mi>L</mi> <mo>=</mo>
    <mi>L</mi> <mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>1</mn></msup>
    <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>,</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>,</mo> <msup><mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover> <mn>2</mn></msup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi> <mi>h</mi></msup> <mo>,</mo>
    <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>,</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper L equals upper L left-parenthesis g left-parenthesis
    upper W Superscript 1 Baseline comma ModifyingAbove omega 0 With right-arrow Superscript
    1 Baseline comma upper W squared comma ModifyingAbove omega 0 With right-arrow
    squared comma ellipsis comma upper W Superscript h Baseline comma ModifyingAbove
    omega 0 With right-arrow Superscript h Baseline comma upper W Superscript h plus
    1 Baseline comma ModifyingAbove omega 0 With right-arrow Superscript h plus 1
    Baseline right-parenthesis right-parenthesis dollar-sign"><mrow><mi>L</mi> <mo>=</mo>
    <mi>L</mi> <mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mn>1</mn></msup>
    <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>,</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>,</mo> <msup><mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover> <mn>2</mn></msup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi> <mi>h</mi></msup> <mo>,</mo>
    <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>,</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
- en: We will compute the partial derivatives of *L* backwards, starting with <math
    alttext="StartFraction normal partial-differential upper L Over normal partial-differential
    upper W Superscript h plus 1 Baseline EndFraction"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mfrac></math>
    and <math alttext="StartFraction normal partial-differential upper L Over normal
    partial-differential ModifyingAbove omega 0 With right-arrow Superscript h plus
    1 Baseline EndFraction"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msup><mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mfrac></math>
    and working our way back to <math alttext="StartFraction normal partial-differential
    upper L Over normal partial-differential upper W Superscript 1 Baseline EndFraction"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mi>W</mi> <mn>1</mn></msup></mrow></mfrac></math> and <math
    alttext="StartFraction normal partial-differential upper L Over normal partial-differential
    ModifyingAbove omega 0 With right-arrow Superscript 1 Baseline EndFraction"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mn>1</mn></msup></mrow></mfrac></math> . Their derivatives
    are taken with respect to each entry in the corresponding matrix or vector.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将反向计算*L*的偏导数，从<math alttext="分数正常偏导数上标L除以正常偏导数上标W上标h加1"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mfrac></math>开始，并从<math
    alttext="分数正常偏导数上标L除以正常偏导数用右箭头上标h加1的omega0"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mfrac></math>开始，逐步回溯到<math
    alttext="分数正常偏导数上标L除以正常偏导数上标W上标1"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msup><mi>W</mi>
    <mn>1</mn></msup></mrow></mfrac></math>和<math alttext="分数正常偏导数上标L除以正常偏导数用右箭头上标1的omega0"><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mn>1</mn></msup></mrow></mfrac></math>。它们的导数是相对于相应矩阵或向量中的每个条目进行的。
- en: Suppose for simplicity, but without loss of generality, that the network is
    a regression network predicting a single numerical value, so that the training
    function *g* is scalar (not a vector). Suppose also that we use the same activation
    function *f* for each neuron throughout the network. The output neuron has no
    activation since this is a regression.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 假设为简单起见，但不失一般性，网络是一个回归网络，预测单个数值，因此训练函数*g*是标量（不是向量）。假设我们在整个网络中使用相同的激活函数*f*。输出神经元没有激活，因为这是一个回归问题。
- en: 'Derivatives with respect to the weights pointing to the output layer:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对指向输出层的权重的导数：
- en: <math alttext="dollar-sign upper L equals upper L left-parenthesis upper W Superscript
    h plus 1 Baseline ModifyingAbove s With right-arrow Superscript h Baseline plus
    omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mi>L</mi>
    <mo>=</mo> <mi>L</mi> <mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup>
    <mo>)</mo></mrow></math>
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper L equals upper L left-parenthesis upper W Superscript
    h plus 1 Baseline ModifyingAbove s With right-arrow Superscript h Baseline plus
    omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mi>L</mi>
    <mo>=</mo> <mi>L</mi> <mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup>
    <mo>)</mo></mrow></math>
- en: so that
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential omega 0 Superscript h plus 1 Baseline EndFraction
    equals 1 times upper L prime left-parenthesis upper W Superscript h plus 1 Baseline
    ModifyingAbove s With right-arrow Superscript h Baseline plus omega 0 Superscript
    h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></mfrac>
    <mo>=</mo> <mn>1</mn> <mo>×</mo> <msup><mi>L</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential omega 0 Superscript h plus 1 Baseline EndFraction
    equals 1 times upper L prime left-parenthesis upper W Superscript h plus 1 Baseline
    ModifyingAbove s With right-arrow Superscript h Baseline plus omega 0 Superscript
    h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msubsup><mi>ω</mi> <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></mfrac>
    <mo>=</mo> <mn>1</mn> <mo>×</mo> <msup><mi>L</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
- en: and
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential upper W Superscript h plus 1 Baseline EndFraction
    equals left-parenthesis ModifyingAbove s With right-arrow Superscript h Baseline
    right-parenthesis Superscript t Baseline upper L prime left-parenthesis upper
    W Superscript h plus 1 Baseline ModifyingAbove s With right-arrow Superscript
    h Baseline plus omega 0 Superscript h plus 1 Baseline right-parenthesis period
    dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msup><mi>W</mi>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mfrac> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>)</mo></mrow>
    <mi>t</mi></msup> <msup><mi>L</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msup><mi>W</mi>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <msup><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo> <msubsup><mi>ω</mi> <mn>0</mn>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential upper W Superscript h plus 1 Baseline EndFraction
    equals left-parenthesis ModifyingAbove s With right-arrow Superscript h Baseline
    right-parenthesis Superscript t Baseline upper L prime left-parenthesis upper
    W Superscript h plus 1 Baseline ModifyingAbove s With right-arrow Superscript
    h Baseline plus omega 0 Superscript h plus 1 Baseline right-parenthesis period
    dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow> <mrow><mi>∂</mi><msup><mi>W</mi>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mfrac> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup> <mo>)</mo></mrow>
    <mi>t</mi></msup> <msup><mi>L</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msup><mi>W</mi>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup> <msup><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover> <mi>h</mi></msup> <mo>+</mo> <msubsup><mi>ω</mi> <mn>0</mn>
    <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
- en: Recall that <math alttext="ModifyingAbove s With right-arrow Superscript h"><msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mi>h</mi></msup></math> is the output
    of the last layer, so it depends on all the weights of the previous layers, namely,
    <math alttext="left-parenthesis upper W Superscript 1 Baseline comma ModifyingAbove
    omega 0 With right-arrow Superscript 1 Baseline comma upper W squared comma ModifyingAbove
    omega 0 With right-arrow squared comma ellipsis comma upper W Superscript h Baseline
    comma ModifyingAbove omega 0 With right-arrow Superscript h Baseline right-parenthesis"><mrow><mo>(</mo>
    <msup><mi>W</mi> <mn>1</mn></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>→</mo></mover> <mn>1</mn></msup> <mo>,</mo> <msup><mi>W</mi>
    <mn>2</mn></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mn>2</mn></msup> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi>
    <mi>h</mi></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mi>h</mi></msup> <mo>)</mo></mrow></math> .
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，<math alttext="用右箭头上标h修改s"><msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mi>h</mi></msup></math> 是最后一层的输出，因此它取决于前几层的所有权重，即，<math alttext="左括号上标W1逗号用右箭头上标1修改的omega0逗号上标W平方逗号用右箭头平方的omega0逗号省略号上标h逗号用右箭头上标h右括号"><mrow><mo>(</mo>
    <msup><mi>W</mi> <mn>1</mn></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>→</mo></mover> <mn>1</mn></msup> <mo>,</mo> <msup><mi>W</mi>
    <mn>2</mn></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mn>2</mn></msup> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi>
    <mi>h</mi></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mi>h</mi></msup> <mo>)</mo></mrow></math>。
- en: Derivatives with respect to the weights pointing to the last hidden layer
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 对指向最后隐藏层的权重的导数
- en: 'To compute these, we show them explicitly in the formula of the loss function:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算这些导数，我们在损失函数的公式中明确显示它们：
- en: <math alttext="dollar-sign upper L equals upper L left-parenthesis upper W Superscript
    h plus 1 Baseline left-parenthesis f left-parenthesis upper W Superscript h Baseline
    ModifyingAbove s With right-arrow Superscript h minus 1 Baseline plus ModifyingAbove
    omega 0 With right-arrow Superscript h Baseline right-parenthesis right-parenthesis
    plus omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mi>L</mi>
    <mo>=</mo> <mi>L</mi> <mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></math>
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper L equals upper L left-parenthesis upper W Superscript
    h plus 1 Baseline left-parenthesis f left-parenthesis upper W Superscript h Baseline
    ModifyingAbove s With right-arrow Superscript h minus 1 Baseline plus ModifyingAbove
    omega 0 With right-arrow Superscript h Baseline right-parenthesis right-parenthesis
    plus omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mi>L</mi>
    <mo>=</mo> <mi>L</mi> <mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></math>
- en: so that
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential ModifyingAbove omega 0 With right-arrow Superscript
    h Baseline EndFraction equals ModifyingAbove 1 With right-arrow left-parenthesis
    upper W Superscript h plus 1 Baseline f prime left-parenthesis upper W Superscript
    h Baseline ModifyingAbove s With right-arrow Superscript h minus 1 Baseline plus
    ModifyingAbove omega 0 With right-arrow Superscript h Baseline right-parenthesis
    right-parenthesis upper L prime left-parenthesis upper W Superscript h plus 1
    Baseline left-parenthesis f left-parenthesis upper W Superscript h Baseline ModifyingAbove
    s With right-arrow Superscript h minus 1 Baseline plus ModifyingAbove omega 0
    With right-arrow Superscript h Baseline right-parenthesis right-parenthesis plus
    omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mi>h</mi></msup></mrow></mfrac> <mo>=</mo> <mover accent="true"><mn>1</mn>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <msup><mi>L</mi> <mo>'</mo></msup>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential ModifyingAbove omega 0 With right-arrow Superscript
    h Baseline EndFraction equals ModifyingAbove 1 With right-arrow left-parenthesis
    upper W Superscript h plus 1 Baseline f prime left-parenthesis upper W Superscript
    h Baseline ModifyingAbove s With right-arrow Superscript h minus 1 Baseline plus
    ModifyingAbove omega 0 With right-arrow Superscript h Baseline right-parenthesis
    right-parenthesis upper L prime left-parenthesis upper W Superscript h plus 1
    Baseline left-parenthesis f left-parenthesis upper W Superscript h Baseline ModifyingAbove
    s With right-arrow Superscript h minus 1 Baseline plus ModifyingAbove omega 0
    With right-arrow Superscript h Baseline right-parenthesis right-parenthesis plus
    omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mi>h</mi></msup></mrow></mfrac> <mo>=</mo> <mover accent="true"><mn>1</mn>
    <mo>→</mo></mover> <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <msup><mi>L</mi> <mo>'</mo></msup>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
- en: and
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential upper W Superscript h Baseline EndFraction equals
    ModifyingAbove s With right-arrow Superscript h minus 1 Baseline left-parenthesis
    upper W Superscript h plus 1 Baseline f prime left-parenthesis upper W Superscript
    h Baseline ModifyingAbove s With right-arrow Superscript h minus 1 Baseline plus
    ModifyingAbove omega 0 With right-arrow Superscript h Baseline right-parenthesis
    right-parenthesis upper L prime left-parenthesis upper W Superscript h plus 1
    Baseline left-parenthesis f left-parenthesis upper W Superscript h Baseline ModifyingAbove
    s With right-arrow Superscript h minus 1 Baseline plus ModifyingAbove omega 0
    With right-arrow Superscript h Baseline right-parenthesis right-parenthesis plus
    omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mi>W</mi> <mi>h</mi></msup></mrow></mfrac> <mo>=</mo> <msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <msup><mi>L</mi> <mo>'</mo></msup>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartFraction normal partial-differential upper L
    Over normal partial-differential upper W Superscript h Baseline EndFraction equals
    ModifyingAbove s With right-arrow Superscript h minus 1 Baseline left-parenthesis
    upper W Superscript h plus 1 Baseline f prime left-parenthesis upper W Superscript
    h Baseline ModifyingAbove s With right-arrow Superscript h minus 1 Baseline plus
    ModifyingAbove omega 0 With right-arrow Superscript h Baseline right-parenthesis
    right-parenthesis upper L prime left-parenthesis upper W Superscript h plus 1
    Baseline left-parenthesis f left-parenthesis upper W Superscript h Baseline ModifyingAbove
    s With right-arrow Superscript h minus 1 Baseline plus ModifyingAbove omega 0
    With right-arrow Superscript h Baseline right-parenthesis right-parenthesis plus
    omega 0 Superscript h plus 1 Baseline right-parenthesis dollar-sign"><mrow><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow>
    <mrow><mi>∂</mi><msup><mi>W</mi> <mi>h</mi></msup></mrow></mfrac> <mo>=</mo> <msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <msup><mi>L</mi> <mo>'</mo></msup>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msup><mi>W</mi> <mi>h</mi></msup>
    <msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>+</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mi>h</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>0</mn> <mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
- en: Recall that <math alttext="ModifyingAbove s With right-arrow Superscript h minus
    1"><msup><mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup></math>
    is the output of the hidden layer before the last hidden layer, so it depends
    on all the weights of the previous layers, namely, <math alttext="left-parenthesis
    upper W Superscript 1 Baseline comma ModifyingAbove omega 0 With right-arrow Superscript
    1 Baseline comma upper W squared comma ModifyingAbove omega 0 With right-arrow
    squared comma ellipsis comma upper W Superscript h minus 1 Baseline comma ModifyingAbove
    omega 0 With right-arrow Superscript h minus 1 Baseline right-parenthesis"><mrow><mo>(</mo>
    <msup><mi>W</mi> <mn>1</mn></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>→</mo></mover> <mn>1</mn></msup> <mo>,</mo> <msup><mi>W</mi>
    <mn>2</mn></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>→</mo></mover> <mn>2</mn></msup> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi>
    <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup> <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>)</mo></mrow></math> .
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下<math alttext="ModifyingAbove s With right-arrow Superscript h minus 1"><msup><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup></math>是倒数第二个隐藏层的输出，因此它取决于前几层的所有权重，即<math
    alttext="left-parenthesis upper W Superscript 1 Baseline comma ModifyingAbove
    omega 0 With right-arrow Superscript 1 Baseline comma upper W squared comma ModifyingAbove
    omega 0 With right-arrow squared comma ellipsis comma upper W Superscript h minus
    1 Baseline comma ModifyingAbove omega 0 With right-arrow Superscript h minus 1
    Baseline right-parenthesis"><mrow><mo>(</mo> <msup><mi>W</mi> <mn>1</mn></msup>
    <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mn>1</mn></msup> <mo>,</mo> <msup><mi>W</mi> <mn>2</mn></msup> <mo>,</mo> <msup><mover
    accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover> <mn>2</mn></msup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msup><mi>W</mi> <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>,</mo> <msup><mover accent="true"><msub><mi>ω</mi> <mn>0</mn></msub> <mo>→</mo></mover>
    <mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></msup> <mo>)</mo></mrow></math>。
- en: We continue the process systematically until we reach the input layer.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统地继续这个过程，直到达到输入层。
- en: Assessing The Significance Of The Features Of The Input Data
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估输入数据特征的重要性
- en: One goal of data analysts is to assess the significance of the input variables
    (data features) with respect to the output or target variable.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师的一个目标是评估输入变量（数据特征）与输出或目标变量之间的相关性。
- en: 'The main question to answer here is: *If we tweak the value of a certain input
    variable, what is the relative change of the output?*'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要回答的主要问题是：*如果我们调整某个输入变量的值，输出的相对变化是多少？*
- en: For example, if we add one more bus on a given bus route, would that affect
    the overall bus ridership?
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果在给定的公交线路上增加一辆公交车，是否会影响整体的公交车乘客量？
- en: 'The math question that we are asking is a *derivative* question: Find the partial
    derivative of the output with respect to the input variable in question.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所问的数学问题是一个*导数*问题：找到关于所讨论的输入变量的输出的偏导数。
- en: We have plenty of literature in statistics on variable significance when the
    models are linear (sensitivity analysis). When the models are nonlinear, such
    as our neural network models, there isn’t as much literature. We cannot make our
    predictions based on nonlinear models then employ variable significance analysis
    that’s built for linear models. Many data analysts who use built in software packages
    for their analysis fall into this trap. This is another reason to seek to understand
    deeply the assumptions of the models that we base our business decisions on.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型是线性的时候（敏感性分析），我们在统计学上有大量关于变量重要性的文献。当模型是非线性的，比如我们的神经网络模型时，相关文献就不那么多了。我们不能基于非线性模型做出预测，然后使用为线性模型设计的变量重要性分析。许多数据分析师在他们的分析中使用内置软件包时会陷入这种陷阱。这是另一个寻求深入理解我们基于业务决策的模型假设的原因。
- en: Summary And Looking Ahead
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和展望
- en: 'This chapter represents our official transition to the deep learning era in
    the AI field. While [Chapter 3](ch03.xhtml#ch03) presented traditional yet still
    very useful machine learning models, [Chapter 4](#ch04) adds neural networks to
    our arsenal of machine learning models. Both chapters built the models with the
    general mathematical structure of: Training function, loss function, and optimization,
    where each of these was tailored to the particular task and model at hand.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章代表了我们正式进入人工智能领域的深度学习时代。虽然[第3章](ch03.xhtml#ch03)介绍了传统但仍然非常有用的机器学习模型，[第4章](#ch04)将神经网络添加到我们的机器学习模型库中。这两章都建立在训练函数、损失函数和优化的一般数学结构上，其中每个都针对特定任务和手头的模型进行了定制。
- en: By employing nonlinear activation functions at each neuron of a neural network,
    over multiple layers, our training function is able to pick up on complex features
    in the data that is otherwise hard to describe using an explicit formula of a
    nonlinear function. Mathematical analysis, in particular, universal approximation
    theorems for neural networks, back up this intuition and provide a theoretical
    background that justifies the wild success of neural networks. These theorems,
    however, still lack the ability to provide us with a map to construct special
    networks tailored to specific tasks and data sets, so we must experiment with
    various architectures, regularizations, hyperparameters, until we obtain a neural
    network model that performs well on new and unseen data.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在神经网络的每个神经元上使用非线性激活函数，跨多个层，我们的训练函数能够捕捉到数据中复杂的特征，这些特征很难用非线性函数的显式公式描述。数学分析，特别是神经网络的通用逼近定理，支持这种直觉，并提供了一个理论背景，证明了神经网络的巨大成功。然而，这些定理仍然缺乏能力为我们提供一个构建特定任务和数据集的特殊网络的地图，因此我们必须尝试各种架构、正则化、超参数，直到我们获得一个在新的和未知数据上表现良好的神经网络模型。
- en: Neural networks are well tailored for large problems with large data sets. The
    optimization task for such large problems requires efficient and computationally
    inexpensive methods, though all computations at that scale can be considered expensive.
    Stochastic gradient descent is the popular optimization method of choice, and
    the back-propagation algorithm is the work horse of this method. More specifically,
    the back-propagation algorithm computes the gradient of the loss function (or
    the objective function when we add weight decay regularization) at the current
    weight choice. Understanding the landscape of the objective function remains central
    for any optimization task, and as a rule of thumb, convex problems are easier
    to optimize than non-convex ones. Loss functions involved in neural network models
    are generally non-convex.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络非常适合处理具有大数据集的大问题。对于这样的大问题，优化任务需要高效且计算成本低廉的方法，尽管在这种规模下的所有计算都可以被视为昂贵的。随机梯度下降是流行的优化方法选择，而反向传播算法是这种方法的核心。更具体地说，反向传播算法计算当前权重选择下损失函数的梯度（或者当我们添加权重衰减正则化时的目标函数）。理解目标函数的景观对于任何优化任务都是至关重要的，作为一个经验法则，凸问题比非凸问题更容易优化。神经网络模型中涉及的损失函数通常是非凸的。
- en: '[Chapter 4](#ch04) is the last foundational (and long) chapter in this book.
    We can finally discuss more specialized AI models, as well as deeper mathematics,
    when needed. The next chapters are independent from each other, so read them in
    the order that feels more relevant to your immediate application area.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[第四章](#ch04)是本书中最后一个基础（且较长）的章节。我们最终可以讨论更专门的人工智能模型，以及在需要时更深入的数学知识。接下来的章节彼此独立，所以按照对您当前应用领域更相关的顺序阅读它们。'
- en: 'Finally, let’s summarize the mathematics that appeared in this chapter, which
    we must elaborate more on as we progress in the field:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们总结一下本章中出现的数学内容，随着我们在这个领域的进展，我们必须更详细地阐述：
- en: Probability and measure
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 概率和测度
- en: This is needed to prove universal approximation type theorems, and will be discussed
    in the probability chapter. It is also related to uncertainty analysis related
    to Dropout.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这是证明通用逼近类型定理所需的，将在概率章节中讨论。这也与与Dropout相关的不确定性分析有关。
- en: Statistics
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学
- en: Input standarizing steps during batch normalization at each layer of the neural
    network, and the resulting reshaping of the related distributions.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的每一层进行批量归一化时的输入标准化步骤，以及相关分布的重塑。
- en: Optimization
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 优化
- en: Gradient descent, stochastic gradient descent, convex and non-convex landscapes.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降，随机梯度下降，凸和非凸景观。
- en: Calculus on Linear Algebra
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数上的微积分
- en: 'Back-propagation algorithm: This is the chain rule from calculus applied on
    functions of matrices of variables.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法：这是在矩阵函数上应用的微积分链式法则。
