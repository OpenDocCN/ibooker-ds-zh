<html><head></head><body><section data-pdf-bookmark="Chapter 15. Multiple Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="multiple_regression">&#13;
<h1><span class="label">Chapter 15. </span>Multiple Regression</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>I don’t look at a problem and put variables in there that don’t affect it.</p>&#13;
    <p data-type="attribution">Bill Parcells</p>&#13;
</blockquote>&#13;
&#13;
<p>Although<a data-primary="predictive models" data-secondary="multiple regression" data-type="indexterm" id="PMmultreg15"/> the VP is pretty impressed with your predictive model, she thinks you can do better.  To that end, you’ve collected additional data: you know how many hours each of your users works each day, and whether they have a PhD.  You’d like to use this additional data to improve your model.</p>&#13;
&#13;
<p>Accordingly, you hypothesize a linear model with more independent variables:</p>&#13;
<div data-type="equation">&#13;
<math alttext="minutes equals alpha plus beta 1 friends plus beta 2 work hours plus beta 3 phd plus epsilon" display="block">&#13;
  <mrow>&#13;
    <mtext>minutes</mtext>&#13;
    <mo>=</mo>&#13;
    <mi>α</mi>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <mtext>friends</mtext>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>2</mn> </msub>&#13;
    <mtext>work</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>hours</mtext>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>3</mn> </msub>&#13;
    <mtext>phd</mtext>&#13;
    <mo>+</mo>&#13;
    <mi>ε</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Obviously, whether a user has a PhD is not a number—but, as we mentioned in <a data-type="xref" href="ch11.html#machine_learning">Chapter 11</a>, we can introduce a <em>dummy variable</em> that equals 1 for users with PhDs and 0 for users without, after which it’s just as numeric as the other variables.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Model" data-type="sect1"><div class="sect1" id="idm45635738437704">&#13;
<h1>The Model</h1>&#13;
&#13;
<p>Recall<a data-primary="multiple regression" data-secondary="model for" data-type="indexterm" id="idm45635738435976"/> that in <a data-type="xref" href="ch14.html#simple_linear_regression">Chapter 14</a> we fit a model of the form:</p>&#13;
<div data-type="equation">&#13;
<math alttext="y Subscript i Baseline equals alpha plus beta x Subscript i Baseline plus epsilon Subscript i" display="block">&#13;
  <mrow>&#13;
    <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>α</mi>&#13;
    <mo>+</mo>&#13;
    <mi>β</mi>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>ε</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Now imagine that each input &#13;
<math>&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> is not a single number but rather a vector of <em>k</em> numbers,&#13;
&#13;
<math>&#13;
  <mrow>&#13;
    <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow> </msub>&#13;
  </mrow>&#13;
</math>. The multiple regression model assumes that:</p>&#13;
<div data-type="equation">&#13;
<math alttext="y Subscript i Baseline equals alpha plus beta 1 x Subscript i Baseline 1 Baseline plus period period period plus beta Subscript k Baseline x Subscript i k Baseline plus epsilon Subscript i Baseline" display="block">&#13;
  <mrow>&#13;
    <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>α</mi>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow> </msub>&#13;
    <mo>+</mo>&#13;
    <mo>.</mo>&#13;
    <mo>.</mo>&#13;
    <mo>.</mo>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mi>k</mi> </msub>&#13;
    <msub><mi>x</mi> <mrow><mi>i</mi><mi>k</mi></mrow> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>ε</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>In multiple regression the vector of parameters is usually called <em>β</em>.  We’ll want this to include the constant term as well, which we can achieve by adding a column of 1s to our data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">beta</code> <code class="o">=</code> <code class="p">[</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta_1</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code> <code class="n">beta_k</code><code class="p">]</code></pre>&#13;
&#13;
<p>and:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">x_i</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="n">x_i1</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code> <code class="n">x_ik</code><code class="p">]</code></pre>&#13;
&#13;
<p>Then our model is just:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">dot</code><code class="p">,</code> <code class="n">Vector</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""assumes that the first element of x is 1"""</code>&#13;
    <code class="k">return</code> <code class="n">dot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code></pre>&#13;
&#13;
<p>In this particular case, our independent variable <code>x</code> will be a list of vectors, each of which looks like this:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="p">[</code><code class="mi">1</code><code class="p">,</code>    <code class="c1"># constant term</code>&#13;
 <code class="mi">49</code><code class="p">,</code>   <code class="c1"># number of friends</code>&#13;
 <code class="mi">4</code><code class="p">,</code>    <code class="c1"># work hours per day</code>&#13;
 <code class="mi">0</code><code class="p">]</code>    <code class="c1"># doesn't have PhD</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Further Assumptions of the Least Squares Model" data-type="sect1"><div class="sect1" id="idm45635738437208">&#13;
<h1>Further Assumptions of the Least Squares Model</h1>&#13;
&#13;
<p>There<a data-primary="multiple regression" data-secondary="assumptions of least square model" data-type="indexterm" id="idm45635738234280"/><a data-primary="least squares solution" data-type="indexterm" id="idm45635738226792"/> are a couple of further assumptions that are required for this model (and our solution) to make sense.</p>&#13;
&#13;
<p>The<a data-primary="linear independence" data-type="indexterm" id="idm45635738225272"/> first is that the columns of <em>x</em> are <em>linearly independent</em>—that there’s no way to write any one as a weighted sum of some of the others.  If this assumption fails, it’s impossible to estimate <code>beta</code>.  To see this in an extreme case, imagine we had an extra field <code>num_acquaintances</code> in our data that for every user was exactly equal to <code>num_friends</code>.</p>&#13;
&#13;
<p>Then, starting with any <code>beta</code>, if we add <em>any</em> amount to the <code>num_friends</code> coefficient and subtract that same amount from the <code>num_acquaintances</code> coefficient, the model’s predictions will remain unchanged. This means that there’s no way to find <em>the</em> coefficient for <code>num_friends</code>.  (Usually violations of this assumption won’t be so obvious.)</p>&#13;
&#13;
<p>The second important assumption is that the columns of <em>x</em> are all uncorrelated with the errors <math>&#13;
  <mi>ε</mi>&#13;
</math>.&#13;
If this fails to be the case, our estimates of <code>beta</code> will be systematically wrong.</p>&#13;
&#13;
<p>For instance, in <a data-type="xref" href="ch14.html#simple_linear_regression">Chapter 14</a>, we built a model that predicted that each additional friend was associated with an extra 0.90 daily minutes on the site.</p>&#13;
&#13;
<p>Imagine it’s also the case that:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>People who work more hours spend less time on the site.</p>&#13;
</li>&#13;
<li>&#13;
<p>People with more friends tend to work more hours.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>That is, imagine that the “actual” model is:</p>&#13;
<div data-type="equation">&#13;
<math alttext="minutes equals alpha plus beta 1 friends plus beta 2 work hours plus epsilon" display="block">&#13;
  <mrow>&#13;
    <mtext>minutes</mtext>&#13;
    <mo>=</mo>&#13;
    <mi>α</mi>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <mtext>friends</mtext>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>2</mn> </msub>&#13;
    <mtext>work</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>hours</mtext>&#13;
    <mo>+</mo>&#13;
    <mi>ε</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>where <math>&#13;
  <msub><mi>β</mi> <mn>2</mn> </msub>&#13;
</math> is negative, and that work hours and friends are positively correlated. In that case, when we minimize the errors of the single-variable model:</p>&#13;
<div data-type="equation">&#13;
<math alttext="minutes equals alpha plus beta 1 friends plus epsilon" display="block">&#13;
  <mrow>&#13;
    <mtext>minutes</mtext>&#13;
    <mo>=</mo>&#13;
    <mi>α</mi>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <mtext>friends</mtext>&#13;
    <mo>+</mo>&#13;
    <mi>ε</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>we will underestimate <math>&#13;
  <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
</math>.</p>&#13;
&#13;
<p>Think about what would happen if we made predictions using the single-variable model with the “actual” value of <math>&#13;
  <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
</math>. (That is, the value that arises from minimizing the errors of what we called the “actual” model.) The predictions would tend to be way too large for users who work many hours and a little too large for users who work few hours, because  <math>&#13;
  <mrow>&#13;
    <msub><mi>β</mi> <mn>2</mn> </msub>&#13;
    <mo>&lt;</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math> and we “forgot” to include it. Because work hours is positively correlated with number of friends, this means the predictions tend to be way too large for users with many friends, and only slightly too large for users with few friends.</p>&#13;
&#13;
<p>The result of this is that we can reduce the errors (in the single-variable model) by decreasing our estimate of <math>&#13;
  <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
</math>, which means that the error-minimizing <math>&#13;
  <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
</math> is smaller than the “actual” value.  That is, in this case the single-variable least squares solution is biased to underestimate <math>&#13;
  <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
</math>.  And, in general, whenever the independent variables are correlated with the errors like this, our least squares solution will give us a biased estimate of <math>&#13;
  <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
</math>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Fitting the Model" data-type="sect1"><div class="sect1" id="idm45635738227240">&#13;
<h1>Fitting the Model</h1>&#13;
&#13;
<p>As<a data-primary="multiple regression" data-secondary="model fitting" data-type="indexterm" id="idm45635738123496"/> we did in the simple linear model, we’ll choose <code>beta</code> to minimize the sum of squared errors.  Finding an exact solution is not simple to do by hand, which means we’ll need to use gradient descent.  Again we’ll want to minimize the sum of the squared errors. The error function is almost identical to the one we used in <a data-type="xref" href="ch14.html#simple_linear_regression">Chapter 14</a>, except that instead of expecting parameters <code>[alpha, beta]</code> it will take a vector of arbitrary length:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">error</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">predict</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">-</code> <code class="n">y</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">squared_error</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">error</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>&#13;
&#13;
<code class="n">x</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]</code>&#13;
<code class="n">y</code> <code class="o">=</code> <code class="mi">30</code>&#13;
<code class="n">beta</code> <code class="o">=</code> <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">]</code>  <code class="c1"># so prediction = 4 + 8 + 12 = 24</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">error</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">==</code> <code class="o">-</code><code class="mi">6</code>&#13;
<code class="k">assert</code> <code class="n">squared_error</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">==</code> <code class="mi">36</code></pre>&#13;
&#13;
<p>If you know calculus, it’s easy to compute the gradient:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">sqerror_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="n">err</code> <code class="o">=</code> <code class="n">error</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="mi">2</code> <code class="o">*</code> <code class="n">err</code> <code class="o">*</code> <code class="n">x_i</code> <code class="k">for</code> <code class="n">x_i</code> <code class="ow">in</code> <code class="n">x</code><code class="p">]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">sqerror_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="o">-</code><code class="mi">12</code><code class="p">,</code> <code class="o">-</code><code class="mi">24</code><code class="p">,</code> <code class="o">-</code><code class="mi">36</code><code class="p">]</code></pre>&#13;
&#13;
<p>Otherwise, you’ll need to take my word for it.</p>&#13;
&#13;
<p>At this point, we’re ready to find the optimal <code>beta</code> using  gradient descent. Let’s first write out a <code>least_squares_fit</code> function that can work with any dataset:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">import</code> <code class="nn">tqdm</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">vector_mean</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.gradient_descent</code> <code class="kn">import</code> <code class="n">gradient_step</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">least_squares_fit</code><code class="p">(</code><code class="n">xs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">],</code>&#13;
                      <code class="n">ys</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">],</code>&#13;
                      <code class="n">learning_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.001</code><code class="p">,</code>&#13;
                      <code class="n">num_steps</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">1000</code><code class="p">,</code>&#13;
                      <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Find the beta that minimizes the sum of squared errors</code>&#13;
<code class="sd">    assuming the model y = dot(x, beta).</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="c1"># Start with a random guess</code>&#13;
    <code class="n">guess</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">xs</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="n">num_steps</code><code class="p">,</code> <code class="n">desc</code><code class="o">=</code><code class="s2">"least squares fit"</code><code class="p">):</code>&#13;
        <code class="k">for</code> <code class="n">start</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">xs</code><code class="p">),</code> <code class="n">batch_size</code><code class="p">):</code>&#13;
            <code class="n">batch_xs</code> <code class="o">=</code> <code class="n">xs</code><code class="p">[</code><code class="n">start</code><code class="p">:</code><code class="n">start</code><code class="o">+</code><code class="n">batch_size</code><code class="p">]</code>&#13;
            <code class="n">batch_ys</code> <code class="o">=</code> <code class="n">ys</code><code class="p">[</code><code class="n">start</code><code class="p">:</code><code class="n">start</code><code class="o">+</code><code class="n">batch_size</code><code class="p">]</code>&#13;
&#13;
            <code class="n">gradient</code> <code class="o">=</code> <code class="n">vector_mean</code><code class="p">([</code><code class="n">sqerror_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">guess</code><code class="p">)</code>&#13;
                                    <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">batch_xs</code><code class="p">,</code> <code class="n">batch_ys</code><code class="p">)])</code>&#13;
            <code class="n">guess</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">guess</code><code class="p">,</code> <code class="n">gradient</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">guess</code></pre>&#13;
&#13;
<p>We can then apply that to our data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.statistics</code> <code class="kn">import</code> <code class="n">daily_minutes_good</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.gradient_descent</code> <code class="kn">import</code> <code class="n">gradient_step</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="c1"># I used trial and error to choose num_iters and step_size.</code>&#13;
<code class="c1"># This will run for a while.</code>&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.001</code>&#13;
&#13;
<code class="n">beta</code> <code class="o">=</code> <code class="n">least_squares_fit</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="n">learning_rate</code><code class="p">,</code> <code class="mi">5000</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>&#13;
<code class="k">assert</code> <code class="mf">30.50</code> <code class="o">&lt;</code> <code class="n">beta</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mf">30.70</code>  <code class="c1"># constant</code>&#13;
<code class="k">assert</code>  <code class="mf">0.96</code> <code class="o">&lt;</code> <code class="n">beta</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">&lt;</code>  <code class="mf">1.00</code>  <code class="c1"># num friends</code>&#13;
<code class="k">assert</code> <code class="o">-</code><code class="mf">1.89</code> <code class="o">&lt;</code> <code class="n">beta</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="o">&lt;</code> <code class="o">-</code><code class="mf">1.85</code>  <code class="c1"># work hours per day</code>&#13;
<code class="k">assert</code>  <code class="mf">0.91</code> <code class="o">&lt;</code> <code class="n">beta</code><code class="p">[</code><code class="mi">3</code><code class="p">]</code> <code class="o">&lt;</code>  <code class="mf">0.94</code>  <code class="c1"># has PhD</code></pre>&#13;
&#13;
<p>In practice, you wouldn’t estimate a linear regression using gradient descent; you’d get the exact coefficients using linear algebra techniques that are beyond the scope of this book. If you did so, you’d find the equation:</p>&#13;
<div data-type="equation">&#13;
<math display="block">&#13;
  <mrow>&#13;
    <mtext>minutes</mtext>&#13;
    <mo>=</mo>&#13;
    <mn>30</mn>&#13;
    <mo>.</mo>&#13;
    <mn>58</mn>&#13;
    <mo>+</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>972</mn>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>friends</mtext>&#13;
    <mo>-</mo>&#13;
    <mn>1</mn>&#13;
    <mo>.</mo>&#13;
    <mn>87</mn>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>work</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>hours</mtext>&#13;
    <mo>+</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>923</mn>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>phd</mtext>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>which is pretty close to what we found.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Interpreting the Model" data-type="sect1"><div class="sect1" id="idm45635738124568">&#13;
<h1>Interpreting the Model</h1>&#13;
&#13;
<p>You<a data-primary="multiple regression" data-secondary="model interpretation" data-type="indexterm" id="idm45635737622616"/> should think of the coefficients of the model as representing all-else-being-equal estimates&#13;
of the impacts of each factor.  All else being equal, each additional friend corresponds to an&#13;
extra minute spent on the site each day.  All else being equal, each additional hour in a user’s workday&#13;
corresponds to about two fewer minutes spent on the site each day.  All else being equal, having a PhD&#13;
is associated with spending an extra minute on the site each day.</p>&#13;
&#13;
<p>What this doesn’t (directly) tell us is anything about the interactions among the variables.&#13;
It’s possible that the effect of work hours is different for people with many friends&#13;
than it is for people with few friends.  This model doesn’t capture that.  One way to handle&#13;
this case is to introduce a new variable that is the <em>product</em> of “friends” and “work hours.”  This effectively&#13;
allows the “work hours” coefficient to increase (or decrease) as the number of friends increases.</p>&#13;
&#13;
<p>Or it’s possible that the more friends you have, the more time you spend on the site <em>up to a point</em>,&#13;
after which further friends cause you to spend less time on the site.  (Perhaps with too many friends&#13;
the experience is just too overwhelming?)  We could try to capture this in our model by adding another variable&#13;
that’s the <em>square</em> of the number of friends.</p>&#13;
&#13;
<p>Once we start adding variables, we need to worry about whether their coefficients “matter.”  There are no limits to the&#13;
numbers of products, logs, squares, and higher powers we could add.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Goodness of Fit" data-type="sect1"><div class="sect1" id="idm45635737617432">&#13;
<h1>Goodness of Fit</h1>&#13;
&#13;
<p>Again<a data-primary="multiple regression" data-secondary="goodness of fit" data-type="indexterm" id="idm45635737616024"/><a data-primary="coefficient of determination" data-type="indexterm" id="idm45635737615016"/><a data-primary="R-squared" data-type="indexterm" id="idm45635737614376"/> we can look at the R-squared:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.simple_linear_regression</code> <code class="kn">import</code> <code class="n">total_sum_of_squares</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">multiple_r_squared</code><code class="p">(</code><code class="n">xs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">],</code> <code class="n">ys</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="n">sum_of_squared_errors</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">error</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>&#13;
                                <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">))</code>&#13;
    <code class="k">return</code> <code class="mf">1.0</code> <code class="o">-</code> <code class="n">sum_of_squared_errors</code> <code class="o">/</code> <code class="n">total_sum_of_squares</code><code class="p">(</code><code class="n">ys</code><code class="p">)</code></pre>&#13;
&#13;
<p>which has now increased to 0.68:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">assert</code> <code class="mf">0.67</code> <code class="o">&lt;</code> <code class="n">multiple_r_squared</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.68</code></pre>&#13;
&#13;
<p>Keep in mind, however, that adding new variables to a regression will&#13;
<em>necessarily</em> increase the R-squared.  After all, the simple regression model is just the special case of the multiple regression model where the coefficients on “work hours” and “PhD” both equal 0.  The optimal multiple regression model will necessarily have an error at least as small as that one.</p>&#13;
&#13;
<p>Because of this, in a multiple regression, we also need to look at the <em>standard errors</em> of the coefficients, which measure how certain we are about our estimates of each <math>&#13;
  <msub><mi>β</mi> <mi>i</mi> </msub>&#13;
</math>. The regression as a whole may fit our data very well, but if some of the independent variables are correlated (or irrelevant), their coefficients might not <em>mean</em> much.</p>&#13;
&#13;
<p>The typical approach to measuring these errors starts with another assumption—that the errors &#13;
<math>&#13;
  <msub><mi>ε</mi> <mi>i</mi> </msub>&#13;
</math> are independent normal random variables with mean 0 and some shared (unknown) standard deviation <math>&#13;
  <mi>σ</mi>&#13;
</math>.  In that case, we (or, more likely, our statistical software) can use some linear algebra to find the standard error of each coefficient.  The larger it is, the less sure our model is about that coefficient.  Unfortunately, we’re not set up to do that kind of linear algebra from scratch.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Digression: The Bootstrap" data-type="sect1"><div class="sect1" id="the_bootstrap">&#13;
<h1>Digression: The Bootstrap</h1>&#13;
&#13;
<p>Imagine<a data-primary="bootstrapping" data-type="indexterm" id="idm45635737415336"/><a data-primary="multiple regression" data-secondary="bootstrapping new datasets" data-type="indexterm" id="idm45635737414600"/> that we have a sample of <em>n</em> data points, generated by some (unknown to us) distribution:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">data</code> <code class="o">=</code> <code class="n">get_sample</code><code class="p">(</code><code class="n">num_points</code><code class="o">=</code><code class="n">n</code><code class="p">)</code></pre>&#13;
&#13;
<p>In <a data-type="xref" href="ch05.html#statistics">Chapter 5</a>, we wrote a function that could compute the <code>median</code> of the sample,&#13;
which we can use as an estimate of the median of the distribution itself.</p>&#13;
&#13;
<p>But how confident can we be about our estimate? If all the data points in the sample are very close to 100, then it seems likely that the actual median is close to 100.  If approximately half the data points in the sample are close to 0 and the other half are close to 200, then we can’t be nearly as certain about the median.</p>&#13;
&#13;
<p>If we could repeatedly get new samples, we could compute the medians of many samples and look at the distribution of those medians.  Often we can’t.  In that case we can <em>bootstrap</em> new datasets by choosing <em>n</em> data points <em>with replacement</em> from our data. And then we can compute the medians of those synthetic datasets:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">TypeVar</code><code class="p">,</code> <code class="n">Callable</code>&#13;
&#13;
<code class="n">X</code> <code class="o">=</code> <code class="n">TypeVar</code><code class="p">(</code><code class="s1">'X'</code><code class="p">)</code>        <code class="c1"># Generic type for data</code>&#13;
<code class="n">Stat</code> <code class="o">=</code> <code class="n">TypeVar</code><code class="p">(</code><code class="s1">'Stat'</code><code class="p">)</code>  <code class="c1"># Generic type for "statistic"</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">bootstrap_sample</code><code class="p">(</code><code class="n">data</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">]:</code>&#13;
    <code class="sd">"""randomly samples len(data) elements with replacement"""</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">data</code><code class="p">)</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">data</code><code class="p">]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">bootstrap_statistic</code><code class="p">(</code><code class="n">data</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">],</code>&#13;
                        <code class="n">stats_fn</code><code class="p">:</code> <code class="n">Callable</code><code class="p">[[</code><code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">]],</code> <code class="n">Stat</code><code class="p">],</code>&#13;
                        <code class="n">num_samples</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">Stat</code><code class="p">]:</code>&#13;
    <code class="sd">"""evaluates stats_fn on num_samples bootstrap samples from data"""</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">stats_fn</code><code class="p">(</code><code class="n">bootstrap_sample</code><code class="p">(</code><code class="n">data</code><code class="p">))</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_samples</code><code class="p">)]</code></pre>&#13;
&#13;
<p>For example, consider the two following datasets:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># 101 points all very close to 100</code>&#13;
<code class="n">close_to_100</code> <code class="o">=</code> <code class="p">[</code><code class="mf">99.5</code> <code class="o">+</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">101</code><code class="p">)]</code>&#13;
&#13;
<code class="c1"># 101 points, 50 of them near 0, 50 of them near 200</code>&#13;
<code class="n">far_from_100</code> <code class="o">=</code> <code class="p">([</code><code class="mf">99.5</code> <code class="o">+</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()]</code> <code class="o">+</code>&#13;
                <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">50</code><code class="p">)]</code> <code class="o">+</code>&#13;
                <code class="p">[</code><code class="mi">200</code> <code class="o">+</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">50</code><code class="p">)])</code></pre>&#13;
&#13;
<p>If you compute the <code>median</code>s of the two datasets, both will be very close to 100.  However, if you look at:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.statistics</code> <code class="kn">import</code> <code class="n">median</code><code class="p">,</code> <code class="n">standard_deviation</code>&#13;
&#13;
<code class="n">medians_close</code> <code class="o">=</code> <code class="n">bootstrap_statistic</code><code class="p">(</code><code class="n">close_to_100</code><code class="p">,</code> <code class="n">median</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code></pre>&#13;
&#13;
<p>you will mostly see numbers really close to 100. But if you look at:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">medians_far</code> <code class="o">=</code> <code class="n">bootstrap_statistic</code><code class="p">(</code><code class="n">far_from_100</code><code class="p">,</code> <code class="n">median</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code></pre>&#13;
&#13;
<p>you will see a lot of numbers close to 0 and a lot of numbers close to 200.</p>&#13;
&#13;
<p>The <code>standard_deviation</code> of the first set of medians is close to 0, while that of the second set of medians is close to 100:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">assert</code> <code class="n">standard_deviation</code><code class="p">(</code><code class="n">medians_close</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">standard_deviation</code><code class="p">(</code><code class="n">medians_far</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">90</code></pre>&#13;
&#13;
<p>(This extreme a case would be pretty easy to figure out by manually inspecting the data, but in general that won’t be true.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Standard Errors of Regression Coefficients" data-type="sect1"><div class="sect1" id="idm45635737416680">&#13;
<h1>Standard Errors of Regression Coefficients</h1>&#13;
&#13;
<p>We<a data-primary="multiple regression" data-secondary="standard errors of regression coefficients" data-type="indexterm" id="idm45635736997752"/><a data-primary="regression coefficients" data-type="indexterm" id="idm45635736996648"/><a data-primary="standard errors" data-type="indexterm" id="idm45635736995976"/> can take the same approach to estimating the standard errors of our regression coefficients.  We repeatedly take a <code>bootstrap_sample</code> of our data and estimate <code>beta</code> based on that sample.  If the coefficient corresponding to one of the independent variables (say, <code>num_friends</code>) doesn’t vary much across samples, then we can be confident that our estimate is relatively tight.  If the coefficient varies greatly across samples, then we can’t be at all confident in our estimate.</p>&#13;
&#13;
<p>The only subtlety is that, before sampling, we’ll need to <code>zip</code> our <code>x</code> data and <code>y</code> data to make sure that corresponding values of the independent and dependent variables are sampled together.  This means that <code>bootstrap_sample</code> will return a list of pairs <code>(x_i, y_i)</code>, which we’ll need to reassemble into an <code>x_sample</code> and a <code>y_sample</code>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Tuple</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">datetime</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">estimate_sample_beta</code><code class="p">(</code><code class="n">pairs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Tuple</code><code class="p">[</code><code class="n">Vector</code><code class="p">,</code> <code class="nb">float</code><code class="p">]]):</code>&#13;
    <code class="n">x_sample</code> <code class="o">=</code> <code class="p">[</code><code class="n">x</code> <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">pairs</code><code class="p">]</code>&#13;
    <code class="n">y_sample</code> <code class="o">=</code> <code class="p">[</code><code class="n">y</code> <code class="k">for</code> <code class="n">_</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">pairs</code><code class="p">]</code>&#13;
    <code class="n">beta</code> <code class="o">=</code> <code class="n">least_squares_fit</code><code class="p">(</code><code class="n">x_sample</code><code class="p">,</code> <code class="n">y_sample</code><code class="p">,</code> <code class="n">learning_rate</code><code class="p">,</code> <code class="mi">5000</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="s2">"bootstrap sample"</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">beta</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code> <code class="c1"># so that you get the same results as me</code>&#13;
&#13;
<code class="c1"># This will take a couple of minutes!</code>&#13;
<code class="n">bootstrap_betas</code> <code class="o">=</code> <code class="n">bootstrap_statistic</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">)),</code>&#13;
                                      <code class="n">estimate_sample_beta</code><code class="p">,</code>&#13;
                                      <code class="mi">100</code><code class="p">)</code></pre>&#13;
&#13;
<p>After which we can estimate the standard deviation of each coefficient:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">bootstrap_standard_errors</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="n">standard_deviation</code><code class="p">([</code><code class="n">beta</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">beta</code> <code class="ow">in</code> <code class="n">bootstrap_betas</code><code class="p">])</code>&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">)]</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="n">bootstrap_standard_errors</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># [1.272,    # constant term, actual error = 1.19</code>&#13;
<code class="c1">#  0.103,    # num_friends,   actual error = 0.080</code>&#13;
<code class="c1">#  0.155,    # work_hours,    actual error = 0.127</code>&#13;
<code class="c1">#  1.249]    # phd,           actual error = 0.998</code></pre>&#13;
&#13;
<p>(We would likely get better estimates if we collected more than 100 samples&#13;
and used more than 5,000 iterations to estimate each <code>beta</code>, but we don’t have all day.)</p>&#13;
&#13;
<p>We can use these to test hypotheses such as “does <math>&#13;
  <msub><mi>β</mi> <mi>i</mi> </msub>&#13;
</math> equal 0?”  Under the null hypothesis <math>&#13;
  <mrow>&#13;
    <msub><mi>β</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math> (and with our other assumptions about the distribution of <math>&#13;
  <msub><mi>ε</mi> <mi>i</mi> </msub>&#13;
</math>), the statistic:</p>&#13;
<div data-type="equation">&#13;
<math alttext="t Subscript j Baseline equals ModifyingAbove beta Subscript j Baseline With caret slash ModifyingAbove sigma Subscript j Baseline With caret" display="block">&#13;
  <mrow>&#13;
    <msub><mi>t</mi> <mi>j</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mover accent="true"><msub><mi>β</mi> <mi>j</mi> </msub> <mo>^</mo></mover>&#13;
    <mo>/</mo>&#13;
    <mover accent="true"><msub><mi>σ</mi> <mi>j</mi> </msub> <mo>^</mo></mover>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>which is our estimate of <math>&#13;
  <msub><mi>β</mi> <mi>j</mi> </msub>&#13;
</math> divided by our estimate of its standard error, follows<a data-primary="Student’s t-distribution" data-type="indexterm" id="idm45635736827416"/> a <em>Student’s t-distribution</em> with “<math>&#13;
  <mrow>&#13;
    <mi>n</mi>&#13;
    <mo>-</mo>&#13;
    <mi>k</mi>&#13;
  </mrow>&#13;
</math> degrees of freedom.”</p>&#13;
<!--bad quote in front of "n-k degrees of freedom"-->&#13;
&#13;
<p>If we had a <code>students_t_cdf</code> function, we could compute <em>p</em>-values for each least-squares coefficient to indicate how likely we would be to observe such a value if the actual coefficient were 0.&#13;
Unfortunately, we don’t have such a function. (Although we would if we weren’t working from scratch.)</p>&#13;
&#13;
<p>However, as the degrees of freedom get large, the <em>t</em>-distribution gets closer and closer to a standard normal.  In a situation like this, where <em>n</em> is much larger than <em>k</em>, we can use <code>normal_cdf</code> and still feel good about ourselves:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.probability</code> <code class="kn">import</code> <code class="n">normal_cdf</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">p_value</code><code class="p">(</code><code class="n">beta_hat_j</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">sigma_hat_j</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="n">beta_hat_j</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>&#13;
        <code class="c1"># if the coefficient is positive, we need to compute twice the</code>&#13;
        <code class="c1"># probability of seeing an even *larger* value</code>&#13;
        <code class="k">return</code> <code class="mi">2</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">normal_cdf</code><code class="p">(</code><code class="n">beta_hat_j</code> <code class="o">/</code> <code class="n">sigma_hat_j</code><code class="p">))</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="c1"># otherwise twice the probability of seeing a *smaller* value</code>&#13;
        <code class="k">return</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">normal_cdf</code><code class="p">(</code><code class="n">beta_hat_j</code> <code class="o">/</code> <code class="n">sigma_hat_j</code><code class="p">)</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">p_value</code><code class="p">(</code><code class="mf">30.58</code><code class="p">,</code> <code class="mf">1.27</code><code class="p">)</code>   <code class="o">&lt;</code> <code class="mf">0.001</code>  <code class="c1"># constant term</code>&#13;
<code class="k">assert</code> <code class="n">p_value</code><code class="p">(</code><code class="mf">0.972</code><code class="p">,</code> <code class="mf">0.103</code><code class="p">)</code>  <code class="o">&lt;</code> <code class="mf">0.001</code>  <code class="c1"># num_friends</code>&#13;
<code class="k">assert</code> <code class="n">p_value</code><code class="p">(</code><code class="o">-</code><code class="mf">1.865</code><code class="p">,</code> <code class="mf">0.155</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.001</code>  <code class="c1"># work_hours</code>&#13;
<code class="k">assert</code> <code class="n">p_value</code><code class="p">(</code><code class="mf">0.923</code><code class="p">,</code> <code class="mf">1.249</code><code class="p">)</code>  <code class="o">&gt;</code> <code class="mf">0.4</code>    <code class="c1"># phd</code></pre>&#13;
&#13;
<p>(In a situation <em>not</em> like this, we would probably be using statistical software that knows how to compute the <em>t</em>-distribution, as well as how to compute the exact standard errors.)</p>&#13;
&#13;
<p>While most of the coefficients have very small <em>p</em>-values (suggesting that they are indeed nonzero), the coefficient for “PhD” is not “significantly” different from 0, which makes it likely that the coefficient for “PhD” is random rather than meaningful.</p>&#13;
&#13;
<p>In more elaborate regression scenarios, you sometimes want to test more elaborate hypotheses about the data, such as “at least one of the <math>&#13;
  <msub><mi>β</mi> <mi>j</mi> </msub>&#13;
</math> is nonzero” or “<math>&#13;
  <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
</math> equals <math>&#13;
  <msub><mi>β</mi> <mn>2</mn> </msub>&#13;
</math> <em>and</em> <math>&#13;
  <msub><mi>β</mi> <mn>3</mn> </msub>&#13;
</math> equals <math>&#13;
  <msub><mi>β</mi> <mn>4</mn> </msub>&#13;
</math>.” You can do this with an <em>F-test</em>, but alas, that falls outside the scope of this book.</p>&#13;
<!--the quotes in the previous sentence are all messed up-->&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Regularization" data-type="sect1"><div class="sect1" id="regularization">&#13;
<h1>Regularization</h1>&#13;
&#13;
<p>In<a data-primary="multiple regression" data-secondary="regularization" data-type="indexterm" id="idm45635736653096"/><a data-primary="regularization" data-type="indexterm" id="idm45635736652088"/> practice, you’d often like to apply linear regression&#13;
to datasets with large numbers of variables.  This creates&#13;
a couple of extra wrinkles.  First, the more variables&#13;
you use, the more likely you are to overfit your model to the training set.&#13;
And second, the more nonzero coefficients you have, the harder it is to make sense&#13;
of them.  If the goal is to <em>explain</em> some phenomenon, a sparse model with three factors&#13;
might be more useful than a slightly better model with hundreds.</p>&#13;
&#13;
<p><em>Regularization</em> is an approach in which we add to the error term&#13;
a penalty that gets larger as <code>beta</code> gets larger.  We then minimize&#13;
the combined error and penalty.  The more importance&#13;
we place on the penalty term, the more we discourage large coefficients.</p>&#13;
&#13;
<p>For example, in <em>ridge regression</em>,&#13;
we add a penalty proportional to the sum of the squares of the <code>beta_i</code> (except that typically we don’t penalize <code>beta_0</code>, the constant term):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># alpha is a *hyperparameter* controlling how harsh the penalty is.</code>&#13;
<code class="c1"># Sometimes it's called "lambda" but that already means something in Python.</code>&#13;
<code class="k">def</code> <code class="nf">ridge_penalty</code><code class="p">(</code><code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">alpha</code> <code class="o">*</code> <code class="n">dot</code><code class="p">(</code><code class="n">beta</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">beta</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">squared_error_ridge</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code>&#13;
                        <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                        <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code>&#13;
                        <code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""estimate error plus ridge penalty on beta"""</code>&#13;
    <code class="k">return</code> <code class="n">error</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code> <code class="o">+</code> <code class="n">ridge_penalty</code><code class="p">(</code><code class="n">beta</code><code class="p">,</code> <code class="n">alpha</code><code class="p">)</code></pre>&#13;
&#13;
<p>We can then plug this into gradient descent in the usual way:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">add</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">ridge_penalty_gradient</code><code class="p">(</code><code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="sd">"""gradient of just the ridge penalty"""</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="mf">0.</code><code class="p">]</code> <code class="o">+</code> <code class="p">[</code><code class="mi">2</code> <code class="o">*</code> <code class="n">alpha</code> <code class="o">*</code> <code class="n">beta_j</code> <code class="k">for</code> <code class="n">beta_j</code> <code class="ow">in</code> <code class="n">beta</code><code class="p">[</code><code class="mi">1</code><code class="p">:]]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sqerror_ridge_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code>&#13;
                           <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                           <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code>&#13;
                           <code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    the gradient corresponding to the ith squared error term</code>&#13;
<code class="sd">    including the ridge penalty</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="n">add</code><code class="p">(</code><code class="n">sqerror_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">),</code>&#13;
               <code class="n">ridge_penalty_gradient</code><code class="p">(</code><code class="n">beta</code><code class="p">,</code> <code class="n">alpha</code><code class="p">))</code></pre>&#13;
&#13;
<p>And then we just need to modify the <code>least_squares_fit</code> function&#13;
to use the <code>sqerror_ridge_gradient</code> instead of <code>sqerror_gradient</code>.&#13;
(I’m not going to repeat the code here.)</p>&#13;
&#13;
<p>With <code>alpha</code> set to 0, there’s no penalty at all and we get the same results as before:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">beta_0</code> <code class="o">=</code> <code class="n">least_squares_fit_ridge</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="mf">0.0</code><code class="p">,</code>  <code class="c1"># alpha</code>&#13;
                                 <code class="n">learning_rate</code><code class="p">,</code> <code class="mi">5000</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>&#13;
<code class="c1"># [30.51, 0.97, -1.85, 0.91]</code>&#13;
<code class="k">assert</code> <code class="mi">5</code> <code class="o">&lt;</code> <code class="n">dot</code><code class="p">(</code><code class="n">beta_0</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">beta_0</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code> <code class="o">&lt;</code> <code class="mi">6</code>&#13;
<code class="k">assert</code> <code class="mf">0.67</code> <code class="o">&lt;</code> <code class="n">multiple_r_squared</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="n">beta_0</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.69</code></pre>&#13;
&#13;
<p>As we increase <code>alpha</code>, the goodness of fit gets worse, but the size of <code>beta</code> gets smaller:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">beta_0_1</code> <code class="o">=</code> <code class="n">least_squares_fit_ridge</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">,</code>  <code class="c1"># alpha</code>&#13;
                                   <code class="n">learning_rate</code><code class="p">,</code> <code class="mi">5000</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>&#13;
<code class="c1"># [30.8, 0.95, -1.83, 0.54]</code>&#13;
<code class="k">assert</code> <code class="mi">4</code> <code class="o">&lt;</code> <code class="n">dot</code><code class="p">(</code><code class="n">beta_0_1</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">beta_0_1</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code> <code class="o">&lt;</code> <code class="mi">5</code>&#13;
<code class="k">assert</code> <code class="mf">0.67</code> <code class="o">&lt;</code> <code class="n">multiple_r_squared</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="n">beta_0_1</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.69</code>&#13;
&#13;
<code class="n">beta_1</code> <code class="o">=</code> <code class="n">least_squares_fit_ridge</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code>  <code class="c1"># alpha</code>&#13;
                                 <code class="n">learning_rate</code><code class="p">,</code> <code class="mi">5000</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>&#13;
<code class="c1"># [30.6, 0.90, -1.68, 0.10]</code>&#13;
<code class="k">assert</code> <code class="mi">3</code> <code class="o">&lt;</code> <code class="n">dot</code><code class="p">(</code><code class="n">beta_1</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">beta_1</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code> <code class="o">&lt;</code> <code class="mi">4</code>&#13;
<code class="k">assert</code> <code class="mf">0.67</code> <code class="o">&lt;</code> <code class="n">multiple_r_squared</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="n">beta_1</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.69</code>&#13;
&#13;
<code class="n">beta_10</code> <code class="o">=</code> <code class="n">least_squares_fit_ridge</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code><code class="mi">10</code><code class="p">,</code>  <code class="c1"># alpha</code>&#13;
                                  <code class="n">learning_rate</code><code class="p">,</code> <code class="mi">5000</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>&#13;
<code class="c1"># [28.3, 0.67, -0.90, -0.01]</code>&#13;
<code class="k">assert</code> <code class="mi">1</code> <code class="o">&lt;</code> <code class="n">dot</code><code class="p">(</code><code class="n">beta_10</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">beta_10</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code> <code class="o">&lt;</code> <code class="mi">2</code>&#13;
<code class="k">assert</code> <code class="mf">0.5</code> <code class="o">&lt;</code> <code class="n">multiple_r_squared</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">,</code> <code class="n">beta_10</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.6</code></pre>&#13;
&#13;
<p>In particular, the coefficient on “PhD” vanishes as we increase the penalty,&#13;
which accords with our previous result that it wasn’t significantly&#13;
different from 0.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Usually you’d want to <code>rescale</code> your data before using this approach.&#13;
After all, if you changed years of experience to centuries of experience,&#13;
its least squares coefficient would increase by a factor of 100 and suddenly get penalized much more, even though it’s the same model.</p>&#13;
</div>&#13;
&#13;
<p>Another approach is <em>lasso regression</em>, which uses the penalty:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">lasso_penalty</code><code class="p">(</code><code class="n">beta</code><code class="p">,</code> <code class="n">alpha</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">alpha</code> <code class="o">*</code> <code class="nb">sum</code><code class="p">(</code><code class="nb">abs</code><code class="p">(</code><code class="n">beta_i</code><code class="p">)</code> <code class="k">for</code> <code class="n">beta_i</code> <code class="ow">in</code> <code class="n">beta</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code></pre>&#13;
&#13;
<p>Whereas the ridge penalty shrank the coefficients overall,&#13;
the lasso penalty tends to force coefficients to be 0,&#13;
which makes it good for learning sparse models.&#13;
Unfortunately, it’s not amenable to gradient descent,&#13;
which means that we won’t be able to solve it from scratch.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635736653912">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Regression<a data-primary="multiple regression" data-secondary="resources for learning about" data-type="indexterm" id="idm45635736054056"/> has a rich and expansive theory behind it.&#13;
This is another place where you should consider reading a textbook, or at least a lot of Wikipedia articles.</p>&#13;
</li>&#13;
<li>&#13;
<p>scikit-learn<a data-primary="scikit-learn" data-type="indexterm" id="idm45635736051944"/><a data-primary="linear_model module" data-type="indexterm" id="idm45635736051208"/><a data-primary="multiple regression" data-secondary="tools for" data-type="indexterm" id="idm45635736050536"/> has a <a href="https://scikit-learn.org/stable/modules/linear_model.html"><code>linear_model</code> module</a>&#13;
that provides a <code>LinearRegression</code> model similar to ours, as well as ridge regression, lasso regression, and other types of regularization.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://www.statsmodels.org">Statsmodels</a> is another<a data-primary="Python" data-secondary="statsmodels module" data-type="indexterm" id="idm45635736046552"/><a data-primary="statsmodels" data-type="indexterm" id="idm45635736045544"/> Python module that contains (among other things) linear regression models.<a data-primary="" data-startref="PMmultreg15" data-type="indexterm" id="idm45635736044744"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>