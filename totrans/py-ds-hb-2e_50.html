<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 45. In Depth: Principal Component Analysis" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0509-principal-component-analysis">
<h1><span class="label">Chapter 45. </span>In Depth: Principal Component Analysis</h1>
<p><a data-primary="machine learning" data-secondary="principal component analysis" data-type="indexterm" id="ix_ch45-asciidoc0"/><a data-primary="principal component analysis (PCA)" data-type="indexterm" id="ix_ch45-asciidoc1"/>Up until now, we have been looking in depth at supervised learning
estimators: those estimators that predict labels based on labeled
training data. Here we begin looking at several unsupervised estimators,
which can highlight interesting aspects of the data without reference to
any known labels.</p>
<p>In this chapter we will explore what is perhaps one of the most broadly
used unsupervised algorithms, principal component analysis (PCA). PCA is
fundamentally a dimensionality reduction algorithm, but it can also be
useful as a tool for visualization, noise filtering, feature extraction
and engineering, and much more. After a brief conceptual discussion of
the PCA algorithm, we will explore a couple examples of these further
applications.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code></pre>
<section data-pdf-bookmark="Introducing Principal Component Analysis" data-type="sect1"><div class="sect1" id="ch_0509-principal-component-analysis_introducing-principal-component-analysis">
<h1>Introducing Principal Component Analysis</h1>
<p><a data-primary="principal component analysis (PCA)" data-secondary="basics" data-type="indexterm" id="ix_ch45-asciidoc2"/>Principal component analysis is a fast and flexible unsupervised method
for dimensionality reduction in data, which we saw briefly in
<a data-type="xref" href="ch38.xhtml#section-0502-introducing-scikit-learn">Chapter 38</a>. Its
behavior is easiest to visualize by looking at a two-dimensional
dataset. Consider these 200 points (see <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_4_0">Figure 45-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">200</code><code class="p">))</code><code class="o">.</code><code class="n">T</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">])</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_4_0">
<img alt="output 4 0" height="194" src="assets/output_4_0.png" width="600"/>
<h6><span class="label">Figure 45-1. </span>Data for demonstration of PCA</h6>
</div></figure>
<p>By eye, it is clear that there is a nearly linear relationship between
the <em>x</em> and <em>y</em> variables. This is reminiscent of the linear regression
data we explored in <a data-type="xref" href="ch42.xhtml#section-0506-linear-regression">Chapter 42</a>, but the problem setting here is slightly different: rather
than attempting to <em>predict</em> the <em>y</em> values from the <em>x</em> values, the
unsupervised learning problem attempts to learn about the <em>relationship</em>
between the <em>x</em> and <em>y</em> values.</p>
<p><a data-primary="principal axes" data-type="indexterm" id="ix_ch45-asciidoc3"/>In principal component analysis, this relationship is quantified by
finding a list of the <em>principal axes</em> in the data, and using those axes
to describe the dataset. Using Scikit-Learn’s <code>PCA</code>
estimator, we can compute this as follows:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>
        <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
        <code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code></pre>
<p>The fit learns some quantities from the data, most importantly the
components and explained variance:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="nb">print</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="p">[[</code><code class="o">-</code><code class="mf">0.94446029</code> <code class="o">-</code><code class="mf">0.32862557</code><code class="p">]</code>
         <code class="p">[</code><code class="o">-</code><code class="mf">0.32862557</code>  <code class="mf">0.94446029</code><code class="p">]]</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="nb">print</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="p">[</code><code class="mf">0.7625315</code> <code class="mf">0.0184779</code><code class="p">]</code></pre>
<p>To see what these numbers mean, let’s visualize them as
vectors over the input data, using the components to define the
direction of the vector and the explained variance to define the squared
length of the vector (see <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_11_0">Figure 45-2</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">draw_vector</code><code class="p">(</code><code class="n">v0</code><code class="p">,</code> <code class="n">v1</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
            <code class="n">ax</code> <code class="o">=</code> <code class="n">ax</code> <code class="ow">or</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>
            <code class="n">arrowprops</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">arrowstyle</code><code class="o">=</code><code class="s1">'-&gt;'</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                            <code class="n">shrinkA</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">shrinkB</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">annotate</code><code class="p">(</code><code class="s1">''</code><code class="p">,</code> <code class="n">v1</code><code class="p">,</code> <code class="n">v0</code><code class="p">,</code> <code class="n">arrowprops</code><code class="o">=</code><code class="n">arrowprops</code><code class="p">)</code>

        <code class="c1"># plot data</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.2</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">length</code><code class="p">,</code> <code class="n">vector</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_</code><code class="p">,</code> <code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">):</code>
            <code class="n">v</code> <code class="o">=</code> <code class="n">vector</code> <code class="o">*</code> <code class="mi">3</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">length</code><code class="p">)</code>
            <code class="n">draw_vector</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">mean_</code><code class="p">,</code> <code class="n">pca</code><code class="o">.</code><code class="n">mean_</code> <code class="o">+</code> <code class="n">v</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_11_0">
<img alt="output 11 0" height="200" src="assets/output_11_0.png" width="600"/>
<h6><span class="label">Figure 45-2. </span>Visualization of the principal axes in the data</h6>
</div></figure>
<p>These vectors represent the principal axes of the data, and the length
of each vector is an indication of how “important” that axis is in
describing the distribution of the data—more precisely, it is a measure
of the variance of the data when projected onto that axis. The
projection of each data point onto the principal axes are the principal
components of the data.</p>
<p>If we plot these principal components beside the original data, we see
the plots shown in <a data-type="xref" href="#fig_images_in_0509-pca-rotation">Figure 45-3</a>.</p>
<figure><div class="figure" id="fig_images_in_0509-pca-rotation">
<img alt="05.09 PCA rotation" height="209" src="assets/05.09-PCA-rotation.png" width="600"/>
<h6><span class="label">Figure 45-3. </span>Transformed principal axes in the data<sup><a data-type="noteref" href="ch45.xhtml#idm45858728565136" id="idm45858728565136-marker">1</a></sup></h6>
</div></figure>
<p>This transformation from data axes to principal axes is an <em>affine
transformation</em>, which means it is composed of a translation, rotation,
and uniform scaling.</p>
<p>While this algorithm to find principal components may seem like just a
mathematical curiosity, it turns out to have very far-reaching
applications in the world of machine learning and data exploration.</p>
<section data-pdf-bookmark="PCA as Dimensionality Reduction" data-type="sect2"><div class="sect2" id="ch_0509-principal-component-analysis_pca-as-dimensionality-reduction">
<h2>PCA as Dimensionality Reduction</h2>
<p><a data-primary="dimensionality reduction" data-secondary="principal component analysis" data-type="indexterm" id="idm45858728515568"/><a data-primary="principal component analysis (PCA)" data-secondary="dimensionality reduction" data-type="indexterm" id="idm45858728514720"/>Using PCA for dimensionality reduction involves zeroing out one or more
of the smallest principal components, resulting in a lower-dimensional
projection of the data that preserves the maximal data variance.</p>
<p>Here is an example of using PCA as a dimensionality reduction transform:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">X_pca</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"original shape:   "</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"transformed shape:"</code><code class="p">,</code> <code class="n">X_pca</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">original</code> <code class="n">shape</code><code class="p">:</code>    <code class="p">(</code><code class="mi">200</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
        <code class="n">transformed</code> <code class="n">shape</code><code class="p">:</code> <code class="p">(</code><code class="mi">200</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>
<p>The transformed data has been reduced to a single dimension. To
understand the effect of this dimensionality reduction, we can perform
the inverse transform of this reduced data and plot it along with the
original data (see <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_18_0">Figure 45-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">X_new</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_pca</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.2</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X_new</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.8</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_18_0">
<img alt="output 18 0" height="383" src="assets/output_18_0.png" width="600"/>
<h6><span class="label">Figure 45-4. </span>Visualization of PCA as dimensionality reduction</h6>
</div></figure>
<p>The light points are the original data, while the dark points are the
projected version. This makes clear what a PCA dimensionality reduction
means: the information along the least important principal axis or axes
is removed, leaving only the component(s) of the data with the highest
variance. The fraction of variance that is cut out (proportional to the
spread of points about the line formed in the preceding figure) is
roughly a measure of how much “information” is discarded in this
reduction of dimensionality.<a data-startref="ix_ch45-asciidoc3" data-type="indexterm" id="idm45858728361504"/></p>
<p>This reduced-dimension dataset is in some senses “good enough” to
encode the most important relationships between the points: despite
reducing the number of data features by 50%, the overall relationships
between the data points are mostly preserved.</p>
</div></section>
<section data-pdf-bookmark="PCA for Visualization: Handwritten Digits" data-type="sect2"><div class="sect2" id="ch_0509-principal-component-analysis_pca-for-visualization-handwritten-digits">
<h2>PCA for Visualization: Handwritten Digits</h2>
<p><a data-primary="principal component analysis (PCA)" data-secondary="visualization with" data-type="indexterm" id="idm45858728358640"/>The <a data-primary="optical character recognition" data-secondary="principal component analysis for visualization" data-type="indexterm" id="ix_ch45-asciidoc4"/><a data-primary="principal component analysis (PCA)" data-secondary="handwritten digit example" data-type="indexterm" id="ix_ch45-asciidoc5"/>usefulness of dimensionality reduction may not be entirely apparent
in only two dimensions, but it becomes clear when looking at
high-dimensional data. To see this, let’s take a quick look
at the application of PCA to the digits dataset we worked with in
<a data-type="xref" href="ch44.xhtml#section-0508-random-forests">Chapter 44</a>.</p>
<p>We’ll start by loading the data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
        <code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>
        <code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">64</code><code class="p">)</code></pre>
<p>Recall that the digits dataset consists of 8 × 8–pixel images, meaning
that they are 64-dimensional. To gain some intuition into the
relationships between these points, we can use PCA to project them into
a more manageable number of dimensions, say two:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>  <code class="c1"># project from 64 to 2 dimensions</code>
         <code class="n">projected</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">projected</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">64</code><code class="p">)</code>
         <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code></pre>
<p>We can now plot the first two principal components of each point to
learn about the data, as seen in <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_25_0">Figure 45-5</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">projected</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">projected</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code>
                     <code class="n">c</code><code class="o">=</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code>
                     <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s1">'rainbow'</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'component 1'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'component 2'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">colorbar</code><code class="p">();</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_25_0">
<img alt="output 25 0" height="438" src="assets/output_25_0.png" width="600"/>
<h6><span class="label">Figure 45-5. </span>PCA applied to the handwritten digits data</h6>
</div></figure>
<p>Recall what these components mean: the full data is a 64-dimensional
point cloud, and these points are the projection of each data point
along the directions with the largest variance. Essentially, we have
found the optimal stretch and rotation in 64-dimensional space that
allows us to see the layout of the data in two dimensions, and we have
done this in an unsupervised manner—that is, without reference to the
labels.</p>
</div></section>
<section data-pdf-bookmark="What Do the Components Mean?" data-type="sect2"><div class="sect2" id="ch_0509-principal-component-analysis_what-do-the-components-mean">
<h2>What Do the Components Mean?</h2>
<p><a data-primary="principal component analysis (PCA)" data-secondary="meaning of components" data-type="indexterm" id="ix_ch45-asciidoc6"/>We can go a bit further here, and begin to ask what the reduced
dimensions <em>mean</em>. This meaning can be understood in terms of
combinations of basis vectors. For example, each image in the training
set is defined by a collection of 64 pixel values, which we will call
the vector <math alttext="x">
<mi>x</mi>
</math>:</p>
<div data-type="equation">
<math alttext="x equals left-bracket x 1 comma x 2 comma x 3 ellipsis x 64 right-bracket" display="block">
<mrow>
<mi>x</mi>
<mo>=</mo>
<mo>[</mo>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mn>3</mn> </msub>
<mo>⋯</mo>
<msub><mi>x</mi> <mn>64</mn> </msub>
<mo>]</mo>
</mrow>
</math>
</div>
<p>One way we can think about this is in terms of a pixel basis. That is,
to construct the image, we multiply each element of the vector by the
pixel it describes, and then add the results together to build the
image:</p>
<div data-type="equation">
<math alttext="normal i normal m normal a normal g normal e left-parenthesis x right-parenthesis equals x 1 dot left-parenthesis normal p normal i normal x normal e normal l 1 right-parenthesis plus x 2 dot left-parenthesis normal p normal i normal x normal e normal l 2 right-parenthesis plus x 3 dot left-parenthesis normal p normal i normal x normal e normal l 3 right-parenthesis ellipsis x 64 dot left-parenthesis normal p normal i normal x normal e normal l 64 right-parenthesis" display="block">
<mrow>
<mi> image </mi>
<mrow>
<mo>(</mo>
<mi>x</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>·</mo>
<mrow>
<mo>(</mo>
<mi> pixel </mi>
<mspace width="3.33333pt"/>
<mn>1</mn>
<mo>)</mo>
</mrow>
<mo>+</mo>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>·</mo>
<mrow>
<mo>(</mo>
<mi> pixel </mi>
<mspace width="3.33333pt"/>
<mn>2</mn>
<mo>)</mo>
</mrow>
<mo>+</mo>
<msub><mi>x</mi> <mn>3</mn> </msub>
<mo>·</mo>
<mrow>
<mo>(</mo>
<mi> pixel </mi>
<mspace width="3.33333pt"/>
<mn>3</mn>
<mo>)</mo>
</mrow>
<mo>⋯</mo>
<msub><mi>x</mi> <mn>64</mn> </msub>
<mo>·</mo>
<mrow>
<mo>(</mo>
<mi> pixel </mi>
<mspace width="3.33333pt"/>
<mn>64</mn>
<mo>)</mo>
</mrow>
</mrow>
</math>
</div>
<p>One way we might imagine reducing the dimensionality of this data is to
zero out all but a few of these basis vectors. For example, if we use
only the first eight pixels, we get an eight-dimensional projection of
the data (<a data-type="xref" href="#fig_images_in_0509-digits-pixel-components">Figure 45-6</a>). However, it is not very reflective of
the whole image: we’ve thrown out nearly 90% of the pixels!</p>
<figure><div class="figure" id="fig_images_in_0509-digits-pixel-components">
<img alt="05.09 digits pixel components" height="100" src="assets/05.09-digits-pixel-components.png" width="600"/>
<h6><span class="label">Figure 45-6. </span>A naive dimensionality reduction achieved by discarding pixels<sup><a data-type="noteref" href="ch45.xhtml#idm45858728053776" id="idm45858728053776-marker">2</a></sup></h6>
</div></figure>
<p>The upper row of panels shows the individual pixels, and the lower row
shows the cumulative contribution of these pixels to the construction of
the image. Using only eight of the pixel-basis components, we can only
construct a small portion of the 64-pixel image. Were we to continue
this sequence and use all 64 pixels, we would recover the original
image.</p>
<p>But the pixel-wise representation is not the only choice of basis. We
can also use other basis functions, which each contain some predefined
contribution from each pixel, and write something like:</p>
<div data-type="equation">
<math alttext="i m a g e left-parenthesis x right-parenthesis equals normal m normal e normal a normal n plus x 1 dot left-parenthesis normal b normal a normal s normal i normal s 1 right-parenthesis plus x 2 dot left-parenthesis normal b normal a normal s normal i normal s 2 right-parenthesis plus x 3 dot left-parenthesis normal b normal a normal s normal i normal s 3 right-parenthesis ellipsis" display="block">
<mrow>
<mi>i</mi>
<mi>m</mi>
<mi>a</mi>
<mi>g</mi>
<mi>e</mi>
<mrow>
<mo>(</mo>
<mi>x</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mi> mean </mi>
<mo>+</mo>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>·</mo>
<mrow>
<mo>(</mo>
<mi> basis </mi>
<mspace width="3.33333pt"/>
<mn>1</mn>
<mo>)</mo>
</mrow>
<mo>+</mo>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>·</mo>
<mrow>
<mo>(</mo>
<mi> basis </mi>
<mspace width="3.33333pt"/>
<mn>2</mn>
<mo>)</mo>
</mrow>
<mo>+</mo>
<msub><mi>x</mi> <mn>3</mn> </msub>
<mo>·</mo>
<mrow>
<mo>(</mo>
<mi> basis </mi>
<mspace width="3.33333pt"/>
<mn>3</mn>
<mo>)</mo>
</mrow>
<mo>⋯</mo>
</mrow>
</math>
</div>
<p>PCA can be thought of as a process of choosing optimal basis functions,
such that adding together just the first few of them is enough to
suitably reconstruct the bulk of the elements in the dataset. The
principal components, which act as the low-dimensional representation of
our data, are simply the coefficients that multiply each of the elements
in this series. <a data-type="xref" href="#fig_images_in_0509-digits-pca-components">Figure 45-7</a> shows a similar depiction of
reconstructing the same digit using the mean plus the first eight PCA
basis functions.</p>
<figure><div class="figure" id="fig_images_in_0509-digits-pca-components">
<img alt="05.09 digits pca components" height="98" src="assets/05.09-digits-pca-components.png" width="600"/>
<h6><span class="label">Figure 45-7. </span>A more sophisticated dimensionality reduction achieved by discarding the least important principal components (compare to <a data-type="xref" href="#fig_images_in_0509-digits-pixel-components">Figure 45-6</a>)<sup><a data-type="noteref" href="ch45.xhtml#idm45858728025120" id="idm45858728025120-marker">3</a></sup></h6>
</div></figure>
<p>Unlike the pixel basis, the PCA basis allows us to recover the salient
features of the input image with just a mean, plus eight components! The
amount of each pixel in each component is the corollary of the
orientation of the vector in our two-dimensional example. This is the
sense in which PCA provides a low-dimensional representation of the
data: it discovers a set of basis functions that are more efficient than
the native pixel basis of the input data.<a data-startref="ix_ch45-asciidoc6" data-type="indexterm" id="idm45858728023328"/></p>
</div></section>
<section data-pdf-bookmark="Choosing the Number of Components" data-type="sect2"><div class="sect2" id="ch_0509-principal-component-analysis_choosing-the-number-of-components">
<h2>Choosing the Number of Components</h2>
<p><a data-primary="principal component analysis (PCA)" data-secondary="choosing number of components" data-type="indexterm" id="idm45858728021136"/>A vital part of using PCA in practice is the ability to estimate how
many components are needed to describe the data. <a data-primary="explained variance ratio" data-type="indexterm" id="idm45858728019936"/>This can be determined
by looking at the cumulative <em>explained variance ratio</em> as a function of
the number of components (see <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_34_0">Figure 45-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'number of components'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'cumulative explained variance'</code><code class="p">);</code></pre>
<p>This curve quantifies how much of the total, 64-dimensional variance is
contained within the first <math alttext="upper N">
<mi>N</mi>
</math> components. For example, we
see that with the digits data the first 10 components contain
approximately 75% of the variance, while you need around 50 components
to describe close to 100% of the variance.</p>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_34_0">
<img alt="output 34 0" height="685" src="assets/output_34_0.png" width="600"/>
<h6><span class="label">Figure 45-8. </span>The cumulative explained variance, which measures how well PCA pre‐ serves the content of the data</h6>
</div></figure>
<p>This tells us that our two-dimensional projection loses a lot of
information (as measured by the explained variance) and that
we’d need about 20 components to retain 90% of the variance.
Looking at this plot for a high-dimensional dataset can help you
understand the level of redundancy present in its features.<a data-startref="ix_ch45-asciidoc5" data-type="indexterm" id="idm45858727982640"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="PCA as Noise Filtering" data-type="sect1"><div class="sect1" id="ch_0509-principal-component-analysis_pca-as-noise-filtering">
<h1>PCA as Noise Filtering</h1>
<p><a data-primary="noise filter, PCA as" data-type="indexterm" id="ix_ch45-asciidoc7"/><a data-primary="optical character recognition" data-secondary="principal component analysis as noise filtering" data-type="indexterm" id="ix_ch45-asciidoc8"/><a data-primary="principal component analysis (PCA)" data-secondary="noise filtering" data-type="indexterm" id="ix_ch45-asciidoc9"/>PCA can also be used as a filtering approach for noisy data. The idea is
this: any components with variance much larger than the effect of the
noise should be relatively unaffected by the noise. So, if you
reconstruct the data using just the largest subset of principal
components, you should be preferentially keeping the signal and throwing
out the noise.</p>
<p>Let’s see how this looks with the digits data. First we will
plot several of the input noise-free input samples (<a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_37_0">Figure 45-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">plot_digits</code><code class="p">(</code><code class="n">data</code><code class="p">):</code>
             <code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code>
                                      <code class="n">subplot_kw</code><code class="o">=</code><code class="p">{</code><code class="s1">'xticks'</code><code class="p">:[],</code> <code class="s1">'yticks'</code><code class="p">:[]},</code>
                                      <code class="n">gridspec_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">hspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>
             <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axes</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
                 <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">),</code>
                           <code class="n">cmap</code><code class="o">=</code><code class="s1">'binary'</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'nearest'</code><code class="p">,</code>
                           <code class="n">clim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">16</code><code class="p">))</code>
         <code class="n">plot_digits</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_37_0">
<img alt="output 37 0" height="413" src="assets/output_37_0.png" width="600"/>
<h6><span class="label">Figure 45-9. </span>Digits without noise</h6>
</div></figure>
<p>Now let’s add some random noise to create a noisy dataset,
and replot it (<a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_40_0">Figure 45-10</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
         <code class="n">rng</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="mf">10.609434159508863</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
         <code class="n">noisy</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
         <code class="n">plot_digits</code><code class="p">(</code><code class="n">noisy</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_40_0">
<img alt="output 40 0" height="234" src="assets/output_40_0.png" width="600"/>
<h6><span class="label">Figure 45-10. </span>Digits with Gaussian random noise added</h6>
</div></figure>
<p>The visualization makes the presence of this random noise clear.
Let’s train a PCA model on the noisy data, requesting that
the projection preserve 50% of the variance:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="mf">0.50</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">noisy</code><code class="p">)</code>
         <code class="n">pca</code><code class="o">.</code><code class="n">n_components_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="mi">12</code></pre>
<p>Here 50% of the variance amounts to 12 principal components, out of the
64 original features. Now we compute these components, and then use the
inverse of the transform to reconstruct the filtered digits; <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_44_0">Figure 45-11</a> shows the result.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="n">components</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">noisy</code><code class="p">)</code>
         <code class="n">filtered</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">components</code><code class="p">)</code>
         <code class="n">plot_digits</code><code class="p">(</code><code class="n">filtered</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_44_0">
<img alt="output 44 0" height="234" src="assets/output_44_0.png" width="600"/>
<h6><span class="label">Figure 45-11. </span>Digits “denoised” using PCA</h6>
</div></figure>
<p>This signal preserving/noise filtering property makes PCA a very useful
feature selection routine—for example, rather than training a classifier
on very high-dimensional data, you might instead train the classifier on
the lower-dimensional principal component representation, which will
automatically serve to filter out random noise in the inputs<a data-startref="ix_ch45-asciidoc9" data-type="indexterm" id="idm45858727589136"/><a data-startref="ix_ch45-asciidoc8" data-type="indexterm" id="idm45858727588432"/><a data-startref="ix_ch45-asciidoc7" data-type="indexterm" id="idm45858727587760"/>.<a data-startref="ix_ch45-asciidoc4" data-type="indexterm" id="idm45858727586960"/><a data-startref="ix_ch45-asciidoc2" data-type="indexterm" id="idm45858727586256"/></p>
</div></section>
<section data-pdf-bookmark="Example: Eigenfaces" data-type="sect1"><div class="sect1" id="ch_0509-principal-component-analysis_example-eigenfaces">
<h1>Example: Eigenfaces</h1>
<p><a data-primary="eigenfaces" data-type="indexterm" id="ix_ch45-asciidoc10"/><a data-primary="face recognition" data-secondary="principal component analysis" data-type="indexterm" id="ix_ch45-asciidoc11"/><a data-primary="principal component analysis (PCA)" data-secondary="eigenfaces example" data-type="indexterm" id="ix_ch45-asciidoc12"/><a data-primary="principal component analysis (PCA)" data-secondary="facial recognition example" data-type="indexterm" id="ix_ch45-asciidoc13"/>Earlier we explored an example of using a PCA projection as a feature
selector for facial recognition with a support vector machine (see
<a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>). Here we will take a look back and explore a bit more of what
went into that. Recall that we were using the Labeled Faces in the Wild
(LFW) dataset made available through Scikit-Learn:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_lfw_people</code>
         <code class="n">faces</code> <code class="o">=</code> <code class="n">fetch_lfw_people</code><code class="p">(</code><code class="n">min_faces_per_person</code><code class="o">=</code><code class="mi">60</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">target_names</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">images</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="p">[</code><code class="s1">'Ariel Sharon'</code> <code class="s1">'Colin Powell'</code> <code class="s1">'Donald Rumsfeld'</code> <code class="s1">'George W Bush'</code>
          <code class="s1">'Gerhard Schroeder'</code> <code class="s1">'Hugo Chavez'</code> <code class="s1">'Junichiro Koizumi'</code> <code class="s1">'Tony Blair'</code><code class="p">]</code>
         <code class="p">(</code><code class="mi">1348</code><code class="p">,</code> <code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">)</code></pre>
<p>Let’s take a look at the principal axes that span this
dataset. Because this is a large dataset, we will use the <code>"random"</code>
eigensolver in the <code>PCA</code> estimator: it uses a randomized method to
approximate the first <math alttext="upper N">
<mi>N</mi>
</math> principal components more quickly
than the standard approach, at the expense of some accuracy. This
trade-off can be useful for high-dimensional data (here, a
dimensionality of nearly 3,000). We will take a look at the first 150
components:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="n">svd_solver</code><code class="o">=</code><code class="s1">'randomized'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
         <code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">svd_solver</code><code class="o">=</code><code class="s1">'randomized'</code><code class="p">)</code></pre>
<p>In this case, it can be interesting to visualize the images associated
with the first several principal components (these components are
technically known as <em>eigenvectors</em>, so these types of images are often
called <em>eigenfaces</em>; as you can see in <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_51_0">Figure 45-12</a>, they are as
creepy as they sound):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">9</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code>
                                  <code class="n">subplot_kw</code><code class="o">=</code><code class="p">{</code><code class="s1">'xticks'</code><code class="p">:[],</code> <code class="s1">'yticks'</code><code class="p">:[]},</code>
                                  <code class="n">gridspec_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">hspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>
         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axes</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'bone'</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_51_0">
<img alt="output 51 0" height="564" src="assets/output_51_0.png" width="600"/>
<h6><span class="label">Figure 45-12. </span>A visualization of eigenfaces learned from the LFW dataset</h6>
</div></figure>
<p>The results are very interesting, and give us insight into how the
images vary: for example, the first few eigenfaces (from the top left)
seem to be associated with the angle of lighting on the face, and later
principal vectors seem to be picking out certain features, such as eyes,
noses, and lips. Let’s take a look at the cumulative
variance of these components to see how much of the data information the
projection is preserving (see <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_53_0">Figure 45-13</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'number of components'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'cumulative explained variance'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_53_0">
<img alt="output 53 0" height="399" src="assets/output_53_0.png" width="600"/>
<h6><span class="label">Figure 45-13. </span>Cumulative explained variance for the LFW data</h6>
</div></figure>
<p>The 150 components we have chosen account for just over 90% of the
variance. That would lead us to believe that using these 150 components,
we would recover most of the essential characteristics of the data. To
make this more concrete, we can compare the input images with the images
reconstructed from these 150 components (see <a data-type="xref" href="#fig_0509-principal-component-analysis_files_in_output_56_0">Figure 45-14</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="c1"># Compute the components and projected faces</code>
         <code class="n">pca</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">components</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">projected</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">components</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="c1"># Plot the results</code>
         <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mf">2.5</code><code class="p">),</code>
                                <code class="n">subplot_kw</code><code class="o">=</code><code class="p">{</code><code class="s1">'xticks'</code><code class="p">:[],</code> <code class="s1">'yticks'</code><code class="p">:[]},</code>
                                <code class="n">gridspec_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">hspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>
         <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'binary_r'</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">projected</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'binary_r'</code><code class="p">)</code>

         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'full-dim</code><code class="se">\n</code><code class="s1">input'</code><code class="p">)</code>
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'150-dim</code><code class="se">\n</code><code class="s1">reconstruction'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0509-principal-component-analysis_files_in_output_56_0">
<img alt="output 56 0" height="395" src="assets/output_56_0.png" width="600"/>
<h6><span class="label">Figure 45-14. </span>150-dimensional PCA reconstruction of the LFW data</h6>
</div></figure>
<p>The top row here shows the input images, while the bottom row shows the
reconstruction of the images from just 150 of the ~3,000 initial
features. This visualization makes clear why the PCA feature selection
used in <a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a> was so successful: although it reduces the
dimensionality of the data by nearly a factor of 20, the projected
images contain enough information that we might, by eye, recognize the
individuals in each image. This means our classification algorithm only
needs to be trained on 150-dimensional data rather than
3,000-dimensional data, which, depending on the particular algorithm we
choose, can lead to much more efficient classification.<a data-startref="ix_ch45-asciidoc13" data-type="indexterm" id="idm45858727025136"/><a data-startref="ix_ch45-asciidoc12" data-type="indexterm" id="idm45858727024464"/><a data-startref="ix_ch45-asciidoc11" data-type="indexterm" id="idm45858727023792"/><a data-startref="ix_ch45-asciidoc10" data-type="indexterm" id="idm45858727023120"/></p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch_0509-principal-component-analysis_summary">
<h1>Summary</h1>
<p><a data-primary="principal component analysis (PCA)" data-secondary="strengths/weaknesses" data-type="indexterm" id="idm45858727020864"/>In this chapter we explored the use of principal component analysis for
dimensionality reduction, visualization of high-dimensional data, noise
filtering, and feature selection within high-dimensional data. Because
of its versatility and interpretability, PCA has been shown to be
effective in a wide variety of contexts and disciplines. Given any
high-dimensional dataset, I tend to start with PCA in order to visualize
the relationships between points (as we did with the digits data), to
understand the main variance in the data (as we did with the
eigenfaces), and to understand the intrinsic dimensionality (by plotting
the explained variance ratio). Certainly PCA is not useful for every
high-dimensional dataset, but it offers a straightforward and efficient
path to gaining insight into high-dimensional data.</p>
<p><a data-primary="outliers, PCA and" data-type="indexterm" id="idm45858727019536"/>PCA’s main weakness is that it tends to be highly affected
by outliers in the data. For this reason, several robust variants of PCA
have been developed, many of which act to iteratively discard data
points that are poorly described by the initial components. Scikit-Learn
includes a number of interesting variants on PCA in the
<code>sklearn​.decom⁠position</code> submodule; one example is <code>SparsePCA</code>, which
introduces a regularization term (see
<a data-type="xref" href="ch42.xhtml#section-0506-linear-regression">Chapter 42</a>) that
serves to enforce sparsity of the components.</p>
<p>In the following chapters, we will look at other unsupervised learning
methods that build on some of the ideas of PCA.<a data-startref="ix_ch45-asciidoc1" data-type="indexterm" id="idm45858727016624"/><a data-startref="ix_ch45-asciidoc0" data-type="indexterm" id="idm45858727015920"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858728565136"><sup><a href="ch45.xhtml#idm45858728565136-marker">1</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/VmpjC">online appendix</a>.</p><p data-type="footnote" id="idm45858728053776"><sup><a href="ch45.xhtml#idm45858728053776-marker">2</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/ixfc1">online appendix</a>.</p><p data-type="footnote" id="idm45858728025120"><sup><a href="ch45.xhtml#idm45858728025120-marker">3</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/WSe0T">online appendix</a>.</p></div></div></section></div></body></html>