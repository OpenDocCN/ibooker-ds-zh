- en: Chapter 5\. Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data scientists are often tasked with automating decisions for business problems.
    Is an email an attempt at phishing? Is a customer likely to churn? Is the web
    user likely to click on an advertisement? These are all *classification* problems,
    a form of *supervised learning* in which we first train a model on data where
    the outcome is known and then apply the model to data where the outcome is not
    known. Classification is perhaps the most important form of prediction: the goal
    is to predict whether a record is a 1 or a 0 (phishing/not-phishing, click/don’t
    click, churn/don’t churn), or in some cases, one of several categories (for example,
    Gmail’s filtering of your inbox into “primary,” “social,” “promotional,” or “forums”).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, we need more than a simple binary classification: we want to know the
    predicted probability that a case belongs to a class. Rather than having a model
    simply assign a binary classification, most algorithms can return a probability
    score (propensity) of belonging to the class of interest. In fact, with logistic
    regression, the default output from *R* is on the log-odds scale, and this must
    be transformed to a propensity. In *Python*’s `scikit-learn`, logistic regression,
    like most classification methods, provides two prediction methods: `predict` (which
    returns the class) and `predict_proba` (which returns probabilities for each class).
    A sliding cutoff can then be used to convert the propensity score to a decision.
    The general approach is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish a cutoff probability for the class of interest, above which we consider
    a record as belonging to that class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate (with any model) the probability that a record belongs to the class
    of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If that probability is above the cutoff probability, assign the new record to
    the class of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The higher the cutoff, the fewer the records predicted as 1—that is, as belonging
    to the class of interest. The lower the cutoff, the more the records predicted
    as 1.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers several key techniques for classification and estimating
    propensities; additional methods that can be used both for classification and
    for numerical prediction are described in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The naive Bayes algorithm uses the probability of observing predictor values,
    given an outcome, to estimate what is really of interest: the probability of observing
    outcome *Y = i*, given a set of predictor values.^([1](ch05.xhtml#idm46522850239256))'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand naive Bayesian classification, we can start out by imagining
    complete or exact Bayesian classification. For each record to be classified:'
  prefs: []
  type: TYPE_NORMAL
- en: Find all the other records with the same predictor profile (i.e., where the
    predictor values are the same).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine what classes those records belong to and which class is most prevalent
    (i.e., probable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign that class to the new record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding approach amounts to finding all the records in the sample that
    are exactly like the new record to be classified in the sense that all the predictor
    values are identical.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Predictor variables must be categorical (factor) variables in the standard naive
    Bayes algorithm. See [“Numeric Predictor Variables”](#NumericPredictors) for two
    workarounds for using continuous variables.
  prefs: []
  type: TYPE_NORMAL
- en: Why Exact Bayesian Classification Is Impractical
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the number of predictor variables exceeds a handful, many of the records
    to be classified will be without exact matches. Consider a model to predict voting
    on the basis of demographic variables. Even a sizable sample may not contain even
    a single match for a new record who is a male Hispanic with high income from the
    US Midwest who voted in the last election, did not vote in the prior election,
    has three daughters and one son, and is divorced. And this is with just eight
    variables, a small number for most classification problems. The addition of just
    a single new variable with five equally frequent categories reduces the probability
    of a match by a factor of 5.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the naive Bayes solution, we no longer restrict the probability calculation
    to those records that match the record to be classified. Instead, we use the entire
    data set. The naive Bayes modification is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For a binary response *Y = i* (*i* = 0 or 1), estimate the individual conditional
    probabilities for each predictor <math alttext="upper P left-parenthesis upper
    X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> ; these are the probabilities that the predictor
    value is in the record when we observe *Y = i*. This probability is estimated
    by the proportion of *X[j]* values among the *Y = i* records in the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply these probabilities by each other, and then by the proportion of records
    belonging to *Y = i*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 and 2 for all the classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate a probability for outcome *i* by taking the value calculated in step
    2 for class *i* and dividing it by the sum of such values for all classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the record to the class with the highest probability for this set of
    predictor values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This naive Bayes algorithm can also be stated as an equation for the probability
    of observing outcome *Y = i*, given a set of predictor values <math alttext="upper
    X 1 comma ellipsis comma upper X Subscript p Baseline"><mrow><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full formula for calculating class probabilities using exact Bayes
    classification:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub>
    <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo stretchy="false">)</mo></mrow> <mrow><mi>P</mi> <mo stretchy="false">(</mo>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>X</mi>
    <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mo>+</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo>
    <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>…</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the naive Bayes assumption of conditional independence, this equation
    changes into:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo stretchy="false">)</mo></mrow>
    <mrow><mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>0</mn>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo stretchy="false">)</mo>
    <mo>+</mo> <mi>P</mi> <mo stretchy="false">(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn>
    <mo stretchy="false">)</mo> <mi>P</mi> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>…</mo> <mi>P</mi>
    <mo stretchy="false">(</mo> <msub><mi>X</mi> <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: Why is this formula called “naive”? We have made a simplifying assumption that
    the *exact conditional probability* of a vector of predictor values, given observing
    an outcome, is sufficiently well estimated by the product of the individual conditional
    probabilities <math alttext="upper P left-parenthesis upper X Subscript j Baseline
    vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math>
    . In other words, in estimating <math alttext="upper P left-parenthesis upper
    X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> instead of <math alttext="upper P left-parenthesis
    upper X 1 comma upper X 2 comma ellipsis upper X Subscript p Baseline vertical-bar
    upper Y equals i right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <msub><mi>X</mi> <mi>p</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi>
    <mo>)</mo></mrow></math> , we are assuming <math alttext="upper X Subscript j"><msub><mi>X</mi>
    <mi>j</mi></msub></math> is *independent* of all the other predictor variables
    <math alttext="upper X Subscript k"><msub><mi>X</mi> <mi>k</mi></msub></math>
    for <math alttext="k not-equals j"><mrow><mi>k</mi> <mo>≠</mo> <mi>j</mi></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Several packages in *R* can be used to estimate a naive Bayes model. The following
    fits a model to the loan payment data using the `klaR` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output from the model is the conditional probabilities <math alttext="upper
    P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub>
    <mo>|</mo> <mi>Y</mi> <mo>=</mo> <mi>i</mi> <mo>)</mo></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python* we can use `sklearn.naive_bayes.MultinomialNB` from `scikit-learn`.
    We need to convert the categorical features to dummy variables before we fit the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It is possible to derive the conditional probabilities from the fitted model
    using the property `feature_log_prob_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model can be used to predict the outcome of a new loan. We use the last
    value of the data set for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we get this value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the model predicts a default (*R*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed, `scikit-learn`’s classification models have two methods—`predict`,
    which returns the predicted class, and `predict_proba`, which returns the class
    probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The prediction also returns a `posterior` estimate of the probability of default.
    The naive Bayesian classifier is known to produce *biased* estimates. However,
    where the goal is to *rank* records according to the probability that *Y* = 1,
    unbiased estimates of probability are not needed, and naive Bayes produces good
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Numeric Predictor Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Bayesian classifier works only with categorical predictors (e.g., with
    spam classification, where the presence or absence of words, phrases, characters,
    and so on lies at the heart of the predictive task). To apply naive Bayes to numerical
    predictors, one of two approaches must be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: Bin and convert the numerical predictors to categorical predictors and apply
    the algorithm of the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a probability model—for example, the normal distribution (see [“Normal Distribution”](ch02.xhtml#NormalDist))—to
    estimate the conditional probability <math alttext="upper P left-parenthesis upper
    X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>j</mi></msub> <mo>|</mo> <mi>Y</mi> <mo>=</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When a predictor category is absent in the training data, the algorithm assigns
    *zero probability* to the outcome variable in new data, rather than simply ignoring
    this variable and using the information from other variables, as other methods
    might. Most implementations of Naive Bayes use a smoothing parameter (Laplace
    Smoothing) to prevent this.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*The Elements of Statistical Learning*, 2nd ed., by Trevor Hastie, Robert Tibshirani,
    and Jerome Friedman (Springer, 2009).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a full chapter on naive Bayes in *Data Mining for Business Analytics*
    by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth
    Lichtendahl (Wiley, 2007–2020, with editions for *R*, *Python*, Excel, and JMP).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminant Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Discriminant analysis* is the earliest statistical classifier; it was introduced
    by R. A. Fisher in 1936 in an article published in the *Annals of Eugenics* journal.^([2](ch05.xhtml#idm46522849542200))'
  prefs: []
  type: TYPE_NORMAL
- en: While discriminant analysis encompasses several techniques, the most commonly
    used is *linear discriminant analysis*, or *LDA*. The original method proposed
    by Fisher was actually slightly different from LDA, but the mechanics are essentially
    the same. LDA is now less widely used with the advent of more sophisticated techniques,
    such as tree models and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: However, you may still encounter LDA in some applications, and it has links
    to other more widely used methods (such as principal components analysis; see
    [“Principal Components Analysis”](ch07.xhtml#PCA)).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linear discriminant analysis should not be confused with Latent Dirichlet Allocation,
    also referred to as LDA. Latent Dirichlet Allocation is used in text and natural
    language processing and is unrelated to linear discriminant analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand discriminant analysis, it is first necessary to introduce the
    concept of *covariance* between two or more variables. The covariance measures
    the relationship between two variables <math alttext="x"><mi>x</mi></math> and
    <math alttext="z"><mi>z</mi></math> . Denote the mean for each variable by <math
    alttext="x overbar"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>
    and <math alttext="z overbar"><mover accent="true"><mi>z</mi> <mo>¯</mo></mover></math>
    (see [“Mean”](ch01.xhtml#Mean)). The covariance <math alttext="s Subscript x comma
    z"><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub></math>
    between <math alttext="x"><mi>x</mi></math> and <math alttext="z"><mi>z</mi></math>
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub>
    <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>z</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where *n* is the number of records (note that we divide by *n* – 1 instead of
    *n*; see [“Degrees of Freedom, and *n* or *n* – 1?”](ch01.xhtml#Nminus1)).
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the correlation coefficient (see [“Correlation”](ch01.xhtml#Correlations)),
    positive values indicate a positive relationship and negative values indicate
    a negative relationship. Correlation, however, is constrained to be between –1
    and 1, whereas covariance scale depends on the scale of the variables <math alttext="x"><mi>x</mi></math>
    and <math alttext="z"><mi>z</mi></math> . The *covariance matrix* <math alttext="normal
    upper Sigma"><mi>Σ</mi></math> for <math alttext="x"><mi>x</mi></math> and <math
    alttext="z"><mi>z</mi></math> consists of the individual variable variances, <math
    alttext="s Subscript x Superscript 2"><msubsup><mi>s</mi> <mi>x</mi> <mn>2</mn></msubsup></math>
    and <math alttext="s Subscript z Superscript 2"><msubsup><mi>s</mi> <mi>z</mi>
    <mn>2</mn></msubsup></math> , on the diagonal (where row and column are the same
    variable) and the covariances between variable pairs on the off-diagonals:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>Σ</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msubsup><mi>s</mi> <mi>x</mi>
    <mn>2</mn></msubsup></mtd> <mtd><msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>s</mi> <mrow><mi>z</mi><mo>,</mo><mi>x</mi></mrow></msub></mtd>
    <mtd><msubsup><mi>s</mi> <mi>z</mi> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recall that the standard deviation is used to normalize a variable to a *z*-score;
    the covariance matrix is used in a multivariate extension of this standardization
    process. This is known as Mahalanobis distance (see [“Other Distance Metrics”](ch06.xhtml#Mahalanobis))
    and is related to the LDA function.
  prefs: []
  type: TYPE_NORMAL
- en: Fisher’s Linear Discriminant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For simplicity, let’s focus on a classification problem in which we want to
    predict a binary outcome *y* using just two continuous numeric variables <math
    alttext="left-parenthesis x comma z right-parenthesis"><mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow></math> . Technically, discriminant analysis
    assumes the predictor variables are normally distributed continuous variables,
    but, in practice, the method works well even for nonextreme departures from normality,
    and for binary predictors. Fisher’s linear discriminant distinguishes variation
    *between* groups, on the one hand, from variation *within* groups on the other.
    Specifically, seeking to divide the records into two groups, linear discriminant
    analysis (LDA) focuses on maximizing the “between” sum of squares <math alttext="normal
    upper S normal upper S Subscript normal b normal e normal t normal w normal e
    normal e normal n"><msub><mi>SS</mi> <mi>between</mi></msub></math> (measuring
    the variation between the two groups) relative to the “within” sum of squares
    <math alttext="normal upper S normal upper S Subscript normal w normal i normal
    t normal h normal i normal n"><msub><mi>SS</mi> <mi>within</mi></msub></math>
    (measuring the within-group variation). In this case, the two groups correspond
    to the records <math alttext="left-parenthesis x 0 comma z 0 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>z</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></math> for which *y* = 0 and the records <math alttext="left-parenthesis
    x 1 comma z 1 right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>z</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math> for which
    *y* = 1. The method finds the linear combination <math alttext="w Subscript x
    Baseline x plus w Subscript z Baseline z"><mrow><msub><mi>w</mi> <mi>x</mi></msub>
    <mi>x</mi> <mo>+</mo> <msub><mi>w</mi> <mi>z</mi></msub> <mi>z</mi></mrow></math>
    that maximizes that sum of squares ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><msub><mi>SS</mi> <mi>between</mi></msub> <msub><mi>SS</mi>
    <mi>within</mi></msub></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: The between sum of squares is the squared distance between the two group means,
    and the within sum of squares is the spread around the means within each group,
    weighted by the covariance matrix. Intuitively, by maximizing the between sum
    of squares and minimizing the within sum of squares, this method yields the greatest
    separation between the two groups.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `MASS` package, associated with the book *Modern Applied Statistics with
    S* by W. N. Venables and B. D. Ripley (Springer, 1994), provides a function for
    LDA with *R*. The following applies this function to a sample of loan data using
    two predictor variables, `borrower_score` and `payment_inc_ratio`, and prints
    out the estimated linear discriminator weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can use `LinearDiscriminantAnalysis` from `sklearn.discriminant_analysis`.
    The `scalings_` property gives the estimated weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Using Discriminant Analysis for Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the predictor variables are normalized prior to running LDA, the discriminator
    weights are measures of variable importance, thus providing a computationally
    efficient method of feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lda` function can predict the probability of “default” versus “paid off”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict_proba` method of the fitted model returns the probabilities for
    the “default” and “paid off” outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of the predictions helps illustrate how LDA works. Using the output
    from the `predict` function, a plot of the estimated probability of default is
    produced as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A similar graph is created in Python using this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot is shown in [Figure 5-1](#LoanLDA). Data points on the left
    of the diagonal line are predicted to default (probability greater than 0.5).
  prefs: []
  type: TYPE_NORMAL
- en: '![LDA prediction of loan default using two variables: a score of the borrower''s
    creditworthiness and the payment-to-income ratio. Data points on the left of the
    diagonal line are predicted to default (probability greater than 0.5).](Images/psd2_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1\. LDA prediction of loan default using two variables: a score of
    the borrower’s creditworthiness and the payment-to-income ratio'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the discriminant function weights, LDA splits the predictor space into
    two regions, as shown by the solid line. The predictions farther away from the
    line in both directions have a higher level of confidence (i.e., a probability
    further away from 0.5).
  prefs: []
  type: TYPE_NORMAL
- en: Extensions of Discriminant Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More predictor variables: while the text and example in this section used just
    two predictor variables, LDA works just as well with more than two predictor variables.
    The only limiting factor is the number of records (estimating the covariance matrix
    requires a sufficient number of records per variable, which is typically not an
    issue in data science applications).'
  prefs: []
  type: TYPE_NORMAL
- en: There are other variants of discriminant analysis. The best known is quadratic
    discriminant analysis (QDA). Despite its name, QDA is still a linear discriminant
    function. The main difference is that in LDA, the covariance matrix is assumed
    to be the same for the two groups corresponding to *Y* = 0 and *Y* = 1. In QDA,
    the covariance matrix is allowed to be different for the two groups. In practice,
    the difference in most applications is not critical.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both *The Elements of Statistical Learning*, 2nd ed., by Trevor Hastie, Robert
    Tibshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, *An
    Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor
    Hastie, and Robert Tibshirani (Springer, 2013), have a section on discriminant
    analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Mining for Business Analytics* by Galit Shmueli, Peter Bruce, Nitin Patel,
    Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions
    for *R*, *Python*, Excel, and JMP) has a full chapter on discriminant analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For historical interest, Fisher’s original article on the topic, “The Use of
    Multiple Measurements in Taxonomic Problems,” as published in 1936 in *Annals
    of Eugenics* (now called *Annals of Genetics*), can be found [online](https://oreil.ly/_TCR8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is analogous to multiple linear regression (see [Chapter 4](ch04.xhtml#Regression)),
    except the outcome is binary. Various transformations are employed to convert
    the problem to one in which a linear model can be fit. Like discriminant analysis,
    and unlike *K*-Nearest Neighbor and naive Bayes, logistic regression is a structured
    model approach rather than a data-centric approach. Due to its fast computational
    speed and its output of a model that lends itself to rapid scoring of new data,
    it is a popular method.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Response Function and Logit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key ingredients for logistic regression are the *logistic response function*
    and the *logit*, in which we map a probability (which is on a 0–1 scale) to a
    more expansive scale suitable for linear modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to think of the outcome variable not as a binary label but
    as the probability *p* that the label is a “1.” Naively, we might be tempted to
    model *p* as a linear function of the predictor variables:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>β</mi> <mi>q</mi></msub> <msub><mi>x</mi>
    <mi>q</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: However, fitting this model does not ensure that *p* will end up between 0 and
    1, as a probability must.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we model *p* by applying a *logistic response* or *inverse logit*
    function to the predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi>
    <mi>q</mi></msub> <msub><mi>x</mi> <mi>q</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This transform ensures that the *p* stays between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the exponential expression out of the denominator, we consider *odds*
    instead of probabilities. Odds, familiar to bettors everywhere, are the ratio
    of “successes” (1) to “nonsuccesses” (0). In terms of probabilities, odds are
    the probability of an event divided by the probability that the event will not
    occur. For example, if the probability that a horse will win is 0.5, the probability
    of “won’t win” is (1 – 0.5) = 0.5, and the odds are 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can obtain the probability from the odds using the inverse odds function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mi>Odds</mi> <mrow><mn>1</mn><mo>+</mo>
    <mi>Odds</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We combine this with the logistic response function, shown earlier, to get:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>e</mi> <mrow><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>2</mn></msub> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi> <mi>q</mi></msub>
    <msub><mi>x</mi> <mi>q</mi></msub></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, taking the logarithm of both sides, we get an expression that involves
    a linear function of the predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>Odds</mi>
    <mrow><mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>β</mi>
    <mi>q</mi></msub> <msub><mi>x</mi> <mi>q</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The *log-odds* function, also known as the *logit* function, maps the probability
    *p* from <math alttext="left-parenthesis 0 comma 1 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math> to any value <math alttext="left-parenthesis
    negative normal infinity comma plus normal infinity right-parenthesis"><mrow><mo>(</mo>
    <mo>-</mo> <mi>∞</mi> <mo>,</mo> <mo>+</mo> <mi>∞</mi> <mo>)</mo></mrow></math>
    —see [Figure 5-2](#LogitFun). The transformation circle is complete; we have used
    a linear model to predict a probability, which we can in turn map to a class label
    by applying a cutoff rule—any record with a probability greater than the cutoff
    is classified as a 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph of the logit function that maps a probability to a scale suitable for
    a linear model](Images/psd2_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Graph of the logit function that maps a probability to a scale
    suitable for a linear model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Logistic Regression and the GLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The response in the logistic regression formula is the log odds of a binary
    outcome of 1. We observe only the binary outcome, not the log odds, so special
    statistical methods are needed to fit the equation. Logistic regression is a special
    instance of a *generalized linear model* (GLM) developed to extend linear regression
    to other settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *R*, to fit a logistic regression, the `glm` function is used with the `family`
    parameter set to `binomial`. The following code fits a logistic regression to
    the personal loan data introduced in [“K-Nearest Neighbors”](ch06.xhtml#KNN):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The response is `outcome`, which takes a 0 if the loan is paid off and a 1 if
    the loan defaults. `purpose_` and `home_` are factor variables representing the
    purpose of the loan and the home ownership status. As in linear regression, a
    factor variable with *P* levels is represented with *P* – 1 columns. By default
    in *R*, the *reference* coding is used, and the levels are all compared to the
    reference level (see [“Factor Variables in Regression”](ch04.xhtml#FactorsRegression)).
    The reference levels for these factors are `credit_card` and `MORTGAGE`, respectively.
    The variable `borrower_score` is a score from 0 to 1 representing the creditworthiness
    of the borrower (from poor to excellent). This variable was created from several
    other variables using *K*-Nearest Neighbor—see [“KNN as a Feature Engine”](ch06.xhtml#KnnFeatureEngine).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, we use the `scikit-learn` class `LogisticRegression` from `sklearn.linear_model`.
    The arguments `penalty` and `C` are used to prevent overfitting by L1 or L2 regularization.
    Regularization is switched on by default. In order to fit without regularization,
    we set `C` to a very large value. The `solver` argument selects the used minimizer;
    the method `liblinear` is the default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In contrast to *R*, `scikit-learn` derives the classes from the unique values
    in `y` (*paid off* and *default*). Internally, the classes are ordered alphabetically.
    As this is the reverse order from the factors used in *R*, you will see that the
    coefficients are reversed. The `predict` method returns the class label and `predict_proba`
    returns the probabilities in the order available from the attribute `logit_reg.classes_`.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized Linear Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generalized linear models (GLMs) are characterized by two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: A probability distribution or family (binomial in the case of logistic regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A link function—i.e., a transformation function that maps the response to the
    predictors (logit in the case of logistic regression)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression is by far the most common form of GLM. A data scientist
    will encounter other types of GLMs. Sometimes a log link function is used instead
    of the logit; in practice, use of a log link is unlikely to lead to very different
    results for most applications. The Poisson distribution is commonly used to model
    count data (e.g., the number of times a user visits a web page in a certain amount
    of time). Other families include negative binomial and gamma, often used to model
    elapsed time (e.g., time to failure). In contrast to logistic regression, application
    of GLMs with these models is more nuanced and involves greater care. These are
    best avoided unless you are familiar with and understand the utility and pitfalls
    of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Predicted Values from Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The predicted value from logistic regression is in terms of the log odds: <math
    alttext="ModifyingAbove upper Y With caret equals log left-parenthesis normal
    upper O normal d normal d normal s left-parenthesis upper Y equals 1 right-parenthesis
    right-parenthesis"><mrow><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mo>=</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>Odds</mi> <mrow><mo>(</mo> <mi>Y</mi>
    <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> . The
    predicted probability is given by the logistic response function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, look at the predictions from the model `logistic_model` in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can convert the probabilities into a data frame and use the
    `describe` method to get these characteristics of the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting these values to probabilities is a simple transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The probabilities are directly available using the `predict_proba` methods
    in `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: These are on a scale from 0 to 1 and don’t yet declare whether the predicted
    value is default or paid off. We could declare any value greater than 0.5 as default.
    In practice, a lower cutoff is often appropriate if the goal is to identify members
    of a rare class (see [“The Rare Class Problem”](#RareClassProblem)).
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Coefficients and Odds Ratios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One advantage of logistic regression is that it produces a model that can be
    scored to new data rapidly, without recomputation. Another is the relative ease
    of interpretation of the model, as compared with other classification methods.
    The key conceptual idea is understanding an *odds ratio*. The odds ratio is easiest
    to understand for a binary factor variable *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>odds</mtext> <mtext>ratio</mtext> <mo>=</mo>
    <mfrac><mrow><mi>Odds</mi> <mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow>
    <mrow><mi>Odds</mi> <mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>0</mn><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This is interpreted as the odds that *Y* = 1 when *X* = 1 versus the odds that
    *Y* = 1 when *X* = 0. If the odds ratio is 2, then the odds that *Y* = 1 are two
    times higher when *X* = 1 versus when *X* = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Why bother with an odds ratio rather than probabilities? We work with odds because
    the coefficient <math alttext="beta Subscript j"><msub><mi>β</mi> <mi>j</mi></msub></math>
    in the logistic regression is the log of the odds ratio for <math alttext="upper
    X Subscript j"><msub><mi>X</mi> <mi>j</mi></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: An example will make this more explicit. For the model fit in [“Logistic Regression
    and the GLM”](#GLM), the regression coefficient for `purpose_small_business` is
    1.21526. This means that a loan to a small business compared to a loan to pay
    off credit card debt reduces the odds of defaulting versus being paid off by <math><mrow><mi>e</mi>
    <mi>x</mi> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>.</mo> <mn>21526</mn> <mo>)</mo>
    <mo>≈</mo> <mn>3</mn> <mo>.</mo> <mn>4</mn></mrow></math> . Clearly, loans for
    the purpose of creating or expanding a small business are considerably riskier
    than other types of loans.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-3](#LogOddsRatio) shows the relationship between the odds ratio and
    the log-odds ratio for odds ratios greater than 1. Because the coefficients are
    on the log scale, an increase of 1 in the coefficient results in an increase of
    <math><mrow><mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>)</mo>
    <mo>≈</mo> <mn>2</mn> <mo>.</mo> <mn>72</mn></mrow></math> in the odds ratio.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The relationship between the odds ratio and the log-odds ratio](Images/psd2_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. The relationship between the odds ratio and the log-odds ratio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Odds ratios for numeric variables *X* can be interpreted similarly: they measure
    the change in the odds ratio for a unit change in *X*. For example, the effect
    of increasing the payment-to-income ratio from, say, 5 to 6 increases the odds
    of the loan defaulting by a factor of <math><mrow><mi>e</mi> <mi>x</mi> <mi>p</mi>
    <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>08244</mn> <mo>)</mo> <mo>≈</mo> <mn>1</mn>
    <mo>.</mo> <mn>09</mn></mrow></math> . The variable `borrower_score` is a score
    on the borrowers’ creditworthiness and ranges from 0 (low) to 1 (high). The odds
    of the best borrowers relative to the worst borrowers defaulting on their loans
    is smaller by a factor of <math><mrow><mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo>
    <mo>-</mo> <mn>4</mn> <mo>.</mo> <mn>61264</mn> <mo>)</mo> <mo>≈</mo> <mn>0</mn>
    <mo>.</mo> <mn>01</mn></mrow></math> . In other words, the default risk from the
    borrowers with the poorest creditworthiness is 100 times greater than that of
    the best borrowers!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear and Logistic Regression: Similarities and Differences'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear regression and logistic regression share many commonalities. Both assume
    a parametric linear form relating the predictors with the response. Exploring
    and finding the best model are done in very similar ways. Extensions to the linear
    model, like the use of a spline transform of a predictor (see [“Splines”](ch04.xhtml#Splines)),
    are equally applicable in the logistic regression setting. Logistic regression
    differs in two fundamental ways:'
  prefs: []
  type: TYPE_NORMAL
- en: The way the model is fit (least squares is not applicable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nature and analysis of the residuals from the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear regression is fit using least squares, and the quality of the fit is
    evaluated using RMSE and R-squared statistics. In logistic regression (unlike
    in linear regression), there is no closed-form solution, and the model must be
    fit using *maximum likelihood estimation* (MLE). Maximum likelihood estimation
    is a process that tries to find the model that is most likely to have produced
    the data we see. In the logistic regression equation, the response is not 0 or
    1 but rather an estimate of the log odds that the response is 1. The MLE finds
    the solution such that the estimated log odds best describes the observed outcome.
    The mechanics of the algorithm involve a quasi-Newton optimization that iterates
    between a scoring step (*Fisher’s scoring*), based on the current parameters,
    and an update to the parameters to improve the fit.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, most practitioners don’t need to concern themselves with the details
    of the fitting algorithm since this is handled by the software. Most data scientists
    will not need to worry about the fitting method, other than understanding that
    it is a way to find a good model under certain assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Factor Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In logistic regression, factor variables should be coded as in linear regression;
    see [“Factor Variables in Regression”](ch04.xhtml#FactorsRegression). In *R* and
    other software, this is normally handled automatically, and generally reference
    encoding is used. All of the other classification methods covered in this chapter
    typically use the one hot encoder representation (see [“One Hot Encoder”](ch06.xhtml#OneHotEncoder)).
    In *Python*’s `scikit-learn`, it is easiest to use one hot encoding, which means
    that only *n – 1* of the resulting dummies can be used in the regression.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like other classification methods, logistic regression is assessed by how accurately
    the model classifies new data (see [“Evaluating Classification Models”](#EvaluatingModels)).
    As with linear regression, some additional standard statistical tools are available
    to examine and improve the model. Along with the estimated coefficients, *R* reports
    the standard error of the coefficients (SE), a *z*-value, and a p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The package `statsmodels` has an implementation for generalized linear model
    (`GLM`) that provides similarly detailed information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Interpretation of the p-value comes with the same caveat as in regression and
    should be viewed more as a relative indicator of variable importance (see [“Assessing
    the Model”](ch04.xhtml#RMSE)) than as a formal measure of statistical significance.
    A logistic regression model, which has a binary response, does not have an associated
    RMSE or R-squared. Instead, a logistic regression model is typically evaluated
    using more general metrics for classification; see [“Evaluating Classification
    Models”](#EvaluatingModels).
  prefs: []
  type: TYPE_NORMAL
- en: 'Many other concepts for linear regression carry over to the logistic regression
    setting (and other GLMs). For example, you can use stepwise regression, fit interaction
    terms, or include spline terms. The same concerns regarding confounding and correlated
    variables apply to logistic regression (see [“Interpreting the Regression Equation”](ch04.xhtml#InterpretingRegression)).
    You can fit generalized additive models (see [“Generalized Additive Models”](ch04.xhtml#GAMS))
    using the `mgcv` package in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The formula interface of `statsmodels` also supports these extensions in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Analysis of residuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One area where logistic regression differs from linear regression is in the
    analysis of the residuals. As in linear regression (see [Figure 4-9](ch04.xhtml#HousePartialResid)),
    it is straightforward to compute partial residuals in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot is displayed in [Figure 5-4](#LogisticPartialResidual). The
    estimated fit, shown by the line, goes between two sets of point clouds. The top
    cloud corresponds to a response of 1 (defaulted loans), and the bottom cloud corresponds
    to a response of 0 (loans paid off). This is very typical of residuals from a
    logistic regression since the output is binary. The prediction is measured as
    the logit (log of the odds ratio), which will always be some finite value. The
    actual value, an absolute 0 or 1, corresponds to an infinite logit, either positive
    or negative, so the residuals (which get added to the fitted value) will never
    equal 0\. Hence the plotted points lie in clouds either above or below the fitted
    line in the partial residual plot. Partial residuals in logistic regression, while
    less valuable than in regression, are still useful to confirm nonlinear behavior
    and identify highly influential records.
  prefs: []
  type: TYPE_NORMAL
- en: There is currently no implementation of partial residuals in any of the major
    *Python* packages. We provide *Python* code to create the partial residual plot
    in the accompanying source code repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![Partial residuals from logistic regression](Images/psd2_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Partial residuals from logistic regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some of the output from the `summary` function can effectively be ignored. The
    dispersion parameter does not apply to logistic regression and is there for other
    types of GLMs. The residual deviance and the number of scoring iterations are
    related to the maximum likelihood fitting method; see [“Maximum Likelihood Estimation”](#MLE).
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The standard reference on logistic regression is *Applied Logistic Regression*,
    3rd ed., by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant (Wiley, 2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also popular are two books by Joseph Hilbe: *Logistic Regression Models* (very
    comprehensive, 2017) and *Practical Guide to Logistic Regression* (compact, 2015),
    both from Chapman & Hall/CRC Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both *The Elements of Statistical Learning*, 2nd ed., by Trevor Hastie, Robert
    Tibshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, *An
    Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor
    Hastie, and Robert Tibshirani (Springer, 2013), have a section on logistic regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Mining for Business Analytics* by Galit Shmueli, Peter Bruce, Nitin Patel,
    Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions
    for *R*, *Python*, Excel, and JMP) has a full chapter on logistic regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is common in predictive modeling to train a number of different models, apply
    each to a holdout sample, and assess their performance. Sometimes, after a number
    of models have been evaluated and tuned, and if there are enough data, a third
    holdout sample, not used previously, is used to estimate how the chosen model
    will perform with completely new data. Different disciplines and practitioners
    will also use the terms *validation* and *test* to refer to the holdout sample(s).
    Fundamentally, the assessment process attempts to learn which model produces the
    most accurate and useful predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple way to measure classification performance is to count the proportion
    of predictions that are correct, i.e., measure the *accuracy*. Accuracy is simply
    a measure of total error:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>accuracy</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Negative</mi></mrow></mrow> <mrow><mi>Sample</mi> <mi>Size</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In most classification algorithms, each case is assigned an “estimated probability
    of being a 1.”^([3](ch05.xhtml#idm46522847087352)) The default decision point,
    or cutoff, is typically 0.50 or 50%. If the probability is above 0.5, the classification
    is “1”; otherwise it is “0.” An alternative default cutoff is the prevalent probability
    of 1s in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of classification metrics is the *confusion matrix*. The confusion
    matrix is a table showing the number of correct and incorrect predictions categorized
    by type of response. Several packages are available in *R* and *Python* to compute
    a confusion matrix, but in the binary case, it is simple to compute one by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the confusion matrix, consider the `logistic_gam` model that
    was trained on a balanced data set with an equal number of defaulted and paid-off
    loans (see [Figure 5-4](#LogisticPartialResidual)). Following the usual conventions,
    *Y* = 1 corresponds to the event of interest (e.g., default), and *Y* = 0 corresponds
    to a negative (or usual) event (e.g., paid off). The following computes the confusion
    matrix for the `logistic_gam` model applied to the entire (unbalanced) training
    set in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The predicted outcomes are columns and the true outcomes are the rows. The diagonal
    elements of the matrix show the number of correct predictions, and the off-diagonal
    elements show the number of incorrect predictions. For example, 14,295 defaulted
    loans were correctly predicted as a default, but 8,376 defaulted loans were incorrectly
    predicted as paid off.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-5](#ConfusionGraphic) shows the relationship between the confusion
    matrix for a binary response *Y* and different metrics (see [“Precision, Recall,
    and Specificity”](#PrecisonRecallSpecificity) for more on the metrics). As with
    the example for the loan data, the actual response is along the rows and the predicted
    response is along the columns. The diagonal boxes (upper left, lower right) show
    when the predictions <math alttext="ModifyingAbove upper Y With caret"><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover></math> correctly predict the response.
    One important metric not explicitly called out is the false positive *rate* (the
    mirror image of precision). When 1s are rare, the ratio of false positives to
    all predicted positives can be high, leading to the unintuitive situation in which
    a predicted 1 is most likely a 0. This problem plagues medical screening tests
    (e.g., mammograms) that are widely applied: due to the relative rarity of the
    condition, positive test results most likely do not mean breast cancer. This leads
    to much confusion in the public.'
  prefs: []
  type: TYPE_NORMAL
- en: '![images/confusion-matrix-terms.png](Images/psd2_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Confusion matrix for a binary response and various metrics
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we present the actual response along the rows and the predicted response
    along the columns, but it is not uncommon to see this reversed. A notable example
    is the popular `caret` package in *R*.
  prefs: []
  type: TYPE_NORMAL
- en: The Rare Class Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, there is an imbalance in the classes to be predicted, with one
    class much more prevalent than the other—for example, legitimate insurance claims
    versus fraudulent ones, or browsers versus purchasers at a website. The rare class
    (e.g., the fraudulent claims) is usually the class of more interest and is typically
    designated 1, in contrast to the more prevalent 0s. In the typical scenario, the
    1s are the more important case, in the sense that misclassifying them as 0s is
    costlier than misclassifying 0s as 1s. For example, correctly identifying a fraudulent
    insurance claim may save thousands of dollars. On the other hand, correctly identifying
    a nonfraudulent claim merely saves you the cost and effort of going through the
    claim by hand with a more careful review (which is what you would do if the claim
    were tagged as “fraudulent”).
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, unless the classes are easily separable, the most accurate classification
    model may be one that simply classifies everything as a 0. For example, if only
    0.1% of the browsers at a web store end up purchasing, a model that predicts that
    each browser will leave without purchasing will be 99.9% accurate. However, it
    will be useless. Instead, we would be happy with a model that is less accurate
    overall but is good at picking out the purchasers, even if it misclassifies some
    nonpurchasers along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Precision, Recall, and Specificity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Metrics other than pure accuracy—metrics that are more nuanced—are commonly
    used in evaluating classification models. Several of these have a long history
    in statistics—especially biostatistics, where they are used to describe the expected
    performance of diagnostic tests. The *precision* measures the accuracy of a predicted
    positive outcome (see [Figure 5-5](#ConfusionGraphic)):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>precision</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Positive</mi></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The *recall*, also known as *sensitivity*, measures the strength of the model
    to predict a positive outcome—the proportion of the 1s that it correctly identifies
    (see [Figure 5-5](#ConfusionGraphic)). The term *sensitivity* is used a lot in
    biostatistics and medical diagnostics, whereas *recall* is used more in the machine
    learning community. The definition of recall is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>recall</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Positive</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Positive</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Negative</mi></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Another metric used is *specificity*, which measures a model’s ability to predict
    a negative outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>specificity</mtext> <mo>=</mo> <mfrac><mrow><mo>∑</mo>
    <mrow><mi>True</mi> <mi>Negative</mi></mrow></mrow> <mrow><mo>∑</mo> <mrow><mi>True</mi>
    <mi>Negative</mi></mrow> <mo>+</mo><mo>∑</mo> <mrow><mi>False</mi> <mi>Positive</mi></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the three metrics from `conf_mat` in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the equivalent code to calculate the metrics in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`scikit-learn` has a custom method `precision_recall_fscore_support` that calculates
    precision and recall/specificity all at once.'
  prefs: []
  type: TYPE_NORMAL
- en: ROC Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can see that there is a trade-off between recall and specificity. Capturing
    more 1s generally means misclassifying more 0s as 1s. The ideal classifier would
    do an excellent job of classifying the 1s, without misclassifying more 0s as 1s.
  prefs: []
  type: TYPE_NORMAL
- en: 'The metric that captures this trade-off is the “Receiver Operating Characteristics”
    curve, usually referred to as the *ROC curve*. The ROC curve plots recall (sensitivity)
    on the y-axis against specificity on the x-axis.^([4](ch05.xhtml#idm46522846485448))
    The ROC curve shows the trade-off between recall and specificity as you change
    the cutoff to determine how to classify a record. Sensitivity (recall) is plotted
    on the y-axis, and you may encounter two forms in which the x-axis is labeled:'
  prefs: []
  type: TYPE_NORMAL
- en: Specificity plotted on the x-axis, with 1 on the left and 0 on the right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1-Specificity plotted on the x-axis, with 0 on the left and 1 on the right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The curve looks identical whichever way it is done. The process to compute
    the ROC curve is:'
  prefs: []
  type: TYPE_NORMAL
- en: Sort the records by the predicted probability of being a 1, starting with the
    most probable and ending with the least probable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the cumulative specificity and recall based on the sorted records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computing the ROC curve in *R* is straightforward. The following code computes
    ROC for the loan data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can use the `scikit-learn` function `sklearn.metrics.roc_curve`
    to calculate the required information for the ROC curve. You can find similar
    packages for *R*, e.g., `ROCR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in [Figure 5-6](#ROCLoan). The dotted diagonal line corresponds
    to a classifier no better than random chance. An extremely effective classifier
    (or, in medical situations, an extremely effective diagnostic test) will have
    an ROC that hugs the upper-left corner—it will correctly identify lots of 1s without
    misclassifying lots of 0s as 1s. For this model, if we want a classifier with
    a specificity of at least 50%, then the recall is about 75%.
  prefs: []
  type: TYPE_NORMAL
- en: '![ROC curve for the loan data](Images/psd2_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. ROC curve for the loan data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Precision-Recall Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to ROC curves, it can be illuminating to examine the [precision-recall
    (PR) curve](https://oreil.ly/_89Pr). PR curves are computed in a similar way except
    that the data is ordered from least to most probable and cumulative precision
    and recall statistics are computed. PR curves are especially useful in evaluating
    data with highly unbalanced outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: AUC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ROC curve is a valuable graphical tool, but by itself doesn’t constitute
    a single measure for the performance of a classifier. The ROC curve can be used,
    however, to produce the area underneath the curve (AUC) metric. AUC is simply
    the total area under the ROC curve. The larger the value of AUC, the more effective
    the classifier. An AUC of 1 indicates a perfect classifier: it gets all the 1s
    correctly classified, and it doesn’t misclassify any 0s as 1s.'
  prefs: []
  type: TYPE_NORMAL
- en: A completely ineffective classifier—the diagonal line—will have an AUC of 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-7](#AUCLoan) shows the area under the ROC curve for the loan model.
    The value of AUC can be computed by a numerical integration in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can either calculate the accuracy as shown for *R* or use `scikit-learn`’s
    function `sklearn.metrics.roc_auc_score`. You will need to provide the expected
    value as 0 or 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The model has an AUC of about 0.69, corresponding to a relatively weak classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Area under the ROC curve for the loan data](Images/psd2_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Area under the ROC curve for the loan data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: False Positive Rate Confusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: False positive/negative rates are often confused or conflated with specificity
    or sensitivity (even in publications and software!). Sometimes the false positive
    rate is defined as the proportion of true negatives that test positive. In many
    cases (such as network intrusion detection), the term is used to refer to the
    proportion of positive signals that are true negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Lift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the AUC as a metric to evaluate a model is an improvement over simple
    accuracy, as it can assess how well a classifier handles the trade-off between
    overall accuracy and the need to identify the more important 1s. But it does not
    completely address the rare-case problem, where you need to lower the model’s
    probability cutoff below 0.5 to avoid having all records classified as 0\. In
    such cases, for a record to be classified as a 1, it might be sufficient to have
    a probability of 0.4, 0.3, or lower. In effect, we end up overidentifying 1s,
    reflecting their greater importance.
  prefs: []
  type: TYPE_NORMAL
- en: Changing this cutoff will improve your chances of catching the 1s (at the cost
    of misclassifying more 0s as 1s). But what is the optimum cutoff?
  prefs: []
  type: TYPE_NORMAL
- en: The concept of lift lets you defer answering that question. Instead, you consider
    the records in order of their predicted probability of being 1s. Say, of the top
    10% classified as 1s, how much better did the algorithm do, compared to the benchmark
    of simply picking blindly? If you can get 0.3% response in this top decile instead
    of the 0.1% you get overall by picking randomly, the algorithm is said to have
    a *lift* (also called *gains*) of 3 in the top decile. A lift chart (gains chart)
    quantifies this over the range of the data. It can be produced decile by decile,
    or continuously over the range of the data.
  prefs: []
  type: TYPE_NORMAL
- en: To compute a lift chart, you first produce a *cumulative gains chart* that shows
    the recall on the y-axis and the total number of records on the x-axis. The *lift
    curve* is the ratio of the cumulative gains to the diagonal line corresponding
    to random selection. *Decile gains charts* are one of the oldest techniques in
    predictive modeling, dating from the days before internet commerce. They were
    particularly popular among direct mail professionals. Direct mail is an expensive
    method of advertising if applied indiscriminately, and advertisers used predictive
    models (quite simple ones, in the early days) to identify the potential customers
    with the likeliest prospect of payoff.
  prefs: []
  type: TYPE_NORMAL
- en: Uplift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes the term *uplift* is used to mean the same thing as lift. An alternate
    meaning is used in a more restrictive setting, when an A/B test has been conducted
    and the treatment (A or B) is then used as a predictor variable in a predictive
    model. The uplift is the improvement in response predicted *for an individual
    case* with treatment A versus treatment B. This is determined by scoring the individual
    case first with the predictor set to A, and then again with the predictor toggled
    to B. Marketers and political campaign consultants use this method to determine
    which of two messaging treatments should be used with which customers or voters.
  prefs: []
  type: TYPE_NORMAL
- en: A lift curve lets you look at the consequences of setting different probability
    cutoffs for classifying records as 1s. It can be an intermediate step in settling
    on an appropriate cutoff level. For example, a tax authority might have only a
    certain amount of resources that it can spend on tax audits, and it wants to spend
    them on the likeliest tax cheats. With its resource constraint in mind, the authority
    would use a lift chart to estimate where to draw the line between tax returns
    selected for audit and those left alone.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluation and assessment are typically covered in the context of a particular
    model (e.g., *K*-Nearest Neighbors or decision trees); three books that handle
    the subject in its own chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data Mining*, 3rd ed., by Ian Whitten, Eibe Frank, and Mark Hall (Morgan Kaufmann,
    2011).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Modern Data Science with R* by Benjamin Baumer, Daniel Kaplan, and Nicholas
    Horton (Chapman & Hall/CRC Press, 2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Mining for Business Analytics* by Galit Shmueli, Peter Bruce, Nitin Patel,
    Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions
    for *R*, *Python*, Excel, and JMP).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for Imbalanced Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section dealt with evaluation of classification models using metrics
    that go beyond simple accuracy and are suitable for imbalanced data—data in which
    the outcome of interest (purchase on a website, insurance fraud, etc.) is rare.
    In this section, we look at additional strategies that can improve predictive
    modeling performance with imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have enough data, as is the case with the loan data, one solution is
    to *undersample* (or downsample) the prevalent class, so the data to be modeled
    is more balanced between 0s and 1s. The basic idea in undersampling is that the
    data for the dominant class has many redundant records. Dealing with a smaller,
    more balanced data set yields benefits in model performance, and it makes it easier
    to prepare the data and to explore and pilot models.
  prefs: []
  type: TYPE_NORMAL
- en: How much data is enough? It depends on the application, but in general, having
    tens of thousands of records for the less dominant class is enough. The more easily
    distinguishable the 1s are from the 0s, the less data needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loan data analyzed in [“Logistic Regression”](#LogisticRegression) was
    based on a balanced training set: half of the loans were paid off, and the other
    half were in default. The predicted values were similar: half of the probabilities
    were less than 0.5, and half were greater than 0.5. In the full data set, only
    about 19% of the loans were in default, as shown in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'What happens if we use the full data set to train the model? Let’s see what
    this looks like in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Only 0.39% of the loans are predicted to be in default, or less than 1/47 of
    the expected number.^([5](ch05.xhtml#idm46522845748808)) The loans that were paid
    off overwhelm the loans in default because the model is trained using all the
    data equally. Thinking about it intuitively, the presence of so many nondefaulting
    loans, coupled with the inevitable variability in predictor data, means that,
    even for a defaulting loan, the model is likely to find some nondefaulting loans
    that it is similar to, by chance. When a balanced sample was used, roughly 50%
    of the loans were predicted to be in default.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling and Up/Down Weighting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One criticism of the undersampling method is that it throws away data and is
    not using all the information at hand. If you have a relatively small data set,
    and the rarer class contains a few hundred or a few thousand records, then undersampling
    the dominant class has the risk of throwing out useful information. In this case,
    instead of downsampling the dominant case, you should oversample (upsample) the
    rarer class by drawing additional rows with replacement (bootstrapping).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can achieve a similar effect by weighting the data. Many classification
    algorithms take a weight argument that will allow you to up/down weight the data.
    For example, apply a weight vector to the loan data using the `weight` argument
    to `glm` in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Most `scikit-learn` methods allow specifying weights in the `fit` function
    using the keyword argument `sample_weight`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The weights for loans that default are set to <math alttext="StartFraction 1
    Over p EndFraction"><mfrac><mn>1</mn> <mi>p</mi></mfrac></math> , where *p* is
    the probability of default. The nondefaulting loans have a weight of 1. The sums
    of the weights for the defaulting loans and nondefaulting loans are roughly equal.
    The mean of the predicted values is now about 58% instead of 0.39%.
  prefs: []
  type: TYPE_NORMAL
- en: Note that weighting provides an alternative to both upsampling the rarer class
    and downsampling the dominant class.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting the Loss Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many classification and regression algorithms optimize a certain criteria or
    *loss function*. For example, logistic regression attempts to minimize the deviance.
    In the literature, some propose to modify the loss function in order to avoid
    the problems caused by a rare class. In practice, this is hard to do: classification
    algorithms can be complex and difficult to modify. Weighting is an easy way to
    change the loss function, discounting errors for records with low weights in favor
    of records with higher weights.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A variation of upsampling via bootstrapping (see [“Oversampling and Up/Down
    Weighting”](#UpDownWeighting)) is *data generation* by perturbing existing records
    to create new records. The intuition behind this idea is that since we observe
    only a limited set of instances, the algorithm doesn’t have a rich set of information
    to build classification “rules.” By creating new records that are similar but
    not identical to existing records, the algorithm has a chance to learn a more
    robust set of rules. This notion is similar in spirit to ensemble statistical
    models such as boosting and bagging (see [Chapter 6](ch06.xhtml#StatisticalML)).
  prefs: []
  type: TYPE_NORMAL
- en: The idea gained traction with the publication of the *SMOTE* algorithm, which
    stands for “Synthetic Minority Oversampling Technique.” The SMOTE algorithm finds
    a record that is similar to the record being upsampled (see [“K-Nearest Neighbors”](ch06.xhtml#KNN))
    and creates a synthetic record that is a randomly weighted average of the original
    record and the neighboring record, where the weight is generated separately for
    each predictor. The number of synthetic oversampled records created depends on
    the oversampling ratio required to bring the data set into approximate balance
    with respect to outcome classes.
  prefs: []
  type: TYPE_NORMAL
- en: There are several implementations of SMOTE in *R*. The most comprehensive package
    for handling unbalanced data is `unbalanced`. It offers a variety of techniques,
    including a “Racing” algorithm to select the best method. However, the SMOTE algorithm
    is simple enough that it can be implemented directly in *R* using the `FNN` package.
  prefs: []
  type: TYPE_NORMAL
- en: The *Python* package `imbalanced-learn` implements a variety of methods with
    an API that is compatible with `scikit-learn`. It provides various methods for
    over- and undersampling and support for using these techniques with boosting and
    bagging classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-Based Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In practice, accuracy and AUC are a poor man’s way to choose a classification
    rule. Often, an estimated cost can be assigned to false positives versus false
    negatives, and it is more appropriate to incorporate these costs to determine
    the best cutoff when classifying 1s and 0s. For example, suppose the expected
    cost of a default of a new loan is <math alttext="upper C"><mi>C</mi></math> and
    the expected return from a paid-off loan is <math alttext="upper R"><mi>R</mi></math>
    . Then the expected return for that loan is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>expected</mtext> <mtext>return</mtext> <mo>=</mo>
    <mi>P</mi> <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>0</mn> <mo>)</mo> <mo>×</mo> <mi>R</mi>
    <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo> <mo>×</mo>
    <mi>C</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Instead of simply labeling a loan as default or paid off, or determining the
    probability of default, it makes more sense to determine if the loan has a positive
    expected return. Predicted probability of default is an intermediate step, and
    it must be combined with the loan’s total value to determine expected profit,
    which is the ultimate planning metric of business. For example, a smaller value
    loan might be passed over in favor of a larger one with a slightly higher predicted
    default probability.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A single metric, such as AUC, cannot evaluate all aspects of the suitability
    of a model for a situation. [Figure 5-8](#ComparisonOfRules) displays the decision
    rules for four different models fit to the loan data using just two predictor
    variables: `borrower_score` and `payment_inc_ratio`. The models are linear discriminant
    analysis (LDA), logistic linear regression, logistic regression fit using a generalized
    additive model (GAM), and a tree model (see [“Tree Models”](ch06.xhtml#TreeModels)).
    The region to the upper left of the lines corresponds to a predicted default.
    LDA and logistic linear regression give nearly identical results in this case.
    The tree model produces the least regular rule, with two steps. Finally, the GAM
    fit of the logistic regression represents a compromise between the tree model
    and the linear model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison of the classification rules for four different methods. Logistic
    and LDA produce nearly identical, overlapping linear classifiers.](Images/psd2_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Comparison of the classification rules for four different methods
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is not easy to visualize the prediction rules in higher dimensions or, in
    the case of the GAM and the tree model, even to generate the regions for such
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, exploratory analysis of predicted values is always warranted.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tom Fawcett, author of *Data Science for Business*, has a [good article on imbalanced
    classes](https://oreil.ly/us2rd).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more on SMOTE, see Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall,
    and W. Philip Kegelmeyer, [“SMOTE: Synthetic Minority Over-sampling Technique,”](https://oreil.ly/bwaIQ)
    *Journal of Artificial Intelligence Research* 16 (2002): 321–357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also see the Analytics Vidhya Content Team’s [“Practical Guide to Deal with
    Imbalanced Classification Problems in *R*,”](https://oreil.ly/gZUDs) March 28,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification, the process of predicting which of two or more categories a
    record belongs to, is a fundamental tool of predictive analytics. Will a loan
    default (yes or no)? Will it prepay? Will a web visitor click on a link? Will
    they purchase something? Is an insurance claim fraudulent? Often in classification
    problems, one class is of primary interest (e.g., the fraudulent insurance claim),
    and in binary classification, this class is designated as a 1, with the other,
    more prevalent class being a 0\. Often, a key part of the process is estimating
    a *propensity score*, a probability of belonging to the class of interest. A common
    scenario is one in which the class of interest is relatively rare. When evaluating
    a classifier, there are a variety of model assessment metrics that go beyond simple
    accuracy; these are important in the rare-class situation, when classifying all
    records as 0s can yield high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.xhtml#idm46522850239256-marker)) This and subsequent sections in
    this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck;
    used with permission.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.xhtml#idm46522849542200-marker)) It is certainly surprising that
    the first article on statistical classification was published in a journal devoted
    to eugenics. Indeed, there is a [disconcerting connection between the early development
    of statistics and eugenics](https://oreil.ly/eUJvR).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.xhtml#idm46522847087352-marker)) Not all methods provide unbiased
    estimates of probability. In most cases, it is sufficient that the method provide
    a ranking equivalent to the rankings that would result from an unbiased probability
    estimate; the cutoff method is then functionally equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.xhtml#idm46522846485448-marker)) The ROC curve was first used during
    World War II to describe the performance of radar receiving stations, whose job
    was to correctly identify (classify) reflected radar signals and alert defense
    forces to incoming aircraft.
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch05.xhtml#idm46522845748808-marker)) Due to differences in implementation,
    results in *Python* differ slightly: 1%, or about 1/18 of the expected number.'
  prefs: []
  type: TYPE_NORMAL
