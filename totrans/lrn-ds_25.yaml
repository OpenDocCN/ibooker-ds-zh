- en: Chapter 19\. Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter continues our foray into the fourth stage of the data science
    lifecycle: fitting and evaluating models to understand the world. So far, we’ve
    described how to fit a constant model using absolute error ([Chapter 4](ch04.html#ch-modeling))
    and simple and multiple linear models using squared error ([Chapter 15](ch15.html#ch-linear)).
    We’ve also fit linear models with an asymmetric loss function ([Chapter 18](ch18.html#ch-donkey))
    and with regularized loss ([Chapter 16](ch16.html#ch-risk)). In all of these cases,
    we aimed to predict or explain the behavior of a numeric outcome—bus wait times,
    smoke particles in the air, and donkey weights are all numeric variables.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we expand our view of modeling. Instead of predicting numeric
    outcomes, we build models to predict nominal outcomes. These sorts of models enable
    banks to predict whether a credit card transaction is fraudulent or not, doctors
    to classify tumors as benign or malignant, and your email service to identify
    spam and set it aside from your usual emails. This type of modeling is called
    *classification* and occurs widely in data science.
  prefs: []
  type: TYPE_NORMAL
- en: Just as with linear regression, we formulate a model, choose a loss function,
    fit the model by minimizing average loss for our data, and assess the fitted model.
    But unlike linear regression, our model is not linear, the loss function is not
    squared error, and our assessment compares different kinds of classification errors.
    Despite these differences, the overall structure of model fitting carries over
    to this setting. Together, regression and classification compose the primary approaches
    for *supervised learning*, the general task of fitting models based on observed
    outcomes and covariates.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by introducing an example that we use throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Wind-Damaged Trees'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1999, a huge storm with winds over 90 mph damaged millions of trees in the
    [Boundary Waters Canoe Area Wilderness](https://oreil.ly/O2qOL) (BWCAW), which
    has the largest tract of virgin forest in the eastern US. In an effort to understand
    the susceptibility of trees to wind damage, a researcher named [Roy Lawrence Rich](https://oreil.ly/plX02)
    carried out a ground survey of the BWCAW. In the years following this study, other
    researchers have used this dataset to model *windthrow*, or the uprooting of trees
    in strong winds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The population under study are the trees in the BWCAW. The access frame are
    *transects*: straight lines that cut through the natural landscape. These particular
    transects begin close to a lake and travel orthogonally to the gradient of the
    land for 250–400 meters. Along these transects, surveyors stop every 25 meters
    and examine a 5-by-5-meter plot. At each plot, trees are counted, categorized
    as blown down or standing, measured in diameter at 6 ft from the ground, and their
    species recorded.'
  prefs: []
  type: TYPE_NORMAL
- en: Sampling protocols like this are common for studying natural resources. In the
    BWCAW, over 80% of the land in the region is within 500 meters of a lake, so the
    access frame nearly covers the population. The study took place over the summers
    of 2000 and 2001, and no other natural disasters happened between the 1999 storm
    and when the data were collected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Measurements were collected on over 3,600 trees, but in this example, we examine
    just the black spruce. There are over 650 of them. We read in these data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|   | diameter | storm | status |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 9.0 | 0.02 | standing |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 11.0 | 0.03 | standing |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 9.0 | 0.03 | standing |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **656** | 9.0 | 0.94 | fallen |'
  prefs: []
  type: TYPE_TB
- en: '| **657** | 17.0 | 0.94 | fallen |'
  prefs: []
  type: TYPE_TB
- en: '| **658** | 8.0 | 0.98 | fallen |'
  prefs: []
  type: TYPE_TB
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Each row corresponds to a single tree and has the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`diameter`'
  prefs: []
  type: TYPE_NORMAL
- en: Diameter of the tree in cm, measured at 6 ft above the ground
  prefs: []
  type: TYPE_NORMAL
- en: '`storm`'
  prefs: []
  type: TYPE_NORMAL
- en: Severity of the storm (fraction of trees that fell in a 25-meter-wide area containing
    the tree)
  prefs: []
  type: TYPE_NORMAL
- en: '`status`'
  prefs: []
  type: TYPE_NORMAL
- en: Tree has “fallen” or is “standing”
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin with some exploratory analysis before we turn to modeling. First,
    we calculate some simple summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|   | diameter | storm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **min** | 5.0 | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 6.0 | 0.21 |'
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 8.0 | 0.36 |'
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 12.0 | 0.55 |'
  prefs: []
  type: TYPE_TB
- en: '| **max** | 32.0 | 0.98 |'
  prefs: []
  type: TYPE_TB
- en: 'Based on the quartiles, the distribution of tree diameter seems skewed right.
    Let’s compare the distribution of diameters for the standing and fallen trees
    with histograms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in01.png)'
  prefs: []
  type: TYPE_IMG
- en: The distribution of the diameter of the trees that fell in the storm is centered
    at 12 cm with a right skew. In comparison, the standing trees were nearly all
    under 10 cm in diameter with a mode at about 6 cm (only trees with a diameter
    of at least 5 cm are included in the study).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another feature to investigate is the strength of the storm. We plot the storm
    strength against the tree diameter using the symbol and marker color to distinguish
    the standing trees from the fallen ones. Since the diameter is essentially measured
    to the nearest cm, many trees have the same diameter, so we jitter the values
    by adding a bit of noise to the diameter values to help reduce overplotting (see
    [Chapter 11](ch11.html#ch-viz)). We also adjust the opacity of the marker colors
    to reveal the denser regions on the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this plot, it looks like both the tree diameter and the strength of the
    storm are related to windthrow: whether the tree was uprooted or left standing.
    Notice that windthrow, the feature we want to predict, is a nominal variable.
    In the next section, we consider how this impacts the prediction problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’d like to create a model that explains the susceptibility of trees to windthrow.
    In other words, we need to build a model for a two-level nominal feature: fallen
    or standing. When the response variable is nominal, this modeling task is called
    *classification*. In this case there are only two levels, so this task is more
    specifically called *binary classification*.'
  prefs: []
  type: TYPE_NORMAL
- en: A Constant Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by considering the simplest model: a constant model that always
    predicts one class. We use <math><mi>C</mi></math> to denote the constant model’s
    prediction. For our windthrow dataset, this model will predict either <math><mi>C</mi>
    <mo>=</mo> <mtext>standing</mtext></math> or <math><mi>C</mi> <mo>=</mo> <mtext>fallen</mtext></math>
    for every input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In classification, we want to track how often our model predicts the correct
    category. For now, we simply use a count of the correct predictions. This is sometimes
    called the *zero-one error* because the loss function takes on one of two possible
    values: 1 when an incorrect prediction is made and 0 for a correct prediction.
    For a given observed outcome <math><msub><mi>y</mi> <mi>i</mi></msub></math> and
    prediction <math><mi>C</mi></math> , we can express this loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi>ℓ</mi></mrow>
    <mo stretchy="false">(</mo> <mi>C</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo>
    <mo>=</mo> <mrow><mo>{</mo> <mtable columnalign="left left" columnspacing="1em"
    rowspacing=".2em"><mtr><mtd><mn>0</mn></mtd> <mtd><mtext>when </mtext> <mi>C</mi>
    <mtext> matches </mtext> <mi>y</mi></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><mtext>when </mtext>
    <mi>C</mi> <mtext> is a mismatch for </mtext> <mi>y</mi></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have collected data, <math><mrow><mi mathvariant="bold">y</mi></mrow>
    <mo>=</mo> <mo stretchy="false">[</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>…</mo> <mo>,</mo> <msub><mi>y</mi> <mi>n</mi></msub> <mo stretchy="false">]</mo></math>
    , then the average loss is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>C</mi> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow> <mo stretchy="false">)</mo></mtd>
    <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mi>n</mi></munderover> <mrow><mi>ℓ</mi></mrow> <mo
    stretchy="false">(</mo> <mi>C</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><mo>=</mo> <mfrac><mrow><mi mathvariant="normal">#</mi> <mtext> mismatches</mtext></mrow>
    <mi>n</mi></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: For the constant model (see [Chapter 4](ch04.html#ch-modeling)), the model minimizes
    the loss when <math><mi>C</mi></math> is set to the most prevalent category.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the black spruce, we have the following proportions of standing
    and fallen trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So our prediction is that a tree stands, and the average loss for our dataset
    is <math><mn>0.35</mn></math> .
  prefs: []
  type: TYPE_NORMAL
- en: That said, this prediction is not particularly helpful or insightful. For example,
    in our EDA of the trees dataset, we saw that the size of the tree is correlated
    with whether the tree stands or falls. Ideally, we could incorporate this information
    into the model, but the constant model doesn’t let us do this. Let’s build some
    intuition for how we can incorporate predictors into our model.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the Relationship Between Size and Windthrow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want to take a closer look at how tree size is related to windthrow. For
    convenience, we transform the nominal windthrow feature into a 0-1 numeric feature
    where 1 stands for a fallen tree and 0 for standing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|   | diameter | storm | status | status_0_1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 9.0 | 0.02 | standing | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 11.0 | 0.03 | standing | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 9.0 | 0.03 | standing | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **656** | 9.0 | 0.94 | fallen | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **657** | 17.0 | 0.94 | fallen | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **658** | 8.0 | 0.98 | fallen | 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This representation is useful in many ways. For example, the average of `status_0_1`
    is the proportion of fallen trees in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Having this 0-1 feature also lets us make a plot to show the relationship between
    tree diameter and windthrow. This is analogous to our process for linear regression,
    where we make scatterplots of the outcome variable against explanatory variable(s)
    (see [Chapter 15](ch15.html#ch-linear)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we plot the tree status against the diameter, but we add a small amount
    of random noise to the status to help us see the density of 0 and 1 values at
    each diameter. As before, we jitter the diameter values too and adjust the opacity
    of the markers to reduce overplotting. We also add a horizontal line at the proportion
    of fallen trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in03.png)'
  prefs: []
  type: TYPE_IMG
- en: This scatterplot shows that the smaller trees are more likely to be standing
    than the larger trees. Notice that the average status for trees (0.35) essentially
    fits a constant model to the response variable. If we consider tree diameter as
    an explanatory feature, we should be able to improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'A starting place might be to compute the proportion of fallen trees for different
    diameters. The following block of code divides tree diameter into intervals and
    computes the proportion of fallen trees in each bin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot these proportions against tree diameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in04.png)'
  prefs: []
  type: TYPE_IMG
- en: The size of the markers reflects the number of trees in the diameter bin. We
    can use these proportions to improve our model. For example, for a tree that is
    6 cm in diameter, we would classify it as standing, whereas for a 20 cm tree,
    our classification would be fallen. A natural starting place for binary classification
    is to model the observed proportions and then use these proportions to classify.
    Next, we develop a model for these proportions.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling Proportions (and Probabilities)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that when we model, we need to choose three things: a model, a loss
    function, and a method to minimize the average loss on our train set. In the previous
    section, we chose a constant model, the 0-1 loss, and a proof to fit the model.
    However, the constant model doesn’t incorporate predictor variables. In this section,
    we address this issue by introducing a new model called the *logistic* model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To motivate these models, notice that the relationship between tree diameter
    and the proportion of downed trees does not appear linear. For demonstration,
    let’s fit a simple linear model to these data to show that it has several undesirable
    features. Using the techniques from [Chapter 15](ch15.html#ch-linear), we fit
    a linear model of tree status to diameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we add this fitted line to our scatterplot of proportions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, the model doesn’t fit the proportions well at all. There are several
    problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The model gives proportions greater than 1 for large trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model doesn’t pick up the curvature in the proportions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An extreme point (such as a tree that’s 30 cm across) shifts the fitted line
    to the right, away from the bulk of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address these issues, we introduce the *logistic model*.
  prefs: []
  type: TYPE_NORMAL
- en: A Logistic Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logistic model is one of the most widely used basic models for classification
    and a simple extension of the linear model. The *logistic function*, often called
    the *sigmoid function*, is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext mathvariant="bold">logistic</mtext> <mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn>
    <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mo>−</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *sigmoid* function is typically denoted by <math><mi>σ</mi> <mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></math> . Sadly, the Greek letter <math><mi>σ</mi></math>
    is widely used to mean a lot of things in data science and statistics, like the
    standard deviation, logistic function, and a permutation. You’ll have to be careful
    when seeing <math><mi>σ</mi></math> and use context to understand its meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the logistic function to reveal its s-shape (sigmoid-shape) and
    confirm that it outputs numbers between 0 and 1\. The function monotonically increases
    with <math><mi>t</mi></math> , and large values of <math><mi>t</mi></math> get
    close to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the logistic function maps to the interval between 0 and 1, it is commonly
    used when modeling proportions and probabilities. Also, we can write the logistic
    as a function of a line, <math><msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <mi>x</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>σ</mi> <mrow><mo>(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mo>−</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>−</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mi>x</mi> <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'To help build your intuition for the shape of this function, the following
    plot shows the logistic function as we vary <math><msub><mi>θ</mi> <mn>0</mn></msub></math>
    and <math><msub><mi>θ</mi> <mn>1</mn></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in06.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that changing the magnitude of <math><msub><mrow><mi>θ</mi></mrow>
    <mn>1</mn></msub></math> changes the sharpness of the curve; the farther away
    from 0, the steeper the curve. Flipping the sign of <math><msub><mrow><mi>θ</mi></mrow>
    <mn>1</mn></msub></math> reflects the curve about the vertical line <math><mi>x</mi>
    <mo>=</mo> <mn>0</mn></math> . Changing <math><msub><mi>θ</mi> <mn>0</mn></msub></math>
    shifts the curve left and right.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic function can be seen as a transformation: it transforms a linear
    function into a nonlinear smooth curve, and the output always lies between 0 and
    1\. In fact, the output of a logistic function has a deeper probabilistic interpretation,
    which we describe next.'
  prefs: []
  type: TYPE_NORMAL
- en: Log Odds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that the odds are the ratio <math><mi>p</mi> <mrow><mo>/</mo></mrow>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo></math>
    for a probability <math><mi>p</mi></math> . For example, when we toss a fair coin,
    the odds of getting heads are 1; for a coin that’s twice as likely to land heads
    as tails ( <math><mi>p</mi> <mo>=</mo> <mn>2</mn> <mrow><mo>/</mo></mrow> <mn>3</mn></math>
    ), the odds of getting heads are 2\. The logistic model is also called the *log
    odds* model because the logistic function coincides with a linear function of
    the log odds.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see this in the following equations. To show this, we multiply the numerator
    and denominator of the sigmoid function by <math><mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>σ</mi> <mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mo>−</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>exp</mi>
    <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow>
    <mrow><mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr> <mtr><mtd><mo
    stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>
    <mn>1</mn> <mo>−</mo> <mfrac><mrow><mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow> <mrow><mn>1</mn> <mo>+</mo> <mi>exp</mi>
    <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo>
    <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we take the logarithm of the odds and simplify:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>log</mi> <mo>⁡</mo> <mrow><mo>(</mo>
    <mfrac><mrow><mi>σ</mi> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow>
    <mrow><mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></mfrac> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo>
    <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>exp</mi> <mo>⁡</mo> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow> <mo stretchy="false">)</mo>
    <mo>=</mo> <mi>t</mi></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for <math><mi>σ</mi> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo stretchy="false">)</mo></math>
    , we find the log odds are a linear function of <math><mi>x</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>log</mi> <mo>⁡</mo> <mrow><mo>(</mo>
    <mfrac><mrow><mi>σ</mi> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo stretchy="false">)</mo></mrow>
    <mrow><mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo
    stretchy="false">)</mo></mrow></mfrac> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo>
    <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>exp</mi> <mo>⁡</mo> <mrow><mo
    stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <mi>x</mi> <mo stretchy="false">)</mo></mrow> <mo stretchy="false">)</mo>
    <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mi>x</mi></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This representation of the logistic in terms of log odds gives a useful interpretation
    for the coefficient <math><msub><mi>θ</mi> <mn>1</mn></msub></math> . Suppose
    the explanatory variable increases by 1\. Then the odds change as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mtext> odds </mtext> <mo>=</mo></mtd>
    <mtd><mi>exp</mi> <mo>⁡</mo> <mrow><mo>(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo stretchy="false">(</mo> <mi>x</mi>
    <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd><mo>=</mo></mtd> <mtd> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo> <mo>×</mo> <mi>exp</mi>
    <mo>⁡</mo> <mrow><mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo stretchy="false">)</mo></mrow></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: We see that the odds increase or decrease by a factor of <math><mi>exp</mi>
    <mo>⁡</mo> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo
    stretchy="false">)</mo></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, the <math><mi>log</mi></math> function is the natural logarithm. Since
    the natural log is the default in data science, we typically don’t bother to write
    it as <math><mi>ln</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s add a logistic curve to our plot of proportions to get a sense of
    how well it might fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Logistic Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following plot, we’ve added a logistic curve on top of the plot of proportions
    of fallen trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the curve follows the proportions reasonably well. In fact,
    we selected this particular logistic by fitting it to the data. The fitted logistic
    regression is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve seen that logistic curves can model probabilities well, we turn
    to the process of fitting logistic curves to data. In the next section, we proceed
    to our second step in modeling: selecting an appropriate loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: A Loss Function for the Logistic Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The logistic model gives us probabilities (or empirical proportions), so we
    write our loss function as <math><mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi>
    <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo></math> , where <math><mi>p</mi></math>
    is between 0 and 1\. The response takes on one of two values because our outcome
    feature is a binary classification. Thus, any loss function reduces to:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi>ℓ</mi></mrow>
    <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo>
    <mo>=</mo> <mrow><mo>{</mo> <mtable columnalign="left left" columnspacing="1em"
    rowspacing=".2em"><mtr><mtd><mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi>
    <mo>,</mo> <mn>0</mn> <mo stretchy="false">)</mo></mtd> <mtd><mrow><mtext>if </mtext>
    <mrow><mi>y</mi></mrow> <mtext> is 0</mtext></mrow></mtd></mtr> <mtr><mtd><mi>ℓ</mi>
    <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mn>1</mn> <mo stretchy="false">)</mo></mtd>
    <mtd><mrow><mtext>if </mtext> <mrow><mi>y</mi></mrow> <mtext> is 1</mtext></mrow></mtd></mtr></mtable></mrow></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, using 0 and 1 to represent the categories has an advantage because
    we can conveniently write the loss as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo>
    <mi>y</mi> <mo stretchy="false">)</mo> <mo>=</mo>  <mi>y</mi> <mi>ℓ</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mo stretchy="false">(</mo>
    <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mi>ℓ</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mo>,</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to confirm this equivalence by considering the two cases <math><mi>y</mi>
    <mo>=</mo> <mn>1</mn></math> and <math><mi>y</mi> <mo>=</mo> <mn>0</mn></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic model pairs well with *log loss*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi>ℓ</mi></mrow> <mo stretchy="false">(</mo>
    <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>=</mo></mtd>
    <mtd><mrow><mo>{</mo> <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em"><mtr><mtd><mo>−</mo>
    <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo></mtd>
    <mtd><mrow><mtext>if </mtext> <mrow><mi>y</mi></mrow> <mtext> is 1</mtext></mrow></mtd></mtr>
    <mtr><mtd><mo>−</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn>
    <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo></mtd> <mtd><mrow><mtext>if </mtext>
    <mrow><mi>y</mi></mrow> <mtext> is 0</mtext></mrow></mtd></mtr></mtable></mrow></mtd></mtr>
    <mtr><mtd><mo>=</mo></mtd> <mtd><mo>−</mo> <mi>y</mi> <mi>log</mi> <mo>⁡</mo>
    <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo>−</mo>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo>
    <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi>
    <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the log loss is not defined at 0 and 1 because <math><mo>−</mo> <mi>log</mi>
    <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo></math>
    tends to <math><mi mathvariant="normal">∞</mi></math> as <math><mi>p</mi></math>
    approaches 0, and similarly for <math><mo>−</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo></math> as <math><mi>p</mi></math>
    tends to 1\. We need to be careful to avoid the end points in our minimization.
    We can see this in the following plot of the two forms of the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in08.png)'
  prefs: []
  type: TYPE_IMG
- en: When <math><mi>y</mi></math> is 1 (solid line), the loss is small for <math><mi>p</mi></math>
    near 1, and when <math><mi>y</mi></math> is 0 (dotted line), the loss is small
    near 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our goal is to fit a constant to the data using log loss, then the average
    loss is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mo>,</mo> <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo>
    <mo>=</mo></mtd> <mtd><mfrac><mn>1</mn> <mi>n</mi></mfrac> <munder><mo>∑</mo>
    <mi>i</mi></munder> <mo stretchy="false">[</mo> <mo>−</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo>
    <mo>−</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo></mtd></mtr>
    <mtr><mtd><mo>=</mo></mtd> <mtd><mo>−</mo> <mfrac><msub><mi>n</mi> <mn>1</mn></msub>
    <mi>n</mi></mfrac> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi>
    <mo stretchy="false">)</mo> <mo>−</mo> <mfrac><msub><mi>n</mi> <mn>0</mn></msub>
    <mi>n</mi></mfrac> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn>
    <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here <math><msub><mi>n</mi> <mn>0</mn></msub></math> and <math><msub><mi>n</mi>
    <mn>1</mn></msub></math> are the number of <math><msub><mi>y</mi> <mi>i</mi></msub></math>
    that are 0 and 1, respectively. We can differentiate with respect to <math><mi>p</mi></math>
    to find the minimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mrow><mi>∂</mi> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mo>,</mo> <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo></mrow>
    <mrow><mi>∂</mi> <mi>p</mi></mrow></mfrac> <mo>=</mo> <mo>−</mo> <mfrac><msub><mi>n</mi>
    <mn>1</mn></msub> <mrow><mi>n</mi> <mi>p</mi></mrow></mfrac> <mo>+</mo> <mfrac><msub><mi>n</mi>
    <mn>0</mn></msub> <mrow><mi>n</mi> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo>
    <mi>p</mi> <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we set the derivative to 0 and solve for the minimizing value <math><mrow><mover><mi>p</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mn>0</mn></mtd> <mtd><mo>=</mo>
    <mo>−</mo> <mfrac><msub><mi>n</mi> <mn>1</mn></msub> <mrow><mi>n</mi> <mrow><mrow><mover><mi>p</mi>
    <mo stretchy="false">^</mo></mover></mrow></mrow></mrow></mfrac> <mo>+</mo> <mfrac><msub><mi>n</mi>
    <mn>0</mn></msub> <mrow><mi>n</mi> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo>
    <mrow><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow></mrow>
    <mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mo>=</mo> <mo>−</mo> <mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow><mover><mi>p</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo> <mfrac><msub><mi>n</mi>
    <mn>1</mn></msub> <mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow></mfrac>
    <mo>+</mo> <mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow><mover><mi>p</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo> <mfrac><msub><mi>n</mi>
    <mn>0</mn></msub> <mrow><mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow><mrow><mover><mi>p</mi>
    <mo stretchy="false">^</mo></mover></mrow></mrow> <mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr>
    <mtr><mtd><mrow><msub><mi>n</mi> <mn>1</mn></msub></mrow> <mo stretchy="false">(</mo>
    <mn>1</mn> <mo>−</mo> <mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mrow><msub><mi>n</mi> <mn>0</mn></msub></mrow>
    <mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow></mtd></mtr>
    <mtr><mtd><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow></mtd>
    <mtd><mo>=</mo> <mfrac><msub><mi>n</mi> <mn>1</mn></msub> <mi>n</mi></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: (The final equation results from noting that <math><msub><mi>n</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>n</mi> <mn>1</mn></msub> <mo>=</mo> <mi>n</mi></math> .)
  prefs: []
  type: TYPE_NORMAL
- en: 'To fit a more complex model based on the logistic function, we can substitute
    <math><mi>σ</mi> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo stretchy="false">)</mo></math>
    for <math><mi>p</mi></math> . And the loss for the logistic model becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi>ℓ</mi></mrow> <mo stretchy="false">(</mo>
    <mi>σ</mi> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo stretchy="false">)</mo> <mo>,</mo>
    <mi>y</mi> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mi>y</mi> <mi>ℓ</mi>
    <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo
    stretchy="false">)</mo> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>+</mo>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo>
    <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mi>x</mi> <mo stretchy="false">)</mo> <mo>,</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi>
    <mo stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mo>=</mo> <mi>y</mi> <mi>log</mi>
    <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mi>x</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo> <mo>+</mo>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo>
    <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mi>x</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Averaging the loss over the data, we arrive at:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mo>,</mo> <mtext mathvariant="bold">x</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munder><mo>∑</mo>
    <mi>i</mi></munder></mtd> <mtd><mo>−</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mi>log</mi>
    <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><mo>−</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Unlike with squared loss, there is no closed-form solution to this loss function.
    Instead, we use iterative methods like gradient descent (see [Chapter 20](ch20.html#ch-gd))
    to minimize the average loss. This is also one of the reasons we don’t use squared
    error loss for logistic models—the average squared error is nonconvex, which makes
    it hard to optimize. The notion of convexity is covered in greater detail in [Chapter 20](ch20.html#ch-gd),
    and [Figure 20-4](ch20.html#gd-convex) gives a picture for intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Log loss is also called *logistic loss* and *cross-entropy loss*. Another name
    for it is the *negative log likelihood*. This name refers to the technique of
    fitting models using the likelihood that a probability distribution produced our
    data. We do not go any further into the background of these alternative approaches
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the logistic model (with the log loss) is called *logistic regression*.
    Logistic regression is an example of a generalized linear model, a linear model
    with a nonlinear transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can fit logistic models with `scikit-learn`. The package designers made
    the API very similar to fitting linear models by least squares (see [Chapter 15](ch15.html#ch-linear)).
    First, we import the logistic regression module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we set up the regression problem with outcome `y`, the status of the tree,
    and covariate `X`, the diameter (which we have log-transformed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we fit the logistic regression and examine the intercept and coefficient
    for diameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When making a prediction, the `predict` function returns the predicted (most
    likely) class, and `predict_proba` returns the predicted probability. For a tree
    with diameter 6, we expect the prediction to be 0 (meaning `standing`) with a
    high probability. Let’s check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the model predicts that a tree with a diameter of 6 has a 0.87 probability
    for the class `standing` and a 0.13 probability for `fallen`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve fit a model with one feature, we might want to see if including
    another feature like the strength of the storm can improve the model. To do this,
    we can fit a multiple logistic regression by adding a feature to `X` and fitting
    the model again.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the logistic regression fits a model to predict probabilities—the
    model predicts that a tree with diameter 6 has a 0.87 probability of class `standing`
    and a 0.13 probability of class `fallen`. Since probabilities can be any number
    between 0 and 1, we need to convert the probabilities back to categories to perform
    classification. We address this classification problem in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: From Probabilities to Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started this chapter by presenting a binary classification problem where
    we want to model a nominal response variable. At this point, we have used logistic
    regression to model proportions or probabilities, and we’re now ready to return
    to the original problem: we use the predicted probabilities to classify records.
    For our example, this means that for a tree of a particular diameter, we use the
    fitted coefficients from the logistic regression to estimate the chance it is
    fallen. If the chance is high, we classify a tree as fallen; otherwise, we classify
    it as standing. But we need to choose a threshold for making this *decision rule*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sklearn` logistic regression model’s `predict` function implements the
    basic decision rule: predict `1` if the predicted probability <math><mi>p</mi>
    <mo>></mo> <mn>0.5</mn></math> . Otherwise, predict 0\. We’ve overlaid this decision
    rule on top of the model predictions as a dotted line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in09.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we consider a more general decision rule. For some choice of
    <math><mi>τ</mi></math> , predict 1 if the model’s predicted probability <math><mi>p</mi>
    <mo>></mo> <mi>τ</mi></math> , otherwise predict 0\. By default, `sklearn` sets
    <math><mi>τ</mi> <mo>=</mo> <mn>0.5</mn></math> . Let’s explore what happens when
    <math><mi>τ</mi></math> is set to other values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing an appropriate value for <math><mi>τ</mi></math> depends on our goals.
    Suppose we want to maximize accuracy. The *accuracy* of a classifier is the fraction
    of correct predictions. We can compute the accuracy for different thresholds,
    meaning different <math><mi>τ</mi></math> values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To understand how accuracy changes with respect to <math><mi>τ</mi></math>
    , we make a plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in10.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the threshold with the highest accuracy isn’t exactly at 0.5\. In
    practice, we should use cross-validation to select the threshold (see [Chapter 16](ch16.html#ch-risk)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The threshold that maximizes accuracy could be a value other than 0.5 for many
    reasons, but a common one is *class imbalance*, where one category is more frequent
    than another. Class imbalance can lead to a model that classifies a record as
    belonging to the more common category. In extreme cases (like fraud detection)
    when only a tiny fraction of the data contain a particular class, our models can
    achieve high accuracy by simply always predicting the frequent class without learning
    what makes a good classifier for the rare class. There are techniques for managing
    class imbalance, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Resampling the data to reduce or eliminate the class imbalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting the loss function to put a larger penalty on the smaller class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example, the class imbalance is not that extreme, so we continue without
    these adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of class imbalance explains why accuracy alone is often not how
    we want to judge a model. Instead, we want to differentiate between the types
    of correct and incorrect classifications. We describe these next.
  prefs: []
  type: TYPE_NORMAL
- en: The Confusion Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A convenient way to visualize errors in a binary classification is to look
    at the confusion matrix. The confusion matrix compares what the model predicts
    with the actual outcomes. There are two types of error in this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*False positives*'
  prefs: []
  type: TYPE_NORMAL
- en: When the actual class is 0 (false) but the model predicts 1 (true)
  prefs: []
  type: TYPE_NORMAL
- en: '*False negatives*'
  prefs: []
  type: TYPE_NORMAL
- en: When the actual class is 1 (true) but the model predicts 0 (false)
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would like to minimize both kinds of errors, but we often need to
    manage the balance between these two sources.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The terms *positive* and *negative* come from disease testing, where a test
    indicating the presence of a disease is called a positive result. This can be
    a bit confusing because having a disease doesn’t seem like something positive
    at all. And <math><mi>y</mi> <mo>=</mo> <mn>1</mn></math> denotes the “positive”
    case. To keep things straight, it’s a good idea to confirm your understanding
    of what <math><mi>y</mi> <mo>=</mo> <mn>1</mn></math> stands for in the context
    of your data.
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` has a function to compute and plot the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_19in11.png)'
  prefs: []
  type: TYPE_IMG
- en: Ideally, we want to see all of the counts in the diagonal squares True negative
    and True positive. That means we have correctly classified everything. But this
    is rarely the case, and we need to assess the size of the errors. For this, it’s
    easier to compare rates than counts. Next, we describe different rates and when
    we might prefer to prioritize one or the other.
  prefs: []
  type: TYPE_NORMAL
- en: Precision Versus Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some settings, there might be a much higher cost to missing positive cases.
    For example, if we are building a classifier to identify tumors, we want to make
    sure that we don’t miss any malignant tumors. Conversely, we’re less concerned
    about classifying a benign tumor as malignant because a pathologist would still
    need to take a closer look to verify the malignant classification. In this case,
    we want to have a high true positive rate among the records that are actually
    positive. The rate is called *sensitivity*, or *recall*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>Recall</mtext> <mo>=</mo> <mfrac><mtext>True Positives</mtext>
    <mrow><mtext>True Positives</mtext> <mo>+</mo> <mtext>False Negatives</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mtext>True Positives</mtext> <mtext>Actually True</mtext></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: Higher recall runs the risk of predicting true on false records (false positives).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, when classifying email as spam (positive) or ham (negative),
    we might be annoyed if an important email gets thrown into our spam folder. In
    this setting, we want high *precision*, the accuracy of the model for positive
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>Precision</mtext> <mo>=</mo> <mfrac><mtext>True
    Positives</mtext> <mrow><mtext>True Positives</mtext> <mo>+</mo> <mtext>False
    Positives</mtext></mrow></mfrac> <mo>=</mo> <mfrac><mtext>True Positives</mtext>
    <mtext>Predicted True</mtext></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: Higher-precision models are often more likely to predict that true observations
    are negative (higher false-negative rate).
  prefs: []
  type: TYPE_NORMAL
- en: 'A common analysis compares the precision and recall at different thresholds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To see how precision and recall relate, we plot them both against the threshold
    <math><mi>τ</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_19in12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another common plot used to evaluate the performance of a classifier is the
    *precision-recall curve*, or PR curve for short. It plots the precision-recall
    pairs for each threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_19in13.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the righthand end of the curve reflects the imbalance in the sample.
    The precision matches the fraction of fallen trees in the sample, 0.35\. Plotting
    multiple PR curves for different models can be particularly useful for comparing
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using precision and recall gives us more control over what kinds of errors
    matter. As an example, let’s suppose we want to ensure that at least 75% of the
    fallen trees are classified as fallen. We can find the threshold where this occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We find that about 41% (1 – precision) of the trees that we classify as fallen
    are actually standing. In addition, we find the fraction of trees below this threshold
    to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have classified 52% of the samples as standing (negative). *Specificity*
    (also called *true negative rate*) measures the proportion of data belonging to
    the negative class that the classifier labels as negative:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>Specificity</mtext> <mo>=</mo> <mfrac><mtext>True
    Negatives</mtext> <mrow><mtext>True Negatives</mtext> <mo>+</mo> <mtext>False
    Positives</mtext></mrow></mfrac> <mo>=</mo> <mfrac><mtext>True Negatives</mtext>
    <mtext>Predicted False</mtext></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The specificity for our threshold is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In other words, 70% of the trees classified as standing are actually standing.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen, there are several ways to use the 2-by-2 confusion matrix. Ideally,
    we want accuracy, precision, and recall to all be high. This happens when most
    predictions fall along the diagonal for the table, so our predictions are nearly
    all correct–true negatives and true positives. Unfortunately, in most scenarios
    our models will have some amount of error. In our example, trees of the same diameter
    include a mix of fallen and standing, so we can’t perfectly classify trees based
    on their diameter. In practice, when data scientists choose a threshold, they
    need to consider their context to decide whether to prioritize precision, recall,
    or specificity.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we fit simple logistic regressions with one explanatory variable,
    but we can easily include other variables in the model by adding more features
    to our design matrix. For example, if some predictors are categorical, we can
    include them as one-hot encoded features. These ideas carry over directly from
    [Chapter 15](ch15.html#ch-linear). The technique of regularization ([Chapter 16](ch16.html#ch-risk))
    also applies to logistic regression. We will integrate all of these modeling techniques—including
    using a train-test split to assess the model and cross-validation to choose the
    threshold—in the case study in [Chapter 21](ch21.html#ch-fake-news) that develops
    a model to classify fake news.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is a cornerstone in machine learning since it naturally
    extends to more complex models. For example, logistic regression is one of the
    basic components of a neural network. When the response variable has more than
    two categories, logistic regression can be extended to multinomial logistic regression.
    Another extension of logistic regression for modeling counts is called Poisson
    regression. These different forms of regression are related to maximum likelihood,
    where the underlying model for the response is binomial, multinomial, or Poisson,
    respectively, and the goal is to optimize the likelihood of the data over the
    parameters of the respective distribution. This family of models is also known
    as generalized linear models. In all of these scenarios, closed-form solutions
    for minimizing loss don’t exist, so optimization of the average loss relies on
    numerical methods, which we cover in the next chapter.
  prefs: []
  type: TYPE_NORMAL
