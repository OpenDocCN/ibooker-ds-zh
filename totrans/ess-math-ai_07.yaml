- en: 'Chapter 7\. Natural Language And Finance AI: Vectorization And Time Series'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*They. Can. Read.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One of the hallmarks of human intelligence is our mastery of language at a
    very early age: Comprehension of written and spoken language, written and spoken
    expression of thoughts, conversation between two or more people, translation from
    one language to another, and the use of language to express empathy, convey emotions,
    and process visual and audio data perceived from our surroundings. Leaving the
    philosophical question of consciousness aside, if machines acquire the ability
    to perform these language tasks, deciphering the intent of words, at a level similar
    to humans, or above humans, then it is a major propeller towards general artificial
    intelligence. These tasks fall under the umbrellas of *natural language processing*,
    *computational linguistics*, *machine learning* and/or *probabilistic language
    modeling*. These fields are vast and it is easy to find interested people wandering
    aimlessly in a haze of various models with big promises. We should not get lost.
    The aim of this chapter is to lay out the natural processing field all at once
    so we can have a bird’s eye view without getting into the weeds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following questions guide us at all times:'
  prefs: []
  type: TYPE_NORMAL
- en: What type of task is at hand? In other words, what is our goal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of data is at hand? What type of data do we need to collect?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What state of the art models are out there that deal with similar tasks and
    similar types of data? If there are none, then we have to come up with the models
    ourselves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we train these models? In what formats do they consume their data? In
    what formats do they produce their outputs? Do they have a training function,
    loss function (or objective function), and optimization structure?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages and disadvantages of various models versus others?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there Python packages or libraries available for their implementation? Luckily,
    nowadays, most models come out accompanied with their Python implementations and
    very simple APIs (application programming interfaces). Even better, there are
    many pre-trained models available to download and ready for use in applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How much computational infrastructure do we need in order to train and/or deploy
    these models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we do better? There is always room for improvement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also need to *extract the math from the best performing models*. Thankfully,
    this is the easy part, since similar mathematics underlies many models, even when
    relating to different types of tasks or from dissimilar application areas, such
    as predicting the next word in a sentence or predicting the stock market behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The state of the art models that we intend to cover in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers or attention models (since 2017). The important math here is extremely
    simple: The dot product between two vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent long short term memory neural networks (since 1995). The important
    math here is *backpropagation in time*. We covered backpropagation in [Chapter 4](ch04.xhtml#ch04),
    but for recurrent nets, we take the derivatives with respect to time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks (since 1989) for time series data. The important
    math is the *convolution* operation, which we covered in [Chapter 5](ch05.xhtml#ch05).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These models are very well suited for *time series data*, that is, data that
    appears sequentially with time. Examples of time series data are movies, audio
    files such as music and voice recordings, financial markets data, climate data,
    dynamical systems data, documents, and books.
  prefs: []
  type: TYPE_NORMAL
- en: 'We might wonder why documents and books can be considered as time dependent,
    even though they have already been written and are just *there*. How come an image
    is not time dependent but a book, and in general, reading and writing, are? The
    answer is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: When we read a book, we comprehend what we read one word at a time, then one
    phrase at a time, then one sentence at a time, then one paragraph at a time, and
    so on. This is how we grasp the concepts and topics of the book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same is true when we write a document, outputting one word at time, even
    though the whole idea we are trying to express is already there, *encoded*, before
    we output the words, *sequentially*, on paper.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we caption an image, the image itself is not time dependent, but our captioning
    (the output) is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we summarize an article, answer a question, or translate from one language
    to another, the output text is time dependent. The input text could be time dependent
    if processed using a recurrent neural network, or stationary if processed all
    at once using a transformer or a convolutional model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Until 2017, the most popular machine learning models to process time series
    data were based either on *convolutional neural neworks* or on *recurrent neural
    networks with long short term memory*. In 2017, *transformers* took over, abandoning
    recurrence altogether in certain application areas. The question whether recurrent
    neural networks are obsolete is out there, but with things changing everyday in
    the AI field, who knows which models die and which models survive the test of
    time. Moreover, recurrent neural networks power many AI engines and are still
    subjects of active research.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we transform natural language text to numerical quantities that retain
    meaning? Our machines only understand numbers, and we need to process natural
    language using these machines. We must *vectorize* our samples of text data, or
    *embed* them into finite dimensional vector spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we lower the dimension of the vectors from the enormous ones initially
    required to represent natural language? For example, the French language has around
    135,000 distinct words, how do we get around having to one-hot code words in a
    French sentence using vectors of 135,000 entries each?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the model at hand consider (as its input and/or output) our natural language
    data as a time dependent sequence fed into it one term at a time, or a stationary
    vector consumed all at once?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How exaclty do various models for natural language processing work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is there finance in this chapter as well?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Along the way, we discuss the types of natural language and finance applications
    that our models are well suited for. We keep the focus on the mathematics and
    not the programming, since such models (especially for language applications)
    require substantive computational infrastructures. For example, [DeepL Translator](https://www.deepl.com/en/translator)
    generates its translations using a supercomputer operated with hydropower from
    Iceland, which reaches 5.1 petaflops. We also note that the AI-specialized chip
    industry is booming, led by Nvidia, Google’s Tensor Processing Unit, Amazon’s
    Inferentia, AMD’s Instinct GPU, and startups like Cerebras and Graphcore. While
    conventional chips have struggled to keep pace with Moore’s Law, which predicted
    a doubling of processing power every 18 months, AI-specialized chips have outpaced
    this law by a wide margin.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we do not write code for this chapter, we note that most programming
    can be accomplished using Python’s TensorFlow and Keras libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout our discussion below, we have to be mindful of whether we are in
    the *training* phase of a model or in the *prediction* phase (using the pre-trained
    model to do tasks). Moreover, it is important to differentiate whether our model
    needs labeled data to be trained, such as English sentences *along with* their
    French translations as labels, or can learn from unlabeled data, such as computing
    *the meanings* of words from their contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Natural language processing applications are ubiquitous. This technology has
    been integrated into many aspects of our lives that we just take it for granted,
    when using apps on our smartphones, digital calendars, digital home assistants,
    Siri, Alexa, and others. The following list is partially adapted from the excellent
    book: *Natural Language Processing In Action* by Lane, Howard, and Hapke. It demonstrates
    how indispencable natural language processing has become:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Search and information retrieval: Web, documents, autocomplete, chatbots'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Email: Spam filter, email classification, email prioritization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Editing: Spelling check, grammar check, style recommendation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentiment Analysis: Product reviews, customer care, monitoring of community
    morale'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dialog: Chatbots, digital assistants such as Amazon’s Alexa, scheduling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Writing: Indexing, concordance, table of contents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text mining: Summarization, knowledge extraction such as mining election campaigns’
    finance and natural language data (finding connections between political donors),
    resume to job matching, medical diagnosis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Law: Legal inference, precedent search, subpoena classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'News: Event detection, fact checking, headline composition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attribution: Plagiarism detection, literary forensics, style coaching'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behavior prediction: Finance applications, election forecasting, marketing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creative Writing: Movie scripts, poetry, song lyrics, bot powered financial
    and sports news stories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Captioning: Computer vision combined with natural language processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Translation: Google Translate and DeepL Translate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even though the past decade has brought impressive feats, machines are still
    nowhere close to mastering natural language. The processes involved are tedious,
    requiring attentive statistical bookkeeping, and substansive *memory*, the same
    way humans require memory to master languages. The point here is: There is plenty
    of room for new innovations and contributions to the field.'
  prefs: []
  type: TYPE_NORMAL
- en: Language models have recently shifted from hand coded to data driven. They do
    not implement hard coded logical and grammar rules. Instead, they rely on detecting
    the statistical relationships between words. Even though there is a school of
    thought in linguistics that asserts grammar as an innate property for humans,
    or in other words, *hardcoded* into our brains, humans have a striking ability
    to master new languages without ever encountering any grammatical rules for these
    languages. From personal experience, attempting to learn the grammar of a new
    language seems to impede the learning process, but do not quote me on that.
  prefs: []
  type: TYPE_NORMAL
- en: One major challenge is that data for natural language is extremely high dimensional.
    There are millions of words across thousands of languages. There are huge corpuses
    of documents, such as entire collections of authors’ works, billions of tweets,
    wikipedia articles, news articles, Facebook comments, movie reviews, *etc*. A
    first goal is then to reduce the number of dimensions for efficient storage, processing,
    and computation, while at the same time avoiding the loss of essential information.
    This has been a common theme in the AI field, and one cannot help but wonder how
    many mathematical innovations would have never seen the light of the day had we
    possessed unlimited storage and computational infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Natural Language Data For Machine Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a machine to process any natural language task, the first thing it must
    do is to break down text and organize it into building blocks that retain meaning,
    intent, context, topics, information, and sentiments. To this end, it must establish
    a correspondence between words and number tags, using processes called *tokenizing*,
    *stemming* (such as giving singular words and their plural variation the same
    token), *lemmatization* (associating several words of similar meaning together),
    *case normalization* (such as giving capitalized and lower case words of same
    spelling the same tokens) and others. This correspondence is not for individual
    characters that make up words, but for full words, pairs or more of words (*2-grams
    or n-grams*), punctuations, significant capitalizations, *etc.*, that carry meaning.
    This creates a *vocabulary* or a *lexicon* of numerical tokens corresponding to
    a given corpus of natural language documents. A vocabulary or a lexicon in this
    sense is similar to a Python dictionary: Each individual natural language *building
    block object* has a unique token.'
  prefs: []
  type: TYPE_NORMAL
- en: An *n-gram* is a sequence of *n* words that carry different meaning when kept
    ordered together than when each word is floating on its own. For example, a 2-gram
    is a couple of words that come together and it would change the meaning if we
    unpair them, such as *ice cream* or *was not*, so the whole 2-gram gets one numerical
    token, retaining the meaning of the two words within their correct context. Similarly,
    a 3-gram is a triplet of ordered words, such as *John F. Kennedy*; and so on.
    A *parser* for natural language is the same as a compiler for computers. Do not
    worry if these new terms confuse you. For our mathematical purposes, all we need
    are the numerical tokens associated with unique words, *n-grams* , emojis, punctuations,
    *etc.*, and the resulting *vocabulary* for a corpus of natural language documents.
    These are saved in dictionary like objects, allowing us to flip back and forth
    easily between text and numerical tokens.
  prefs: []
  type: TYPE_NORMAL
- en: We leave the actual details of tokenizing, stemming, lemmatization, parsing,
    and other natural language data preparations for computer scientists and their
    collaborations with linguists. In fact, collaboration with linguists has become
    less important as the models mature in their ability to detect patterns directly
    from the data, thus, the need for coding hand crafted linguistic rules into natural
    language models has diminished. Note also that not all natural language pipelines
    include stemming and lemmatization. They all, however, involve tokenizng. The
    quality of tokenizing text data is crucial for the performance of our natural
    language pipeline. It is the first step containing fundamental building blocks
    representing the data that we feed into our models. The quality of both the data
    and the way it is tokenized affects the outputs of the entire natural language
    processing pipeline. For your production applications, use *spaCy* parser, which
    does sentence segmentation, tokenization, and multiple other things in one pass.
  prefs: []
  type: TYPE_NORMAL
- en: After tokenizing and possessing a healthy vocabulary (the collection of numerical
    tokens and the entities they correspond to in the natural language text), we need
    to represent entire natural language documents using vectors of numbers. These
    documents can range from very long, such as a book series, to very short, such
    as a Twitter tweet or a simple search query for Google Search or DuckDuckGo. We
    can then express a corpus of one million documents as a collection of one million
    numerical vectors, or a matrix with one million columns. These columns will be
    as long as our chosen vocabulary, or shorter if we decide to *compress* these
    documents further. In linear algebra language, the length of these vectors is
    the dimension of our *vector space* that our documents are *embedded in*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole point of the above process is to obtain numerical vector representations
    of our documents so that we can do math on them: Comes linear algebra with its
    arsenal of linear combinations, projections, dot products, and singular value
    decompositions. There is, however, one caveat: For natural language applications,
    the lengths of the vectors representing our documents, or the size of our vocabulary,
    are prohibitively enormous to do any useful computations with. The curse of dimensionality
    becomes a real thing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The Curse Of Dimensionality'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vectors become exponentially farther apart in terms of Euclidean distance as
    the number of dimensions increases. One natural language example is sorting documents
    based on their *distance* from another document, such as a search query. This
    simple operation becomes impractical when we go above 20 dimensions or so, if
    we use the Euclidean distance to measure the *closeness* of documents (see [Wikipedia’s
    Curse Of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)
    for more details). Thus, for natural language applications, we must use another
    measure for distance between documents. We will discuss *cosine similarity* shortly,
    which measures the *angle* between two document vectors, as opposed to their Euclidean
    distance.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a main driver for natural language processing models is to represent
    these documents using shorter vectors that convey the main topics and retain meaning.
    Think how many unique tokens or combinations of tokens we have to use in order
    to represent this book while at the same time preserving its most important information.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, our natural language processing pipeline proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From text to numerical tokens, then to an acceptable vocabulary for an entire
    corpus of documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From documents of tokens to high dimensional vectors of numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From high dimensional vectors of numbers to lower dimensional vectors of topics
    using techniques like *direct projection onto a smaller subset of the vocabulary
    space* (just dropping part of the vocabulary, making the corresponding entries
    zero), *latent semantic analysis* (projecting onto special vectors determined
    by special linear combinations of the document vectors), *Word2Vec*, *Doc2Vec*,
    *thought vectors*, *Dirichlet allocation*, and others. We dicuss these shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As it is usually the case in mathematical modeling, there is more than one way
    to represent a given document as a vector of numbers. It is us who decide on the
    vector space which our documents inhabit, or get *embedded in*. Each vector representation
    has advantages and disadvantages, depending on the goal of our natural language
    task. Some are simpler than others too.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Models And The *log* Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When representing a document as a vector of numbers starts with counting the
    number of times certain terms appear in the document, then our document vectorizing
    model is *statistical*, since it is frequency based.
  prefs: []
  type: TYPE_NORMAL
- en: When we deal with term frequencies, it is better to apply the *log* function
    to our counts as opposed to using raw counts. The *log* function is advantageous
    when we deal with quantities that could get extremely large, extremely small,
    or could have extreme variations in scale. Viewing these extreme counts or variations
    within a logarithmic scale brings them back to the normal realm.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the number <math alttext="10 Superscript 23"><msup><mn>10</mn>
    <mn>23</mn></msup></math> is huge, but <math alttext="log left-parenthesis 10
    Superscript 23 Baseline right-parenthesis equals 23 log left-parenthesis 10 right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msup><mn>10</mn> <mn>23</mn></msup> <mo>)</mo></mrow>
    <mo>=</mo> <mn>23</mn> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>10</mn>
    <mo>)</mo></mrow></mrow></math> is not. Similarly, if the term *shark* appears
    in two documents of a corpus of 20 million documents (20 million/2=10 million),
    and the term *whale* appears in twenty documents of this corpus (20 million/20=1
    million), then that is a 9 million difference, which seems excessive for terms
    that appeared in two and twenty documents respectively. Computing the same quantities
    but on a *log* scale, we get *7log(10)* and *6log(10)* respectively (it doesn’t
    matter which *log* base we use), which doesn’t seem excessive anymore, and more
    in line with the terms’ occurence in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The need for using the *log* function when dealing with word counts in particular
    is reinforced by *Zipf law*. This law says that term counts in a corpus of natural
    language naturally follow a power law, so it is best to temper that with a log
    function, transforming differences in term frequencies into a linear scale. We
    discuss this next.
  prefs: []
  type: TYPE_NORMAL
- en: Zipf’s Law For Term Counts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Zipf’s law*](https://en.wikipedia.org/wiki/Zipf%27s_law) for natural language
    has to do with word counts. It is very interesting and so surprising that I am
    tempted to try and see if it applies to my own book. It is hard to imagine that
    as I a write each word in this book, my unique word counts are actually following
    some law. Are we, along with the way we word our ideas and thoughts, *that predictable*?
    It turns out that Zipf’s law extends to counting many things around us, not only
    words in documents and corpuses.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Zipf’s Law***: *For a corpus of natural language where the terms have been
    ordered according to their frequencies, the frequency of the first item is twice
    as that of the second item, three times as the third item, and so on.* That is,
    the frequency with which an item appears in a corpus is related to its ranking:
    <math alttext="f 1 equals 2 f 2 equals 3 f 3 equals period period period"><mrow><msub><mi>f</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>2</mn> <msub><mi>f</mi> <mn>2</mn></msub> <mo>=</mo>
    <mn>3</mn> <msub><mi>f</mi> <mn>3</mn></msub> <mo>=</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo></mrow></math>'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify if Zipf’s law applies by plotting the frequency of the terms
    against their respective ranks and verifying the power law: <math alttext="f Subscript
    r Baseline equals f left-parenthesis r right-parenthesis equals f 1 r Superscript
    negative 1"><mrow><msub><mi>f</mi> <mi>r</mi></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>r</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>f</mi> <mn>1</mn></msub> <msup><mi>r</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math> . To verify power laws,
    it is easier to make a *log-log* plot, plotting <math alttext="log left-parenthesis
    f Subscript r Baseline right-parenthesis"><mrow><mo form="prefix">log</mo> <mo>(</mo>
    <msub><mi>f</mi> <mi>r</mi></msub> <mo>)</mo></mrow></math> against <math alttext="log
    left-parenthesis r right-parenthesis"><mrow><mo form="prefix">log</mo> <mo>(</mo>
    <mi>r</mi> <mo>)</mo></mrow></math> . If we obtain a straight line in the *log-log*
    plot, then <math alttext="f Subscript r Baseline equals f left-parenthesis r right-parenthesis
    equals f 1 r Superscript alpha"><mrow><msub><mi>f</mi> <mi>r</mi></msub> <mo>=</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>r</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>f</mi>
    <mn>1</mn></msub> <msup><mi>r</mi> <mi>α</mi></msup></mrow></math> where <math
    alttext="alpha"><mi>α</mi></math> is the slope of the straight line.'
  prefs: []
  type: TYPE_NORMAL
- en: Various Vector Representations For Natural Language Documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s list the most common document vector representations for state of the
    art natural language processing models. The first two, Term Frequency and Term
    Frequency times Inverse Document Frequency, are statistical representations since
    they are frequency based, relying on counting word appearances in documents. They
    are slightly more involved than a simple binary representation detecting the presence
    or nonpresence of certain words, nevertheless, they are still shallow, merely
    counting words. Even with this shallowness, they are very useful for applications
    such as spam filtering and sentiment analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Term Frequency (TF) vector representation of a document or bag of words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we represent a document using a *bag of words*, discarding the order in
    which words appear in the document. Even though word order encodes important information
    about a document’s content, ignoring it is usually an okay approximation for short
    sentences and phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that we want to embed our given document in a *vocabulary space* of
    10,000 tokens. Then the vector representing this document will have 10,000 entries,
    with each entry counting how many times each particular token appears in the document.
    For the obvious reasons, this is called the *Term Frequency* or *bag of words*
    vector representation of the document, where each entry is a nonnegative integer
    (a whole number).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a Google search query: *What’s the weather tomorrow?* will be
    vectorized as zeros everywhere except for ones at the tokens representing the
    words *what*, *the* *weather*, and *tomorrow* if they exist in the vocabulary.
    We then *normalize* this vector, dividing each entry by the total number of terms
    in the document, so that the length of the document doesn’t skew our analysis.
    That is, if a document has 50,000 terms and the term *cat* gets mentioned a hundred
    times, and another document has a hundred terms only and the term *cat* gets mentioned
    10 times, then obviously the word cat is more important for the second document
    than for the first, and a mere word count without normalizing would not be able
    to capture that.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, some natural language processing classes take the log of each term
    in the document vector for the reasons mentioned in the previous two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Term Frequency times Inverse Document Frequency (TF-IDF) vector representation
    of a document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, for each entry of the vector representing the document we still count
    the number of times the token appears in the document, *but then we divide by
    the number of documents in our corpus in which the token occurs*.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that if a term appears many times in one document and not as much
    in the others, then this term must be important for this one document, getting
    a higher score in the corresponding entry of the vector representing this document.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to avoid division by zero, in the case a term does not appear in any
    document, it is common practice to add one to the denominator. For example, the
    inverse document frequency of the token *cat* is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign IDF for cat equals StartFraction number of documents
    in corpus Over number of documents containing cat plus 1 EndFraction dollar-sign"><mrow><mtext>IDF</mtext>
    <mtext>for</mtext> <mtext>cat</mtext> <mo>=</mo> <mfrac><mrow><mtext>number</mtext><mtext>of</mtext><mtext>documents</mtext><mtext>in</mtext><mtext>corpus</mtext></mrow>
    <mrow><mtext>number</mtext><mtext>of</mtext><mtext>documents</mtext><mtext>containing</mtext><mtext>cat</mtext><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, using TF-IDF representation, the entries of the document vectors
    will be nonnegative rational numbers, each providing a measure of the *importance*
    of that particular token to the document. Finally, we take the log of each entry
    in this vector, for the same reasons stated in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: There are many alternative TF-IDF approaches relevant to information retrieval
    systems, such as Okapi BM25\. See Molino 2017.
  prefs: []
  type: TYPE_NORMAL
- en: Topic vector represenation of a document determined by latent semantic analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TF-IDF vectors are very high dimensional (as many dimensions as tokens in the
    corpus, so it could be in the millions), sparse, and have no special meaning when
    added or subtracted from each other. We need more compact vectors, in the hundreds
    of dimensions or less, which is a big squeeze from millions of dimensions. In
    addition to the dimension reduction advantage, these vectors capture some meaning
    not only word counts and statistics. We call them *topic vectors*. Instead of
    focusing on the *statistics of words in documents*, we focus on the *statistics
    of connections between words in documents and across corpuses*. The topics produced
    here will be linear combinations of word counts.
  prefs: []
  type: TYPE_NORMAL
- en: First we process the whole TF-IDF matrix *X* of our corpus, producing our *topic
    space*. Processing in this case means that we compute the *singular value decomposition*
    of the TF-IDF matrix from linear algebra, namely, <math alttext="upper X equals
    upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> . We have
    a whole chapter on the singular value decomposition in this book, so we will not
    go into its details now, but we will explain how it is used for producing our
    topic space for a corpus. Singular value decomposition from linear algebra is
    called [*latent semantic analysis*](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
    in natural language processing. We will use both terms synonymously.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to pay attention as to whether the columns of the corpus’s TF-IDF matrix
    *X* represent the word tokens or the documents. Different authors and software
    packages use one or the other, so we must be careful and process either the matrix
    or its transpose in order to produce our topic space. In this section we follow
    the representation: The rows are all the words (tokens for words, n-grams, etc.)
    of the entire corpus, and the columns are the TF-IDF vector representations for
    each document in the corpus. This is slightly divergent from the usual representation
    of a data matrix, where the features (the words within each document) are in the
    columns and the instances (the documents) are in the rows. The reason for this
    switch will be apparent shortly. However, this is not divergent from our represetation
    for documents as column vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, given a new document with its TF-IDF vector representation, we convert
    it to a much more compact *topic vector* by *projecting it onto* the topic space
    produced by the singular value decomposition of the corpus’s TF-IDF matrix. *Projecting*
    in linear algebra is merely computing the *dot product* between the appropriate
    vectors, and saving the resulting scalar numbers into the entries of a new *projected*
    vector. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We have a TF-IDF vector of a document that has as many entries as the number
    of tokens in the entire corpus;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have *topic weight vectors*, which are the columns of the matrix *U* produced
    by the singular value decomposition of the TF-IDF matrix <math alttext="upper
    X equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> . Again,
    each topic weight vector has as many entries as tokens in our corpus. Initially,
    we also have as many topic weight vectors as tokens in our entire corpus (columns
    of *U*). The *weights* in the column of *U* tell us how much a certain token contributes
    to the topic, with a big contribution if it a positive number close to one, ambivalent
    contribution if it is close to zero, and even a negative contribution if it is
    a negative number close to -1\. Note that the entries of *U* are always numbers
    between -1 and 1, so we interpret them as weighing factors for our corpus’s tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might be wondering: If we have as many topic weight vectors as tokens in
    our corpus, each having as many entries as tokens as well, then where are the
    savings, and when will compression or dimension reduction happen? Keep reading.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goal 1: Compute how much of a certain topic our document contains. This is
    simply the dot product between the document’s TF-IDF vector and the column of
    *U* corresponding to the topic that we care for. Record this as the first scalar
    number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal 2: Compute how much of *another* topic our document contains. This is
    the dot product between the document’s TF-IDF vector and the column of *U* corresponding
    to this other topic that we care for. Record this as the second scalar number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal 3: Repeat this for as many (as there are columns of *U*, which is the
    same as the total number of tokens in the corpus) or as little (as one) *topics*
    as we like, recording the scalar number from each dot product that we compute.
    It is clear now that *a topic* in this context means a column vector containing
    weights between -1 and 1 assigned to each token in the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal 4: *Reduce the dimension* by keeping only the topics that matter. That
    is, if we decide to keep only two topics, then the *compressed vector representation*
    of our document will be the *two dimensional* vector containing the two scalar
    numbers produced using the two dot products between the document’s TF-IDF vector
    and the two topics’ weight vectors. This way, we would have reduced the dimension
    of our document from possibly *millions* to just two. Pretty cool stuff.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal 5: *Choose the right topics* to represent our documents. This is where
    the singular value decomposition works its magic. The columns of *U* are organized
    in order, from the most important topic across the corpus to the least important.
    In the language of statistics, the columns are organized from the topic with the
    *most variance* across the corpus and hence encodes more information, to the one
    with the *least variance* and hence encodes little information. We explain how
    variance and singular value decomposition are related in [Chapter 10](ch10.xhtml#ch10).
    Thus, if we decide to project our high dimensional document onto the first few
    column vectors of *U* only, we are guaranteed that we are not missing much in
    terms of capturing enough variation of possible topics across the corpus, and
    assessing how much of these our document contains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal 6: Understand that *this is still a statistical method for capturing topics
    in a document*. We started with the TF-IDF matrix of a corpus, simply counting
    token occurences in documents. In this sense, a topic is captured based only on
    the premise that documents that refer to similar things use similar words. This
    is different than capturing topics based on the *meanings* of the words they use.
    That is, if we have two documents discussing the same topic but using entirely
    different vocabulary, they will be far apart in topic space. The remedy to this
    would be to store words together with other words of similar meaning, which is
    the Word2Vec approach, discussed later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question 1: What happens if we add another document to our corpus? Luckily,
    we do not have to reprocess the whole corpus to produce the document’s topic vector,
    we just project it onto the corpus’s existing topic space. This of course breaks
    down if we add a new document that has nothing in common with our corpus, such
    as an article on pure mathematics added to a corpus on Shakespeare’s love sonnets.
    Our math article in this case will be represented by a bunch of zeros or close
    to zero entries, which does not capture the ideas in the article adequately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question 2: What about the matrix <math alttext="upper V Superscript t"><msup><mi>V</mi>
    <mi>t</mi></msup></math> in the singular value decomposition <math alttext="upper
    X equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> , what
    does it mean in the context of natural language processing of our corpus? The
    matrix <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    has the same number of rows and columns as the number of documents in our corpus.
    It is the document-document matrix and gives the shared meaning between documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question 3: When we move to a lower dimensional topic space using latent semantic
    analysis, are large distances between documents preserved? Yes, since the singular
    value decomposition focuses on *maximizing* the variance across the corpus’s documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question 4: Are small distances preserved, meaning does latent semantic analysis
    preserve the *fine structure* of a document that separates it from *not so different*
    other documents? No. Latent Dirichlet allocation, discussed soon, does a better
    job here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Question 5: Can we improve latent semantic analysis inorder to also keep close
    document vectors together in the lower dimensional topic space? Yes, we can *steer*
    the vectors by taking advantage of extra information, or *meta-data*, of the documents,
    such as messages having the same sender, or by penalizing using a cost function
    so that the method spills out topic vectors that preserve *closeness* as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To summarize**: Latent semantic analysis chooses the topics in an optimal
    way that maximizes the diversity in the topics across the corpus. The matrix *U*
    from the singular value decomposition of the TF-IDF matrix is very important for
    us. It returns the directions along which the variance is maximal. We usually
    get rid of the topics that have the least amount of variance between the documents
    in the corpus, throwing away the last columns of *U*. This is similar to manually
    getting rid of stop words (and, a, the, *etc.*) during text preparation, but latent
    semantic analysis does that for us, in an optimized way. The matrix *U* has the
    same number of rows and columns as our vocabulary. It is the cross correlation
    between words and topics based on word co-occurence in the same document. When
    we multiply a new document by *U* (project it onto the columns of *U*), we would
    get the amount of each topic in the document. We can truncate *U* as we wish and
    throw away less important topics, reducing the dimension to as little topics as
    we want.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent semantic analysis has shortcomings**: The topic spaces it produces,
    or the columns of *U*, are mere *linear combinations* of tokens that are thrown
    together in a way that captures as much variance in the usage across the vocabulary’s
    tokens as possible. This doesn’t necessarily translate into word combinations
    that are in any way meaningful to humans. Bummer. Word2Vec, discussed later, addresses
    these shortcomings.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the topic vectors produced via latent semantic analysis are just linear
    transformations performed on the TF-IDF vectors. They should be the first choice
    for semantic searches, clustering documents, and content based recommendation
    engines. All of this can be accomplished by measuring distances between these
    topic vectors, which we explain later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Topic vector represenation of a document determined by latent Dirichlet allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike topic vectors using latent semantic analysis, with [*latent Dirichlet
    allocation*](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) we do
    have to reprocess the entire corpus if we add a new document to the corpus in
    order to produce its topic vector. Moreover, we use a nonlinear statistical approach
    to bundle words together into topics: We assume a Dirichlet distribution of word
    frequencies. This makes the method more precise than latent semantic analysis
    in terms of the statistics of allocating words to topics. Thus, the method is
    explainable: The way words are allocated to topics, based on how often they occured
    together in a document, and the way topics are allocated to documents, tend to
    make sense to us as humans.'
  prefs: []
  type: TYPE_NORMAL
- en: This nonlinear method takes longer time to train than the linear latent semantic
    analysis. For this reason it is impractical for applications involving corpuses
    of documents, even though it is explainable. We can use it instead for summarizing
    single documents, where each sentence in the document becomes its own *document*,
    and the mother document becomes the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Latent Dirichlet allocation was invented in 2000 by geneticists for the purpose
    of infering population structure, and adopted in 2003 for natural language processing.
    The following are the its assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with raw word counts (rather than normalized TF-IDF vectors), but there
    is still no sequencing of words in order to make sense of them. Instead, we are
    still relying on modeling the statistics of words for each document, except this
    time we will incorporporate the word distribution explicitly into the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A document is a linear combination of an arbitrary number of topics (specify
    this number ahead of time so that the method allocates the document’s tokens to
    this number of topics).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can represent each topic by a certain distribution of words based on their
    term frequencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of occurence of a certain topic in a document follows a Dirichlet
    probability distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of a certain word being assigned to a topic also follows a Dirichlet
    probability distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, topic vectors obtained using Dirichlet allocation are sparse, indicating
    clean separation between the topics in the sense of which words they contain,
    which makes them explainable.
  prefs: []
  type: TYPE_NORMAL
- en: With Dirichlet allocation, words that occur frequently together are assigned
    to the same topics. So this method keeps tokens that were close together close
    together when we move to the lower dimensional topic space. Latent semantic analysis,
    on the other hand keeps tokens that were spread apart spread apart when we move
    to the lower dimensional topic space, so this is better for classification problems
    where the separation between the classes is maintained even as we move to the
    lower dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Topic vector represenation of a document determined by latent discriminant analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike latent semantic analysis and latent Dirichlet allocation which break
    down a document into as many topics as we choose, latent discriminant analysis
    breaks down a document into *only one topic* such as spamness, sentiment, *etc.*.
    This is good for binary classification, such as classifying messages as spam or
    nonspam, or classifying reviews as positive or negative. Rather than what latent
    semantic analysis does, maximizing the separation between all the vectors in the
    new topic space, latent discriminant analysis maximizes the separation only between
    the centroids of the vectors belonging to each class.
  prefs: []
  type: TYPE_NORMAL
- en: But how do we determine the vector representing this one topic? Given the TF-IDF
    vectors of labeled spam and nonspam documents, we compute the centroid of each
    class, then our vector is along the line connecting the two centroids (see [Figure 7-1](#Fig_LDA)).
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Latent discriminant analysis.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each new document can now be projected onto this one dimension. The coordinate
    of our document along that line is the dot product between its TF-IDF and the
    direction vector of the cetroids line. The whole document (with millions of dimensions)
    is now squashed into one number along one dimension (one axis) that carries the
    two centroids along with their midpoint. We can then classify the document as
    belonging to one class or the other depending on its distance from each centroid
    along that one line. Note that the decision boundary for separating classes using
    this method is linear.
  prefs: []
  type: TYPE_NORMAL
- en: Meaning vector representations of words and of documents determined by neural
    network embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous models for vectorizing documents of natural language text only
    considered linear relationships between words, or, in latent Dirichlet allocation,
    we had to use human judgement to be able to select the model’s parameters and
    extract features. We now know that the power of neural networks lies in their
    ability to capture nonlinear relationships, extract features, and find appropriate
    model parameters automatically. We will now use neural networks to create vectors
    that represent individual words and terms, and we will employ similar methods
    to create vectors representing the meanings of entire paragraphs. Since these
    vectors encode the meaning and the logical and contexual usage of each term, we
    can reason with them simply by doing usual vector additions and subtractions.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec vector representation of individual terms by incorporating continuous
    *ness* attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By using TF vectors or TF-IDF vectors as a starting point for our topic vector
    models, we have ignored the nearby context of words and the effect that has on
    the words’ meanings. Word vectors solve this problem. A word vector is a numerical
    vector representation of a word’s meaning, so every single term in the corpus
    becomes a vector of semantics. This vector representation, with floating point
    number entries, of single words, enables semantic queries and logical reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Word vector represenations are *learned* using a neural network. They usually
    have 100 to 500 dimensions encoding how much of each meaning dimension a word
    carries within it. During training a word vector model, the text data is unlabeled.
    Once trained, two terms can be determined to be close in meaning or far apart
    by comparing their vectors via some closeness metrics. Cosine similarity, discussed
    next, is the to-go-to method.
  prefs: []
  type: TYPE_NORMAL
- en: In 2013, Google created this word-to-vector model, Word2Vec, that it trained
    on Google News feed containing 100 billion words. The resulting pre-trained word2vec
    model contains 300 dimensional vectors for 3 million words and phrases. It is
    freely available to download at the word2vec the [Google Code Archive page for
    the word2vec project](https://code.google.com/archive/p/word2vec/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector that Word2Vec builds up captures much more of a word’s meaning than
    the topic vectors discussed earlier in this chapter. The abstract of the paper,
    [*Efficient Estimation of Word Representations in Vector Space (2013)*](https://arxiv.org/abs/1301.3781),
    is informative: *We propose two novel model architectures for computing continuous
    vector representations of words from very large data sets. The quality of these
    representations is measured in a word similarity task, and the results are compared
    to the previously best performing techniques based on different types of neural
    networks. We observe large improvements in accuracy at much lower computational
    cost, i.e. it takes less than a day to learn high quality word vectors from a
    1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art
    performance on our test set for measuring syntactic and semantic word similarities.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper published a month later [*Distributed Representations of Words and
    Phrases and their Compositionality*](https://arxiv.org/pdf/1310.4546.pdf) addressed
    the represenation of word phrases that mean something different than their individual
    components, such as *Air Canada*: *The recently introduced continuous Skip-gram
    model is an efficient method for learning high quality distributed vector representations
    that capture a large number of precise syntactic and semantic word relationships.
    In this paper we present several extensions that improve both the quality of the
    vectors and the training speed. By subsampling of the frequent words we obtain
    significant speedup and also learn more regular word representations. We also
    describe a simple alterna- tive to the hierarchical softmax called negative sampling.
    An inherent limitation of word representations is their indifference to word order
    and their inability to represent idiomatic phrases. For example, the meanings
    of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated
    by this example, we present a simple method for finding phrases in text, and show
    that learning good vector representations for millions of phrases is possible*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The publication that introduced Word2Vec representations [Linguistic Regularities
    in Continuous Space Word Representations (2013)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rvecs.pdf)
    demonstrates how these meaning vectors for words encode logical regularities and
    how this enables us to answer regular analogy questions: *Continuous space language
    models have recently demonstrated outstanding results across a variety of tasks.
    In this paper, we examine the vector space word representations that are implicitly
    learned by the input layer weights. We find that these representations are surprisingly
    good at capturing syntactic and semantic regularities in language, and that each
    relationship is characterized by a relation specific vector offset. This allows
    vector oriented reasoning based on the offsets between words. For example, the
    male/female relationship is automatically learned, and with the induced vector
    representations, “King - Man + Woman” results in a vector very close to “Queen.”
    We demonstrate that the word vectors capture syntactic regularities by means of
    syntactic analogy questions (provided with this paper), and are able to correctly
    answer almost 40% of the questions. We demonstrate that the word vectors capture
    semantic regularities by using the vector offset method to answer SemEval-2012
    Task 2 questions. Remarkably, this method outperforms the best previous systems.*'
  prefs: []
  type: TYPE_NORMAL
- en: The performance of word2vec improved dramatically since 2013, by training it
    on much larger corpuses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2Vec takes one word and assigns to it a vector of attributes such as: place-ness,
    animal-ness, city-ness, positivity (sentiment), brightness, gender, *etc.* Each
    attribute is a dimension, capturing how much of the attribute the meaning of the
    word contains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These word meaning vectors and the attributes are not endcoded manually, but
    during training, where the model learns the meaning of a word from the company
    it keeps: The five or so nearby words in the same sentence. This is different
    from latent semantic analysis where the topics are learned only from words occuring
    in the same document, not necessarily close to each other. For applications involving
    short documents and statements, Word2Vec embeddings have actually replaced topic
    vectors obtained through latent semantic analysis. We can also use word vectors
    to derive word clusters from huge data sets: Performing K-means clustering on
    top of the word vector represenations. See [Google Code Archive page for the word2vec
    project](https://code.google.com/archive/p/word2vec/) for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of representing words through vectors that mean something (rather
    than count something) is that we can reason with them. For example, if we subtract
    the vector representing man from the vector representing king and add the vector
    representing woman, then we get a vector very close to the vector representing
    the word queen. Another example is capturing the relationship between singular
    and plural words: If we subtract vectors representing the singular form of words
    from vectors representing their plural forms, we obtain vectors that are roughly
    the same for all words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next questions are: How do we compute Word2Vec embeddings? That is, how
    do we train a Word2Vec model? What are training data, the neural network’s architecture,
    and its input and output? The neural networks that train Word2Vec models are shallow
    with only one hidden layer. The input is a large corpus of text and the outpute
    are vectors of several hundred dimensions, one for each unique term in the corpus.
    Words that share common liguistic contexts end up with vectors that are *close*
    to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two learning algorithms for Word2Vec, which we will not detail here
    [for now, I will detail this in the next round of editing], however, by now, we
    have a very good idea of how neural networks work, especially shallow ones with
    only one hidden layer. The two learning algorithms are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Continuous bag-of-words*: This predicts the current word from a window of
    surrounding context words; the order of the context words does not influence the
    prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Continuous skip-gram*: This uses the current word to predict the surrounding
    window of context words; the algorithm weighs nearby context words more heavily
    than more distant context words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both algorithms learn the vector representation of a term that is useful for
    prediction of other terms in a sentence. Continuous-bag-of-words is apparently
    faster than continuous-skip-gram, while skip-gram is better for infrequent words.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details, we refer to the tutorial [The Amazing Power Of Word Vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/),
    the [Wikipedia page on Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and the
    three original papers on the subject (2013): [*Efficient Estimation of Word Representations
    in Vector Space*](https://arxiv.org/pdf/1301.3781.pdf), [Distributed Representations
    of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf),
    [Linguistic Regularities in Continuous Space Word Representations](https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The trained Google News Word2vec model has three million words, each represented
    with a vector of 300 dimensions. To download this, you would need 3 GB of available
    memory. There are ways around downloading the whole pre-trained model if we have
    limited memory or if we only care for a fraction of the words.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to visualize vectors representing words?**'
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors are very high dimensional (100-500 dimensions), but humans can
    only visualize two and three dimensional vectors, so we need to project our high
    dimensional vectors onto these drastically lower dimensional spaces and still
    retain their most essential characteristics. By now we know that the singular
    value decomposition (principal component analysis) accomplishes that for us, giving
    us the vectors along which to project in decreasing order of importance, or the
    directions along which a given collection of word vectors vary the most. That
    is, the singular value decomposition ensures that this projection gives the best
    possible view of the word vectors, keeping these as far apart as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many nice worked examples on the web. In the publication [*https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset*](https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset)
    [*Word Embedding-Topic distribution vectors for MOOC (Massive Open Online Courses)
    video lectures dataset*], the authors generate two things from a data set from
    the education domain, namely, the transcripts of 12,032 video lectures from 200
    courses collected from [Coursera](https://www.coursera.org): Word vectors using
    Word2Vec model, and document topic vectors using Latent Dirichlet allocation.
    The data set has 878 thousand sentences and more than 79 million tokens. The vocabulary
    size is over 68 thousand unique words. The individual video transcripts are of
    different lengths, varying from 228 to 32,767 tokens, with an average of 6622
    tokens per video transcript. The authors use Word2Vec’s and Latent Dirichlet Allocation
    implementations in the Gensim package in Python. [Figure 7-2](#Fig_word2vec_visualization_pca)
    shows the publication’s three dimensional visualization of a subset of the word
    vectors using principal component analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-2\. Three dimensional visualization of word vectors using the first
    three principal components. This example highlights the vector representing the
    word ‘studying’ and its neighbours: academic, studies, institution, reading, *etc.*
    ([image source](https://www.researchgate.net/publication/338405739_WET_Word_Embedding-Topic_distribution_vectors_for_MOOC_video_lectures_dataset)).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note that word vectors and document topic vectors are not an end by themselves;
    instead they are a means to an end, which is usually a natural language processing
    task, such as: Classiﬁcation within specific domains, such as the massive open
    online courses (MOOCS) in the example above; benchmarking and performance analysis
    of existing and new models; transfer learning, recommendation systems, contextual
    analysis, short text enrichment with topics, personalized learning; organizing
    content in a way that is easy to search and for maximum visibility. We visit such
    tasks shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: Facebook’s fastText vector representation of individual n-character grams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Facebook’s fastText is similar to Word2Vec, but instead of representing full
    words or n-grams as vectors, it is trained to output a vector representation for
    every *n-character* gram. This enables fastText to handle rare, mispelled, and
    even partial words, such as the ones frequently appearing in social media posts.
    During training, Word2Vec’s skip-gram algorithm learns to predict the surrounding
    context of a given word. Similarly, fastText’s n-character gram algorithm learns
    to predict a word’s surrounding n-character grams, hence providing more granularity
    and flexibility. For example, instead of only representing the full word lovely
    as a vector, it will represent the 2 and 3 grams as vectors as well: lo, lov,
    ov, ove, ve, vel, el, ely, ly.'
  prefs: []
  type: TYPE_NORMAL
- en: Facebook released their pretrained fastText models for 294 languages, trained
    on available Wikipedea corpora for these languages. These range from Abkhazian
    to Zulu, and include rare languages only spoken by a handful of people. Of course,
    the accuracy of the released models vary across languages and depend on the availability
    and quality of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Doc2Vec or Par2Vec vector representation of a document
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How about representing documents semantically? In previous sections we were
    able to represent entire documents as topic vectors, but Word2Vec only represents
    individual words or phrases as vectors. Can we then extend Word2Vec model to represent
    entire documents as vectors carrying meaning? The (2014) paper [*Distributed Representations
    of Sentences and Documents*](https://arxiv.org/abs/1405.4053) does exactly that,
    with an unsupervised algorithm that learns fixed-length dense vectors from variable-length
    pieces of texts, such as sentences, paragraphs, and documents. The tutorial [*Doc2Vec
    tutorial using Gensim*](https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1)
    walks through the Python implementation process, producing a fixed size vector
    for each full document in a given corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Global Vector or (GloVe) vector represenation of words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are other ways to produce vectors representing meanings of words. [Gobal
    vector GloVe (2014)](https://nlp.stanford.edu/projects/glove/) is a model that
    obtains such vectors using the singular value decomposition. It is trained only
    on the non-zero entries of a global word-word co-occurrence matrix, which tabulates
    how frequently words co-occur with one another across an entire corpus.
  prefs: []
  type: TYPE_NORMAL
- en: GloVe is essentially a *log-bilinear model* with a weighted least-squares objective.
    The log-bilinear model is perhaps the simplest neural language model. Given the
    preceding *n-1* words, the log-bilinear model computes an initial vector representation
    for the next word simply by linearly combining the vector representations of these
    preceding *n-1* words. Then the probability for the occurence of the next word
    given those *n-1* preceding words is computed based on computing the similarity
    (dot product) between the linear combination vector representation and the representations
    of all words in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper P r o b left-parenthesis w Subscript n Baseline
    equals w vertical-bar w 1 comma w 2 comma ellipsis comma w Subscript n minus 1
    Baseline right-parenthesis equals StartFraction e x p left-parenthesis w Subscript
    v o c a b Sub Subscript i Subscript Superscript t Baseline w right-parenthesis
    Over e x p left-parenthesis w Subscript v o c a b 1 Superscript t Baseline w right-parenthesis
    plus e x p left-parenthesis w Subscript v o c a b 2 Superscript t Baseline w right-parenthesis
    plus ellipsis plus e x p left-parenthesis w Subscript v o c a b Sub Subscript
    v o c a b minus s i z e Subscript Superscript t Baseline w right-parenthesis EndFraction
    dollar-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi> <mrow><mo>(</mo>
    <msub><mi>w</mi> <mi>n</mi></msub> <mo>=</mo> <mi>w</mi> <mo>|</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>w</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mi>i</mi></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mn>1</mn></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mn>2</mn></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><msubsup><mi>w</mi>
    <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><msub><mi>b</mi> <mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>-</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msub></mrow>
    <mi>t</mi></msubsup> <mi>w</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The main intuition underlying the Global Vector model is the simple observation
    that ratios of word-word co-occurrence probabilities potentially encode some form
    of meaning. The example on the GloVe project website considers the co-occurrence
    probabilities for target words ice and steam with various probe words from the
    vocabulary. [Figure 7-3](#Fig_GloVe_probability_table) shows the actual probabilities
    from a 6 billion word corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Probability table showing the occurence of the words ice and steam
    with the words solid, gas, water and fashion ([image source](https://nlp.stanford.edu/projects/glove/)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Observing the table in [Figure 7-3](#Fig_GloVe_probability_table), we notice
    that, as expected, the word ice co-occurs more frequently with the word solid
    than it does with the word gas, whereas the word steam co-occurs more frequently
    with the word gas than it does with the word solid. Both ice and steam co-occur
    with their shared property water frequently, and both co-occur with the unrelated
    word fashion infrequently. Calculating the the ratio of probabilities cancels
    out the noise from non-discriminative words like water, so that large values,
    much greater than 1, correlate well with properties specific to ice, and small
    values, much less than 1, correlate well with properties specific of steam. This
    way, the ratio of probabilities encodes some crude form of meaning associated
    with the abstract concept of thermodynamic phase.
  prefs: []
  type: TYPE_NORMAL
- en: The training objective of GloVe is to learn word vectors such that their dot
    product equals the logarithm of the probability of co-occurrence of words. Since
    the logarithm of a ratio is equal to the difference of logarithms, this objective
    considers vector differences in the word vector space. Because these ratios can
    encode some form of meaning, this information gets encoded as vector differences
    as well. For this reason, the resulting word vectors perform very well on word
    analogy tasks, such as those discussed in the Word2Vec package.
  prefs: []
  type: TYPE_NORMAL
- en: Since singular value decomposition algorithms have been optimized for decades,
    GloVe has an advantage in training over Word2Vec, which is a neural network and
    relies on gradient descent and backpropagation to perform its error minimization.
    If training our own word vectors from a certain corpus that we care for, we are
    probably better off using a Gobal Vector model than Word2Vec, even though Word2vec
    is the first to accomplish semantic and logical reasoning with words, since Global
    Vector trains faster, has better RAM and CPU efficiency, and gives more accurate
    results than Word2Vec even on smaller corpora.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine Similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this chapter we have worked towards one goal only: Convert a document
    of natural language text into a vector of numbers. Our document can be one word,
    one sentence, a paragraph, multiple paragraphs, or longer. We discovered multiple
    ways to get our vectors, some are more semantically representative of our documents
    than others.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a document’s vector representation, we can feed it into machine
    learning models, such as classification algorithms, clustering algoritms, or others.
    One example is to cluster the document vectors of a corpus with some clustering
    algorithm such as *k-means* in order to create a document classifier. We can also
    determine how semantically similar our document is to other documents, for search
    engines, information retrieval systems, and other applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have established that due to the curse of dimensionality, measuring the
    Euclidean distance between two very high dimensional document vectors is useless,
    since they would come out extremely far apart, only because of the vastness of
    the space they inhabit. So how do we determine whether vectors representing documents
    are *close* or *far*, or *similar* or *different*? One successful way is to use
    *cosine similarity*, measuring the cosine of the angle between the two document
    vectors. This is given by the dot product of the vectors, each normalized by its
    length (had we normalized the document vectors ahead of time, then their lengths
    would’ve already been one):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign cosine left-parenthesis angle between ModifyingAbove
    d o c 1 With right-arrow and ModifyingAbove d o c 2 With right-arrow right-parenthesis
    equals StartFraction ModifyingAbove d o c 1 With right-arrow Superscript t Baseline
    Over l e n g t h left-parenthesis ModifyingAbove d o c 1 With right-arrow right-parenthesis
    EndFraction StartFraction ModifyingAbove d o c 2 With right-arrow Over l e n g
    t h left-parenthesis ModifyingAbove d o c 2 With right-arrow right-parenthesis
    EndFraction dollar-sign"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo> <mtext>angle</mtext>
    <mtext>between</mtext> <mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>1</mn></msub></mrow> <mo>→</mo></mover> <mtext>and</mtext> <mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>2</mn></msub></mrow> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>1</mn></msub></mrow>
    <mo>→</mo></mover> <mi>t</mi></msup> <mrow><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo>(</mo><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>1</mn></msub></mrow>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac> <mfrac><mover accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi>
    <mn>2</mn></msub></mrow> <mo>→</mo></mover> <mrow><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo>(</mo><mover
    accent="true"><mrow><mi>d</mi><mi>o</mi><msub><mi>c</mi> <mn>2</mn></msub></mrow>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-4](#Fig_cosine_similarity) shows three documents represented in a
    two dimensional vector space. We care about the angles between them'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. three documents represented in a two dimensional vector space.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The cosine of an angle is always a number between -1 and 1\. When two document
    vectors are perfectly aligned and pointing in the same direction along all the
    dimensions, their cosine similarity is 1; when they are perfect opposites of each
    other regarding every single dimension, their cosine similarity is -1; and when
    they are orthogonal to each other, their cosine similarity is zero.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bulk of this chapter was about converting a given document of natural language
    text to a vector of numbers. We have established that there are multiple ways
    to get our document vectors, all leading to varying representations (and hence
    conclusions), or emphasizing certain aspects of the given natural language data
    over others. For people entering the natural language processing subfield of AI,
    this is one of the hardest barriers to overcome, especially if they are from a
    quantitative background, where the entities they work with are inherently numerical,
    ripe for mathematical modeling and analysis. Now that we have overcome this barrier,
    equipped with concrete vector representations for natural language data, we can
    think mathematically about popular applications. It is important to be aware that
    there are multiple ways to accomplish each of the following. Traditional approaches
    are hard coded rules, assigning scores to words, punctuations, emojis, etc., then
    relying on the existence of these in a data sample to produce a result. Modern
    approaches rely on various machine learning models, which in turn rely on (mostly)
    labeled training data sets. To excell in this field, we must set time aside and
    try different models on the same task, compare performance, and gain an in depth
    understanding of each model along with its strengths, weaknesses, and mathematical
    justifications of its successes and failures.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hard coded rules: A successful algorithm is VADER or Valence Aware Dictionary
    for sEntiment Reasoning. A tokenizer here needs to handle punctuation and emojis
    properly, since these convey a lot of sentiment. We also have to manually compile
    thousands of words along with their sentiment score, as opposed to having the
    machine accomplish this automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier):
    This is a set of classifying algorithms based in Bayes Theorem from probability.
    The decision rule for classification on maximum likelihood. This will be discussed
    in the probability and measure chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Latent discriminant analysis: In the previous section, we learned how to classify
    documents using latent discriminant analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using latent semantic analysis: Clusters of document vectors formed using latent
    semantic analysis can be used for classification. Ideally, positive reviews cluster
    away from negative reviews in latent semantic analysis’s topic spaces. Given a
    bunch of reviews labeled positive or negative, we first compute their topic vectors
    using latent semantic analysis. Now inorder to classify a new review, we can compute
    its topic vector, then that topic vector’s cosine similarity with the positive
    and negative topic vectors. Finally we classify the review as positive if it is
    more similar to positive topic vectors, and negative if more similar to negative
    topic vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers, convolutional neural network, recurrent long short term memory
    neural network: All of these modern machine learning methods require passing our
    document in vector form into a neural network with a certain architecture. We
    will spend time on these state of the art methods shortly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam Filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mathematically, spam filtering is a similar classification problem as sentiment
    analysis discussed above, when the sentiment of a document is either positive
    or negative. Thus, the same methods for sentiment classification apply for spam
    filtering. In all cases, it doesn’t matter how we create our document vectors,
    we can use them to predict whether a social post is spam or not spam, predict
    how likely it is to get *likes*, *etc.*.
  prefs: []
  type: TYPE_NORMAL
- en: Search And Information Retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, no matter how we create the numerical vectors representing documents,
    we can use them for search and information retrieval tasks. The search can be
    index based or a semantic based, finding documents based on their meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full text search: When we search for a document based on a word or a partial
    word that it contains. Search engines break documents into words that can be indexed,
    similar indexes we find at the end of textbooks. Of course spelling errors and
    typos require a lot of tracking and sometimes guessing. Indices, when available,
    work pretty well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Semantic search: When our search for documents takes into account the meaning
    of the words in both our query and in the documents among which we are searching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on cosine similarity between TF-IDF of documents (for corpuses containing
    billions of documents). Any search engine with a milisecond response utilizes
    a TF-IDF matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Based on semantics: Cosine similarity between topic vectors of documents obtained
    through Latent Semantic Analysis (for corpuses containing millions of documents)
    or Latent Dirichlet allocation (for much smaller corpuses). This is similar to
    how we classified whether a message is spammy or not using latent semantic analysis,
    except that now we compute the cosine similarity between the new document’s topic
    vector and *all* the topic vectors of our database, returning the ones that are
    most similar to our document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on eigenvector iteration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Semantic search and queries using word vectors (Word2Vec or GloVe): Consider
    a search like this (this example is adopted from the book Natural Language Processing
    In Action): *She invented something to do with physics in Europe in the early
    20th century*. When we enter our search sentence into Google or Bing, we may not
    get the direct answer *Marie Curie*. Google Search will most likely only give
    us links to lists of famous physicists, both men and women. After searching several
    pages we find *Marie Curie*, our answer. Google will take note of that, and refine
    our results net time we search. Now using word vectors, we can do simple arithmetic
    on the word vectors representing: woman+Europe+physics+scientist+famous, then
    we would obtain a new vector, close in cosine similarity to the vector representing
    *Marie Curie*, and Voila! We have our answer. We can even subtract gender bias
    in the natural sciences from word vectors by simply subtracting the vector representing
    the token *man*, *male*, *etc.*, so we can search for the word vector closest
    to: woman+Europe+physics+scientist-male-2*man.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search based on analogy questions: To compute a search such as *They are to
    music what Marie Curie is to science*, all we have to do is simple vector arithmetic
    of the word vectors representing: Marie Curie-science+music.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This note is paraphrased from the book Natural Language Processing In Action:
    Traditional indexing approaches work with binary word occurrence vectors, discrete
    vectors (bag of word vectors), sparse floating point number vectors (Term Frequency
    times Inverse Document Frequency vectors), and low-dimensional floating point
    number vectors (such as three dimensional Geographic Information Systems data).
    But high-dimensional floating point number vectors, such as topic vectors from
    Latent Semantic Analysis or Latent Dirichlet Allocation, are challenging. Inverted
    indexes work for discrete vectors or binary vectors, because the index only needs
    to maintain an entry for each nonzero discrete dimension. Either that value of
    that dimension is present or not present in the referenced vector or document.
    Because TF-IDF vectors are sparse, mostly zero, we do not need an entry in our
    index for most dimensions for most documents. Now Latent Semantic Analysis and
    Latent Dirichlet Allocation produce topic vectors that are high-dimensional, continuous,
    and dense, where zeros are rare. Moreover, the semantic analysis algorithm does
    not produce an efficient index for scalable search. This is exasperated by the
    curse of dimensionality, which makes an exact index impossible. One solution to
    the challenge of high-dimensional vectors is to index them with a locality sensitive
    hash, like a ZIP code, that designates a region of hyperspace. Such a hash is
    similar to a regular hash: It is discrete and it only depends on the values in
    the vector. But even this doesn’t work perfectly once we exceed about 12 dimensions.
    Exact semantic search wouldn’t work for a large corpus, such as Google Search
    or even Wikipedia semantic search. The key is to settle for *good enough* rather
    than striving for a perfect index or a latent hashing algorithm for our high-dimensional
    vectors. There are now several open source implementations of some efficient and
    accurate approximate nearest neighbors algorithms that use latent semantic hashing
    to efficiently implement semantic search. Technically these indexing or hashing
    solutions cannot guarantee that we will find all the best matches for our semantic
    search query. But they can get a good list of close matches almost as fast as
    with a conventional reverse index on a TF-IDF vector or bag-of-words vector, if
    we are willing to give up a little precision. Neural network models fine tune
    the concepts of topic vectors so that the vectors associated with words are more
    precise and useful, hence enhancing searches.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Translating a sequence of tokens of any length (such as a sentence or a paragraph)
    to sequence of any length in a different language. The Coder-decoder architecture,
    dicussed below in the context of transformers and recurrent neural networks, has
    proven successful for translation tasks. The coder-decoder architecture is different
    than the auto-encoder architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Image Captioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This combines computer vision with natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the ultimate application of natural language processing. A chatbot
    requires more than one kind of processing: Parse language, search, analyze, generate
    responses, respond to requests and execute them. Moreover, it requires a database
    to maintain a memory of past statements and responses.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Other applications include [*named-entity recognition*](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d),
    [*conceptual focus*](https://www.sciencedirect.com/science/article/pii/S1877042815035193),
    relevant information extraction from text (such as dates), and language generation,
    which we visit in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers And Attention Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers and attention models are the state of the art for natural language
    processing applications such as machine translation, question answering, language
    generation, named-entity recognition, image captioning, and chatbots (as of 2022).
    Currently, they underlie large language models such as [Google’s BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
    and OpenAI’s [GPT-2](https://en.wikipedia.org/wiki/GPT-2) and [GPT-3](https://en.wikipedia.org/wiki/GPT-3).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers by-pass both recurrence and convolution architectures, which were
    the go-to architectures for natural language processing applications up until
    2017, when the paper, [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762),
    introduced the first transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The de-throned recurrent and convolutional neural network architectures are
    still in use (and work well) for certain natural language processing applications,
    as well as other applications such as finance. We elaborate on these models later
    in this chapter. However, the reasons that led to abandoning them for natural
    language are:'
  prefs: []
  type: TYPE_NORMAL
- en: For short input sequences of natural language tokens, the attention layers that
    are involved in transformer models are faster than recurrent layers. Even for
    long sequences, we can modify attention layers to focus only certain neighbourhoods
    within the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of sequential operations required by a recurrent layer depends on
    the length of the input sequence. This number stays constant for an attention
    layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In convolutional neural networks, the width of the kernel directly affects the
    long-term dependencies between pairs of inputs and corresponding outputs. Tracking
    long-term dependencies then requires large kernels, or stacks of convolutional
    layers, all increasing the computational cost of the natural language model employing
    them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Transformer Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transformers are an integral part of enormous language models such as OpenAI’s
    [GPT-2](https://en.wikipedia.org/wiki/GPT-2) (Generative Pre-trained Transformer),
    [GPT-3](https://en.wikipedia.org/wiki/GPT-3), and Google’s BERT (Bidirectional
    Encoder Representations from Transformers, which trains the language model by
    looking at the sequential text data from both left to right and right to left)
    and [Wu Dao’s transformer](https://en.wikipedia.org/wiki/Wu_Dao). These models
    are massive: GPT-2 has around 1.5 billion parameters trained on millions of documents,
    drawn from 8 million websites from all around the internet. GPT-3 has 175 billion
    parameters trained on an even larger data set. Wu Dao’s transformer has a whopping
    1.75 trillion parameters, consuming even more computational resources for training
    and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers were originally designed for language translation tasks, so they
    have an *encoder-decoder* structure. [Figure 7-5](#Fig_transformer_architecture)
    illustrates the architecture of the transformer model originally introduced by
    the paper [Attention Is All You Need (2017)](https://arxiv.org/pdf/1706.03762.pdf).
    However, each of the encoder and decoder are their own modules so they can be
    used separately to perform various tasks. For example, we can use the encoder
    alone to perform a classification task such as part of speech tagging, meaning
    we input the sentence: *I love cooking in my kitchen*, and the output will be
    a class for each word: *I*: pronoun; *love*: verb, cooking: [my grammar is failing
    me editors help me here].'
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. The simple encoder-decoder architecture of a transformer model
    ([image source](https://arxiv.org/pdf/1706.03762.pdf)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input to the full transformer model (with both the encoder and decoder included)
    is a sequence of natural language tokens, of any length, such as a question to
    a chatbot, a paragraph in English that requires translation to French, or summarization
    into a headline. The output is another sequence of natural language tokens, also
    of any length, such as the chatbot’s answer, the translated paragraph in French,
    or the headline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do not confuse the training phase from the inference phase of a model:'
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model is fed both the data and the labels, such as an English
    sentence (input data sample) along with its French translation (label), and the
    model learns a mapping from the input to the target label that generalizes well
    to hopefully the whole of vocabularies and grammars of both languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During inference, the model is fed only the English sentence, and outputs its
    French translation. Transformers output the French sentence one new token at a
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder, on the left half of the Transformer architecture ([Figure 7-5](#Fig_transformer_architecture))
    receives an input of tokens, such as an English sentence, *How was your day?*,
    and produces multiple numerical vector representations each token of this sentence,
    encoding the token’s contextual information from within the sentence. The decoder
    part of the architecture receives these vectors as its input.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder, on the right half of the architecture, receives the vector output
    of the encoder together with the decoder’s output at the previous time step [explain
    this]. Ultimately, it generates an output of tokens, such as the French translation
    of the input sentence, *Comme se passe ta journée* (see [Figure 7-6](#Fig_attention_mechanism)).
    What the decoder actually computes is a *probability* for each word in the French
    vocabulary (say 50,000 tokens) using a softmax function, then produces the token
    with the highest probability. In fact, since computing a softmax for such a high
    dimensional vocabulary is expensive, the decoder uses a *sampled softmax* which
    computes the probability for each token in a random sample of the French vocabulary
    at each step. During training, it has to include the target token in this sample,
    but during inference, there is no target token.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers use a process called *attention* to capture long term dependencies
    in sequences of tokens. The word *sequence* is confusing here, especially for
    mathematicians, who have clear distinctions between the terms *sequence*, *series*,
    *vector*, and *list*. Sequences are usually processed one term at a time, meaning
    one term is processed, then the next, then the next, and so on, until the whole
    input is consumed. Transformers do not process input tokens sequentially. They
    process them altogether, in parallel. This is different than the way recurrent
    neural networks process input tokens, which have to be fed sequentially, in effect
    prohibiting parallel computation. If it was up to us to correct this terminology,
    we should call a natural language sentence a *vector* if we are processing it
    using a transformer model, or a *matrix* since each word in the sentence is represented
    as its own vector, or a *tensor* if we process a batch of sentences at a time,
    which the architecture of the transformer allows. If we want to process the same
    exact sentence using a recurrent neural network model, then we should call it
    a *sequence*, since this model consumes its input data sequentially, one token
    at a time. If we process it using a convolutional neural network, then we would
    call it a vector (or matrix) again, since the network consumes it as a whole,
    not broken down into one token at a time.
  prefs: []
  type: TYPE_NORMAL
- en: It is an advantage when a model does not need to consume the input sequentially,
    because such architectures allow for parallel processing. That said, even though
    parallelization makes transformers computationally efficient, they cannot take
    full advantage of the inherent sequential nature of the natural language input
    and the information encoded within this sequentiality. Think of how humans process
    text. There are new transformer models that try to leverage this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Transformer model runs as follows [explain this better]:'
  prefs: []
  type: TYPE_NORMAL
- en: Represent each word from the input sequence as a d-dimensional vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Incorporate the order of words into the model by adding to the word vector
    information about its position (*positional encoding*): Introduce positional information
    into the input by accompanying each vector of each word with a positional encoding
    vector of the same length. The positional encoding vectors have the same dimension
    the word vector embeddings (this allows the two vectors to be added together).
    There are many choices of positional encodings, some are learned during training,
    others are fixed. Discretized sine and cosine functions with varying frequencies
    are common.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We next feed the positionally encoded word vectors to the encoder block. The
    encoder attends to all words in the input sequence, irrespective if they precede
    or succeed the word under consideration, thus the transformer encoder is bidirectional.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder receives as input its own predicted output word at time step t–1,
    along with the output vectors of the encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input to the decoder is also augmented by positional encoding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The augmented decoder input is fed into the three sublayers. The decoder cannot
    attend to succeeding words, so we apply masking in its first sublayer. At the
    second sublayer, the decoder also receives the output of the encoder, which now
    allows the decoder to attend to all of the words in the input sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the decoder finally passes through a fully connected layer, followed
    by a softmax layer, to generate a prediction for the next word of the output sequence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Attention Mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transformer’s magic is largely due to their built in *attention mechanisms*.
    An attention mechanism comes with bonuses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Explainability**: Pointing out which parts of the input sentence (or document)
    the model paid attention to when producing a particular output (see [Figure 7-6](#Fig_attention_mechanism)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Leveraging pre-trained attention models**: We can adapt pretrained models
    to domain specific tasks. That is, we can further tweak their parameter values
    with extra training on domain specific data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**More accurate modeling of longer sentences**: Another value of attention
    mechanisms is that they allow the modeling of dependencies in sequences of natural
    language tokens without regard to how far apart tokens which are related to each
    other occur in these sequences.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 7-6](#Fig_attention_mechanism) illustrates attention for a translation
    task from English to French.'
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7-6\. Illustrating attention via a translation task: How weights assigned
    to input tokens show which ones the model paid more attention to in order to produce
    each output token ([image source](https://blog.floydhub.com/attention-mechanism/)).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There is no hard core mathematics involved in an attention mechanism: We only
    have to compute a scaled dot product. The main goal of attention is to highlight
    the most relevant parts of the input sequence, how strongly they relate to each
    other within the input itself, and how strongly they contribute to certain parts
    of the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Self attention* is when a sequence of vectors computes alignment within its
    own members. We are now familiar with the fact that the dot product measures the
    compatibility between two vectors. We can compute the simplest possible self attention
    weights by finding the dot products between all the members of the sequence of
    vectors. For example, for the sentence *I love cooking in my kitchen*, we would
    compute all the dot products between the word vectors representing the words *I*,
    *love*, *cooking*, *in*, *my*, and *kitchen*. We would expect the dot product
    between *I* and *my* to be high, similarly between *cooking* and *kitchen*. However,
    the dot product will be highest between *I* and *I*, *love* and *love*, *etc.*,
    because these vectors are perfectly aligned with themsleves, but there is no valuable
    information gleaned from there. The transformer’s solution to avoiding this waste
    is multifold:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, apply three different tranformations to each vector of the input sequence
    (each word of the sentence), multiplying them by three different weight matrices.
    We then obtain three different sets of vectors corresponding to each input word
    vector <math alttext="ModifyingAbove w With right-arrow"><mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></math> :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *query* vector <math alttext="ModifyingAbove q u e r y With right-arrow
    equals upper W Subscript q Baseline ModifyingAbove w With right-arrow"><mrow><mover
    accent="true"><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>q</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math> , whose purpose is to be the vector *attended
    from*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The *key* vector <math alttext="ModifyingAbove k e y With right-arrow equals
    upper W Subscript k Baseline ModifyingAbove w With right-arrow"><mrow><mover accent="true"><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>k</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math> , whose purpose is to be the vector *attended
    to*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The *value* vector <math alttext="ModifyingAbove v a l u e With right-arrow
    equals upper W Subscript v Baseline ModifyingAbove w With right-arrow"><mrow><mover
    accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <msub><mi>W</mi> <mi>v</mi></msub> <mover accent="true"><mi>w</mi>
    <mo>→</mo></mover></mrow></math> , whose purpose is to capture the context that
    is being generated.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Second, obtain alignment scores between the the query and key vectors for all
    words in the sentence, by computing their dot product scaled by the inverse of
    the square root of the length of these vectors <math alttext="StartRoot l EndRoot"><msqrt><mi>l</mi></msqrt></math>
    . We apply this scaling for numerical stability, in order to keep the dot products
    from becoming large (these dot products will soon be passed into a softmax function.
    Since the softmax function has a very small gradient when its input has a large
    magnitude, we offset this effect by dividing each dot product by <math alttext="s
    q r t l"><mrow><mi>s</mi> <mi>q</mi> <mi>r</mi> <mi>t</mi> <mi>l</mi></mrow></math>
    .). Moreover, alignment of two vectors is independent from the lengths of these
    vectors. Therefore, the alignment score between *cooking* and *kitchen* in our
    sentence will be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign a l i g n m e n t Subscript c o o k i n g comma k
    i t c h e n Baseline equals StartFraction 1 Over StartRoot l EndRoot EndFraction
    ModifyingAbove q u e r y With right-arrow Subscript c o o k i n g Superscript
    t Baseline ModifyingAbove k e y With right-arrow Subscript k i t c h e n dollar-sign"><mrow><mi>a</mi>
    <mi>l</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mi>m</mi> <mi>e</mi> <mi>n</mi> <msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mi>l</mi></msqrt></mfrac> <msubsup><mover
    accent="true"><mrow><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow>
    <mi>t</mi></msubsup> <msub><mover accent="true"><mrow><mi>k</mi><mi>e</mi><mi>y</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that this will be different than the alignment score between *kitchen*
    and *cooking*, since the query and key vectors for each are different. Thus, the
    resulting alignment matrix is not symmetric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, transform each alignment score between each two words in the sentence
    into a probability, by passing the score into the *softmax* function. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign omega Subscript c o o k i n g comma k i t c h e n
    Baseline equals s o f t m a x left-parenthesis a l i g n m e n t Subscript c o
    o k i n g comma k i t c h e n Baseline right-parenthesis equals StartFraction
    e x p left-parenthesis a l i g n m e n t Subscript c o o k i n g comma k i t c
    h e n Baseline right-parenthesis Over e x p left-parenthesis a l i g n m e n t
    Subscript c o o k i n g comma upper I Baseline right-parenthesis plus e x p left-parenthesis
    a l i g n m e n t Subscript c o o k i n g comma l o v e Baseline right-parenthesis
    plus e x p left-parenthesis a l i g n m e n t Subscript c o o k i n g comma c
    o o k i n g Baseline right-parenthesis plus e x p left-parenthesis a l i g n m
    e n t Subscript c o o k i n g comma i n Baseline right-parenthesis plus e x p
    left-parenthesis a l i g n m e n t Subscript c o o k i n g comma m y Baseline
    right-parenthesis plus e x p left-parenthesis a l i g n m e n t Subscript c o
    o k i n g comma k i t c h e n Baseline right-parenthesis EndFraction dollar-sign"><mrow><msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mi>s</mi> <mi>o</mi> <mi>f</mi> <mi>t</mi> <mi>m</mi> <mi>a</mi> <mi>x</mi>
    <mrow><mo>(</mo> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mi>m</mi>
    <mi>e</mi> <mi>n</mi> <msub><mi>t</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow> <mrow><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>I</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>m</mi><mi>y</mi></mrow></msub>
    <mo>)</mo></mrow><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mrow><mo>(</mo><mi>a</mi><mi>l</mi><mi>i</mi><mi>g</mi><mi>n</mi><mi>m</mi><mi>e</mi><mi>n</mi><msub><mi>t</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, encode the context of each word, by linearly combining the value vectors
    using the alignment probabilities as weights for the linear combination. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign c o n t e x t Subscript c o o k i n g Baseline equals
    omega Subscript c o o k i n g comma upper I Baseline ModifyingAbove v a l u e
    With right-arrow Subscript upper I Baseline plus omega Subscript c o o k i n g
    comma l o v e Baseline ModifyingAbove v a l u e With right-arrow Subscript l o
    v e Baseline plus omega Subscript c o o k i n g comma c o o k i n g Baseline ModifyingAbove
    v a l u e With right-arrow Subscript c o o k i n g Baseline plus omega Subscript
    c o o k i n g comma i n Baseline ModifyingAbove v a l u e With right-arrow Subscript
    i n Baseline plus omega Subscript c o o k i n g comma m y Baseline ModifyingAbove
    v a l u e With right-arrow Subscript m y Baseline plus omega Subscript c o o k
    i n g comma k i t c h e n Baseline ModifyingAbove v a l u e With right-arrow Subscript
    k i t c h e n dollar-sign"><mrow><mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>t</mi> <mi>e</mi>
    <mi>x</mi> <msub><mi>t</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>=</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>I</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mi>I</mi></msub> <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>l</mi><mi>o</mi><mi>v</mi><mi>e</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>i</mi><mi>n</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mi>n</mi></mrow></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>m</mi><mi>y</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>m</mi><mi>y</mi></mrow></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>,</mo><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <msub><mover accent="true"><mrow><mi>v</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>k</mi><mi>i</mi><mi>t</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We have thus managed to capture in one vector the context of each word in the
    given sentence, with high worth assigned to those words in the sentence it mostly
    aligns with.
  prefs: []
  type: TYPE_NORMAL
- en: The good news here is that we can compute the context vector for all the words
    of a sentence (data sample) simultaneously, since we can pack the vectors above
    in matrices and use efficient and parallelizable matrix computations to get the
    contexts for all the terms at once.
  prefs: []
  type: TYPE_NORMAL
- en: We implement all of the above in one *attention head*. That is, one attention
    head produces one context vector for each token in the data sample. We would benefit
    from producing multiple context vectors for the same token, since some information
    gets lost with all the averaging happening on the way to a context vector. The
    idea here is to be able to extract information using different representations
    of the terms of a sentence (data sample), as opposed to a single representation
    corresponding to a single attention head. So we implement a *multi-head attention*,
    choosing for each head new transformation matrices <math alttext="upper W Subscript
    q"><msub><mi>W</mi> <mi>q</mi></msub></math> , <math alttext="upper W Subscript
    k"><msub><mi>W</mi> <mi>k</mi></msub></math> and <math alttext="upper W Subscript
    v"><msub><mi>W</mi> <mi>v</mi></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Note that during the training process, the entries of the transformation matrices
    are model parameters that have to be learned from the training data samples. Imagine
    then how fast the number of the model’s parameters balloons.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-7](#Fig_multihead_attention) illustrates the *multi-head attention
    mechanism*, implementing *h* heads that receive different linearly transformed
    versions of the queries, keys and values each, to produce *h* context vectors
    for each token, that are then concacenated to produce the output of the *multi-head
    attention* part of the model’s structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Multi-head attention mechanism ([image source](https://arxiv.org/pdf/1706.03762.pdf)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The decoder uses a similar self attention mechanism, but here each word can
    only attend to the words before it, since text is generated from left to right.
    Moreover, the decoder has an extra attention mechanism attending to the outputs
    it receives from the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers Are Far From Perfect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though transformer models have revolutionalized the natural language processing
    field, they are far from perfect. Language models, in general, are *mindless mimics.
    They undertand neither their inputs nor their outputs*. Critical articles, such
    as [this](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)
    and [this](https://www.technologyreview.com/2021/02/24/1017797/gpt3-best-worst-ai-openai-natural-language/)
    by the *MIT Technology Review*, among others, detail their shortcomings, such
    as lack of comprehension of language, repitition when used to generate long passages
    of text, and other. That said, the transformer model brought about a tidal wave
    for natural language, and it is making its way to other AI fields such as biomedicine,
    computer vision and image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks For Time Series Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *time series* in the natural language processing and finance domains
    should be instead *time sequence*. *Series* in mathematics refers to adding up
    the terms of an infinite *sequence*. So when our data is *not summed*, which is
    the case for all of natural language data and most of finance data, we actually
    have *sequences*, not *series*, of numbers, vectors, *etc.*. Oh well, vocabulary
    collisions are unavoidable even across different fields that heavily rely on one
    another.
  prefs: []
  type: TYPE_NORMAL
- en: Other than the definition of a word in a dictionary, their meanings are mostly
    correlated to the way they occur relative to each other. This is conveyed through
    the way words are *ordered* in sentences, as well words’ context and their proximity
    to other words in sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first emphasize the two ways which we can explore the meanings behind words
    and terms in documents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spatially: Exploring a sentence all at once as one vector of tokens, whiever
    way these tokens are represented mathematically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Temporarily: Exploring a sentence sequentially, one token at a time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolutional neural networks, discussed in [Chapter 5](ch05.xhtml#ch05), explore
    sentences spatially, via sliding a fixed-width window (kernel or filter) along
    the tokens of the sentence. When using convolutional neural networks to analyze
    text data, the network expects an input of fixed dimensions. On the other hand,
    when using recurrent neural networks (discussed next) to analyze text data, the
    network expects tokens sequentially, hence, the input does not need to be of fixed
    length.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.xhtml#ch05), we were sliding two dimensional windows (kernels
    or filters) over images, and this chapter, we will slide one dimensional kernels
    over text tokens. We now know that each token is itself represented as a vector
    of numbers. We can either use one hot encoding or word vectors of the Word2Vec
    model. One hot encoded tokens are represented with a very long vector that has
    a 0 for every possible vocabulary word that we want to include from the corpus,
    and a 1 in the position of the token we are encoding. Alternatively, we can use
    trained word vectors produced via Word2Vec. Thus, an input data sample to the
    convolutional neural network is a matrix, made up of column vectors, one column
    for each token in the data sample. If we use Word2Vec to represent tokens, then
    each column vector would have 100 to 500 entries, depending the particular Word2Vec
    model used. Recall that for a convolutional neural network, each data sample has
    to have the exact same number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, one data sample (a sentence or a paragraph) is represented with
    a two dimensional matrix, where the number of rows is is the full length of the
    word vector. In this context, saying that we are sliding a *one-dimensional* kernel
    over our data sample is slightly misleading, but here is the explanation: The
    vector representation of the sample’s tokens extends *downwards*, however, the
    filter covers the whole length of that dimension all at once. That is, if the
    filter is three tokens wides, it would be a matrix of weights with three columns
    and as many rows as the vector representation of our tokens. Thus, one dimensional
    convolution here refers to convolving only *horizontally*. This is different than
    two-dimensional convolution for images, where the two dimensional filter travels
    across the image both horizontally and vertically.'
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous chapter, during a forward pass, the weight values in the
    filters are the same for one data sample. This means that we can parallelize the
    process, which is why convolutional neural networks are efficient to train.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that convolutional neural networks can also process more than one channel
    of input at the same time, that is, three dimensional tensors of input, not only
    two dimensional matrices of numbers. For images this was processing the red, green,
    and blue channels of an input image all at once. For natural language, one input
    sample is a bunch of words represented as columnn vectors lined up next to each
    other. We now know that there are multiple ways to represent the same word as
    a vector of numbers, each perhaps capturing different semantics of the same word.
    These different vector representations of the same word are not necessarily of
    the same length. If we restrict them to be of the same length, then each of these
    representations can be a word’s *channel*, and the convolutional neural network
    can process all the channels of the same data sample at once.
  prefs: []
  type: TYPE_NORMAL
- en: As in [Chapter 5](ch05.xhtml#ch05), convolutional neural networks are efficient
    due to weight sharing, pooling layers, dropout, and the small filters sizes. We
    can run the model with multiple size filters then concatenate the output of each
    size filter into a longer thought vector before passing it into the fully connected
    at the last layer. Of course, the last layer of the network is the one that accomplishes
    the desired task, such as sentiment classification, spam filetering, text generation,
    and others. We went over this in the past two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks For Time Series Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following three sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: She bought tickets to watch the movie.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: She, having free time, bought tickets to watch the movie.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: She, having heard about it nonstop for two weeks in a row, finally bought tickets
    to watch the movie.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In all three sentences, the predicate *bought tickets to watch the movie* corresponds
    to the sentence’s subject *She*. A natural language model will be able to learn
    this if it is designed to handle long term dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional neural network, with its narrow filtering window ranging 3-5
    tokens scanning the sentence, will be able to learn from the first sentence easily,
    and maybe the second sentence, given that the predicate’s position changed only
    a little bit (pooling layers help with the network’s resistence to small variations),
    but the third sentence will be tough, unless we use larger filters (which increases
    the computation cost and make the network more like a fully connected network
    than a convolutional network), or if we deepen the network, stacking convolutional
    layers on top of each other so that the coverage widens as the sentence makes
    its way deeper into the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A completely different approach is to feed the sentence into the network sequentially,
    one token at a time, and maintain a *state* and a *memory* that hold onto important
    information for a certain amount of time. The network produces an outcome when
    all the tokens in the sentence have passed through it. If this is during training,
    only the outcome produced after the last token has been processed gets compared
    to the sentence’s label, then the error *backpropagates through time*, to adjust
    the weights. Compare this to the way we hold onto information when reading a long
    sentence or paragraph. Recurrent neural networks with long-short-term-memory units
    are designed to this way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer models, which we discussed earlier, abolish both convolution and
    recurrence, relying only on attention to capture the relationship between the
    subject of the sentence *she*, and the predicate *bought tickets to watch the
    movie*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is one more thing that differentiates recurrence models from convolutional
    and transformer models: Does the model expect its input to be of the same length
    for all data samples? Meaning can we only input sentences of the same length?
    The answer for transformers and convolutional nets is that they expect only fixed
    length data samples, so we have to preprocess our samples and make them all of
    the same length. On the other hand, recurrent neural networks have variable length
    inputs really well, since after all they only take them one token at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: The main idea for a recurrent neural network is that it network holds onto past
    information as it processes new information. How does this holding happen? In
    a feedforward network, the output of a neuron leaves it and never gets back to
    it. In a recurrent network, the output loops back into the neuron, along with
    new input, in essence, creating a *memory*. Such algorithms are great for autocompletion
    and grammar check. They have been integrated into Gmail’s *Smart Compose* since
    2018.
  prefs: []
  type: TYPE_NORMAL
- en: How do recurrent neural networks work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are the steps for how a recurrent gets trained on a set of labeled data
    samples. Each data sample is made up of a bunch of tokens and a label. As always,
    the goal of the network is to learn the general features and patterns within the
    data that end up producing a certain label (or output) versus others. When tokens
    of each sample are input sequentially, our goal is then to detect features, across
    all the data samples, that emerge when certain tokens appear in patterns relative
    to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: Grab one tokenized and labeled data sample from your data set (such as a movie
    review labeled positive, or a tweet labeled fake news).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the first token <math alttext="t o k e n 0"><mrow><mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub></mrow></math> of your
    sample into the network. Remember that tokens are vectorized so you are really
    passing a vector of numbers into the network. In mathematical terms, we are evaluating
    a function at that token’s vector and producing another vector. So far, our network
    has calculated <math alttext="f left-parenthesis t o k e n 0 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now pass the second token <math alttext="t o k e n 1"><mrow><mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub></mrow></math> of your
    sample into the network, *along with the output of the first token*, <math alttext="f
    left-parenthesis t o k e n 0 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow></math>
    . The network now will evaluate <math alttext="f left-parenthesis t o k e n 1
    plus f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
    . This is the recurrence step, and this is how the network does not forget <math
    alttext="t o k e n 0"><mrow><mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>0</mn></msub></mrow></math> as it processes <math alttext="t o k e n 1"><mrow><mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub></mrow></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now pass the third token <math alttext="t o k e n 2"><mrow><mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub></mrow></math> of your
    sample into the network, *along with the output of the previous step*, <math alttext="f
    left-parenthesis t o k e n 1 plus f left-parenthesis t o k e n 0 right-parenthesis
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math> . The network now will evaluate <math
    alttext="f left-parenthesis t o k e n 2 plus f left-parenthesis t o k e n 1 plus
    f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep going until you finish all the tokens of your one sample. Suppose this
    sample only had five tokens, then our recurrent network will output <math alttext="f
    left-parenthesis t o k e n 4 plus f left-parenthesis t o k e n 3 plus f left-parenthesis
    t o k e n 2 plus f left-parenthesis t o k e n 1 plus f left-parenthesis t o k
    e n 0 right-parenthesis right-parenthesis right-parenthesis right-parenthesis
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi>
    <mi>e</mi> <msub><mi>n</mi> <mn>4</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>3</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>1</mn></msub> <mo>+</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>)</mo></mrow></math> . Note that this output looks very similar to the output
    of the feed forward fully connected networks that we discussed in [Chapter 4](ch04.xhtml#ch04),
    except that this output *unfolds through time* as we input a sample’s tokens one
    at a time *into one recurrent neuron*, while in [Chapter 4](ch04.xhtml#ch04),
    the network’s output *unfolds through space*, as one data sample moves from one
    layer of the neural network to the next. Mathematically, when we write the formulas
    of each, they are the same, so we don’t need more math beyond what we learned
    in the past three chapters. That’s why we love math.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When training the network to produce the right thing, it is the final output
    of the sample, <math alttext="f left-parenthesis t o k e n 4 plus f left-parenthesis
    t o k e n 3 plus f left-parenthesis t o k e n 2 plus f left-parenthesis t o k
    e n 1 plus f left-parenthesis t o k e n 0 right-parenthesis right-parenthesis
    right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>4</mn></msub>
    <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi>
    <msub><mi>n</mi> <mn>3</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi>
    <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi> <mi>k</mi> <mi>e</mi> <msub><mi>n</mi>
    <mn>1</mn></msub> <mo>+</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>t</mi> <mi>o</mi>
    <mi>k</mi> <mi>e</mi> <msub><mi>n</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></math> , that gets compared
    against the sample’s true label via evaluating a loss function, exaclty as we
    did in Chapters [3](ch03.xhtml#ch03), [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now pass the next data sample one token at a time into the network and do the
    same thing again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We update the network’s weights in exactly the same way we updated them in [Chapter 4](ch04.xhtml#ch04),
    by minimizing the loss function via a gradient descent based algorithm, where
    we calculate the required gradient (the derivatives with respect to all the network’s
    weights) via backpropagation. As we just said, this is exactly the same backpropagation
    mathematics we learned in [Chapter 4](ch04.xhtml#ch04), except now of course we
    get to say *we’re backpropagating through time*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In finance, dynamics, and feedback control, the process above is called an [auto-regressive
    moving average (ARMA) model](https://en.wikipedia.org/wiki/Autoregressive_model).
  prefs: []
  type: TYPE_NORMAL
- en: 'Training a recurrent neural net can be expensive, especially for data samples
    of any significant length, say 10 tokens or more, since the number of weights
    to learn is directly related to the number of tokens in data samples: The more
    tokens, the more *depth in time* the recurrent network has. Other than the computational
    cost, this depth comes with all the troubles encountered by regular feed forward
    networks with many layers: Vanishing or exploding gradients, especially with samples
    of data with hundreds of tokens, which will be the mathematical equivalent of
    a fully connected feed forward neural network with hundreds of layers! The same
    remedies for exploding and vanishing gradients for feed forward networks work
    here.'
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrent Units And Long Short Term Memory Units
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recurrent neurons in recurrent networks are not enough to capture long term
    dependencies in a sentence: A token’s effect gets diluted and stepped on by the
    new information as more tokens pass through the recurrent neuron. In fact, a token’s
    information is almost completely gone only after two tokens have passed. This
    problem can be addressed if we add memory units, called *long short term memory
    units* to the architecture of the network. These help learning dependencies stretching
    across a whole data sample.'
  prefs: []
  type: TYPE_NORMAL
- en: Long short term memory units themselves contain neural networks, and they can
    be trained to find only the new information that needs to be retained for the
    upcoming input, and to forget, or reset to zero, information that is no longer
    relevant to learning. Therefore, long short term memory units learn which information
    to hold on to, while the rest of the network learns to predict the target label.
  prefs: []
  type: TYPE_NORMAL
- en: There is not any new mathematics beyond what we learned in [Chapter 4](ch04.xhtml#ch04)
    here, so we will not go into the weeds digging into the specific architecture
    of a long short term memory unit, or a *gated unit*. In summary, the input token
    for each time step passes through the forget and update gates (functions), gets
    multiplied by weights and masks, then gets stored in a memory cell. The network’s
    next output depends on a combination of the input token and the memory unit’s
    current state. Moreover, long short term memory units share the weights they learned
    across samples, so they do not have to relearn basic information about language
    as they go through each sample’s tokens. [I will add couple things here explaining
    the best way to think about these]
  prefs: []
  type: TYPE_NORMAL
- en: Humans are able to process language on a subconscious level, and long short
    term memory are a step into modeling that. They are able to detect patterns in
    language that allow us to address more complex tasks than mere classification,
    such as language generation. We can generate novel text from learned probability
    distributions. This is the next chapter’s topic.
  prefs: []
  type: TYPE_NORMAL
- en: An Example Of Natural Language Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When faced with narratives about different models it always makes things easier
    when we have specific examples in mind with real data, along with the models’
    hyper-paramters. We can find the [IMDB](https://www.imdb.com/chart/top/) movie
    review dataset at [the Stanford AI website](https://ai.stanford.edu/~amaas/data/sentiment/).
    Each data sample is labeled with a 0 (negative review) or a 1 (positive review).
    We can start with the raw text data if we want to practice preprocessing natural
    language text. Then we can tokenize it and vectorize it using one-hot encoding
    over a chosen vocabulary, the Google Word2vec model, or some other model. Do not
    forget to split the data into the training and test sets. Then choose the hyper-paramters,
    for example: length of word vectors around 300, number of tokens per data sample
    around 400, mini-batches of 32, number of epochs 2\. We can play around with these
    to get a feel for the models’ performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Finance AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI models have wide use for the finance field. By now, we know the underlying
    structure of most of AI models (except for graphs, which have a different mathematical
    structure; we discuss graph networks in [Chapter 9](ch09.xhtml#ch09)). At this
    point, only mentioning an application area from finance is enough for us to have
    a very good idea about how to go about modeling it using AI. Moreover, many finance
    applications are naturally interwined with natural language processing applications,
    such as marketing decisions based on customer reviews, or a natural language processing
    system used to predict economic trends and trigger large financial transactions
    based only on the models’ outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following only two AI applications in finance, among many. Think of ways
    we can put what we have learned so far to good use modeling these problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stock Market: Time series prediction. A recurrent neural network can take a
    sequence of inputs and produce a sequence of outputs. This is useful for the time
    series prediction required for stock prices. We input the prices over the past
    *n* days, and the networks outputs the prices from the past *n-1* days *along
    with tomorrow’s price*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[auto-regressive moving average (ARMA) model](https://en.wikipedia.org/wiki/Autoregressive_model)
    in finance, dynamics, and feedback control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stock market appears multiple times in this book. Keep an eye for it when
    discussing stochastic processes in [Chapter 11](ch11.xhtml#ch11) on probability.
  prefs: []
  type: TYPE_NORMAL
- en: Summary And Looking Ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There was almost no new math in this chapter, however, it was one of the hardest
    to write. The goal was to summarize the most important ideas in the whole natural
    language processing field. Moving from words to relatively low dimensional vectors
    of numbers that carry meaning was the main barrier to overcome. Once we learned
    multiple ways to do this, whether vectorizing one word at a time or the main topics
    in a long document or an entire corpus, feeding those vectors to different machine
    learning models with different architectures and purposes was just business as
    usual.
  prefs: []
  type: TYPE_NORMAL
- en: Calculus
  prefs: []
  type: TYPE_NORMAL
- en: The *log* scale for Term Frequencies and Inverse Document Frequencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics
  prefs: []
  type: TYPE_NORMAL
- en: Zipf law for word counts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Dirichlet probability distribution for assigning words to topics and topics
    to documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear Algebra
  prefs: []
  type: TYPE_NORMAL
- en: Vectorizing documents of natural language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dot product of two vectors and how it provides a measure of similarity or
    compatibility between the entities that the vectors represent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition i.e. latent semantic analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bilinear log model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time Series Data
  prefs: []
  type: TYPE_NORMAL
- en: What it means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it is fed into machine learning models (as one bulk, or one item at a time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI Model Of The Day
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer
  prefs: []
  type: TYPE_NORMAL
