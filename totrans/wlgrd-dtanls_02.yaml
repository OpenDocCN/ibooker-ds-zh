- en: 3 Data modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modeling data as a fundamental analytical activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to define business entities from raw data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to structure a data model to best suit the analytical question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an analyst, you will find yourself applying the same logic to raw data over
    and over again. For example, every time you calculate revenue, you might need
    to remember to remove internal money transfers between departments. Or when you
    look at customer spending, you might need to exclude a certain customer because
    they operate differently. Whenever these business rules need to be applied constantly
    to ensure data is accurate, it is a good opportunity to build a *data model*.
  prefs: []
  type: TYPE_NORMAL
- en: A data model is a dataset created from raw data that has been cleaned, with
    specific business rules built into it. Creating reusable data models will save
    you time and maintenance headaches in the future. Data modeling also forces you
    to think deeply about your or your stakeholder’s question, which leads to a more
    valuable answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Real business case: Customer deduplication'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once, when working in industry, I spent months on a customer deduplication project.
    We wanted to track the number of customers over time, but our customers were spread
    across multiple databases. Deduplicating them was not a trivial task, especially
    because, in some databases, customers appeared as company names, such as “South
    West Motors,” and in others, they were recorded as individuals, such as “John
    Smith,” with no information about the company they worked for.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, our solution involved text similarity algorithms to find instances
    where “South West Motors” from one database existed as “South West Motors Limited”
    in another. We also used graph theory to link customers together across our company
    network. These are advanced algorithms for the seemingly simple task of counting
    customers. Entity resolution problems are everywhere, which is why this chapter
    explores the topic and lets you practice on a real problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will review the fundamentals and importance of data modeling
    and practice converting raw data into a reusable data model using a real-world
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The importance of data modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data modeling is a foundational step in the analytical workflow. It is the process
    of taking raw data, mapping it to business-specific entities, and creating new
    data models. We can think of it as converting data in its raw state to a more
    useful form, which we can call information. Data analysis is then the process
    of converting this information into insight. The intermediate step is required
    because, in its raw form, data is often not ready for analysis. Figure 3.1 shows
    where data modeling fits into both the abstract and concrete versions of a data
    science workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 Data modeling and analysis as mapped to the data science process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Your data models should encode any business logic required to transform the
    raw data to be suitable for analysis. If you always need to remember to filter
    out certain rows from your raw data, you should have an intermediate data model
    where that filter has already been applied. What does a “lapsed customer” mean
    for your business? Is it someone who hasn’t purchased anything for a certain time?
    Or perhaps someone who hasn’t even logged in to your platform for a while? Whatever
    that definition, it should be encoded in a data model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating data models increases transparency because there is a single place
    to look for how a customer, a vehicle, or a purchase event is defined. All other
    analyses should be done using these intermediate models, not the raw data. Another
    benefit is that this kind of cleaner data model could be exposed to data-savvy
    stakeholders to work with directly, thus enabling self-service. Business intelligence
    tools such as Tableau and Power BI allow power users to create their own reports.
    If that is done using centralized data models, analytical mistakes are less likely.
  prefs: []
  type: TYPE_NORMAL
- en: As analysts, we should be looking out for opportunities to standardize business
    logic by encoding it in data models. These don’t have to be technically complex
    since they can simply be additional tables in our database. Let’s look at some
    tasks involved in data modeling, which we will then practice in the project.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Common data modeling tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data modeling usually involves some combination of
  prefs: []
  type: TYPE_NORMAL
- en: Repetitive data cleaning tasks, such as fixing date formats or converting text
    columns into their numeric equivalents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining business entities, concepts, and activities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deduplicating the source data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restructuring the raw data to be in a format more useful to the analytical questions
    it is designed to answer. This might involve making a choice between wide or long
    data, which we will discuss later in this section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zooming in or out, altering the level of granularity for different analytical
    questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are all tasks you do not want to perform every time you need to do some
    analysis. They should be done once, and the output should be captured in an appropriate
    data model.
  prefs: []
  type: TYPE_NORMAL
- en: Agreeing on terminology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a junior analyst, you might end up in an industry that you are unfamiliar
    with. It is important to ask questions to clarify the terminology because even
    everyday terms like “customer” might have ambiguous meanings. Does a customer
    mean a single person or an organization? What if your business deals with both?
    Part of the data modeling process is defining these terms so that they can be
    encoded in a data table.
  prefs: []
  type: TYPE_NORMAL
- en: Note  When it comes to definitions, you cannot work in a vacuum; decisions about
    what concepts mean concretely need to be made in collaboration with your stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Handling duplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data you will work with will inevitably contain some duplication, which
    might be in the form of duplicated rows of data or duplicate records across multiple
    systems. If you worked in the automotive industry, you could spend a nontrivial
    amount of time figuring out whether “John Smith Motors” in one database is the
    same customer as “JS Motors” in another. Time invested in reconciling this at
    the data modeling stage is time well spent.
  prefs: []
  type: TYPE_NORMAL
- en: Another important data modeling task is deciding the structure of your data,
    such as whether the data should be stored in a wide or a long format.
  prefs: []
  type: TYPE_NORMAL
- en: Wide vs. long data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In many cases, your data will consist of one row per entity, such as a customer.
    Each row represents a customer, and each column represents a property or attribute
    of that customer, such as their name, age, department, and so forth. This is called
    a *wide* format because as the number of measurements grows, the data gains additional
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, *long* data is when one row represents a single measurement of
    an entity. This means an entity, such as a customer, will require multiple rows.
    When a new measurement about the entities is added, the data gains additional
    rows.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a concrete example. Suppose you are working for a sports analytics
    company and want to analyze the factors that go into sports teams that win major
    competitions. Table 3.1 shows the dataset of football results that you have.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.1 Football results in a wide format
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Match ID | Date | Competition | Round | Home team | Away team | Home goals
    | Away goals |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | 2014-07-04  | World Cup 2014  | Quarter-final  | France  | Germany  |
    0  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | 2014-07-04  | World Cup 2014  | Quarter-final  | Brazil  | Colombia  |
    2  | 1  |'
  prefs: []
  type: TYPE_TB
- en: '| 3  | 2014-07-05  | World Cup 2014  | Quarter-final  | Argentina  | Belgium  |
    1  | 0  |'
  prefs: []
  type: TYPE_TB
- en: This is wide data because each row represents an entity, in this case, a match.
    With some enhancement (for instance, adding a “Winner” column), this dataset would
    allow easy analysis of questions such as “Which country has won the most games
    at a World Cup?”
  prefs: []
  type: TYPE_NORMAL
- en: However, what if someone asked, “Which country participated in the most World
    Cup games?” This is trickier because the level of granularity of each row is one
    row per match, so we would have to consider both the “Home team” and “Away team”
    columns. What we would need to answer this second question more easily is one
    row per *participant*. We could consider creating a long-format version of the
    data that would look more like table 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.2 The same football results in a long format, one row per match participant
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Match ID | Date | Competition | Round | Team | Home or away? | Goals scored
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | 2014-07-04  | World Cup 2014  | Quarter-final  | France  | Home  | 0  |'
  prefs: []
  type: TYPE_TB
- en: '| 1  | 2014-07-04  | World Cup 2014  | Quarter-final  | Germany  | Away  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | 2014-07-04  | World Cup 2014  | Quarter-final  | Brazil  | Home  | 2  |'
  prefs: []
  type: TYPE_TB
- en: '| 2  | 2014-07-04  | World Cup 2014  | Quarter-final  | Colombia  | Away  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 3  | 2014-07-05  | World Cup 2014  | Quarter-final  | Argentina  | Home  |
    1  |'
  prefs: []
  type: TYPE_TB
- en: '| 3  | 2014-07-05  | World Cup 2014  | Quarter-final  | Belgium  | Away  |
    0  |'
  prefs: []
  type: TYPE_TB
- en: This is now long data because rows don’t represent unique entities. The table
    encodes the same information, but each match is duplicated on purpose. From this
    table, it is easier to focus only on the “Team” column to find the team with the
    most World Cup matches. The downside of this format is that we cannot simply count
    the number of rows to find statistics such as the number of games played at a
    World Cup because we would be double counting.
  prefs: []
  type: TYPE_NORMAL
- en: Neither a wide nor a long format is better than the other; the choice between
    them depends on the question you are trying to answer using the data. Assessing
    what format is best suited to your analytical question is the essence of data
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the right level of granularity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Just like the football example, you will encounter datasets that have the wrong
    level of granularity for your analysis. US election result data might be at an
    individual county level, but you might have analytical questions about individual
    candidates. Having a candidate-level data model would help answer candidate-specific
    questions much faster. The information is the same; it is just stored in a format
    that is more appropriate for your analytical questions.
  prefs: []
  type: TYPE_NORMAL
- en: When beginning this project, start by asking, “What should the structure of
    the final data model be?” Working toward that goal (in a results-focused way!)
    will guide the concrete steps you will need to take.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Project 2: Who are your customers?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at our project, in which we will extract a customer database from
    a series of retail transactions. We will look at the problem statement, which
    is what our stakeholders want to achieve in their own words. I provide an overview
    of the available data and discuss some of the technical specifics of the example
    solution. Reading section 3.2 is sufficient to get started, but you may find section
    3.3 helpful to see how the results-driven approach would be applied in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The data is available at [https://davidasboth.com/book-code](https://davidasboth.com/book-code).
    You will find the datasets with which you can attempt the project, as well as
    the example solution in the form of a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Problem statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, you have been hired by Ebuy Emporium, a new e-commerce startup.
    They have been up and running for a month and have had unexpected success. They
    are starting to have an active interest in their customer base. Who are their
    customers? What do they buy? What drives their purchasing behavior? However, before
    they do any serious analysis, they need to be able to count their customers, which
    happens to be more difficult than anticipated. One problem is there are multiple
    sources of customer data, which are
  prefs: []
  type: TYPE_NORMAL
- en: The e-commerce platform’s customer database, where customer details are recorded
    when they sign up for an account online. This is where most of the customer details
    should be found.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The in-house CRM (customer relationship management) system, where customer details
    are recorded when they make a purchase over the phone or are otherwise onboarded
    as customers (except because of purchasing online with a registered account).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The raw transaction data, which we will hereafter refer to as “purchases” or
    “sales,” and which also contains purchases made “as a guest,” meaning customer
    records are not explicitly created at the time of purchase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NOTE  Original transaction data courtesy of REES46 ([https://mng.bz/6eZo](https://mng.bz/6eZo)),
    enhanced with fictitious customer data from a European Union Collaboration in
    Research and Methodology (CROS) training program on record linkage ([https://mng.bz/oKad](https://mng.bz/oKad)).
    Thank you to the dataset owners for providing permission to repurpose the original
    source data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another problem is that the existing data sources may not be mutually exclusive—there
    might be overlaps across them all. There is almost certainly some duplication,
    either because the same customer had their details entered into multiple systems
    or because they have made purchases both as a guest and with a registered account.
    Duplicate accounts may not contain exactly the same information; there may be
    typos or misspellings. These complications are why the startup needs the help
    of an analyst to answer their question: “Who are our customers?”'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Data dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tables 3.3 and 3.4 show the data dictionaries for the three data sources, and
    figures 3.2 and 3.3 show sample data.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.3 Data dictionary for the “purchases” dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Column | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `event_time`  | The exact date and time the purchase occurred.  |'
  prefs: []
  type: TYPE_TB
- en: '| `product_id`  | The unique identifier of the purchased product.  |'
  prefs: []
  type: TYPE_TB
- en: '| `category_id`  | The unique identifier of the purchased product’s specific
    category.  |'
  prefs: []
  type: TYPE_TB
- en: '| `category_code`  | A broad category for the purchased product. In a hierarchy,
    category codes contain multiple category IDs, and one `category_id` should only
    be linked to one `category_code`.  |'
  prefs: []
  type: TYPE_TB
- en: '| `brand`  | The purchased item’s brand (if applicable).  |'
  prefs: []
  type: TYPE_TB
- en: '| `price`  | The price the item was bought for (in USD).  |'
  prefs: []
  type: TYPE_TB
- en: '| `session_id`  | A unique identifier for a purchase session. If multiple items
    are purchased in a transaction, each item will have a row in the table, and the
    rows will share a `session_id`.  |'
  prefs: []
  type: TYPE_TB
- en: '| `customer_id`  | The unique identifier of the customer if they purchased
    using a registered account. For guest purchases, this value will be missing.  |'
  prefs: []
  type: TYPE_TB
- en: '| `guest_first_name`  | The first name that was supplied if a purchase was
    made as a guest. For purchases made using registered accounts, this value will
    be missing.  |'
  prefs: []
  type: TYPE_TB
- en: '| `guest_surname`  | The surname that was supplied if a purchase was made as
    a guest. For purchases made using registered accounts, this value will be missing.  |'
  prefs: []
  type: TYPE_TB
- en: '| `guest_postcode`  | The postcode that was supplied if a purchase was made
    as a guest. For purchases made using registered accounts, this value will be missing.  |'
  prefs: []
  type: TYPE_TB
- en: '![figure](../Images/3-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 A snapshot of the purchases dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Table 3.4 Data dictionary for the CRM and customers datasets, which share an
    identical structure
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Column | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `customer_id`  | The unique identifier of the customer in this system  |'
  prefs: []
  type: TYPE_TB
- en: '| `first_name`  | The customer’s first name  |'
  prefs: []
  type: TYPE_TB
- en: '| `surname`  | The customer’s surname  |'
  prefs: []
  type: TYPE_TB
- en: '| `postcode`  | The customer’s postal code  |'
  prefs: []
  type: TYPE_TB
- en: '| `age`  | The customer’s age, in years  |'
  prefs: []
  type: TYPE_TB
- en: '![figure](../Images/3-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 A snapshot of the customer data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NOTE  It is important to remember that the data dictionary refers to the assumptions
    about what data is present in each column. It is good practice to verify some
    of these assumptions as part of the exploratory data analysis phase. For example,
    do the customer IDs provided in the purchase data always match a record in one
    of the customer databases?
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the data dictionaries are self-explanatory, but the first step
    in working with a new dataset should always be making sure we’ve read any relevant
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Desired outcomes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output of this project should be a single dataset representing your customer
    data model—your best estimate of the entire customer base the startup currently
    has. Data will come from the three sources provided, and you will need to consolidate
    and deduplicate accordingly. You will need to decide on the structure of the data
    model based on the columns available in the datasets. This data model should be
    structured so that all the logic to define customers is already in place, and
    answering the question “How many customers do we have?” should be done as simply
    as counting the rows.
  prefs: []
  type: TYPE_NORMAL
- en: There is no single right solution you’re aiming for and no ground truth to check
    your answers against. This is partly because analysis contains so much ambiguity
    that different analysts will make different assumptions and arrive at different
    conclusions, and partly because tasks like this never have answers to check against
    in the real world. Part of being a good analyst is embracing constant uncertainty
    and ambiguity and being comfortable with an answer that may never be a complete
    one. The important thing is being able to provide an answer your stakeholders
    can use.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Required tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the projects are technology agnostic, for this example solution, I
    use the Python library `pandas` to manipulate the datasets and `numpy` for numerical
    functions. I also introduce the `recordlinkage` library used for entity resolution.
    I keep the code snippets to a minimum and focus on the conceptual solution, but
    the full solution is presented as a Jupyter notebook. This is a tool for presenting
    code, data, and text in a single document, making it easy to share your findings,
    as well as the underlying methods. You can attempt this exercise with any number
    of tools as long as they satisfy the following criteria, that is, they are able
    to
  prefs: []
  type: TYPE_NORMAL
- en: Load a dataset of tens of thousands of rows from a CSV file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create new columns and manipulate existing ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join datasets together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform basic analysis tasks such as sorting, grouping, and reshaping data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.3 Planning our approach to customer data modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s use the results-driven approach to break the problem down into its logical
    components. This will give us a deeper understanding of our problem before we
    start working on it. We will also explicitly decide what not to do, that is, we
    will figure out which features of the problem are not essential to a first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Applying the results-driven process to data modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![figure](../Images/3-unnumb-1.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we need to understand the question. In this case, the question is vaguely
    “Who are our customers?” which would normally require some pushback for clarification.
    In this instance, whatever the actual analytical question about customers is,
    the data modeling step is fundamental to answering it. We must first consolidate
    the customer data from the three data sources.
  prefs: []
  type: TYPE_NORMAL
- en: TIP  We could think about what additional data to augment it with to better
    fit the needs of the analytical questions. We know we want to end up with one
    row per customer, but we don’t yet know if a summary of the customer’s purchase
    history would be a useful addition. In this case, we don’t want to spend time
    preemptively adding information to our data model because we think someone might
    ask for it down the line.
  prefs: []
  type: TYPE_NORMAL
- en: '*![figure](../Images/3-unnumb-2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Our end product is quite well defined, but, specifically, starting from the
    end here means having an idea of the structure of the final data model, the existence
    of which will allow us to count customers to produce our minimum viable answer.
    We know that one of its most important properties should be that it contains one
    row per customer. That’s the level of granularity we’re aiming for. Any purchase
    data we augment it with would, therefore, need to be aggregated to the customer
    level; we couldn’t include individual products purchased by a customer but could
    include their total spending, the date they first signed up, the number of unique
    purchases they made, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of looking ahead at our end result is to identify the schema
    of our final data model. What columns are common across our datasets? Are there
    columns we will have to drop before we combine the data sources, or are they important
    enough that we will accept some missing data in our final solution? In data modeling,
    these are all important aspects to think about up front, so we can keep them in
    mind while in the weeds of coding our solution, and we don’t spend time manipulating
    data that we cannot use in the final data model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-unnumb-3.png)'
  prefs: []
  type: TYPE_IMG
- en: The datasets have been identified and provided for us in this case, so our “Identify”
    and “Obtain” steps have been done for us. However, in a real-world scenario, it
    would be prudent to think about any additional sources of customer data that might
    exist within our organization. This often includes spreadsheets kept by various
    sales managers on their computer desktops!
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-unnumb-4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To actually do the data modeling task, here are some key steps to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: We would start by exploring all three datasets. We want to ascertain whether
    the columns contain what the data dictionaries have said. For example, are guest
    details always complete when we don’t have a customer ID? We also want to see
    whether the values make sense. We’re concentrating on customer data but might
    also want to see if the `price` column contains any unrealistic values. Are the
    date and time values all within the same period? Is there anything amiss with
    any of the postcode values? This is an iterative process, so we may not exhaust
    all our exploration at the beginning; some of these questions may only present
    themselves later on. We also don’t want to spend too much time exploring columns
    we won’t use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have verified some key assumptions about the data, one idea would be
    to deduplicate each dataset to just unique customers before merging them. This
    is especially true for purchases, where any time a customer buys multiple items,
    their details are repeated. We would also make some of the key decisions about
    differences in schema. If there is data that is only present in some of the sources,
    what do we do with it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to combine these separate datasets; we want a dataset that
    contains all possible customer records. The combined dataset may contain duplicates.
    We can get rid of some obvious duplication by removing exact duplicates, which
    may arise if a customer record existed in both the customer database and the CRM
    data, and the records were otherwise identical. If the information in two customer
    records is identical, but the unique identifier differs, we would need to be careful.
    Casually removing a duplicate would result in the loss of what we might refer
    to as “data lineage,” that is, the traceability of where our data originally came
    from. If we have a customer record for Jane Smith, it’s good practice to keep
    all possible identifiers for that customer that we’ve encountered across datasets.
    Perhaps she is customer 8834 in one dataset and 931 in another, and we would want
    to know that somehow in our final data model. This not only makes it easier to
    trace her accounts back to their sources, but also increases trust in our final
    solution. Anyone using our data model knows the assumptions we’ve made about which
    customer accounts make up the customer “entity” for Jane Smith.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we could look at deduplicating our combined customer data beyond simply
    identifying exact duplicates. Fuzzy string matching might be a good approach here;
    in this case, we compare two strings and judge them as identical if they almost
    are. When using fuzzy matching, typos are taken into account, and “London” and
    “Lodnon” are seen as the same string. Research in the field of record linkage
    and entity resolution may be helpful to read up on. These are entire topics dedicated
    to figuring out whether two slightly different versions of an entity are, in fact,
    the same. We would need to make a judgment call on whether this additional work
    has tangible benefits and is a good investment of our time. It might even be a
    task we leave for a second draft, as we may prefer to show our stakeholders our
    first findings before committing to this more complex step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we would clean up the data model so that it has the schema we want,
    ensuring the column names are meaningful. Depending on how we choose to handle
    duplicate accounts, we may need to decide on a main account for each customer
    entity, for example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/3-unnumb-5.png)'
  prefs: []
  type: TYPE_IMG
- en: The way to present a data model is by focusing on the implications of our work.
    Creating a small presentation outlining what we’ve done, how many customers we’ve
    found, and what assumptions our work is founded on would get more attention than
    giving access to the data model in the database. It is unlikely our stakeholders
    would ever use our data model to analyze data directly, anyway; the main benefit
    of the work is improved accuracy and more opportunities in customer analytics
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting our findings also creates an opportunity to work with our stakeholders
    to make some of the analytical decisions together. Sometimes, we don’t have the
    right intuition to choose between two seemingly similar options. A problem I have
    faced myself is when customers exist as companies in one database and individuals
    in another. I, as the analyst, shouldn’t be the one that has the final say over
    whether “Jane Smith” is the same customer as her company “JS Motors”; that’s a
    decision that needs wider business input, especially if the business is going
    to measure and track customer numbers over time. You can use the first iteration
    of your data model to present some of these key questions for your stakeholders
    to think about.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-unnumb-6.png)'
  prefs: []
  type: TYPE_IMG
- en: After getting feedback on your initial findings, it is usually apparent what
    the next iteration of your solution needs. In the context of the project, since
    we lack direct feedback from a stakeholder, iteration might mean you create a
    minimum viable data model quickly, perhaps stopping before doing any meaningful
    deduplication. Beyond allowing you to get feedback on your work quicker, getting
    to a minimum viable solution soon can give you the confidence that you’re on the
    right track. You’ll also have a solution in place that is easier to iteratively
    improve, rather than spending a long time on a more complex one, having nothing
    tangible to show for it until the end of a longer process. Alternatively, this
    might be the point at which it is apparent that there is no tangible business
    value in improving your solution further, so rather than iterating further, the
    project is considered complete.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Questions to consider
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you work through this project, here are some key questions to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the possible ways in which customers can be represented across these
    datasets? It may be helpful to list all scenarios (customers in CRM, customers
    who made a purchase, guest checkouts, etc.) to hone in on exactly what you will
    need to code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you consider duplicates, how will linked accounts (i.e., a main account
    versus linked accounts) be represented?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When deduplicating records, what fields do you want to use for deduplication?
    Are two people with the same name living at the same postcode necessarily the
    same person? How much do two customers’ details need to differ before we consider
    them different people?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3.4 An example solution: Identifying customers from transactional data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s dive into the details of an actual example solution. I strongly recommend
    attempting the project yourself before reviewing the example solution. As with
    every project, the data files, as described in section 3.2, are in the supplementary
    materials. I also recommend reading this section even if you’ve got a solution
    you’re happy with, not because the example solution is the only solution but because
    we can learn a lot from seeing how others approach the same problem.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Developing an action plan
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will first explore the data to see if our assumptions about it hold, whether
    any data is missing, and so on. Then, we will decide on a common schema for our
    datasets and trim them all down to this common schema, meaning we will have three
    smaller customer datasets—one from purchases, one from the customer database,
    and one from the CRM data—which are all structured the same way and can be easily
    combined. After combining the datasets, we will deduplicate our records to arrive
    at the best guess of our customer base.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Exploring, extracting, and combining multiple sources of data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will explore each of the three datasets, extract common customer information
    from them, and combine them into one “master” view of customers. We will then
    look at deduplicating that combined dataset. Let’s start with the raw purchases.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a new dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We start by importing the necessary libraries and reading in the sales data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports necessary libraries'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Reads in our purchases CSV file as a pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Inspects the size of the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this code is `(71519,` `11)`, meaning just over 71,000 rows of
    data and 11 columns, so there are over 71,000 transactions in the purchases table.
    We know from the problem statement that guest checkouts will not make up all our
    transactions, so checking for missing data should at least reveal some missing
    guest information. The following code produces the output in figure 3.4, showing
    the count of missing values per column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A pandas trick to “add up” rows with missing values'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 Missing values in our purchases data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are 18,448 missing customer IDs, which should relate to guest checkouts,
    and 53,071 missing guest values. Adding those up gives us 71,519, which is the
    total number of records, meaning that guest checkouts and registered user checkouts
    make up our entire dataset. There are seemingly no rows with either all this information
    missing or both being present, but we should verify this. First, let’s create
    a new column to track guest checkouts, which happen when a customer ID is not
    provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our `is_guest` column takes the Boolean value of `True` if the customer
    ID is missing. We can use this column to verify our assumption about the guest
    checkouts and the customer IDs being mutually exclusive. The first line of this
    code checks for cases where the transaction was a guest checkout, but we also
    had a customer ID, and the second returns cases where we had neither:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The output for both lines is presented in figure 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 The output of our code to check whether guest and registered user
    checkouts overlap
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is Python’s way of telling us that there are no records for our criteria,
    meaning we can be sure that all rows are either a guest checkout or a purchase
    made by a registered customer. Next on our data quality agenda is checking what
    percentage of records are guest checkouts. This is not just for informational
    purposes, but also for us to get a sense of how many customer records we will
    have to infer. As guest checkouts are our weakest signal for a customer record,
    any guests we add to our customer database are assumed customers. They are inferred
    rather than concrete customer accounts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if someone entered John Smith as their guest checkout name, it
    could be because Mr. Smith was buying something on behalf of someone else, perhaps
    as a gift. Is John Smith the customer in this case? Or maybe it was someone using
    Mr. Smith’s credit card, maybe one of his children. In this case, is the customer
    John Smith or the child? Either way, we have no more information to go on than
    the guest name, John Smith, and that is what we would need to put in the customer
    database. Counting the number of guest accounts is useful to determine what percentage
    of our customer data will be “assumed” cases like this. Using the `is_guest` column
    we created earlier, we can calculate its distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The output is shown in figure 3.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 The proportion of guest vs. registered user purchases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This tells us that 25% of the rows are guest checkouts, but we need to remember
    that one row represents a purchased *item*, not a customer record, so the proportion
    of customers who checked out as guests is not necessarily 25%. We can calculate
    this actual proportion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Gets a unique combination of guest columns'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Gets all unique customer IDs'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Subtracts 1 from the unique customer count because NULL is also counted'
  prefs: []
  type: TYPE_NORMAL
- en: This prints a value of `8301` and another just under `0.25`, meaning we have
    8,301 unique combinations of guest columns, and once we extract unique customers,
    it turns out a quarter are indeed not registered and checked out as guests instead.
    This number won’t be exact because there could be typos. We are assuming every
    combination of name and postcode is a unique customer, but a single customer making
    a typo during one of their checkouts would result in us double-counting them here.
    This, of course, assumes they are allowed to make a typo during the checkout process.
    We would need to know more about the actual e-commerce system to understand whether
    these guest columns relate to billing or credit card information, for example,
    where typos might cause the purchase to be rejected. Knowing the data-generating
    process is vital.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize what we have so far. Figure 3.7 shows the progress we’ve made
    exploring the purchases dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 Progress in exploring the purchases dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Identifying a common structure between datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are around 25,000 unique customer IDs, which represent registered customers,
    and another roughly 8,000 unique inferred guests, so from purchases alone, we
    estimate the upper bound of the number of customers to be around 33,000\. I say
    upper bound because we will have to investigate duplicate accounts later, and
    this number may decrease if we find any. We also know that we have a first name,
    surname, and postcode available for guest checkouts. We will need to bear this
    in mind when we look at the other customer databases.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we export our first intermediate dataset, we need to decide
    on a schema for our data model. We know our guest customers have names and postcodes,
    and if we look at the data dictionary, we can see the customer and CRM datasets
    also have customer age. We don’t really want to drop that column just because
    it is missing for guest accounts, so our final schema will include it.
  prefs: []
  type: TYPE_NORMAL
- en: It is generally a good idea to also keep track of where our records came from
    once they are combined into a single table. We could do this by adding a `source`
    column, which could have values of `purchases`, `customer` `database`, or `CRM`,
    but this structure would assume a record can only come from one place. We may
    encounter duplication, so a better choice is an indicator column for each data
    source, that is, a column to mark whether the record is present in the purchase
    data, another to indicate whether it’s present in the customer database, and so
    on. These are mostly for data lineage purposes, so the source of the information
    is more transparent. We can also decide to explicitly track whether a record came
    from a guest checkout because this may be important later if a stakeholder wants
    to know what percentage of customers don’t register when buying. Table 3.5 shows
    the final schema, which is what each of the three datasets needs to be transformed
    into.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3.5 The data model schema that all data sources need to be transformed
    into
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Column | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `customer_id`  | The unique ID of the customer record, or NULL for guests.  |'
  prefs: []
  type: TYPE_TB
- en: '| `first_name`  | Either from the customer or CRM tables or the guest information.  |'
  prefs: []
  type: TYPE_TB
- en: '| `surname`  | Either from the customer or CRM tables or the guest information.  |'
  prefs: []
  type: TYPE_TB
- en: '| `postcode`  | Either from the customer or CRM tables or the guest information.  |'
  prefs: []
  type: TYPE_TB
- en: '| `age`  | From the customer or CRM tables, unavailable for guests.  |'
  prefs: []
  type: TYPE_TB
- en: '| `is_guest`  | `True` if the data comes from a guest checkout.  |'
  prefs: []
  type: TYPE_TB
- en: '| `in_purchase_data`  | `True` if this customer record appears in the purchase
    table. It is not exclusive since the customer could also appear in the customer
    or CRM datasets.  |'
  prefs: []
  type: TYPE_TB
- en: '| `in_crm_data`  | `True` if the customer record exists in the CRM database.  |'
  prefs: []
  type: TYPE_TB
- en: '| `in_customer_data`  | `True` if the customer record exists in the customer
    database.  |'
  prefs: []
  type: TYPE_TB
- en: Let’s go ahead and transform our first raw dataset, sales, into this desired
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: Restructuring a dataset to the common structure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The easiest way to export our customers from the purchase data is to extract
    the guests and non-guests separately and then combine them. These two subsets
    will not have the same structure because we have three columns for guests (first
    name, surname, and postcode) and for registered customers, we only have their
    IDs. We could join data from the customer and CRM tables to find the relevant
    names and postcodes for these IDs, or we could do that when we get around to exploring
    and manipulating the customer datasets. This choice is more personal preference
    than anything else. I’ve chosen to leave the joining until later, so for now,
    we will export incomplete data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To export only the guests, we can filter using our `is_guest` column and export
    only the relevant columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We drop duplicates to ensure we only have unique guest information.'
  prefs: []
  type: TYPE_NORMAL
- en: The output is as presented in figure 3.8.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 Guest data from the purchases table ready to be combined with customer
    data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For non-guest checkouts, we won’t have these columns; we will have only a customer
    ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Customer ID is a single column, so we need to explicitly make it a DataFrame.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We extract unique customer IDs from non-guest rows.'
  prefs: []
  type: TYPE_NORMAL
- en: The output is shown in figure 3.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 Non-guest data from purchases, which is just a column of customer
    IDs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The data shown in figures 3.8 and 3.9 are of a different structure. However,
    when we combine them, we will have all the columns from both datasets and missing
    data where a column did not exist in one of the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we concatenate (or “union” if you are used to SQL terminology) our two
    datasets. Then, we rename our columns and remove the guest prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to make sure we don’t have missing data, so we fill in the missing
    values for the `is_guest` column. Technically, we could leave it blank to indicate
    that someone isn’t a guest, but explicitly using `True`/`False` values is clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we add the `in_purchase_data` column we decided on for our schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we’re working with text data, another important step is to ensure there
    is no trailing whitespace and that the names all use the same capitalization.
    This is so that customer names are treated as being the same even if one is lowercase
    and the other uppercase. We can use the `pandas` built-in `.str` accessor class,
    which lets us manipulate entire string columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, the customer data extracted from our purchases looks like figure 3.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 A preview of our customer data extracted from purchases
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first rows show the non-guest checkouts and the registered customers. For
    now, we have no names or postcodes for them because we decided to join those afterward.
    The last few rows in the preview show our guests, hence the missing customer ID.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, let’s summarize what we have done so far. Figure 3.11 shows
    the steps we took when exploring and reshaping the purchases dataset. Text that
    appears without shapes represents steps and decisions from previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 The steps we took on the purchases dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We are now ready to move on and explore the customer datasets and merge them
    with the customer data we have just exported from our raw purchases.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a second dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We know from the data dictionary that both customer datasets have the same
    schema. What we’re looking for in both datasets is whether there is any missing
    data, whether the customer IDs are all filled in, and whether there are any duplicate
    records. Since customer ID is a unique identifier, we don’t anticipate any duplicates,
    but you cannot assume anything. We start with the CRM data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The shape of the dataset is `(7825,` `5)`, meaning 7,825 rows and 5 columns.
    Figure 3.12 shows a preview of the CRM dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 The first few rows of the raw CRM data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We check for missing data with the following code, the output of which is shown
    in figure 3.13:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 No missing data in the CRM table
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The next bit of sanity checking is ensuring that customer IDs are unique. One
    way to do this is to group by the customer ID and find instances where there are
    multiple rows in a group. If customer IDs are unique, no records should be returned.
    Let’s verify this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here we use `groupby` and `size` to count how many records we have per customer
    ID and use `loc` to filter instances where there is more than one. The Python
    output is `Series([],` `dtype:` `int64),` which indicates no records were found,
    as the empty square brackets represent an empty collection in Python. This means
    customer IDs are indeed unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this does not mean that customer *details* are unique in the table.
    If we look at how many unique combinations of name, postcode, and age we have,
    we can see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The output is 7,825 and 7,419, respectively, meaning that while there are 7,825
    rows in the CRM data, there are only 7,419 unique combinations of columns once
    we drop the customer ID, which indicates we have about 400 duplicate customer
    details where the same information is spread across multiple IDs. This might not
    mean 400 duplicate customers because we could also have multiple people with very
    common names living at the same postcode, but because we have also factored age
    into it, it is more likely these are all redundant duplicates. Incorrect duplicates,
    if there are any, are likely to be a very small percentage when we consider the
    size of the dataset, so it makes sense not to dwell on this, and for now, say
    that every unique combination of name, postcode, and age is a unique customer.
    The nature of data modeling work is that there will always be a margin of error.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 summarizes what we have done so far with the CRM data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 First steps in processing the CRM data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Joining datasets to enhance one with information from another
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The next step is to transform the CRM data to the same schema as the customers
    from the purchase table, and we also need to enhance the registered customers
    in the purchase history with details from the CRM data. So far, we only have IDs
    for those customers, but we need their names, postcodes, and ages. Not all of
    them will be found in the CRM data, but we can join the two and populate as many
    rows as we can. We will use a left join for this as that will ensure we keep all
    the rows in the original data regardless of whether we find a match in the other.
    The code for this is as follows, and the result is shown in figure 3.15:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 Checking for missing values after merging the sales and CRM data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One `pandas`-specific peculiarity is that columns that appear in both tables
    get an `_x` and `_y` suffix by default. We overrode this here to be more descriptive,
    so the ones with `_sales` are from the source data—the purchases—and the `_crm`
    suffix is given to the merged data, in this case, the CRM data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that there were around 33,000 rows in the sales data and 26,000 rows
    missing from the newly added CRM customer columns, we can see we matched around
    7,000 rows on customer ID. That is, customers in 7,000 purchases had their records
    stored in the CRM table. What we have now is a dataset where 7,000 customer records
    are in columns ending in `_crm`, which we should merge into the ones marked `_sales`,
    which contain customer data from guest checkouts. First, we define a filter to
    select only rows with a customer ID, thus excluding guests and rows with customer
    information in the `crm`-suffixed columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This filter can then be applied to identify these rows as having been found
    in the CRM data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The output is shown in figure 3.16.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 Number of rows with customer information coming from the CRM data,
    after merging
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This 7,114 number tallies with what we observed earlier, that around 7,000
    rows have now had their customer information updated. Time to copy over data from
    `crm`-suffixed columns to our `_sales` suffixed ones and only keep the latter
    to get back to our original schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we simply copied over the first name, surname, and postcode to overwrite
    the missing values in `sales`-suffixed columns with CRM customer data in the `crm`-suffixed
    ones. Now we’re ready to remove the latter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The output is shown in figure 3.17 and is what we’d expect. The schema is now
    the same as before, apart from a new `in_crm_data` flag, and the customer data
    from purchases has been enhanced with CRM data where possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 Combined purchase and CRM customer data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Immediately in the second row, we notice an example of a customer who purchased
    with a registered account, so they are not guests, but their details have been
    filled in via the CRM dataset. What remains is to check for and add customer details
    that exist in our CRM system but do not appear in our purchases. There may be
    reasons for this; perhaps those customers bought something on the phone, and those
    sales do not get recorded in the same place. Whatever the reason, it is a possibility
    that we need to account for to ensure full coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Using sets to cross-reference two datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To find these records, we employ a Python trick to subtract one set of customer
    IDs from another, leaving us with the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, “set” refers to the rigorous mathematical definition of a unique collection
    of items, and “difference” means the subtraction of one set from another, leaving
    us with only customer IDs that appear in the CRM data but not in purchases. The
    output tells us there are 711 such customers whose details need to be added to
    our growing customer dataset. We simply concatenate/union the data with the customers
    corresponding to the IDs we have just selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'One final aspect of this data to clean up is that for these new customers,
    we don’t have data for our source flags, so we fill them in with default values.
    Customers added from the CRM data will have their `in_crm_data` flag set to `True`,
    `in_purchase_data` as `False`, and since they weren’t guests, `is_guest` as `False`.
    The output of the following code is shown in figure 3.18:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 After merging the CRM data and cleaning it, there are no missing
    values for our source flags.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s review what we have done with the CRM data before moving on. Figure 3.19
    shows all the steps we have taken.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 The steps we took to explore the CRM data and export customer information
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To summarize, we now have a customer dataset of all customers who made a purchase,
    including guest checkouts, as well as data from our CRM system, including customers
    who exist only in the CRM data and have no recorded purchases. We now need to
    repeat this process with data in the customer database, which is structurally
    similar to the CRM data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a third dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Time to look at our customer database. The code will be similar to the one
    used to manipulate the CRM data but is included for completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output is `(23476,` `5)`, meaning we have over 23,000 customer records,
    which is significantly more than in our CRM data. A preview of what this data
    looks like is shown in figure 3.20.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 The first few rows of the customer data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following code checks for missing data and produces the output shown in
    figure 3.21:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-21.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 No missing data in the customer database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We sanitize our columns like we did for the CRM data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check whether customer ID is unique and how many unique combinations
    of customer information we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This code yields the same output as in the CRM data, namely an empty Python
    collection, meaning there are no instances of the same customer ID appearing twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output of the previous code tells us that 23,476 rows represent 19,889 unique
    combinations of customer details, so we potentially have around 3,500 duplicate
    records to handle. We will do this once all the customer data has been merged
    into a single table.
  prefs: []
  type: TYPE_NORMAL
- en: Merging all our data sources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The next step is to merge the customer information into the growing customer
    data by joining them. Again, we give the duplicate column names meaningful suffixes
    to show which table they came from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The output of this merge is shown in figure 3.22.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 The first few rows of the sales and CRM customers merged with the
    customer database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As with the CRM data, we have duplicates of the customer detail columns. We
    will now identify which rows were successfully merged with the customer database,
    mark them with a final flag `in_customer_data`, and copy those details over to
    the `_sales`-suffixed columns before finally removing the redundant columns and
    arriving at our final schema. The output for the following code is shown in figure
    3.23:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-23.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 The distribution of the new flag, showing whether a customer’s details
    appear in the customer database
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Almost two-thirds of purchases relate to customers whose details are stored
    in the customer database. We can now update the original customer details, those
    with a `_sales` suffix, with details from the customer database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A filter to mark customers whose details we copy over'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We then overwrite those customers’ details.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Finally, we merge the columns into the final schema.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to add any customers present in the customer database who do not
    appear in the purchases table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns 1,423 additional customers to add:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we update the source flags to reflect their correct defaults if they
    are missing. Any newly added customers are not guests, didn’t come from purchases
    or the CRM data, but are present in the customer database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now inspect the final schema of our combined customer data model, shown
    in figure 3.24:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-24.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 A preview of the customer data merged from all three sources
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: From figure 3.24, we can already understand the combinations of where customer
    data comes from. The third row shows us another edge case we had to be prepared
    for—a purchase from a registered customer whose details we do not have in either
    the CRM data or the customer database. We know very little about this customer,
    but they were given an ID, so their details may be in another system. As analysts,
    we would reach out internally to the business and find an explanation. Until then,
    we should keep these rows as they are potentially legitimate customer entities
    and should thus be counted in our data model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize all our steps so far. We explored three different sources of
    customer data, transformed them into the same schema, and finally merged them
    into a single table of customer information. Figure 3.25 shows all our steps so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-25.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 Steps to process three sources of customer data and finally merge
    them
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Taking stock of what we have so far, we have merged our three sources of customer
    data, taking care to cover all eventualities, totaling 35,395 records. One step
    we will take before moving on to deduplication is to get a sense of the size of
    our various customer record types. There are four ways we could have added customers
    to our data model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Identified customers*—Customers who made a purchase and their details are
    present in either the CRM data or the customer database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Guest checkouts*—Customers whose details come from what they entered as a
    guest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unidentified customer IDs*—Customers with a valid ID but with no corresponding
    record in either customer dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Customers with no purchases*—Customers who are present in either customer
    dataset but do not appear in the purchases data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These customer types are mutually exclusive, and their numbers should add up
    to the entire data model. We can verify this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the following: 35,395 records in the entire data model, 23,713
    of which are identified customers, 8,300 guests, 1,248 unidentified customer IDs,
    and 2,134 customers with no purchases. The first number is the sum of the others,
    so we can be confident we did not miss any of our eventualities, and they don’t
    overlap. At this stage, we know there might be some duplication, so we move on
    to the final part of modeling our customer data—entity resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Applying entity resolution to deduplicate records
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One possible kind of duplication we might have is the same customer’s details
    appearing in both the CRM data and the customer database. In this case, we might
    have rows in our data model that are exact duplicates, which are easy to drop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Checks the count of rows before deduplicating'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Checks the row count again to see any impact'
  prefs: []
  type: TYPE_NORMAL
- en: The output for both statements is the same—35,395 records—meaning there are
    no exact duplicates. In this case, this is because we updated our flags along
    the way. That is, customers present in multiple data sources simply have multiple
    source flags set to `True`, so there were no exact duplicates to drop.
  prefs: []
  type: TYPE_NORMAL
- en: 'One consideration at this point is that guest checkout customers are missing
    the `age` column. There is a choice to make here: Do we drop this column because
    our final data model would contain missing data, or do we include it but avoid
    using it for deduplication? If you’re going to drop data, it’s better to do it
    as late as possible, so it makes sense to keep that column. When we deduplicate
    the records, we can decide whether two customers with otherwise identical information,
    but one has their age filled in and the other doesn’t, are the same customer.'
  prefs: []
  type: TYPE_NORMAL
- en: Filling in missing data with unique identifiers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another decision we can make is whether to give guest accounts fake customer
    IDs rather than leaving them as missing data. When we get to the deduplication
    step, it is a good idea to link accounts together, that is, identify customer
    IDs that relate to the same underlying customer entity. With guest accounts, we
    cannot do this unless they also have unique identifiers, so it makes sense to
    give them their own IDs. One idea is to allocate a range of integers only for
    guest accounts. We can look at the existing IDs to see the current range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output tells us the current IDs range from the number 1 all the way to
    9-digit integers, so a safe range would need to be well outside this. One option
    is to use negative numbers instead to identify each unique guest (i.e., each combination
    of guest customer data points). Negative IDs are unusual for unique identifiers,
    but the alternative approach of allocating an ID range for guests, say, in the
    12-digit range, feels just as artificial. Another option could be to create alphanumeric
    identifiers, like a number preceded by a “G” for “guest,” but since customer IDs
    are otherwise all integers, it’s a personal choice not to branch out into the
    alphanumeric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We create an automatic range of values starting from –1 and decreasing.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, our guest customers have IDs ranging from –1 to –8300\. At this stage,
    we should have no duplicate records for customers that appeared in multiple datasets.
    However, we could still have duplicate records for the same customer if they somehow
    received two different customer IDs in different systems. Customer John Smith,
    with an ID of 123, might still be the same customer as John Smith, with an ID
    of 456.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are at the final deduplication phase; we might not be talking about
    a large percentage of duplicate records. As I mentioned when discussing iteration,
    in the first pass, we might even choose to ignore the duplication, in which case
    we already have our first customer data model. However, this is a task where we
    want to be as accurate as possible and would ideally reduce duplication to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Finding and linking duplicate records
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most straightforward initial deduplication approach is to say that if two
    customers have the same values for first name, surname, postcode, and age, they
    are the same customer. However, this might be a problem if we have a guest account
    who is the same customer as one already in the CRM system, which we wouldn’t know
    if we included age in the comparison, as it would be missing for the guest record.
    Using only first name, surname, and postcode may be a combination good enough
    to start with. We can write some code now to find all customer IDs whose first
    name, last name, and postcode match exactly.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create an object, `duplicates`, which is a list of all customer records
    that are identical to one another in the columns we specified. The `keep=False`
    parameter ensures we keep all relevant records, not just the duplicate ones. Having
    the `keep` parameter as anything else would drop the first instance and only keep
    the other rows, the duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a lookup dictionary where each customer ID is linked to all
    the other records, which are its duplicates. A sample of this dictionary is shown
    in figure 3.26:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-26.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.26 A sample of the duplicate lookup dictionary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This dictionary tells us that, for example, there are two customer IDs for
    Aaliyah Harvey at postcode SO760SX: 22648 and 27397\. We can use this dictionary
    to create a new column, `other_customer_ids`, where we store this list for accounts
    that have duplicates. A sample of the resulting data is shown in figure 3.27:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-27.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.27 A sample of rows with the new `other_customer_ids` column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure 3.27 shows that, for example, Harley Palmer at postcode HR250EJ has
    two customer records: IDs 31266 and 5411\. Strictly speaking, our `other_customer_ids`
    column should not be self-referential, so we should remove a customer’s own ID
    from it. We can create a small function to do that and apply it to the rows with
    duplicates. Figure 3.28 shows the data after we run the following code. From this,
    we can notice an instance where a guest account for Max Moore, at postcode M902XX,
    is linked to a registered customer ID as a duplicate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-28.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.28 The new `duplicate_customer_ids` column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As a reminder, what we want our database to contain is one row per customer,
    and each customer who has duplicate records has this fact marked somehow. The
    data, as it stands, has multiple rows for the same customer entity, one from the
    point of view of each, an example of which is shown in figure 3.29\. Customers
    31266 and 5411 are likely the same entity, and their duplication is recorded from
    both points of view.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-29.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.29 The same customer entity represented as two rows
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are two ways to tackle this problem. One is to delete one of the duplicate
    records entirely, thus reducing the data down to one row per entity. The `duplicate_customer_
    ids` column would still record the fact that this customer entity is referred
    to by multiple customer IDs, but the rest of the customer data for the duplicate
    ID would no longer be there. Ideally, each row would be a deduplicated customer
    record, but another option is to create an `is`_`main` flag against rows to identify
    them this way. The advantage is that all data lineage is preserved, and the downside
    is that you can’t simply count the rows anymore; you would need to remember to
    filter on `is`_`main` each time. Choosing which representation is better for your
    data model will again depend on the context in which the data model will be used.
    Technically, if customers 1480 and 1481 are the same customer, they’re the same
    customer *entity* but two distinct *records,* and my personal preference is to
    delete as little data as possible, so in the example solution, I’ve used the `is_main`
    flag approach.
  prefs: []
  type: TYPE_NORMAL
- en: Whichever representation you choose, you still need to decide which customer
    record is the main one. One method is to simply use the first one you encounter.
    It is unlikely to make a big difference, but a more principled way would be to
    use a better metric, like number of transactions, total spending, and so forth,
    to decide which customer record deserves “main” status.
  prefs: []
  type: TYPE_NORMAL
- en: 'The technical trick to create the flag is to generate a column that gives each
    duplicate a rank and a row number in the order they are encountered. Anything
    with a `rank` of 1 simply becomes a main account. This approach will work for
    duplicates and unique records, as the first instance of a combination of customer
    details will always have a `rank` of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Figure 3.30 shows the same duplicate pair as in figure 3.24, with the new `rank`
    column added.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-30.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.30 Our data with a new `rank` column
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now, we have one row per customer record, but to count distinct customer entities,
    we can create our `is_main` flag to make the data model more obvious. Once we’ve
    done this, we no longer need our `rank` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can count the records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that out of 35,395 records, 27,394 are unique/main records.
    This assumes that two customers with the same name and postcode are the same customer
    and that there are only *exact* duplicates. To ensure our solution is as accurate
    as possible, given the data, we can try to match records that are *almost* identical.
  prefs: []
  type: TYPE_NORMAL
- en: Using entity resolution tools to improve deduplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once our dataset is deduplicated, around the 27,000 mark, we may wish to apply
    more advanced ideas in future iterations. One is fuzzy string matching to link
    accounts that differ by a simple typo. Another idea is to investigate whether
    you could use purchasing patterns to identify identical customers. In the case
    where two customers match on most columns but, say, differ in their age, you could
    use additional information like their purchases to decide whether they refer to
    the same customer. For this example, we’ll identify accounts that are almost identical
    and see if that makes a difference. When it comes to more complex topics, such
    as record linkage, there is often a Python package we can use rather than implement
    any algorithms ourselves. In this case, we will use the Python Record Linkage
    Toolkit, which implements multiple algorithms for record deduplication and linking
    efficiently. An important aspect of being an analyst today is identifying when
    to use the work of others. We don’t always need a deep understanding of the underlying
    algorithms and implementations before we use external libraries as long as we
    know what to expect from the output and can investigate problems when they occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s feed our data into this toolkit, a module called `recordlinkage`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can effectively follow the basic tutorial from the toolkit’s data deduplication
    page ([https://mng.bz/nRYa](https://mng.bz/nRYa)) and modify it to our needs.
    First, we index our dataset so that we don’t try to compare every pair of records
    to each other but tell the code that two records at the same postcode should be
    tested for duplication. This assumes there are no typos in the postcode, which
    may not be the case, but it will make our code run much faster as there are fewer
    comparisons to make. Sometimes, we need to trade off accuracy and performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates an Index object'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Marks postcode as a column to use for indexing'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Applies the indexing to the data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the index to be the `customer_id` column because that way, our final
    dataset of matches will retain this name, as we will see. Next, we set up the
    comparison rules: How should our records be compared against each other? Should
    matches be exact or allowed to be fuzzy? We can also choose the algorithm with
    which to make a fuzzy comparison between strings. Here, we use the Damerau–Levenshtein
    method, which is a measure of *edit distance*, that is, the number of individual
    character edits required to get from one string to another. The higher the distance,
    the less similar the two strings are. Here, you could experiment with different
    comparison methods and observe the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a comparison object'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Names should be fuzzy comparisons; anything over 85% similar is a match.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Postcodes should match exactly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to use our comparison rules to perform the pairwise comparisons.
    This creates a DataFrame containing all compared pairs and whether each of our
    comparison criteria was met. A sample of the output is shown in figure 3.31:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-31.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.31 A sample of the output of our record linkage attempt
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This sample shows that, for example, customer IDs 7523 and 7466 match on postcode
    but not on name. We can reduce this data down to only the cases where all comparisons
    returned a match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the same structure as figure 3.31 but with only perfect matches.
    Next, we should merge these customer IDs back to the original dataset and see
    if we’ve improved our record-linking attempts from before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a new DataFrame containing only the two columns of customer IDs'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Joins customer data on the first customer ID'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Joins customer data on the second customer ID'
  prefs: []
  type: TYPE_NORMAL
- en: The `matches` dataset is already deduplicated in the sense that if customers
    1 and 2 are duplicates, we do not have two records from each of their perspectives,
    so joining on both customer IDs means we ensure we have merged the main customer
    with all the duplicates found by `recordlinkage`. Figure 3.32 shows the relevant
    columns of our current merged data. There is an instance where both the `duplicate_customer_ids`
    column, created by looking for exact matches, and the new `customer_id` columns,
    created by the latest fuzzy matching attempt, agree with each other on records
    5411 and 31226.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-32.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.32 New columns added by merging the linked records back to our customer
    data model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One problem we need to resolve with this new merged data is that records with
    multiple duplicates are now repeated (e.g., customer ID 30730), as shown in figure
    3.33.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-33.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.33 Three duplicates means three rows of data, which we need to merge
    into one
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To merge these rows into a single row, as we have in the `duplicate_customer_ids`
    column, we can write a small function to collect all linked customer IDs for a
    given customer ID. We joined our linked pairs twice, so both the `customer_id_1_customers`
    and `customer_id_2_matches` columns could refer to our customer, and the `customer_id_2_
    customers` and `customer_id_1_matches` columns could refer to the duplicate ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Our function collects the necessary values from multiple rows into a single
    list, and we apply this function to each customer ID, in turn, to reduce the dataset
    down to one row per customer ID again. The output is shown in figure 3.34.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-34.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.34 A preview of our customer IDs linked to their duplicates
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can use the `customer_id` column to merge this data back to our data model,
    so we have two sets of duplicates to compare side by side—the one that only uses
    exact matches and the latest one, which also uses fuzzy string matching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Our data now looks like that in figure 3.35.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-35.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.35 Customer data containing results of two different deduplication
    methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now, we can compare instances where the two columns `duplicate_customer_ids`
    and `linked_duplicates` do not agree. Figure 3.36 shows some of the rows as the
    output of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/3-36.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.36 Instances where the two deduplication methods disagree
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s inspect one of these cases, shown in figure 3.37\. We observe that, for
    Scarlett Jackson, two additional duplicate records were found by `recordlinkage`,
    one of them referring to a Sgarlett Jagkson and another to a Scariett Jackson.
    All of these are likely to be the same customer, so it makes sense to use the
    fuzzy method to find all possible duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-37.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.37 An instance where the two deduplication methods found different
    results
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Ultimately, there are just 13 cases where the two methods disagree, so it seems
    using `recordlinkage` only gave us a marginal improvement. However, we now have
    code for the future that can deduplicate our customer data more intelligently,
    and the accuracy of our results has improved.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to the conclusions and recommendations, let’s review the entire
    analysis process shown in figure 3.38\. As with all analyses, your specific path
    may have diverged from the ones I chose.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/3-38.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.38 The entire analytical process diagram for this project
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we have done the work and documented our analysis, let’s move on to
    the conclusions and recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Conclusions and recommendations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our initial problem statement asked for help counting customers. Our final data
    model has 35,395 rows corresponding to 27,394 unique customers, which fits our
    original estimate of “at most 33,000 customers.” However, it is a more precise
    and therefore more useful number as it is a result of all the analysis we have
    done so far. This number may be different if you decide to use the results of
    the `recordlinkage` library to create a deduplicated data model.
  prefs: []
  type: TYPE_NORMAL
- en: How do we assess the quality of the final solution? This is hard to quantify
    as there is no ground truth to check against, but it’s a good idea to get a sense
    of the different amounts of completeness that exist in the data model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we have around 26,000 customer IDs in the purchase data that have
    no corresponding records in either customer dataset. This means we have 26,000
    customers who have signed up to make a purchase online, but we don’t have their
    details, and because they weren’t using a guest checkout, we have nothing but
    a customer ID for them in our final data model. We could choose to drop them as
    incomplete records, but we would skew our measurement of the size of the customer
    base. It is better to understand that our data model varies in completeness and
    is suitable for some tasks—like counting customers—but is not wholly suitable
    for other tasks, such as customer segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: This is a conclusion we should share with our stakeholders when presenting an
    analysis based on our data models. It is also the kind of uncertainty and ambiguity
    we need to learn to embrace and communicate.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever methods you use, entity resolution is hard to automate 100%. There
    will always be edge cases that make the data model less than 100% accurate. The
    value in this task lies in the fact that once you have a “best guess” customer
    data model, you can be sure that all subsequent analyses, while not perfectly
    accurate, will be the best you can do given the data that you have. Also, each
    analysis does not have to start with defining what we mean by a customer since
    that work has already been done in the data model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity: Further project ideas with this data'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'E-commerce data is rich with patterns and trends waiting to be discovered.
    Think about some other research questions that can be answered with this data.
    Here are some ideas to get you thinking:'
  prefs: []
  type: TYPE_NORMAL
- en: Is there a way to deduplicate customers with the same name based on their purchase
    history? If there are two John Smiths at the same postcode but different customer
    IDs and different purchasing profiles, does that make it less likely they’re the
    same person?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the data contain any information about households? Perhaps people with
    the same surname at the same postcode?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do purchasing behaviors differ between registered customers and people who checked
    out as guests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.5 Closing thoughts on data modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you attempted a data modeling task, perhaps without formal
    data modeling training. If you are interested in this topic more deeply, one place
    to start is the canonical *The Data Warehouse Toolkit* by Ralph Kimball and Margy
    Ross (Wiley, 2013). There are key data modeling concepts such as “star schemas”
    and “fact tables” to explore to get a deeper understanding of data modeling best
    practices. A less technical, more business-oriented approach would be to study
    the Business Event Analysis & Modeling (BEAM) technique. The idea behind it is
    that the core business entity to focus on is events that happen in the business
    lifecycle. Your data models would take the form of “customer buys product,” where
    one record is one instance of a customer buying the product, complete with details
    about the customer, the product, and the purchasing event. Thinking in terms of
    events forces you to think about how the business actually works and, ultimately,
    the process that generated your raw data. A relevant text to explore would be
    *Agile Data Warehouse Design* (DecisionOne Press, 2011) by Lawrence Corr and Jim
    Stagnitto.
  prefs: []
  type: TYPE_NORMAL
- en: What is most important is that you consider the purpose, and therefore the necessary
    details, of your data and get into the habit of creating data models, which abstract
    away the complexities of the raw data into a more business-relevant form. This
    skill will come up in most of your analytical projects.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 Data modeling skills for any project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we focused on the creation of new datasets, which we called
    data models. The specific skills learned for data modeling, which can be used
    for any problem, include
  prefs: []
  type: TYPE_NORMAL
- en: Exploring multiple datasets to identify a common structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping a dataset to adhere to this common structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying what form a data model should be stored in (e.g., wide or long)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joining multiple datasets to enhance one with information from another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-referencing two datasets (i.e., finding rows that appear in one but not
    the other)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining smaller data models into a master data model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using simple methods to deduplicate records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using advanced methods, such as entity resolution tools, to deduplicate records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thinking about the purpose of a dataset helps identify the right structure for
    your data model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data modeling is a crucial analyst skill that should be applied to create clean,
    defined, deduplicated, restructured, and usable data from raw datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even basic analytical tasks such as counting are easier when the data is modeled
    correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper data modeling provides easy reuse of the same data to answer additional
    analytical questions by adjusting the level of granularity or providing wide or
    long looks at the data.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
