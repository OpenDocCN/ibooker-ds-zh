- en: Chapter 9\. Stratified Randomization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we saw the simplest form of randomization: a customer
    shows up, and we toss a metaphorical coin or dice. Heads and they see version
    A, tails and they see version B. The probabilities may be different from 50/50,
    but they are constant, and independent of the customer characteristics. No “my
    control group is a bit older than my treatment group, let’s make sure the next
    Millennial who shows up goes into the control group.” As a consequence, your control
    and treatment groups are “probabilistically equivalent,” which is statistics’
    way of saying that if you kept running your experiment forever, your two groups
    would have the exact same proportions as your general population. In practice,
    however, your experimental groups; can end up being quite different from each
    other. Adding explanatory variables to your final analysis can somewhat compensate
    for these imbalances, but as we’ll now see, we can do better than that if we know
    ahead of time who is going to be part of our experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ll introduce you to stratified randomization, which will
    allow us to ensure that our experimental groups are as similar as possible. This
    starkly increases the explanatory power of an experiment, which is especially
    useful when you can’t have large sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Stratified randomization can be applied to any situation where we have a predetermined
    list of customers/employees/etc. to build our experimental groups from. Given
    that A/B tests are most often discussed in relation to minor changes to an email
    or website, I could have taken the example of an email campaign. But I wanted
    to demonstrate that larger business initiatives, which are often taken by company
    executives based on their “strategic sense,” can also be tested and validated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The business context here is that by default, AirCnC gives owners at least
    24 hours to clean their property between two bookings. In high-demand markets
    where properties are booked as soon as they’re available, this represents a significant
    limiting factor. Business leaders are keen to reduce it and increase monthly profit
    per property. As is often the case, there are two schools of thought in the company:'
  prefs: []
  type: TYPE_NORMAL
- en: The finance department advocates offering owners the opportunity to set a minimum
    duration of two nights per booking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The customer experience department believes a minimum duration would adversely
    impact customer satisfaction; instead, it advocates offering owners the services
    of a professional cleaning company for free in exchange for a reduction of the
    cleaning window from 24 to 8 hours.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Situations like this often arise in business. Both sides have somewhat compelling
    arguments that speak to different aspects of the problem or emphasize different
    metrics (here, booking profit versus customer experience), and/or provide anecdotal
    evidence in favor of their position (“this other company does X, so we should
    do it too”). A common outcome is that whoever has the most sway “wins” and their
    solution gets implemented, a.k.a. organizational politics rule.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you probably expect me to say something along the lines of “but
    experimentation allows you to bypass all politics and reach the best solution
    without any fuss.” I wish it were that easy! The truth is that experimentation
    can be of tremendous help in such situations, but it’s not a cure-all, for two
    reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first reason is that unless a solution turns out to be superior to the
    other on all fronts, there will be some genuine trade-offs that need to be made
    between competing objectives: how much decrease in customer satisfaction is the
    company willing to tolerate for an increase in profit? That question is inherently
    political because different stakeholders in the company have different preferences
    on that matter. You’ll have to present these trade-offs as clearly as possible
    to your leaders and get them resolved as much as possible ahead of time if you
    want your experiment to be successful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second reason is that your experiment will make at most one side happy
    (it can also make both sides unhappy if the control group has the best outcomes!).
    Unhappy business leaders, like any other unhappy human being, can be very good
    at finding rationalizations after the fact: “The San Francisco Bay area is different
    from other high-demand markets,” “Survey-measured customer satisfaction is down,
    but Net Promoter Score is up, and NPS is a better measure of ‘true’ customer satisfaction,”
    etc.'
  prefs: []
  type: TYPE_NORMAL
- en: These two reasons make it extra important to properly plan and run your experiment,
    not just from an experimental design perspective, but also from a business perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the Experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we’ve seen in the previous chapter, successfully planning an experiment
    requires us to clearly articulate its theory of change:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the business goal and the target metric?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the definition of our intervention?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How are these connected by our behavioral logic?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should hopefully by now be familiar with that process, so I will not elaborate
    on how to do it, and I’ll just go through the steps quickly to give you a deeper
    understanding of the experiment. In particular, I’ll use the opportunity to call
    out some peculiarities of the experiment from a behavioral perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Business Goal and Target Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The business goal for this experiment, or equivalently the business problem
    we’re trying to solve, is to increase profitability by reducing the amount of
    downtime in high-demand markets. Because the cost of offering free cleaning services
    to owners is significant (finance has estimated it to be $10/day), we’ll need
    to include it in our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll do so by modifying our target metric accordingly. Our basic metric is
    average booking profit per day; we’ll use instead average booking profit per day
    *net of extra cost*. This simply means that we’ll have to deduce $10 from the
    basic metric for the free cleaning treatment.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are also some concerns that the minimum-duration intervention
    would negatively impact customer satisfaction. How can we take that into account?
  prefs: []
  type: TYPE_NORMAL
- en: A solution I have seen other authors advocate is to use a weighted average of
    metrics, (sometimes called an Overall Evaluation Criterion [OEC]). In our present
    example, this would mean assigning weights, e.g., 50% each, to our two variables,
    and then using that new metric as our target. You should certainly feel free to
    use that approach if you or your business partners wish to, but I would recommend
    instead picking a unique target metric, and if necessary have a few other “guardrail”
    metrics on a watch list, for several reasons.
  prefs: []
  type: TYPE_NORMAL
- en: The first one is that the trade-offs between business goals are ultimately strategic
    and political decisions. There is no objective best answer as to how many CSAT
    points an increase in profit is worth; it depends on the organization, its context,
    and its current priorities. Using a weighted average determined at one point in
    time gives the process the appearance of technical objectivity but it’s really
    only fossilized subjectivity.
  prefs: []
  type: TYPE_NORMAL
- en: The second is that an OEC makes these trade-offs linear. If one CSAT point is
    equivalent to $10 million of profit with an OEC, then five CSAT points are equivalent
    to $50 million of profit. But a decrease of one CSAT point might mean fewer delighted
    customers while a decrease of five points might mean a social media storm. A dollar
    is always worth another dollar, but for pretty much anything else, a series of
    small changes is generally preferred to a single big one. Relying blindly on an
    OEC can lead to riskier decisions. Proponents of the OEC approach may object at
    this point that obviously you shouldn’t rely on it blindly; but if you’re going
    to look at the various components and have an open discussion anyway, I’m not
    quite sure how having an OEC helps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, an OEC takes business interventions as fixed. Sticking with the 1
    point CSAT = $10 million equivalence, the following two options would have an
    equal rating of zero:'
  prefs: []
  type: TYPE_NORMAL
- en: The first intervention increases profit by $1 million and decreases CSAT by
    0.1 points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second intervention increases profit by $50 million and decreases CSAT by
    5 points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But there is a big difference from a behavioral perspective. The first option
    is basically a dead horse and there is little hope of beating life into it, whereas
    the second option is more like a mercurial purebred. By targeting it to a specific
    segment of customers, changing its terms or the way it’s presented, there might
    be a way to reap at least some of its benefits without its costs. In that sense,
    an intervention with both big positive and negative effects calls for exploration
    and design iterations rather than the binary decisions encouraged by an OEC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I believe that in some cases, an OEC is used as a shortcut. Let’s
    say that an intervention increases short-term profit but also increases the probability
    of defection. This is not a genuine strategic trade-off: we should measure the
    impact of the probability of defection on lifetime value, and then determine the
    net effect on profitability. Saying that your OEC will be 90% short-term profit
    and 10% effect on defection rate is a way of taking a guess instead of measuring
    the true exchange rate. With the causal-behavioral framework of this book, we
    can do better than taking guesses.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, for the rest of the chapter, we’ll use average booking profit per
    day as our single target metric, and assume that CSAT is being monitored in the
    background for any worrying change.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That’s all well and good, but when we’re talking about tracking customer satisfaction,
    which customers are we talking about? If customers want to book a location for
    just one night and are offered a location with a duration minimum, they might
    decide to book a different one (either in the control group or the free-cleaning
    group), or else completely give up on booking through AirCnC and book a hotel
    instead. Therefore, we can’t simply measure customer experience for a clearly
    defined group of customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, that problem is not rare: whenever you run an experiment where
    the experimental unit for random assignment is not a customer, you have to ask
    yourself how this will play out on the customer side. What we can do is leverage
    the fact that minimum duration would impact only customers who are looking for
    a one-night booking; we also know that customers enter their desired duration
    before being shown available properties. Therefore, we could track all customers
    in our experiment who have a one-night desired duration and check whether they
    end up booking several nights in the same property or one night in another property,
    or end up not booking at all. Whenever they make a booking, we would also track
    how they rate their stay. That’s certainly far from perfect because we would be
    tracking different metrics for different subgroups of customers, but that’s the
    best we can do, and is another example of why I believe much more in monitoring
    guardrail variables than in aggregating them in an OEC.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of the Intervention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After determining the criteria for success, we need to make sure we have clarity
    on what we are testing. This is especially important when the organizational stakes
    are high, e.g., business leaders are butting heads on the question. Here, it is
    important to point out that we’re offering owners the opportunity to set a minimum
    duration, which is not the same thing as making a minimum duration mandatory.
    Similarly, owners may or may not take the free-cleaning offer. In addition, the
    treatments themselves could still be open to interpretation. How thorough and
    costly for the company is the free cleaning? What is the minimum duration we’re
    enforcing on customers?
  prefs: []
  type: TYPE_NORMAL
- en: Because both of our interventions are somewhat complex and rely on owners understanding
    an offer and choosing to take it, it is probably a good idea to create a few different
    designs and test them qualitatively through UX research. In addition, after running
    the experiment, it would be a good idea to consider slightly different variations
    of whichever treatment we decide to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, you’ll want to make sure that all the stakeholders are satisfied
    with the designs for the experiment and are willing to sign off on it. It will
    reduce (but not eliminate!) the risk that when the results come in, they will
    argue that what was tested did not adequately represent their proposed solution.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral Logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The behavioral logic is different for the two treatments: the minimum-duration
    approach would increase the duration and amount per booking but may reduce the
    overall number of bookings; on the other hand, the free-cleaning approach would
    increase the number of bookings but will reduce the profit per booking due to
    the added cost. In addition, we need to take into account that our experimental
    treatments/interventions are *offers*, which owners may or may not accept ([Figure 9-1](#the_cd_of_the_two_treatments_under_cons)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The CD of the two treatments under consideration](Images/BEDA_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The CD of the two treatments under consideration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data and Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [GitHub folder for this chapter](https://oreil.ly/BehavioralDataAnalysisCh9)
    contains two CSV files with the variables listed in [Table 9-1](#variables_in_our_data-id00081).
    The check mark (✓) indicates the variables present in that file, while the cross
    (☓) indicates the variables that are not present.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-1\. Variables in our data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Variable description | chap9-historical_data.csv | chap9-experimental_data.csv
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *ID* | Property/owner ID, 1-5000 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *sq_ft* | Square footage of property, 460-1120 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *tier* | Tier of property, categorical, from 1 to 3 in descending tiers |
    ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *avg_review* | Average review of property, 0-10 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *BPday* | Booking profit per day, target variable, 0-126 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Period* | Month index, 1-35 in historical data, 36 implicit in experimental
    data | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Month* | Month of year, 1-12 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *group* | Experimental assignment, “ctrl,” “treat1” (free cleaning), “treat2”
    (min. booking duration) | ☓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *compliant* | Binary variable indicating if owner was treated according to
    their assigned group | ☓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'In this chapter, we’ll use the following packages in addition to the common
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Determining Random Assignment and Sample Size/Power
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this experiment, we’ll assign experimental groups all at once, based on
    the list of our owners at a point in time. This gives us an opportunity to significantly
    improve over purely random allocation by ensuring from the get-go that our two
    groups are well balanced through a method called stratification. This will allow
    us to get more statistical power out of any given sample size.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I’ll first explain the method for the random assignment, so that
    we can use it in the simulations for our power analysis. Finally, we’ll compare
    the results of these simulations with a traditional statistical power analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Random Assignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before getting into stratification, let’s see what standard randomization would
    look like.
  prefs: []
  type: TYPE_NORMAL
- en: Random assignment level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first consideration for our random assignment is the level at which we’ll
    implement it and then measure the outcomes of the experiment. In the previous
    chapter, I discussed the question of whether the random assignment should happen
    at the level of a customer or of a booking. In the present case, the logistics
    of the experimental treatments, especially the free-cleaning one, precludes implementation
    at the booking level. Therefore, we’ll do the random assignment at the level of
    a property owner, which number 5,000 in AirCnC’s data at this point in time.
  prefs: []
  type: TYPE_NORMAL
- en: Standard randomization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process is similar to the one we used in the previous experiment, but simpler
    because it can be done offline instead of in real time: first, we assign a random
    number between 0 and 1 to each individual in our experimental population. Then,
    we assign groups based on that random number: if K is the number of groups we
    want (including a control group), then all individuals with a random number less
    than 1/K are in the first group, all individuals with a random number between
    1/K and 2/K are in the second group, and so on. The following code illustrates
    the approach with three groups and a sample size of (for illustration purposes)
    5,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'One nice aspect of this approach is that it can easily be generalized to an
    arbitrarily large number of groups, by creating a simple loop; the control group
    is then tagged with `0`, the first treatment group `1`, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: However, an issue with the previous approach is that experimental groups are
    unlikely to be perfectly balanced across customer characteristics. In order to
    create balanced experimental groups, we’ll want to use a technique called stratification.
  prefs: []
  type: TYPE_NORMAL
- en: Stratified randomization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Why is a purely random allocation not our best choice? Let’s imagine that we
    are running an experiment on 20 customers, 10 of which are male and 10 of which
    are female. If we randomly assign each customer to either the control or the treatment
    group with probability 50%, we expect that on average, there will be 5 males and
    5 females in each of the two groups. “On average” here means that if we repeated
    this assignment a large number of times, the average number of males in the control
    group across assignments would be 5\. But in any given experiment, there’s only
    a 34.4% chance that we get exactly 5 males and 5 females in each group, and there’s
    a 8.9% chance that we get 7 or more men in one group, based on the hypergeometric
    distribution.^([1](ch09.xhtml#ch01fn18)) Obviously, this problem gets less pronounced
    with larger sample sizes. With 100 males and 100 females, the probability of getting
    70 men or more in one group becomes negligible. But we don’t care only about gender:
    ideally, we would also want a good balance of age, state of residence, usage pattern,
    etc. This would ensure that our results are relevant for our entire customer base
    as much as possible, and not just to a specific subgroup of it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, when we have the luxury of assigning experimental groups to all
    individuals at once, for instance, we can do significantly better than just crossing
    our fingers and hoping for the best. We can stratify our data: we create “layers”
    of similar customers, called strata,^([2](ch09.xhtml#ch01fn19)) and we split them
    between our experimental groups. In the case of our 10 male and 10 female customers,
    we would create a layer of men, 5 of which would go to the control group and 5
    of which to the treatment group, and similarly for women. This implies that each
    individual still has a 50% chance of ending in either group but our control and
    treatment groups are now perfectly balanced for gender.'
  prefs: []
  type: TYPE_NORMAL
- en: Stratification can be applied to any number of variables. With gender and state
    of residence, we would create a stratum of all women from Kansas and split it
    equally between our control and treatment group, and so on. With a large number
    of variables, or with continuous variables, it becomes impossible to find exact
    matches; in our data, we may not have two women in Kansas with the exact same
    age and whose properties have the exact same square footage. The solution is to
    create pairs of individuals that are “as similar as possible,” e.g., a 58-year-old
    woman with a 900-square-foot property and a 56-year-old woman with a 930-square-foot
    property, and then assign at random one of them to the control group and the other
    to the treatment group. This way, they still have the same probability individually
    of ending in any experimental group. When we have only two individuals per strata,
    this is also called “matching,” because we’re creating matching pairs of customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As often, the intuition is clear enough, but the devil is in the detail of
    the implementation. There are two steps here:'
  prefs: []
  type: TYPE_NORMAL
- en: Give a mathematical meaning to the phrase “as similar as possible.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Efficiently go through our data to allocate each customer to a pair.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mathematical concept that we’ll use to express “as similar as possible”
    is distance. Distance can be applied easily to a single numeric variable. If one
    owner is 56 years old and another is 58 years old, the distance between them is
    58 − 56 = 2 years. Similarly, we could say that the distance between a 900-square-foot
    property and a 930-square-foot property is 30 square feet.
  prefs: []
  type: TYPE_NORMAL
- en: The first complication comes in aggregating multiple numeric variables. We could
    simply add (or equivalently, take the mean of) the two numbers and say that our
    two owners are “distant” by 2 + 30 = 32 distance units. The problem with that
    approach is that square-footage numbers are much bigger than age numbers, as we
    can see in our example. A difference of 30 years between two owners is likely
    to be much more important behaviorally than a difference of 30 square feet between
    their properties. This can be resolved by rescaling all our numeric variables
    so that their minimum is reset to 0 and their maximum to 1\. This means that the
    “distance” between the youngest and the oldest owners is 1, and the distance between
    the smallest and the biggest property is also 1\. This is not a perfect solution,
    especially when you have outliers, but it’s fast and good enough for most purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second complication comes from categorical variables. What is the “distance”
    between a townhouse and an apartment? Or between having a pool or not? A common
    solution is to say that the distance between two properties is 0 if they are in
    the same category and 1 otherwise. For example, a townhouse and a house would
    have a distance of 1 for the property type variable. Mathematically, this is done
    by *one-hot encoding* categorical variables: that is, we create as many binary
    0/1 variables as we have categories. For example, we would transform property
    type = (“house,” “townhouse,” “apartment”) into three variables, *type.house*,
    *type.townhouse,* and *type.apartment*. A property that is an apartment would
    have a value of 1 for the variable *type.apartment* and 0 for the other two variables.
    This also has the added advantage of making categorical “distances” comparable
    with numerical distance. In effect, we’re saying that the difference between a
    townhouse and an apartment is as important as the difference between the smallest
    and the largest properties. This is again debatable from a behavioral perspective,
    but that’s a good starting point, and often a good stopping point as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have written R and Python functions that prepare our data by rescaling numeric
    variables and one-hot encoding categorical variables. This is just boilerplate
    code, so just skip that bit of code if you don’t care about the details of the
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have prepared our data, the second step is to create pairs. This computationally
    intensive problem quickly becomes intractable with larger data (at least if you
    want the optimal solution). Fortunately, algorithms have been created that will
    handle it for you. In R, we can use the function `block()` from the `blockTools`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters for that function are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id.vars`'
  prefs: []
  type: TYPE_NORMAL
- en: This is the variable(s) used to identify the individuals in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '`n.tr`'
  prefs: []
  type: TYPE_NORMAL
- en: This is the number of experimental groups, including the control group.
  prefs: []
  type: TYPE_NORMAL
- en: '`algorithm`'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates the name of the algorithm to use. Predictably, `"``optimal"` will
    yield the best pairing overall but can quickly become unfeasible with large data
    and limited computational power; `"``naiveGreedy"` is the least computationally
    demanding and a good start. `"optGreedy"` is generally a good compromise for when
    you’re ready to do the final assignment.
  prefs: []
  type: TYPE_NORMAL
- en: '`distance`'
  prefs: []
  type: TYPE_NORMAL
- en: Indicates how to calculate distances between individuals. `"``euclidean"` is
    the distance function from high school, and appropriate for the data we have prepared.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `block()` function returns the stratified allocation in an unwieldy format,
    so I created a convenience wrapper that transforms its output into a usable format.
    Feel free to look at its code on [GitHub](https://oreil.ly/BehavioralDataAnalysisCh9):
    I’ll just show you its output here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that 5,000 is not divisible by 3, so we need to drop at random two rows,
    the closest smaller number divisible by 3 being 4,998\. Comparing the two types
    of random assignments would show that the experimental groups obtained by stratified
    randomization are much more similar than the groups obtained by standard randomization.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond helping reduce noise in our experiments, stratification is also helpful
    if you intend to do subgroup or moderation analysis (which we’ll discuss later
    in the book). As the phrase goes, “block [with stratification] what you can and
    randomize what you cannot” (Gerber and Green, 2012).
  prefs: []
  type: TYPE_NORMAL
- en: 'Stratified randomization is an effective and robust approach to experimental
    allocation. Its robustness comes largely from its transparency: you can always
    check afterward that your experimental groups are well-balanced in terms of the
    means of numeric variables and the proportions of categories for categorical variables.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, because each individual in a pair has the same probability of ending
    up in any experimental group, even a poorly or wrongly defined distance function
    will leave you no worse than a purely random allocation. The main risk is to include
    too many irrelevant variables, which then drown out the relevant variables. That
    is however easily remedied by including only variables that are part of your causal
    diagram or the main demographic variables. Don’t add a load of other variables
    simply because you can.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables with a large number of categories can also sometimes add
    noise to your stratification because of their coarseness. Taking the example of
    employment, a data scientist is different from a statistician, but intuitively,
    that difference is less than their joint difference with, say, a firefighter.
    Very granular variables ignore such nuances, and are better replaced with broader
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lest these caveats discourage you: stratification is effective and robust;
    don’t be afraid to use it. Even stratifying based only on a few key demographic
    variables leads to significant improvements and should be your default approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve determined how we’re going to do the random allocation, let’s
    do our power analysis to determine the sample size.
  prefs: []
  type: TYPE_NORMAL
- en: Power Analysis with Bootstrap Simulations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After concertation with the business partners, we determine that we want to
    have 90% power for an increase in net booking profit (BP) per day of $2, as it
    is the minimal observable effect they would be interested in. This translates
    into an increase in “raw” BP/day of $12 for the free-cleaning intervention (treatment
    1) and $2 for the minimum-duration intervention (treatment 2). This won’t impact
    our analyses in any material way, as we can just shift the outcome variable by
    deducing the $10 cost from the BP/day for properties in the free-cleaning group,
    but we’ll need to keep that in mind and remember to do it. For the sake of simplicity,
    I’ll only discuss the minimum-duration intervention in our power analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulation methods really shine in situations such as this one, where there
    are often no available dedicated formulas to calculate power or sample sizes,
    or available formulas get horribly complex. The alternative would be to use standard
    formulas that disregard the specifics of the situation (here, the stratification
    of our experimental data) and cross our fingers that things go well (Murphy’s
    law: they probably won’t).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our process will be the same as in [Chapter 8](ch08.xhtml#experimental_design_the_basics):'
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll define our metric function and our decision function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we’ll create a function that simulates a single experiment for a given
    sample size and effect size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we’ll create a function that simulates a large number of experiments
    and count how many of them result in a true positive (i.e., our decision function
    adequately captures the effect); the percentage of true positives is our power
    for that sample size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Single simulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our metric function for the minimum-duration treatment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The metric function for treatment 1 would be defined similarly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll reuse the `boot_CI_fun()` and `decision_fun()` functions from [Chapter 8](ch08.xhtml#experimental_design_the_basics).
    In other words, our decision rule will be to implement the treatment if its 90%-CI
    is strictly above zero. I’m repeating their code in the following, just for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then write the function to run a single simulation, which embeds the
    logic we’ve seen so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker91b)'
  prefs: []
  type: TYPE_NORMAL
- en: Select a month at random, to mimic the way we will run our actual experiment
    (we don’t want to use data from 10 years apart in our power analysis).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker92b)'
  prefs: []
  type: TYPE_NORMAL
- en: Generate stratified random assignment for a sample of desired size.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker93b)'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the assignment to the data and apply the target effect size to treatment
    group 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#comarker94b)'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the decision function and return its output.
  prefs: []
  type: TYPE_NORMAL
- en: Simulations at scale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From there, we can apply the same overall function for power simulations as
    in [Chapter 8](ch08.xhtml#experimental_design_the_basics) (repeated in the following
    for reference):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Our maximum sample size would be 5,000, because that’s the total number of
    property owners AirCnC has. That’s a manageable number for simulation purposes,
    so let’s first run 100 simulations with that sample size. There’s no point in
    working our way up to that if it turns out that we would need to use our entire
    population for the experiment. We find a power of 1, which is comforting: the
    required sample size is less than our total population. From there, we try different
    sample sizes, progressively increasing the number of simulations as we zero in
    on the sample size with a power of 0.90 ([Figure 9-2](#iterative_power_simulations_with_increa)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iterative power simulations with increasing number of simulations, with labels
    indicating the order of runs](Images/BEDA_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Iterative power simulations with increasing number of simulations,
    with labels indicating the order of runs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It looks like a sample size of 1,500 will do. As in [Chapter 8](ch08.xhtml#experimental_design_the_basics),
    let’s now determine the power curve for various effect sizes at that sample size
    ([Figure 9-3](#power_to_detect_various_effect_sizes_an)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Power to detect various effect sizes and significance (sample size = 1,500)](Images/BEDA_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Power to detect various effect sizes and significance (sample size
    = 1,500)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in [Figure 9-3](#power_to_detect_various_effect_sizes_an), our
    power curve drops very steeply, going from an effect size of $2 to $1, and our
    power is almost nil for effect sizes less than 1; that is, if we assume that the
    treatment increases *BPday* by $1, we’re very likely to end up with a CI that
    includes zero and then conclude that there’s no effect. At the leftward end of
    the curve, our simulated significance is equal to zero, instead of the expected
    5%. Let’s discuss what causes that and whether we should worry about it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the power/significance trade-off
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As I mentioned in the last chapter, if our data is “well-behaved” (i.e., normally
    distributed, allocated between experimental groups purely at random, etc.) and
    there is no true effect, we would expect a 90%-CI to include zero 90% of the time,
    be strictly above it 5% of the time, and be strictly under it 5% of the time.
    Here, because of the stratified randomization, our false positive rate appears
    lower than 5%: I observed none whatsoever in 500 simulations. By reducing the
    noise in our data, stratified randomization also reduces the risk of false positives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s nice but in this case it may be too much of a good thing, because it
    also brings down the power curve for small positive effects up to 1, as we can
    see in [Figure 9-3](#power_to_detect_various_effect_sizes_an). Let’s put it in
    a different way: if we had a 5% chance of assuming there is an effect when there
    is none, then we would have at least a 5% chance of assuming that there is an
    effect when there is a small one. In that sense, significance gives us some “free”
    power for low effect sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare the previous power curve with the power curves for lower confidence
    levels. By definition, this will give us narrower confidence intervals, meaning
    a higher significance and a higher power, especially for small effect sizes ([Figure 9-4](#comparison_of_power_curves_for_confiden)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison of power curves for confidence levels 0.90 (solid line), 0.80
    (long-dash line), 0.60 (dashed line), and 0.40 (dotted line)](Images/BEDA_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Comparison of power curves for confidence levels 0.90 (solid line),
    0.80 (long-dash line), 0.60 (dashed line), and 0.40 (dotted line)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, with a 40%-CI, we only get a small increase in significance
    but a large increase in power, with a power of approximately 50% to detect an
    effect size of 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean that we should use a 40%-CI instead of a 90%-CI? It depends.
    Let’s get back to the business problem. Our business partners have asked for a
    power of 90% for an effect size of 2 because they’re not interested in going through
    the hassle of implementing either of the treatments if the benefits are lower
    than that. Thus, capturing a true effect size of 0.5 with a CI that doesn’t include
    zero, or having a CI that includes zero, is essentially the same thing from a
    business perspective. Either way, no treatment will be implemented. Therefore,
    the power curve for the 90%-CI better reflects our business goals.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, with an experiment like the “1-click button” in [Chapter 8](ch08.xhtml#experimental_design_the_basics),
    costs and risks of implementation are limited. The 90% power threshold is just
    a baseline, and virtually any strictly positive effect would warrant implementation.
    In such a situation, increasing power for small effect sizes is probably worth
    a slight increase in significance.
  prefs: []
  type: TYPE_NORMAL
- en: More broadly, when your data or your experimental design diverge from the standard
    framework, power analysis stops being a simple matter of plugging conventional
    numbers into a formula and requires understanding what’s going on and making judgment
    calls about the right decisions. Fortunately, power curves offer a great tool
    to visualize the possible outcomes of an experiment under different scenarios
    and different decision rules.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and Interpreting Experimental Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we’ve run the experiment, we can analyze its results. Our target metric,
    average booking profit per day, is continuous and not binary; therefore, the two
    appropriate methods are the T-test of means and linear regression. I will refer
    you to Gerber and Green (2012) if you want to learn more about the T-test, and
    I’ll cover linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Before getting into the quantitative analysis, remember that we could not force
    owners to have a two-night minimum duration or to agree to a reduction in the
    duration of the cleaning window in exchange for free cleaning services. We could
    only offer them the opportunity to opt in, which some took and others did not.
    In technical terms, this approach is called an *encouragement design*, because
    we’re encouraging subjects to take up our offer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Encouragement designs are very common, but they introduce some additional considerations,
    because we now have two categories of people in a treatment group: those who opted
    in and those who didn’t. For practical purposes, this means we have two different
    questions we can try to answer:'
  prefs: []
  type: TYPE_NORMAL
- en: What would happen if we offered the possibility of opting into the treatment
    to our entire owner population?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What would happen if we enforced the treatment on our entire owner population,
    without giving them the option of opting out?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer to the first question is called the *i**ntention-to-treat* (ITT)
    estimate because we intend for people to be treated but we don’t make it mandatory.
    The second question is more complex, and we can’t answer it fully based only on
    an encouragement design (or at least it requires additional assumptions), but
    we can get a closer approximation than the ITT estimate, with the *complier average
    causal effect* (CACE) estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate both of these estimates in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Intention-to-Treat Estimate for Encouragement Intervention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first calculate the ITT estimate, which will be very easy: it’s simply
    the coefficient for the effect of the experimental assignment, as we’ve calculated
    in the previous chapter. Should we factor in the fact that the majority of the
    owners in the treatment groups did not opt in? No. The fact that the ITT coefficient
    is diluted by people who opted out is a feature, not a bug: the same dilution
    would happen at a larger scale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s subtract $10 for the owners in the cleaning group who opted in, to account
    for the extra cost, then run a linear regression. We could apply our metric functions
    separately, but I prefer running the whole regression at once to be able to see
    the other coefficients as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We do a regression of our variable of interest, booked profit per day, on the
    square footage of the property, the city tier, the average of the customer reviews,
    and the experimental groups. The coefficient for *Grouptreat1* refers to the minimum-duration
    treatment while *Grouptreat2* refers to the free-cleaning treatment.
  prefs: []
  type: TYPE_NORMAL
- en: The first treatment increases *BPday* by approximately $0.97 on average, but
    the p-value is moderately high, at approximately 0.27\. This suggests that the
    coefficient may not be significantly different from zero, and indeed the corresponding
    Bootstrap 90%-CI is approximately [0.002; 2.66].
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you were to run a T-test comparing the first treatment group with the control
    group, you’d find that the absolute value of the test statistic is 0.96, close
    to the coefficient in the regression we just ran. Similarly, the raw difference
    in *BPday* averages between the control group and the first treatment group is
    approximately 0.85\. Should that surprise us? No. Thanks to stratification, our
    experimental groups are very well balanced and therefore other independent variables
    have the same average effect across groups. This means that even metrics that
    don’t account for covariates are unbiased (their p-values would be off, however,
    because they don’t take into account the stratification).
  prefs: []
  type: TYPE_NORMAL
- en: The second treatment decreases *BPday* by approximately $0.17, after cost, not
    a great value proposition. The corresponding CI is [-2.23; 1.61].
  prefs: []
  type: TYPE_NORMAL
- en: Remember that our business partners are interested only in implementing an intervention
    if it generates $2 of additional BP/day above costs. On the face of it, this would
    preclude implementing the minimum-duration intervention—not just because the statistical
    significance is borderline, but primarily because of the lack of economic significance.
    Even if the lower bound of the confidence interval was squarely above zero, this
    would still not change our business partners’ decision.
  prefs: []
  type: TYPE_NORMAL
- en: If this had to be the end of the story, our business partners would not implement
    either of the encouragement interventions. However, it might be worth considering
    what would happen if we enforced the minimum-duration treatment across the board
    without offering people the option to opt out.
  prefs: []
  type: TYPE_NORMAL
- en: Complier Average Causal Estimate for Mandatory Intervention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we have an encouragement design, can we estimate the effect of making the
    treatment mandatory? A tempting but incorrect way to answer it would be to compare
    the value of the business metric for the opt-in category (a.k.a. the “treated”)
    on the one hand, with the value for the opt-out category and the control group
    on the other hand, lumping the latter two together as “untreated.” One could assume
    that this comparison would reflect the expected outcome of enforcing the treatment
    across the board, i.e., imposing a two-night minimum in hot markets or unilaterally
    shortening the cleaning window while offering free cleaning, regardless of owners’
    preferences. It does not, because opting in to the treatment is not randomized
    and is likely to be confounded. Within the treatment groups, it is plausible that
    the owners who opt in are in some regards different from the owners who don’t,
    e.g., they have financial needs or other characteristics that make them give more
    attention and effort to their property ([Figure 9-5](#the_experimental_allocation_is_randomi)).
  prefs: []
  type: TYPE_NORMAL
- en: '![The experimental allocation is randomized but accepting the free-cleaning
    treatment is not](Images/BEDA_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. The experimental allocation is randomized but accepting the free-cleaning
    treatment is not
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If that CD is correct, then accepting the free-cleaning offer is correlated
    with behaviors that increase booked profit per day and bias upward our coefficient.
    In other words, we would wrongly attribute to the offer some of the effect of
    those behaviors if we compared people who opt in with people who don’t. The random
    allocation ensures that comparisons between experimental groups are unbiased,
    but it doesn’t guarantee anything for subgroups further down the line.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the case of email A/B tests, this limitation on the effect of randomization
    means that rates (e.g., opening rate, click-through rate, etc.) should all have
    as numerator the number of people in the experimental group and not the number
    of people from the previous stage. If 50% of people open your email and 50% of
    those who open the email click through, the click-through rate should be expressed
    as 25%, not 50%.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an encouragement design, we intend for the people in the treatment group
    to opt in and get treated, but we also intend for people in the control group
    to *not* be treated. However, in certain situations we can’t prevent them from
    accessing the treatment. In our present example, the free-cleaning treatment has
    features that are entirely under our control: property owners may use professional
    cleaning services but they have to pay for it, and the window between bookings
    is baked in the software. Therefore no one outside of that treatment group can
    access that precise treatment. Things are less clear-cut however with the two-night
    minimum treatment: property owners outside of that treatment group may unofficially
    enforce a two-night minimum by rejecting single-night requested bookings ([Figure 9-6](#the_experimental_allocation_is_randomiz)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The experimental allocation is randomized but accepting the minimum booking
    treatment is not, and it can happen outside of the treatment group](Images/BEDA_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. The experimental allocation is randomized but accepting the minimum
    booking treatment is not, and it can happen outside of the treatment group
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'With the minimum booking treatment, there are four possible cases for owners
    that we can observe:'
  prefs: []
  type: TYPE_NORMAL
- en: Being in the control group and not having a two-night minimum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Being in the control group and having a two-night minimum nonetheless
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Being in the treatment group and having a two-night minimum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Being in the treatment group and not having a two-night minimum nonetheless
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This categorization doesn’t yet answer our question but it’s giving us some
    important building blocks to distinguish between the effect of the treatment per
    se and the effect of unobserved factors conducive to setting up a two-night minimum.
    Let’s imagine for a second that we could observe these factors, and rank all the
    owners in decreasing order of these factors. Because of randomization, we can
    assume that the control and the treatment groups are reasonably identical in terms
    of the distribution of unobserved factors ([Figure 9-7](#distribution_of_unobserved_factors_and)).
  prefs: []
  type: TYPE_NORMAL
- en: All owners in the control group whose value for unobserved factors is high enough
    will implement a two-night minimum (group B) and all other owners in the control
    group won’t (group A). For the treatment group, we can reasonably assume that
    the owners with a high factor value are still implementing a two-night minimum
    and that our encouragement intervention simply lowers the threshold, by getting
    some owners who wouldn’t otherwise to implement it (together they form the group
    C). Finally, owners who are too low still don’t implement it, despite our best
    efforts (group D).
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution of unobserved factors and observed behaviors in the two groups](Images/BEDA_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Distribution of unobserved factors and observed behaviors in the
    two groups
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In econometrics lingo, subjects who would always be treated regardless of their
    experimental assignment (group B in the control group and the corresponding part
    of group C in the treatment group) are called *always-takers*. Subjects who would
    never be treated regardless of their experimental assignment (group D in the treatment
    group and the corresponding part of group A in the control group) are predictably
    called *never-takers*. Subjects who are treated if and only if they are in the
    treatment group (the overlap between group A and group C)” are called *compliers.*
  prefs: []
  type: TYPE_NORMAL
- en: Theoretically, you could have a fourth category, namely subjects who are treated
    if and only if they are in the control group. They are called *defiers* because
    they are always doing the exact opposite of what we want them to do. The technical
    term for this in psychology is *reactance*. While it can happen in real life (cough,
    teenagers, cough), it’s rarely a concern in business settings, unless you’re trying
    to force people to do things they don’t want to do and I won’t help you with that.
  prefs: []
  type: TYPE_NORMAL
- en: 'By definition, we can’t observe the unobserved factors in our experiment, which
    means that we can only identify with certainty two groups: the always-takers assigned
    to control (B) and the never-takers assigned to treatment (D). We can’t know if
    the owners in the treatment group implementing the two-night minimum (C) are always-takers
    or compliers, and we can’t know if the owners in the control group not implementing
    it (A) are compliers or never-takers. However, and this is where the trick is,
    we can net out the always-takers and never-takers across experimental groups to
    measure the effect of the treatment on the compliers, which is called the *c**omplier
    average causal effect* (CACE). The formula for the CACE is very simple:^([4](ch09.xhtml#ch01fn21))'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mrow><mi>C</mi><mi>A</mi><mi>C</mi><mi>E</mi><mo>=</mo><mrow><mfrac><mstyle
    scriptlevel="0" displaystyle="true"><mrow><mi>I</mi><mi>T</mi><mi>T</mi></mrow></mstyle><mstyle
    scriptlevel="0"><mrow><mi>P</mi><mrow><mo>(</mo><mi>t</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi><mrow><mo>|</mo><mi>T</mi><mi>G</mi></mrow><mo>)</mo><mo>−</mo><mi>P</mi><mrow><mo>(</mo><mi>t</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi><mrow><mo>|</mo><mi>C</mi><mi>G</mi></mrow><mo>)</mo></mrow></mrow></mrow></mstyle></mfrac></mrow></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, to determine the effect of the treatment on the compliers,
    we simply need to weight our previous ITT estimate by a measure of the noncompliance
    in our experiment: if we have full compliance in both groups, meaning that no
    one gets access to the treatment in the control group (P(treated|CG) = 0) and
    everyone is treated in the treatment group (P(treated|TG) = 1), this simplifies
    to the ITT estimate. Very often with encouragement designs, we can prevent people
    in the control group from accessing the treatment, but only a fraction of the
    people in the treatment group are actually treated. In that case, the CACE is
    a multiple of the ITT: if only 10% of the people in the treatment group are treated,
    then our effect is very diluted and our CACE is equal to 10 times the ITT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CACE is very useful in two regards: first, it gives us an estimate of the
    effect of implementing the treatment across the board, without the possibility
    of opting out. Second, looking at the relationship between the ITT and the CACE
    allows us to distinguish between two possible situations:'
  prefs: []
  type: TYPE_NORMAL
- en: The ITT is low, but *P*(*treated*|*TG*) - *P*(*treated*|*CG*) is high, meaning
    that the intervention has a low impact on compliers but a high level of compliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversely, the ITT is high, but *P*(*treated*|*TG*) - *P*(*treated*|*CG*) is
    low, meaning that the intervention has a high impact on compliers but a low level
    of compliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the first case, we would focus our effort on increasing the effectiveness
    of the intervention whereas in the second case we would focus on increasing the
    take-up rate, possibly by making the intervention mandatory. These insights can
    also help us explore alternative designs: maybe 8 hours is too short, but 12 hours
    would be acceptable? Maybe we don’t need to offer *free* cleaning, simply suggesting
    a reputable service provider to owners would be enough?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our present experiment, the uptake rates are quite low in the treatment
    groups, at about 20% on average:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that our CACE estimate for the effect of the minimum-duration treatment
    will be significantly higher than the ITT estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '*CACE*[1] = *ITT*[1]/*ComplianceRate*[1] = 0.97/0.24 ≈ 4.06'
  prefs: []
  type: TYPE_NORMAL
- en: Now, that’s a much more interesting value. The low uptake rate and high CACE
    suggests that our intervention is fundamentally sound and does generate value
    when implemented. We can either try to increase the uptake rate by changing the
    design, or make the intervention mandatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CACE has a very neat but narrow interpretation: because we’re (implicitly)
    comparing the same people, the compliers, across the control and the treatment
    group, our estimate of the effect of the treatment is unbiased. We’re not inadvertently
    capturing the influence of other factors. However, we’re measuring it only for
    that narrow slice of our population, so generalizability is not a given. Compliers
    might have characteristics that *interact* with our treatment. That is, they might
    have traits that influence not (or not only) whether they take up the treatment,
    but how much the treatment affects them. This is where we need to go from a causal
    to a behavioral lens: is our treatment a tide that lifts all boats, or do the
    people involved matter? For example, we’ll see in the next chapter the example
    of a talk path in call centers. In that case, complying doesn’t just mean applying
    the treatment, but exerting effort to do so convincingly and not just going through
    the motions.'
  prefs: []
  type: TYPE_NORMAL
- en: Here our intervention is implemented through AirCnC’s website. A two-night minimum
    is a two-night minimum, whoever you’re renting from. This means we can be confident
    that our treatment would be implemented as planned if rolled out across the board,
    and we can give our business partners the green light to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we had to randomize our experimental assignment “on
    the fly” as customers connected to the website. In this chapter, we were able
    to do the random assignment all at once, and therefore *stratify* (a.k.a. *block*)
    our sample by creating pairs of similar subjects, one of which is assigned to
    the control group and the other to the treatment group. While this added a layer
    of complexity, it also significantly increased the effectiveness (in statistical
    terms, the power) of our experiment. Once you get familiar with stratification,
    you’ll appreciate the ability to extract insights even from small samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also introduced a second complication: our experimental intervention was
    an *encouragement* treatment. We offered possibilities to owners, but we couldn’t
    force them to take them up, and the uptake was not random. In situations like
    this, we can easily measure the effect of the encouragement intervention per se,
    but measuring the effect of accepting the offer (a.k.a. opting in to the treatment)
    is trickier. Fortunately, we have in the CACE an unbiased estimate of that effect
    for the compliers in our experimental population. When we can assume the absence
    of interaction between personal characteristics and the treatment, the CACE can
    be generalized to our entire experimental population. Even when we can’t generalize
    that far, it provides a less biased estimate than simply comparing the control
    and treatment group (i.e., the intention-to-treat estimate).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we had multiple treatments. This did not change anything fundamentally,
    but it also added some complexity. I would recommend starting your experimentation
    journey with only one treatment, but I believe in the longer run you’ll come to
    appreciate the organizational “fixed costs” of running an experiment: getting
    approval from all stakeholders (business partners, legal department, etc.) and
    getting the technology and data pipeline in place takes barely more time with
    two treatments than with one. Therefore, running experiments with multiple treatments
    at once is a key step in increasing the number of treatments tested in a year.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.xhtml#ch01fn18-marker)) Thanks to Andreas Kaltenbrunner for pointing
    out that this is a hypergeometric and not a binomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.xhtml#ch01fn19-marker)) This is the plural of the Latin word for
    layer, *stratum*, hence the word stratification.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch09.xhtml#ch01fn20-marker)) Technically, there are functions for stratified
    sampling, such as `sklearn.utils.resample()`, but these don’t allow distance-based
    matching like we’re doing here.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch09.xhtml#ch01fn21-marker)) If you’re curious where it comes from, the
    derivation is available in the [book’s GitHub repo](https://github.com/FlorentBuissonOReilly/BehavioralDataAnalysis).
  prefs: []
  type: TYPE_NORMAL
