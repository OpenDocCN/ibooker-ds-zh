<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Workflow Context"><div class="chapter" id="ch06">
<h1><span class="label">Chapter 5. </span>Workflow Context</h1>


<p class="byline">Boyan Angelov</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45127450495464">
<h5>A note for Early Release readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
</div></aside>

<p>A common source of frustration for data scientists is discussing their work with colleagues from adjacent fields. Let’s take the example of someone who has been working primarily in developing machine learning (ML) models, having a chat about their work with a colleague from the Business Intelligence (BI) team, more focused on reporting. More often than not, such a discussion can make both parties uncomfortable due to a perceived lack of knowledge about each other’s work domain (and associated workflows) - despite sharing the same job title. The ML person might wonder, what D3.js is, the grammar of graphics, and all that? On the other hand, the BI data scientist might feel insecure about not knowing how to build a deployable API. The feelings that might arise from such a situation have been termed “impostor syndrome,” where doubts about your competency arise. Such a situation is a by-product of the sheer volume of possible applications of data science. A single person is rarely familiar to the same extent with more than several sub-fields. Flexibility is still often required in this fast-evolving field.</p>

<p>This complexity sets the foundation for the workflow focus in this chapter. We’ll cover the primary data science workflows and how the languages’ different ecosystems support them. Much like <a data-type="xref" href="ch04.xhtml#ch05">Chapter 4</a>, at the end of this chapter, you’ll have everything needed for making educated decisions regarding your workflows.</p>






<section data-type="sect1" data-pdf-bookmark="Defining workflows"><div class="sect1" id="idm45127450490616">
<h1>Defining workflows</h1>

<p>Let’s take a step back, and define a workflow:</p>
<blockquote>
<p><strong>A workflow is a complete collection of tools and frameworks to perform all tasks required from a specific job function.</strong></p></blockquote>

<p>For this example, let’s say you’re an ML engineer. Your daily tasks might include tools to obtain data, process it, train a model on it, and deployment frameworks. Those, collectively, represent the ML engineer workflow. An overview of the data workflows for this and other titles and their supporting tools, is presented in <a data-type="xref" href="#workflows-table">Table 5-1</a>.</p>
<table id="workflows-table">
<caption><span class="label">Table 5-1. </span>Common data science workflows and their enabling tools.</caption>
<thead>
<tr>
<th>Method</th>
<th>Python package</th>
<th>R package</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Data Munging<sup><a data-type="noteref" id="idm45127450466184-marker" href="ch05.xhtml#idm45127450466184">a</a></sup></p></td>
<td><p><code>pandas</code></p></td>
<td><p><code>dplyr</code></p></td>
</tr>
<tr>
<td><p>EDA</p></td>
<td><p><code>matplotlib</code>, <code>seaborn</code>, <code>pandas</code></p></td>
<td><p><code>ggplot2</code>, <code>base-r</code>, <code>leaflet</code></p></td>
</tr>
<tr>
<td><p>Machine Learning</p></td>
<td><p><code>scikit-learn</code></p></td>
<td><p><code>mlr</code>, <code>tidymodels</code>, <code>caret</code></p></td>
</tr>
<tr>
<td><p>Deep Learning</p></td>
<td><p><code>keras</code>, <code>tensorflow</code>, <code>pytorch</code></p></td>
<td><p><code>keras</code>, <code>tensorflow</code>, <code>torch</code></p></td>
</tr>
<tr>
<td><p>Data Engineering<sup><a data-type="noteref" id="idm45127450450408-marker" href="ch05.xhtml#idm45127450450408">b</a></sup></p></td>
<td><p><code>flask</code>, <code>bentoML</code>, <code>fastapi</code></p></td>
<td><p><code>plumber</code></p></td>
</tr>
<tr>
<td><p>Reporting</p></td>
<td><p><code>jupyter</code>, <code>streamlit</code></p></td>
<td><p><code>rmarkdown</code>, <code>shiny</code></p></td>
</tr>
</tbody>
<tbody><tr class="footnotes"><td colspan="3"><p data-type="footnote" id="idm45127450466184"><sup><a href="ch05.xhtml#idm45127450466184-marker">a</a></sup> Data munging (or wrangling) is such a fundamental topic in data science that it was already covered in <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a>.</p><p data-type="footnote" id="idm45127450450408"><sup><a href="ch05.xhtml#idm45127450450408-marker">b</a></sup> There is much more to data engineering than model deployment, but we decided to focus on this subset to illustrate Python’s ability.</p></td></tr></tbody></table>

<p>We omitted some areas in the hope that the listed ones are the most common and critical. Those selected workflows are related to each other, as presented on <a data-type="xref" href="#meta_workflow">Figure 5-1</a>. This diagram borrows heavily from the <a href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining">CRISP-DM</a> framework, which shows all significant steps in a typical data science project. Each of the diagram’s steps has a separate workflow associated with it, generally assigned to an individual or a team.</p>

<figure><div id="meta_workflow" class="figure">
<img src="Images/prds_0501.png" alt="" width="2356" height="1697"/>
<h6><span class="label">Figure 5-1. </span>Meta-workflow in data science and engineering.</h6>
</div></figure>

<p>Now that we have defined a workflow, what are the defining properties of a “good” one? We can compile a checklist with three main factors to consider:</p>
<ol>
<li>
<p>It’s well established. It’s widely adopted by the community (also across different application domains, such as Computer Vision or Natural Language Processing).</p>
</li>
<li>
<p>It’s supported by a well-maintained, open-source ecosystem and community. A workflow that relies heavily on closed-source and commercial applications (such as Matlab) is not considered acceptable.</p>
</li>
<li>
<p>It’s suitable for overlapping job functions. The best workflows are similar to lego bricks - their modular design and extensibility can support diverse tech stacks.</p>
</li>

</ol>

<p>With the big picture and definitions out of the way, let’s dive deeper into the different workflows and how they are supported by R and Python!</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Exploratory data analysis"><div class="sect1" id="idm45127450490024">
<h1>Exploratory data analysis</h1>

<p>Looking at numbers is <em>hard</em>. Looking at rows of data containing millions upon millions of them is even more challenging. Any person dealing with data faces this challenge daily. This need has led to considerable developments in data visualization (DV) tools. A recent trend in the area is the explosion of self-serving analytics tools, such as <a href="https://www.tableau.com/">Tableau</a>, <a href="https://www.alteryx.com/">Alteryx</a>, and <a href="https://powerbi.microsoft.com/en-us/">Microsoft PowerBI</a>. These are very useful, but the open-source world has many alternatives available, often rivaling or even exceeding their commercial counterparts’ capabilities (except, in some cases, ease of use). Such tools collectively represent the EDA workflow.</p>
<div data-type="tip"><h1>When to use a GUI for EDA</h1>
<p>Many data scientists frown at the notion of using a graphical user interface (GUI) for their daily work. They would much rather prefer the flexibility and utility of command-line tools instead. Nevertheless, one area where using a GUI makes more sense (for productivity reasons) is EDA. It can be quite time-consuming to generate multiple plots, especially at the beginning of a data science project. Usually, one would need to create tens, if not hundreds of them. Imagine writing the code for each one (even if you improve your code’s organization by refactoring into functions). For some larger datasets, it’s sometimes much easier to use some GUI, such as AWS Quicksight or Google Data Studio. By using a GUI the data scientist can quickly generate a lot of plots first and only then write the code for the ones that make the cut after screening. There are a few good open-source GUI tools, for example <a href="https://orange.biolab.si/">Orange</a>.</p>
</div>

<p>EDA is a fundamental step at the beginning of the analysis of any data source. It is typically performed directly after data loading, at the stage where there’s a significant need for business understanding. This explains why it’s an essential step. You are probably familiar with the <em>garbage in</em>, <em>garbage out</em> paradigm - the quality of any data project depends on the quality of the input data and the domain knowledge behind it. EDA enables the success of the downstream workflows (such as ML), ensuring both the data and the assumptions behind it are correct and of sufficient quality.</p>

<p>In EDA, R has far better tools available than Python. As we discussed in <a data-type="xref" href="ch01.xhtml#ch01">Chapter 1</a> and <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a>, R is a language made <em>by</em> statisticians and <em>for</em> statisticians (remember FUBU from <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a>?), and data visualization (plotting) has been of great importance in statistics for decades. Python has made some forward strides in recent years but is still seen as lagging (you need just to look at example <code>matplotlib</code> plot to realize this fact<sup><a data-type="noteref" id="idm45127450420728-marker" href="ch05.xhtml#idm45127450420728">1</a></sup>). Enough praise for R; let’s have a look at why it’s great for EDA!</p>








<section data-type="sect2" data-pdf-bookmark="Static visualizations"><div class="sect2" id="idm45127450417832">
<h2>Static visualizations</h2>

<p>You should already be acquainted with base R’s powers in terms of DV from <a data-type="xref" href="ch04.xhtml#ch05">Chapter 4</a>, especially regarding time series plotting. Here we’ll take a step further and discuss one of the most famous R packages - <code>ggplot2</code>. It’s one of the main reasons why Pythonistas want to switch to R<sup><a data-type="noteref" id="idm45127450414536-marker" href="ch05.xhtml#idm45127450414536">2</a></sup>. What makes <code>ggplot2</code> so successful in EDA work is that it’s based on a well thought-through methodology - the Grammar of Graphics (GoG). It was developed by L. Wilkinson, and the package by Hadley Wickham<sup><a data-type="noteref" id="idm45127450412248-marker" href="ch05.xhtml#idm45127450412248">3</a></sup>.</p>

<p>What <em>is</em> the GoG? The <a href="https://vita.had.co.nz/papers/layered-grammar.html">original paper</a> behind it has the title “A layered grammar of graphics,” and the word “layered” holds the key. Everything you see on a plot contributes to a larger stack or system. For example, the axes and grids form a separate layer compared to the lines, bars, and points. Those latter elements constitute the “data” layer. The complete stack of layers forms the result - a complete <code>ggplot</code>. Such a modular design pattern allows for great flexibility and provides a new way of thinking about data visualization. The logic behind GoG is illustrated in <a data-type="xref" href="#gog">Figure 5-2</a>.</p>

<figure><div id="gog" class="figure">
<img src="Images/prds_0502.png" alt="" width="5387" height="2969"/>
<h6><span class="label">Figure 5-2. </span>The layered Grammar of Graphics.</h6>
</div></figure>

<p>To illustrate the different procedures for a regular EDA workflow we’ll use the <code>starwars</code> dataset (available from the <code>dplyr</code> package<sup><a data-type="noteref" id="idm45127450403976-marker" href="ch05.xhtml#idm45127450403976">4</a></sup>). This dataset contains information on characters in the Star Wars movies, such as their gender, height and species. Let’s have a look!</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">ggplot2</code><code class="p">)</code><code>
</code><code class="nf">library</code><code class="p">(</code><code class="n">dplyr</code><code class="p">)</code><code>

</code><code class="nf">data</code><code class="p">(</code><code class="s">"</code><code class="s">starwars"</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO1-1" href="#callout_workflow_context_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_workflow_context_CO1-1" href="#co_workflow_context_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This will make the dataset visible in your RStudio environment, but it’s not strictly necessary.</p></dd>
</dl>

<p>As a first step, let’s do a basic plot:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">starwars</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">hair_color</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_bar</code><code class="p">()</code></pre>

<p>This plots the counts of the hair color variable. Here, we see a familiar operator, <code>+</code>, used unconventionally. We use <code>+</code> in ggplot2 to <em>add</em> layers on top of each other in <code>ggplot2</code>. Let’s build on this with a more involved case. Note that we omitted a filtering step from the code here (there’s an outlier - Jabba the Hut):
<sup><a data-type="noteref" id="idm45127450350536-marker" href="ch05.xhtml#idm45127450350536">5</a></sup>.</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">starwars</code><code class="p">,</code><code> </code><code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code> </code><code class="o">=</code><code> </code><code class="n">height</code><code class="p">,</code><code> </code><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="n">mass</code><code class="p">,</code><code> </code><code class="n">fill</code><code> </code><code class="o">=</code><code> </code><code class="n">gender</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><a class="co" id="co_workflow_context_CO2-1" href="#callout_workflow_context_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
  </code><code class="nf">geom_point</code><code class="p">(</code><code class="n">shape</code><code> </code><code class="o">=</code><code> </code><code class="m">21</code><code class="p">,</code><code> </code><code class="n">size</code><code> </code><code class="o">=</code><code> </code><code class="m">5</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><a class="co" id="co_workflow_context_CO2-2" href="#callout_workflow_context_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
  </code><code class="nf">theme_light</code><code class="p">(</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><a class="co" id="co_workflow_context_CO2-3" href="#callout_workflow_context_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
  </code><code class="nf">geom_smooth</code><code class="p">(</code><code class="n">method</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">lm"</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><a class="co" id="co_workflow_context_CO2-4" href="#callout_workflow_context_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
  </code><code class="nf">labs</code><code class="p">(</code><code class="n">x</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Height (cm)"</code><code class="p">,</code><code> </code><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Weight (cm)"</code><code class="p">,</code><code>
       </code><code class="n">title</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">StarWars profiles "</code><code class="p">,</code><code>
       </code><code class="n">subtitle</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Mass vs Height Comparison"</code><code class="p">,</code><code>
       </code><code class="n">caption</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Source: The Star Wars API"</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO2-5" href="#callout_workflow_context_CO2-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_workflow_context_CO2-1" href="#co_workflow_context_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Specify which data and features to use.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO2-2" href="#co_workflow_context_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Select a points plot (the most suitable for continuous data).</p></dd>
<dt><a class="co" id="callout_workflow_context_CO2-3" href="#co_workflow_context_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Use a built-in <code>theme</code> - a collection of specific layer styles.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO2-4" href="#co_workflow_context_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Fit a linear model and show the results as a layer on the plot.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO2-5" href="#co_workflow_context_CO2-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Add title and axes labels.</p></dd>
</dl>

<p>The results of this plotting operation are shown on <a data-type="xref" href="#adv_ggplot_1">Figure 5-3</a>. With just several lines of code, we created a beautiful plot, which can be extended even further.</p>

<figure><div id="adv_ggplot_1" class="figure">
<img src="Images/prds_0503.png" alt="" width="853" height="756"/>
<h6><span class="label">Figure 5-3. </span>An advanced ggplot2 plot.</h6>
</div></figure>

<p>Now that we covered static visualizations let’s see how to make them more interesting by adding interactivity!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Interactive visualizations"><div class="sect2" id="idm45127450417160">
<h2>Interactive visualizations</h2>

<p>Interactivity can be a great aid to exploratory plots. Two excellent R packages stand out: <a href="https://rstudio.github.io/leaflet/"><code>leaflet</code></a> and <a href="https://plotly.com/"><code>plotly</code></a>.</p>
<div data-type="warning" epub:type="warning"><h1>Beware of JavaScript</h1>
<p>Interactivity in Python and R is often based on an underlying JavaScript codebase. Packages like <code>leaflet</code> and <code>plotly</code> take care of this for us, but keen to learn pure JavaScript. Low-level packages for interactive graphics, like <a href="https://d3js.org/">D3.js</a>, can be overwhelming to learn for the novice. Thus, we’d encourage learning a high-level framework, such as <a href="http://dimplejs.org/">Dimple.js</a> instead.</p>
</div>

<p>Different datasets require different visualization methods. We covered the case of a standard tabular dataset (<code>starwars</code>), but how about something different? We’ll have a go at visualizing data with a spatial dimension and use it to show R’s excellent capabilities in producing interactive plots. For this, we selected the <a href="https://www.kaggle.com/gidutz/autotel-shared-car-locations">Shared Cars Locations dataset</a>. It provides the locations of car-sharing vehicles in Tel-Aviv, Israel. Can we show those on a map?</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">leaflet</code><code class="p">)</code>
<code class="nf">leaflet</code><code class="p">(</code><code class="n">data</code> <code class="o">=</code> <code class="n">shared_cars_data</code><code class="p">[</code><code class="m">1</code><code class="o">:</code><code class="m">20</code><code class="p">,</code> <code class="p">])</code> <code class="o">%&gt;%</code>
        <code class="nf">addTiles</code><code class="p">()</code> <code class="o">%&gt;%</code>
        <code class="nf">addMarkers</code><code class="p">(</code><code class="n">lng</code> <code class="o">=</code> <code class="n">longitude</code><code class="p">,</code> <code class="n">lat</code> <code class="o">=</code> <code class="n">latitude</code><code class="p">)</code></pre>

<p>In this case, we subset the data using the first 20 rows only (to make the visualization less cluttered). The <code>addTiles</code> function provides the map background, with the street and city names<sup><a data-type="noteref" id="idm45127450129960-marker" href="ch05.xhtml#idm45127450129960">6</a></sup>. The next step is to add the markers which specify the car locations by using <code>addMarkers</code>. The result of this relatively simple operation is shown in <a data-type="xref" href="#leaflet">Figure 5-4</a>.</p>

<figure><div id="leaflet" class="figure">
<img src="Images/prds_0504.png" alt="" width="863" height="340"/>
<h6><span class="label">Figure 5-4. </span>An interactive map plot with leaflet</h6>
</div></figure>

<p>As with the best data science tools, packages like <code>leaflet</code> hide a lot of complexity under the hood. They do much of the heavy lifting necessary for advanced visualization and enable the data scientist to do what they do best - focus on the data. There are many more advanced features available in <code>leaflet</code>, and we encourage the motivated user to explore them.</p>
<div data-type="tip"><h1>Make ggplot2 interactive</h1>
<p>As our book’s subtitle suggests, we are always attempting to take the best of both worlds. So one easy way to do it is to use the <code>ggplotly</code> command from the <code>plotly</code> package and pass it a <code>ggplot2</code> plot. This will make the plot interactive!</p>
</div>

<p>Hopefully, this section has made clear why the EDA workflow makes using R and tools such as <code>ggplot2</code> and <code>leaflet</code> the best options. We’ve just scratched the surface on what’s possible, and if one decides to go deeper into the data visualization aspects, there are a ton of great resources available.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Machine learning"><div class="sect1" id="idm45127450162632">
<h1>Machine learning</h1>

<p>Nowadays, data science is used almost synonymously with machine learning (ML). While there are many different workflows necessary for a data science project (<a data-type="xref" href="#meta_workflow">Figure 5-1</a>), ML often steals the focus of aspiring data scientists. This is partly due to an increasing growth surge in recent years due to the availability of large amounts of data, better computing resources (such as better CPUs and GPUs), and the need for predictions and automation in modern business. In the early days of the field, it was known under a different name - statistical learning. As previously mentioned, statistics has been historically the primary domain of R. Thus there were good tools available early on for doing ML in it. However, this has changed in recent years, and Python’s tools have mostly overtaken its statistical competitor.</p>

<p>One can trace Python’s ML ecosystem’s success to one specific package - <a href="https://scikit-learn.org/stable/"><code>scikit-learn</code></a>. Since its early versions, the core development team has focused on designing an accessible and easy-to-use API. They supported this with some of the most complete and accessible documentation available in the open-source world. It’s not only a reference documentation but contains excellent tutorials on various specific modern ML applications, such as <a href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">working with text data</a>. <code>scikit-learn</code> provides access to almost all common ML algorithms out of the box<sup><a data-type="noteref" id="idm45127450113112-marker" href="ch05.xhtml#idm45127450113112">7</a></sup>.</p>

<p>Let’s have a look at some proof of why <code>scikit-learn</code> is so great for ML. First, we can demonstrate the model imports:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code></pre>

<p>Here we can already see how consistently those models are designed - similar to books in a well-organized library; everything is at the right place. ML algorithms in <code>scikit-learn</code> are grouped based on their similarities. In this example, tree-based methods such as Decision Tree belong to the <code>tree</code> module. In contrast, linear algorithms can be found in the <code>linear_model</code> one (i.e., if you want to perform a Lasso model, you can predictably find it in <code>linear_model.Lasso</code>). Such hierarchical design makes it easier to focus on writing code and not to search for documentation since any good autocomplete engine will find the relevant model for you.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We discussed modules in <a data-type="xref" href="ch03.xhtml#ch04">Chapter 3</a>, but it’s a concept that bears repeating since it might be confusing for some R users. Modules in Python are nothing more than collections of organized scripts (based on some similarities, such as “data_processing” for example), which allows them to be imported into your applications, improving readability and making the codebase more organized.</p>
</div>

<p>Next, we need to prepare the data for modeling. An essential element of any ML project is splitting the data into train and test sets. While newer R packages such as <code>mlr</code> improve on this as well, <code>scikit-learn</code> has better (in terms of both consistency and syntax) functions available:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code>
                                                    <code class="n">test_size</code><code class="o">=</code><code class="mf">0.33</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>Suppose we have been consistent in the steps before and have followed traditional ML convention. In that case, we have the <code>X</code> object to store our features and <code>y' - the labels (in the case of a supervised learning problemfootnote:[For those readers new to ML, supervised learning is concerned with prediction tasks where a target is available (label), as compared to unsupervised learning where it's missing, and the prediction task is on uncovering groups in the data.]). In this case, the data will be randomly split. The official way to do this in R's `mlr</code> is:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">train_set</code> <code class="o">=</code> <code class="nf">sample</code><code class="p">(</code><code class="n">task</code><code class="o">$</code><code class="n">nrow</code><code class="p">,</code> <code class="m">0.8</code> <code class="o">*</code> <code class="n">task</code><code class="o">$</code><code class="n">nrow</code><code class="p">)</code>
<code class="n">test_set</code> <code class="o">=</code> <code class="nf">setdiff</code><code class="p">(</code><code class="nf">seq_len</code><code class="p">(</code><code class="n">task</code><code class="o">$</code><code class="n">nrow</code><code class="p">),</code> <code class="n">train_set</code><code class="p">)</code></pre>

<p>This can be harder to understand, and if one needs documentation on how to perform a more advanced split, such as by stratification, there’s little available, and another package might be required, increasing the learning curve and cognitive load on the data scientist. <code>scikit-learn</code>, on the other hand, provides a handy function in <code>StratifiedShuffleSplit</code>. The capabilities only increase further when we start to perform the actual modeling:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>

<p>These three code lines are all we need to initialize the model with default parameters, fit (train) it on the training dataset, and predict on the test one. This pattern is consistent across projects (except for the model initialization, where one selects their algorithm of choice and its parameters - those do differ, of course). A visual comparison between several different packages (from other developers and purposes) is shown in <a data-type="xref" href="#consistent_api_ml">Figure 5-6</a>. Finally, let’s compute some performance metrics; many of them are handily available:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">metrics</code>

<code class="n">acc</code> <code class="o">=</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="n">conf_matrix</code> <code class="o">=</code> <code class="n">metrics</code><code class="o">.</code><code class="n">confusion_matrix</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="n">classif_report</code> <code class="o">=</code> <code class="n">metrics</code><code class="o">.</code><code class="n">classification_report</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code></pre>

<p>The <code>metrics</code> module contains everything needed to check our model’s performance, with a simple and predictable Application Programming Interface (API). The pattern of <code>fit</code> and <code>predict</code> we saw earlier has been so influential in the open-source world that it has been widely adopted by other packages, such as <code>yellowbrick</code> (a package for model performance visualization):</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">yellowbrick.regressor</code> <code class="kn">import</code> <code class="n">ResidualsPlot</code>

<code class="n">visualizer</code> <code class="o">=</code> <code class="n">ResidualsPlot</code><code class="p">(</code><code class="n">regr</code><code class="p">)</code>

<code class="n">visualizer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">visualizer</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="n">visualizer</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

<p>There are many other visualizations available in <code>yellowbrick</code>, all obtained with a similar procedure. Some are presented in <a data-type="xref" href="#yellowbrick">Figure 5-5</a>.</p>

<figure><div id="yellowbrick" class="figure">
<img src="Images/prds_0505.png" alt="" width="1937" height="516"/>
<h6><span class="label">Figure 5-5. </span>Different possible <code>yellowbrick</code> regression plots.</h6>
</div></figure>

<p>The consistency and ease of use are among the significant reasons users want to use Python for ML. It enables the user to focus on the task at hand and not on writing code and sifting through tedious documentation pages. There were changes in R packages in recent years aiming at reducing those deficiencies. Such packages most notably include <code>mlr</code> and <code>tidymodels</code>. Still, they are not widely used, but perhaps this pattern can change in the future. There is an additional factor to consider here, which is similar to the ecosystem interoperability we saw in <a data-type="xref" href="ch04.xhtml#ch05">Chapter 4</a>. <code>scikit-learn</code> works very well with other tools Python, which are necessary for the development and deployment of ML models. Such tools include database connections, high-performance computing packages, testing frameworks, and deployment frameworks. Writing the ML code in <code>scikit-learn</code> will enable the data scientists to be a more productive part of a data team (just imagine the expression of your data engineering colleagues’ faces when you deliver an <code>mlr</code> model to them for deployment).</p>

<figure><div id="consistent_api_ml" class="figure">
<img src="Images/prds_0506.png" alt="" width="1833" height="545"/>
<h6><span class="label">Figure 5-6. </span>API consistency overview in the Python ML ecosystem.</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45127449779624">
<h5>Deep Learning</h5>
<p>We won’t cover deep learning (DL) extensively in this section since most of the rationale from <code>scikit-learn</code> (and  Python in general) applies to it as well. Still, due to its increasing importance in modern data science, it deserves a few additional comments.</p>

<p>The DL workflow has been mostly supported by two competing open-source frameworks - <a href="https://www.tensorflow.org/">TensorFlow</a> (from Google) and <a href="https://pytorch.org/">pytorch</a> (Facebook). There is an additional framework, which was eventually included in TensorFlow, called <a href="https://keras.io/">Keras</a>. It provides a higher level of abstraction API to the TensorFlow functions, lowering the learning curve. There have been two notable developments in the R ecosystem regarding those DL frameworks. TensorFlow and Keras have been ported by using the <a href="https://rstudio.github.io/reticulate/"><code>reticulate</code></a> package (we’ll cover it in <a data-type="xref" href="ch06.xhtml#ch07">Chapter 6</a>), which calls Python under the hood. Pytorch on the other hand has been faithfully recreated on top of <code>libtorch</code>, the C++ backend of Pytorch in the <a href="https://torch.mlverse.org/"><code>torch</code></a> package.</p>

<p>Due to those points, our recommendation is to use the Python tools for a DL workflow, based on Keras and TensorFlow, except using <code>torch,</code> in the case you have an existing R codebase.</p>
</div></aside>

<p>To wrap up this section, we can summarize the main points about the ML workflow and why Python tools better support it:</p>
<ol>
<li>
<p>Focus has moved to real-time predictions and automation.</p>
</li>
<li>
<p>The Python ML workflow provides a more consistent and easy-to-use API.</p>
</li>
<li>
<p>Python is more of a glue language <sup><a data-type="noteref" id="idm45127449767160-marker" href="ch05.xhtml#idm45127449767160">8</a></sup>, ideal for combining different software components (i.e.,frontend/backend and databases).</p>
</li>

</ol>

<p>In the next section, we’ll go deeper into the third part of this list and demonstrate the recommended Data Engineering workflow.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Data engineering"><div class="sect1" id="idm45127450118536">
<h1>Data engineering</h1>

<p>Despite the ML tools’ advancements in recent years, the completion rate of such projects in companies remains low. One reason which is often credited for this is the lack of data engineering (DE) support. To apply ML and advanced analytics, companies need the infrastructural foundation provided by data engineers, including databases, data processing pipelines, testing, and deployment tools. Of course, this forms a separate job title - data engineer. Still, data scientists need to interface (and sometimes implement themselves) with those technologies to ensure data science projects are completed successfully.</p>

<p>While DE is a massive field, we’ll focus on a subset for this section. We selected model deployment for this since it’s the most common DE workflow that a data scientist might need to participate in. So what is ML deployment? Most of the time, this means creating an application programming interface (API) and making it available to other applications, either internally or externally (to customers, this is called “exposing” an API, to be “consumed”). Commonly ML models are deployed via a REST interface<sup><a data-type="noteref" id="idm45127449761224-marker" href="ch05.xhtml#idm45127449761224">9</a></sup>.</p>

<p>ML model deployment, compared to the other topics in this chapter, requires interfacing with many different technologies, not directly related to data science. These include web frameworks, CSS, HTML, JavaScript, cloud servers, load balancers, and others. Thus it’s not surprising that Python tools dominate here<sup><a data-type="noteref" id="idm45127449733704-marker" href="ch05.xhtml#idm45127449733704">10</a></sup> - as we covered before, it’s a fantastic glue language.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The model deployment workflow requires code to be executed on other machines rather than the local one where the data scientist performs their daily work. This hits “it works on my machine” problem right on the head. There are different ways to deal with managing different environments consistently, ranging from simple to complex. A simple way to do this is to use a <code>requirements.txt</code> file, where all dependencies are specified. A more complex option, which is often used in large-scale, critical deployments, uses container solutions such as <a href="https://www.docker.com/">Docker</a>. This dependency management is much easier to achieve in Python than in R.</p>
</div>

<p>One of the most popular tools to create an API is Python’s <a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a> - a <a href="https://en.wikipedia.org/wiki/Microframework#:~:text=A%20microframework%20is%20a%20term,Accounts%2C%20authentication%2C%20authorization%2C%20roles">micro-framework</a>. It provides a minimalist interface that is easy to extend with other tools, such as ones providing user authentication or better design. To get started, we’ll go through a small example. We would need a typical Python installation with some other additional configurations such as a virtual environment<sup><a data-type="noteref" id="idm45127449727448-marker" href="ch05.xhtml#idm45127449727448">11</a></sup> and a GUI to query the API. Let’s get started!</p>
<div data-type="tip"><h1>ML-focused API frameworks</h1>
<p>Recently competitors to Flask have sprung up. They serve the same purpose but with an increased focus on ML. Two popular examples include <a href="https://www.bentoml.ai/">BentoML</a> and <a href="https://fastapi.tiangolo.com/">FastAPI</a>. Those frameworks provide you with some additional options that make ML deployment easier. Remember that Flask was initially built for web development APIs, and the needs of an ML project can be different.</p>
</div>

<p>We’ll be building an API that predicts housing prices<sup><a data-type="noteref" id="idm45127449720408-marker" href="ch05.xhtml#idm45127449720408">12</a></sup>. It’s always prudent to start with the end goal in mind and how we’d like such a predictive model to be used by an external application or an end-user. In this case, we can imagine our API to be integrated into an online house rental portal.</p>

<p>For brevity, we’ll omit the model training part. Imagine that you have followed a traditional <code>scikit-learn</code> model development. The results of the predictive model are stored in a <code>.pkl</code> (<code>Pickle</code> object, the standard Python way to store objects on disk). This process is called serialization, and we need to do it to use the model in the API later:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pickle</code>

<code class="c1"># model preparation and training part</code>
<code class="c1"># ...</code>

<code class="c1"># model serialization</code>
<code class="n">outfile</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"models/regr.pkl"</code><code class="p">,</code> <code class="s2">"wb"</code><code class="p">)</code>
<code class="n">pickle</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="n">regr</code><code class="p">,</code> <code class="n">outfile</code><code class="p">)</code>
<code class="n">outfile</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Model trained &amp; stored!"</code><code class="p">)</code></pre>

<p>We can save this code in a script called <code>train_model.py</code>. By running it: <code>python train_model.py</code>, the pickled model will be produced and saved. <a data-type="xref" href="#ml_api_diagram">Figure 5-7</a> provides an overview of how the different components fit.</p>

<figure><div id="ml_api_diagram" class="figure">
<img src="Images/prds_0507.png" alt="" width="1616" height="618"/>
<h6><span class="label">Figure 5-7. </span>Example architecture for an ML API.</h6>
</div></figure>
<pre>Let's use Flask next:</pre>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code><code> </code><code class="nn">pickle</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">ast</code><code> </code><code class="kn">import</code><code> </code><code class="n">literal_eval</code><code> </code><a class="co" id="co_workflow_context_CO3-1" href="#callout_workflow_context_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="kn">from</code><code> </code><code class="nn">flask</code><code> </code><code class="kn">import</code><code> </code><code class="n">Flask</code><code class="p">,</code><code> </code><code class="n">request</code><code class="p">,</code><code> </code><code class="n">jsonify</code><code>
</code><code>
</code><code class="n">app</code><code> </code><code class="o">=</code><code> </code><code class="n">Flask</code><code class="p">(</code><code class="nv-Magic">__name__</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO3-2" href="#callout_workflow_context_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">infile</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">models/regr.pkl</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">rb</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO3-3" href="#callout_workflow_context_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code class="n">regr</code><code> </code><code class="o">=</code><code> </code><code class="n">pickle</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">infile</code><code class="p">)</code><code>
</code><code class="n">infile</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="nd">@app.route</code><code class="p">(</code><code class="s1">'</code><code class="s1">/</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO3-4" href="#callout_workflow_context_CO3-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code class="k">def</code><code> </code><code class="nf">predict</code><code class="p">(</code><code class="n">methods</code><code class="o">=</code><code class="p">[</code><code class="s2">"</code><code class="s2">GET</code><code class="s2">"</code><code class="p">]</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">payload</code><code> </code><code class="o">=</code><code> </code><code class="n">request</code><code class="o">.</code><code class="n">json</code><code class="p">[</code><code class="s2">"</code><code class="s2">data</code><code class="s2">"</code><code class="p">]</code><code>
</code><code>    </code><code class="n">input_data</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">literal_eval</code><code class="p">(</code><code class="n">payload</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="o">-</code><code class="mi">1</code><code class="p">)</code><code>
</code><code>    </code><code class="n">prediction</code><code> </code><code class="o">=</code><code> </code><code class="n">regr</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">input_data</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO3-5" href="#callout_workflow_context_CO3-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">jsonify</code><code class="p">(</code><code class="p">{</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">prediction</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="nb">round</code><code class="p">(</code><code class="nb">float</code><code class="p">(</code><code class="n">prediction</code><code class="p">)</code><code class="p">,</code><code> </code><code class="mi">3</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO3-6" href="#callout_workflow_context_CO3-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a><code>
</code><code>    </code><code class="p">}</code><code class="p">)</code><code>
</code><code>
</code><code>
</code><code class="k">if</code><code> </code><code class="nv-Magic">__name__</code><code> </code><code class="o">==</code><code> </code><code class="s1">'</code><code class="s1">__main__</code><code class="s1">'</code><code class="p">:</code><code>
</code><code>    </code><code class="n">app</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">debug</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_workflow_context_CO3-1" href="#co_workflow_context_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>We use this function to specify that the payload string object is actually a dictionary.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO3-2" href="#co_workflow_context_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Here we create an object that holds the app.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO3-3" href="#co_workflow_context_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>In those several lines we load the serialized model.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO3-4" href="#co_workflow_context_CO3-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>This Python decorator creates an “end-point” (see info box below).</p></dd>
<dt><a class="co" id="callout_workflow_context_CO3-5" href="#co_workflow_context_CO3-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>At this step, the serialised model is used for inference.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO3-6" href="#co_workflow_context_CO3-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>The inference results are returned in a JSON format.</p></dd>
</dl>

<p>This code is added to a file <code>app.py</code>. Once you execute this script, the command line will output a local URL. We can then use a tool such as Postman to query it<sup><a data-type="noteref" id="idm45127449396760-marker" href="ch05.xhtml#idm45127449396760">13</a></sup>. Have a look at <a data-type="xref" href="#postman">Figure 5-8</a> to see how such a query works. Voilà - we built an ML API!</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In our example, the API provides just one functionality - the ability to predict a housing price on a dataset. Often in the real world, the same application might need to do different things. This is organized by creating different end-points. For example, there might be an end-point for triggering a data preparation script and a separate inference one.</p>
</div>

<figure><div id="postman" class="figure">
<img src="Images/prds_0508.png" alt="" width="3288" height="2050"/>
<h6><span class="label">Figure 5-8. </span>Querying the ML API with Postman.</h6>
</div></figure>
<div data-type="tip"><h1>Cloud deployment</h1>
<p>After you’re done with writing and testing the ML API code, the next phase would be to deploy it. Of course, you could use your computer as a server and expose it to the internet, but you can imagine that doesn’t scale very well (you have to keep your machine running, and it might run out of resources). One of the significant changes seen in recent years in terms of DE tools is the advent of cloud computing. Cloud platforms such as Amazon Web Services (AWS), or Google Cloud Provider (GCP) provide you with excellent opportunities and deploy your apps. Your Flask API can be deployed via a cloud service such as <a href="https://aws.amazon.com/elasticbeanstalk/">Elastic Beanstalk</a> or <a href="https://cloud.google.com/appengine">Google App Engine</a>.</p>
</div>

<p>Due to the “glue-like” nature of Python packages, they dominate the DE workflow. If a data scientist can write such applications on their own in Python, the success of the complete data project is ensured.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Reporting"><div class="sect1" id="idm45127449763656">
<h1>Reporting</h1>

<p>Every data scientist is aware (perhaps painfully so) of how vital communication is for their daily work. It’s also an often underrated skill, so this mantra bears repeating. So, what is more important than one of the essential deliverables of a data science project - reporting your results?</p>

<p>There are different reporting methods available. The most typical use case for a data scientist is to create a document, or a slide deck, containing the results of the analysis they have performed on a dataset. This is usually a collection of visualizations with an associated text and a consistent storyline (i.e., going through the different stages of a project lifecycle - data importing, cleaning, and visualization). There are other situations where the report has to be referred to often and updated in real-time - called dashboards. And finally, some reports allow the end-user to explore them more interactively. We’ll go through those three report types in the following subsections.</p>








<section data-type="sect2" data-pdf-bookmark="Static reporting"><div class="sect2" id="idm45127449385592">
<h2>Static reporting</h2>

<p>The popularization of the markdown (MD) language helps data scientists focus on writing code and associated thoughts instead of the tool itself. A flavor of this language - R Markdown (RMD) is widely used in the R community. This allows for the concept of “literate programming”, where the code is mixed with the analysis. The RStudio IDE provides even further functionality with tools such as <a href="https://rmarkdown.rstudio.com/lesson-10.html">R notebooks</a>. This is how easy writing an RMD report is:</p>

<pre data-type="programlisting" data-code-language="r"><code class="c1"># Analysing Star Wars</code>

<code class="n">First</code> <code class="n">we</code> <code class="n">start</code> <code class="n">by</code> <code class="n">importing</code> <code class="n">the</code> <code class="n">data.</code>

<code class="n">```{r}</code>
<code class="n">library(dplyr)</code>

<code class="n">data(starwars)</code>
<code class="n">```</code>

<code class="n">Then</code> <code class="n">we</code> <code class="n">can</code> <code class="n">have</code> <code class="n">a</code> <code class="n">look</code> <code class="n">at</code> <code class="n">the</code> <code class="n">result.</code></pre>

<p>This <code>.rmd</code> file can then be <code>knit</code> (compiled) into a <code>.pdf</code> or an <code>.html</code> (best for interactive plots), creating a beautiful report. There are additional templates to create even slides, dashboards and websites from RMD files. Have a look at <a data-type="xref" href="#example_rmd">Figure 5-9</a> to check it out in action.</p>

<figure><div id="example_rmd" class="figure">
<img src="Images/prds_0509.png" alt="" width="1908" height="926"/>
<h6><span class="label">Figure 5-9. </span>RMarkdown editing within RStudio</h6>
</div></figure>

<p>As with everything in the open-source world, data scientists worldwide have contributed to the further development of RMD. There are many templates available for RMD, enabling users to create everything from a custom-styled report to a dynamically generated blogging website.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The widely adopted alternative to RMD in the Python world is the <a href="https://jupyter.org/">Jupyter</a> Notebook (along with its newer version - <a href="https://jupyterlab.readthedocs.io/en/stable/">Jupyter Lab</a>). The “r” in Jupyter stands for R, and it is certainly possible to use that, but we argue that the RMD notebooks in RStudio provide a better interface, at least for R work.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Interactive reporting"><div class="sect2" id="idm45127449356936">
<h2>Interactive reporting</h2>

<p>What if we want to be able to let the recipients of our report do some work as well? If we allow for some interactivity, our end-users would answer questions for themselves without relying on us to go back, change the code and regenerate the graphs. There are several tools available<sup><a data-type="noteref" id="idm45127449354904-marker" href="ch05.xhtml#idm45127449354904">14</a></sup>, but most of them pale in comparison to the ease of use and capabilities of R’s <code>shiny</code> package<sup><a data-type="noteref" id="idm45127449353176-marker" href="ch05.xhtml#idm45127449353176">15</a></sup>.</p>

<p>Using this package requires a bit of a different way of writing R code, but you will create fantastic applications once you get used to it. Let’s go through a basic yet practical example. <code>shiny</code> apps consist of two fundamental elements: the user interface (UI) and the server logic. Those are often even separated into two files. For simplicity we’ll use the single file layout and use two functions for the app.</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">shiny</code><code class="p">)</code><code>

</code><code class="n">ui</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">fluidPage</code><code class="p">(</code><code> </code><a class="co" id="co_workflow_context_CO4-1" href="#callout_workflow_context_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>

    </code><code class="nf">titlePanel</code><code class="p">(</code><code class="s">"</code><code class="s">StarWars Characters"</code><code class="p">)</code><code class="p">,</code><code>

    </code><code class="nf">sidebarLayout</code><code class="p">(</code><code>
        </code><code class="nf">sidebarPanel</code><code class="p">(</code><code>
            </code><code class="nf">numericInput</code><code class="p">(</code><code class="s">"</code><code class="s">height"</code><code class="p">,</code><code> </code><code class="s">"</code><code class="s">Minimum Height:"</code><code class="p">,</code><code> </code><code class="m">0</code><code class="p">,</code><code> </code><code class="n">min</code><code> </code><code class="o">=</code><code> </code><code class="m">1</code><code class="p">,</code><code> </code><code class="n">max</code><code> </code><code class="o">=</code><code> </code><code class="m">1000</code><code class="p">)</code><code class="p">,</code><code> </code><a class="co" id="co_workflow_context_CO4-2" href="#callout_workflow_context_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
            </code><code class="nf">numericInput</code><code class="p">(</code><code class="s">"</code><code class="s">weight"</code><code class="p">,</code><code> </code><code class="s">"</code><code class="s">Minimum Weight:"</code><code class="p">,</code><code> </code><code class="m">0</code><code class="p">,</code><code> </code><code class="n">min</code><code> </code><code class="o">=</code><code> </code><code class="m">1</code><code class="p">,</code><code> </code><code class="n">max</code><code> </code><code class="o">=</code><code> </code><code class="m">1000</code><code class="p">)</code><code class="p">,</code><code>
            </code><code class="nf">hr</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code>
            </code><code class="nf">helpText</code><code class="p">(</code><code class="s">"</code><code class="s">Data from `dplyr` package."</code><code class="p">)</code><code>
        </code><code class="p">)</code><code class="p">,</code><code>

        </code><code class="nf">mainPanel</code><code class="p">(</code><code>
           </code><code class="nf">plotOutput</code><code class="p">(</code><code class="s">"</code><code class="s">distPlot"</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO4-3" href="#callout_workflow_context_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
        </code><code class="p">)</code><code>
    </code><code class="p">)</code><code>
</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_workflow_context_CO4-1" href="#co_workflow_context_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This function specifies that we want to have a “fluid” layout - that makes the app “responsive” - easy to read on a variety of devices, such as smartphones.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO4-2" href="#co_workflow_context_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Add the dynamic input for the user.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO4-3" href="#co_workflow_context_CO4-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Add a dedicated area for the output.</p></dd>
</dl>

<p>The <code>ui</code> object contains all the “frontend” parts of the application. The actual computation happens in the following function; we’ll be adding the <code>ggplot</code> from the DV section:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">server</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">function</code><code class="p">(</code><code class="n">input</code><code class="p">,</code><code> </code><code class="n">output</code><code class="p">)</code><code> </code><code class="p">{</code><code> </code><a class="co" id="co_workflow_context_CO5-1" href="#callout_workflow_context_CO5-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>

    </code><code class="n">output</code><code class="o">$</code><code class="n">distPlot</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">renderPlot</code><code class="p">(</code><code class="p">{</code><code> </code><a class="co" id="co_workflow_context_CO5-2" href="#callout_workflow_context_CO5-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
        </code><code class="n">starwars_filtered</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">starwars</code><code> </code><code class="o">%&gt;%</code><code>
            </code><code class="nf">filter</code><code class="p">(</code><code class="n">height</code><code> </code><code class="o">&gt;</code><code> </code><code class="n">input</code><code class="o">$</code><code class="n">height</code><code> </code><code class="o">&amp;</code><code> </code><code class="n">mass</code><code> </code><code class="o">&gt;</code><code> </code><code class="n">input</code><code class="o">$</code><code class="n">weight</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO5-3" href="#callout_workflow_context_CO5-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
        </code><code class="nf">ggplot</code><code class="p">(</code><code class="n">starwars_filtered</code><code class="p">,</code><code> </code><code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code> </code><code class="o">=</code><code> </code><code class="n">height</code><code class="p">,</code><code> </code><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="n">mass</code><code class="p">,</code><code> </code><code class="n">fill</code><code> </code><code class="o">=</code><code> </code><code class="n">gender</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">+</code><code>
            </code><code class="nf">geom_point</code><code class="p">(</code><code class="n">pch</code><code> </code><code class="o">=</code><code> </code><code class="m">21</code><code class="p">,</code><code> </code><code class="n">size</code><code> </code><code class="o">=</code><code> </code><code class="m">5</code><code class="p">)</code><code> </code><code class="o">+</code><code>
            </code><code class="nf">theme_light</code><code class="p">(</code><code class="p">)</code><code> </code><code class="o">+</code><code>
            </code><code class="nf">geom_smooth</code><code class="p">(</code><code class="n">method</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">lm"</code><code class="p">)</code><code> </code><code class="o">+</code><code>
            </code><code class="nf">labs</code><code class="p">(</code><code class="n">x</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Height"</code><code class="p">,</code><code> </code><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Mass"</code><code class="p">,</code><code>
            </code><code class="n">title</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">StarWars Characters Mass vs Height Comparison"</code><code class="p">,</code><code>
            </code><code class="n">subtitle</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Each dot represents a separate character"</code><code class="p">,</code><code>
            </code><code class="n">caption</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">Data Source: starwars (dplyr)"</code><code class="p">)</code><code> </code><a class="co" id="co_workflow_context_CO5-4" href="#callout_workflow_context_CO5-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
    </code><code class="p">}</code><code class="p">)</code><code>
</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_workflow_context_CO5-1" href="#co_workflow_context_CO5-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The server needs two things: input and output.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO5-2" href="#co_workflow_context_CO5-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>There is just one output in our case.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO5-3" href="#co_workflow_context_CO5-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>We can add all kinds of R computations here, as in a regular R script.</p></dd>
<dt><a class="co" id="callout_workflow_context_CO5-4" href="#co_workflow_context_CO5-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>The most recent item (in this case, a plot) is returned for display in the frontend.</p></dd>
</dl>

<p>The computation happens in this function. In the end, we need to pass those two functions here to start the app. The results of this are shown on <a data-type="xref" href="#shiny_example">Figure 5-10</a>.</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">shinyApp</code><code class="p">(</code><code class="n">ui</code> <code class="o">=</code> <code class="n">ui</code><code class="p">,</code> <code class="n">server</code> <code class="o">=</code> <code class="n">server</code><code class="p">)</code></pre>

<figure><div id="shiny_example" class="figure">
<img src="Images/prds_0510.png" alt="" width="891" height="489"/>
<h6><span class="label">Figure 5-10. </span>An interactive report with Shiny.</h6>
</div></figure>

<p>One difference for our Shiny app that might make it tricky to use than our markdown files is that you would need to host the application on a remote machine. For a normal <code>.rmd</code> on the files, you need to knit the file into a PDF and then share it. How such applications are deployed is beyond this book’s scope.</p>

<p>Creating reports is a small but vital component of data science work. This is how your work is shown to the outside world, be it your manager or another department. Even if you have done a great job in your analysis, it will often be judged by how well you communicate the process and results. Tools of literate programming such as RMD and more advanced interactive reports in <code>shiny</code> can go a long way to creating a state of the art reports. In the final chapter of this book, <a data-type="xref" href="ch07.xhtml#ch08">Chapter 7</a>, we’ll provide a great example of this in action.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Final thoughts"><div class="sect1" id="idm45127449356312">
<h1>Final thoughts</h1>

<p>In this chapter,, we went through the most essential workflows in a data science project and discovered the best tools in R and Python. In terms of EDA and reporting, R can be crowned the king. Packages such as <code>ggplot2</code> are peerless in the data science community, and <code>shiny</code> can allow for fascinating new ways to present data science results to stakeholders and colleagues. In the ML and DE worlds, Python’s glue-like nature provides fantastic options, enabling modern data scientists to focus on the work rather than the tools.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45127450420728"><sup><a href="ch05.xhtml#idm45127450420728-marker">1</a></sup> It’s a bit unfair to present <code>matplotlib</code> as the only viable alternative from Python. The <a href="https://seaborn.pydata.org/">seaborn</a> package also enables the creation of beautiful plots quickly but still lags behind the <code>ggplot</code> features. It’s worth mentioning that newer versions of <code>pandas</code> have plotting capabilities as well, so we should watch this space.</p><p data-type="footnote" id="idm45127450414536"><sup><a href="ch05.xhtml#idm45127450414536-marker">2</a></sup> There have been attempts to recreate this package in Python, such as <a href="https://pypi.org/project/ggplot/">ggplot</a> but they have not caught on in the community so far.</p><p data-type="footnote" id="idm45127450412248"><sup><a href="ch05.xhtml#idm45127450412248-marker">3</a></sup> He wrote many other packages, and in some ways almost single-handedly changed the way people use R in a modern context. Have a look at <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a> for more information on his packages.</p><p data-type="footnote" id="idm45127450403976"><sup><a href="ch05.xhtml#idm45127450403976-marker">4</a></sup> More information on the dataset is available <a href="https://rdrr.io/cran/dplyr/man/starwars.html">here</a>.</p><p data-type="footnote" id="idm45127450350536"><sup><a href="ch05.xhtml#idm45127450350536-marker">5</a></sup> Did you know that his real name is Jabba Desilijic Tiure?</p><p data-type="footnote" id="idm45127450129960"><sup><a href="ch05.xhtml#idm45127450129960-marker">6</a></sup> Explore the official documentation <a href="https://rstudio.github.io/leaflet/">here</a> for different map styles.</p><p data-type="footnote" id="idm45127450113112"><sup><a href="ch05.xhtml#idm45127450113112-marker">7</a></sup> An overview of those is available <a href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">here</a>.</p><p data-type="footnote" id="idm45127449767160"><sup><a href="ch05.xhtml#idm45127449767160-marker">8</a></sup> For a visual appreciation of the complexity of ML architectures, have a look at <a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">this</a> MLOps document from Google.</p><p data-type="footnote" id="idm45127449761224"><sup><a href="ch05.xhtml#idm45127449761224-marker">9</a></sup> To learn more about what is REST, have a look at <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">this</a> resource.</p><p data-type="footnote" id="idm45127449733704"><sup><a href="ch05.xhtml#idm45127449733704-marker">10</a></sup> The R alternative to Flask is <code>plumber</code>. The RStudio IDE provides a friendly interface to use this tool, but still, it is lagging in options and adoption in the ML community.</p><p data-type="footnote" id="idm45127449727448"><sup><a href="ch05.xhtml#idm45127449727448-marker">11</a></sup> For brevity, we will not go deeper into setting up virtual environments here. We urge the dedicated reader to read up upon the <a href="https://virtualenv.pypa.io/en/latest/"><code>virtualenv</code></a> and <a href="https://rstudio.github.io/renv/articles/renv.html"><code>renv</code></a> tools, covered in <a data-type="xref" href="ch03.xhtml#ch04">Chapter 3</a>.</p><p data-type="footnote" id="idm45127449720408"><sup><a href="ch05.xhtml#idm45127449720408-marker">12</a></sup> The dataset is “Boston Housing”, available <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html">here</a>.</p><p data-type="footnote" id="idm45127449396760"><sup><a href="ch05.xhtml#idm45127449396760-marker">13</a></sup> If you are more of a command-line person, have a look at <code>curl</code>.</p><p data-type="footnote" id="idm45127449354904"><sup><a href="ch05.xhtml#idm45127449354904-marker">14</a></sup> There’s an advanced new tool in Python, called <a href="https://www.streamlit.io/">streamlit</a>, but it is yet to gain in popularity and adoption.</p><p data-type="footnote" id="idm45127449353176"><sup><a href="ch05.xhtml#idm45127449353176-marker">15</a></sup> To get inspired with what’s possible in Shiny, look at the gallery of use cases at the <a href="https://shiny.rstudio.com/gallery/">RStudio website</a>.</p></div></div></section></div></body></html>