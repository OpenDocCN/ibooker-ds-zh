["```py\nimport pandas as pd\n\nposts_file = \"rspct.tsv.gz\"\nposts_df = pd.read_csv(posts_file, sep='\\t')\n\nsubred_file = \"subreddit_info.csv.gz\"\nsubred_df = pd.read_csv(subred_file).set_index(['subreddit'])\n\ndf = posts_df.join(subred_df, on='subreddit')\n\n```", "```py\nprint(df.columns)\n\n```", "```py\nIndex(['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2',\n       'category_3', 'in_data', 'reason_for_exclusion'],\n      dtype='object')\n\n```", "```py\ncolumn_mapping = {\n    'id': 'id',\n    'subreddit': 'subreddit',\n    'title': 'title',\n    'selftext': 'text',\n    'category_1': 'category',\n    'category_2': 'subcategory',\n    'category_3': None, # no data\n    'in_data': None, # not needed\n    'reason_for_exclusion': None # not needed\n}\n\n# define remaining columns\ncolumns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n\n# select and rename those columns\ndf = df[columns].rename(columns=column_mapping)\n\n```", "```py\ndf = df[df['category'] == 'autos']\n\n```", "```py\ndf.sample(1).T\n\n```", "```py\ndf.to_pickle(\"reddit_dataframe.pkl\")\n\n```", "```py\nimport sqlite3\n\ndb_name = \"reddit-selfposts.db\"\ncon = sqlite3.connect(db_name)\ndf.to_sql(\"posts\", con, index=False, if_exists=\"replace\")\ncon.close()\n\n```", "```py\ncon = sqlite3.connect(db_name)\ndf = pd.read_sql(\"select * from posts\", con)\ncon.close()\n\n```", "```py\ntext = \"\"\"\nAfter viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\nit got me thinking about the best match ups.\n<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\nCaptain America<lb>\"\"\"\n\n```", "```py\nimport re\n\nRE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n\ndef impurity(text, min_len=10):\n    \"\"\"returns the share of suspicious characters in a text\"\"\"\n    if text == None or len(text) < min_len:\n        return 0\n    else:\n        return len(RE_SUSPICIOUS.findall(text))/len(text)\n\nprint(impurity(text))\n\n```", "```py\n0.09009009009009009\n\n```", "```py\n# add new column to data frame\ndf['impurity'] = df['text'].apply(impurity, min_len=10)\n\n# get the top 3 records\ndf[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)\n\n```", "```py\nfrom blueprints.exploration import count_words\ncount_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))\n\n```", "```py\nimport html\n\ndef clean(text):\n    # convert html escapes like &amp; to characters.\n    text = html.unescape(text)\n    # tags like <tab>\n    text = re.sub(r'<[^<>]*>', ' ', text)\n    # markdown URLs like [Some text](https://....)\n    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n    # text or code in brackets like [0]\n    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n    # standalone sequences of specials, matches &# but not #cool\n    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n    # standalone sequences of hyphens like --- or ==\n    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n    # sequences of white spaces\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\n```", "```py\nclean_text = clean(text)\nprint(clean_text)\nprint(\"Impurity:\", impurity(clean_text))\n\n```", "```py\nAfter viewing the PINKIEPOOL Trailer it got me thinking about the best\nmatch ups. Here's my take: Deadpool Captain America\nImpurity: 0.0\n\n```", "```py\ndf['clean_text'] = df['text'].map(clean)\ndf['impurity']   = df['clean_text'].apply(impurity, min_len=20)\n\n```", "```py\ndf[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False) \\\n                              .head(3)\n\n```", "```py\ntext = \"The caf√© ‚ÄúSaint-Rapha√´l‚Äù is loca-\\nted on C√¥te d ºAzur.\"\n\n```", "```py\nimport textacy.preprocessing as tprep\n\ndef normalize(text):\n    text = tprep.normalize_hyphenated_words(text)\n    text = tprep.normalize_quotation_marks(text)\n    text = tprep.normalize_unicode(text)\n    text = tprep.remove_accents(text)\n    return text\n\n```", "```py\nprint(normalize(text))\n\n```", "```py\nThe cafe \"Saint-Raphael\" is located on Cote d'Azur.\n\n```", "```py\nfrom textacy.preprocessing.resources import RE_URL\n\ncount_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)\n\n```", "```py\nfrom textacy.preprocessing.replace import replace_urls\n\ntext = \"Check out https://spacy.io/usage/spacy-101\"\n\n# using default substitution _URL_\nprint(replace_urls(text))\n\n```", "```py\nCheck out _URL_\n\n```", "```py\ndf['clean_text'] = df['clean_text'].map(replace_urls)\ndf['clean_text'] = df['clean_text'].map(normalize)\n\n```", "```py\ndf.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)\ndf.drop(columns=['impurity'], inplace=True)\n\ncon = sqlite3.connect(db_name)\ndf.to_sql(\"posts_cleaned\", con, index=False, if_exists=\"replace\")\ncon.close()\n\n```", "```py\ntext = \"\"\"\n2019-08-10 23:32: @pete/@louis - I don't have a well-designed\nsolution for today's problem. The code of module AC68 should be -1.\nHave to think a bit... #goodnight ;-) üò©üò¨\"\"\"\n\n```", "```py\ntokens = re.findall(r'\\w\\w+', text)\nprint(*tokens, sep='|')\n\n```", "```py\n2019|08|10|23|32|pete|louis|don|have|well|designed|solution|for|today\nproblem|The|code|of|module|AC68|should|be|Have|to|think|bit|goodnight\n\n```", "```py\nRE_TOKEN = re.compile(r\"\"\"\n ( [#]?[@\\w'‚Äô\\.\\-\\:]*\\w     # words, hashtags and email addresses\n | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n )\n \"\"\", re.VERBOSE)\n\ndef tokenize(text):\n    return RE_TOKEN.findall(text)\n\ntokens = tokenize(text)\nprint(*tokens, sep='|')\n\n```", "```py\n2019-08-10|23:32|@pete|@louis|I|don't|have|a|well-designed|solution\nfor|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think\na|bit|#goodnight|;-)|üò©|üò¨\n\n```", "```py\nimport nltk\n\ntokens = nltk.tokenize.word_tokenize(text)\nprint(*tokens, sep='|')\n\n```", "```py\n2019-08-10|23:32|:|@|pete/|@|louis|-|I|do|n't|have|a|well-designed\nsolution|for|today|'s|problem|.|The|code|of|module|AC68|should|be|-1|.\nHave|to|think|a|bit|...|#|goodnight|;|-|)||üò©üò¨\n```", "```py\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\n```", "```py\nnlp.pipeline\n\n```", "```py\n[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fbd766f84c0>),\n ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fbd813184c0>),\n ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fbd81318400>)]\n\n```", "```py\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n\n```", "```py\nnlp = spacy.load(\"en_core_web_sm\")\ntext = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)\n\n```", "```py\nfor token in doc:\n    print(token, end=\"|\")\n\n```", "```py\nMy|best|friend|Ryan|Peters|likes|fancy|adventure|games|.|\n\n```", "```py\ndef display_nlp(doc, include_punct=False):\n    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n    rows = []\n    for i, t in enumerate(doc):\n        if not t.is_punct or include_punct:\n            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_,\n                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n                   'pos_': t.pos_, 'dep_': t.dep_,\n                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n            rows.append(row)\n\n    df = pd.DataFrame(rows).set_index('token')\n    df.index.name = None\n    return df\n\n```", "```py\ntext = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) üòãüëç\"\ndoc = nlp(text)\n\nfor token in doc:\n    print(token, end=\"|\")\n\n```", "```py\n@Pete|:|choose|low|-|carb|#|food|#|eat|-|smart|.|_|url|_|;-)|üòã|üëç|\n\n```", "```py\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.util import compile_prefix_regex, \\\n                       compile_infix_regex, compile_suffix_regex\n\ndef custom_tokenizer(nlp):\n\n    # use default patterns except the ones matched by re.search\n    prefixes = [pattern for pattern in nlp.Defaults.prefixes\n                if pattern not in ['-', '_', '#']]\n    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n                if pattern not in ['_']]\n    infixes  = [pattern for pattern in nlp.Defaults.infixes\n                if not re.search(pattern, 'xx-xx')]\n\n    return Tokenizer(vocab          = nlp.vocab,\n                     rules          = nlp.Defaults.tokenizer_exceptions,\n                     prefix_search  = compile_prefix_regex(prefixes).search,\n                     suffix_search  = compile_suffix_regex(suffixes).search,\n                     infix_finditer = compile_infix_regex(infixes).finditer,\n                     token_match    = nlp.Defaults.token_match)\n\nnlp = spacy.load('en_core_web_sm')\nnlp.tokenizer = custom_tokenizer(nlp)\n\ndoc = nlp(text)\nfor token in doc:\n    print(token, end=\"|\")\n\n```", "```py\n@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|üòã|üëç|\n\n```", "```py\ntext = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\ndoc = nlp(text)\n\nnon_stop = [t for t in doc if not t.is_stop and not t.is_punct]\nprint(non_stop)\n\n```", "```py\n[Dear, Ryan, need, sit, talk, Regards, Pete]\n\n```", "```py\nnlp = spacy.load('en_core_web_sm')\nnlp.vocab['down'].is_stop = False\nnlp.vocab['Dear'].is_stop = True\nnlp.vocab['Regards'].is_stop = True\n\n```", "```py\n[Ryan, need, sit, down, talk, Pete]\n\n```", "```py\ntext = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)\n\nprint(*[t.lemma_ for t in doc], sep='|')\n\n```", "```py\n-PRON-|good|friend|Ryan|Peters|like|fancy|adventure|game|.\n\n```", "```py\ntext = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)\n\nnouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\nprint(nouns)\n\n```", "```py\n[friend, Ryan, Peters, adventure, games]\n\n```", "```py\nimport textacy\n\ntokens = textacy.extract.words(doc,\n            filter_stops = True,           # default True, no stopwords\n            filter_punct = True,           # default True, no punctuation\n            filter_nums = True,            # default False, no numbers\n            include_pos = ['ADJ', 'NOUN'], # default None = include all\n            exclude_pos = None,            # default None = exclude none\n            min_freq = 1)                  # minimum frequency of words\n\nprint(*[t for t in tokens], sep='|')\n\n```", "```py\nbest|friend|fancy|adventure|games\n\n```", "```py\ndef extract_lemmas(doc, **kwargs):\n    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n\nlemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\nprint(*lemmas, sep='|')\n\n```", "```py\ngood|friend|fancy|adventure|game\n\n```", "```py\nMy_best|best_friend|friend_Ryan|Ryan_Peters|Peters_likes|likes_fancy\nfancy_adventure|adventure_games\n\n```", "```py\ntext = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)\n\npatterns = [\"POS:ADJ POS:NOUN:+\"]\nspans = textacy.extract.matches(doc, patterns=patterns)\nprint(*[s.lemma_ for s in spans], sep='|')\n\n```", "```py\ngood friend|fancy adventure|fancy adventure game\n\n```", "```py\nprint(*doc.noun_chunks, sep='|')\n\n```", "```py\nMy best friend|Ryan Peters|fancy adventure games\n\n```", "```py\ndef extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n    patterns = []\n    for pos in preceding_pos:\n        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n    spans = textacy.extract.matches(doc, patterns=patterns)\n    return [sep.join([t.lemma_ for t in s]) for s in spans]\n\nprint(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')\n\n```", "```py\ngood_friend|fancy_adventure|fancy_adventure_game|adventure_game\n\n```", "```py\ntext = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\ndoc = nlp(text)\n\nfor ent in doc.ents:\n    print(f\"({ent.text}, {ent.label_})\", end=\" \")\n\n```", "```py\n(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE)\n\n```", "```py\nfrom spacy import displacy\n\ndisplacy.render(doc, style='ent')\n\n```", "```py\ndef extract_entities(doc, include_types=None, sep='_'):\n\n    ents = textacy.extract.entities(doc,\n             include_types=include_types,\n             exclude_types=None,\n             drop_determiners=True,\n             min_freq=1)\n\n    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]\n\n```", "```py\nprint(extract_entities(doc, ['PERSON', 'GPE']))\n\n```", "```py\n[\"James_O'Neill/PERSON\", 'San_Francisco/GPE']\n\n```", "```py\ndef extract_nlp(doc):\n    return {\n    'lemmas'          : extract_lemmas(doc,\n                                     exclude_pos = ['PART', 'PUNCT',\n                                        'DET', 'PRON', 'SYM', 'SPACE'],\n                                     filter_stops = False),\n    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n    'noun_phrases'    : extract_noun_phrases(doc, ['NOUN']),\n    'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n    }\n\n```", "```py\ntext = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)\nfor col, values in extract_nlp(doc).items():\n    print(f\"{col}: {values}\")\n\n```", "```py\nlemmas: ['good', 'friend', 'Ryan', 'Peters', 'like', 'fancy', 'adventure', \\\n         'game']\nadjs_verbs: ['good', 'like', 'fancy']\nnouns: ['friend', 'Ryan', 'Peters', 'adventure', 'game']\nnoun_phrases: ['adventure_game']\nadj_noun_phrases: ['good_friend', 'fancy_adventure', 'fancy_adventure_game']\nentities: ['Ryan_Peters/PERSON']\n\n```", "```py\nnlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\nprint(nlp_columns)\n\n```", "```py\n['lemmas', 'adjs_verbs', 'nouns', 'noun_phrases', 'adj_noun_phrases', 'entities']\n\n```", "```py\ndb_name = \"reddit-selfposts.db\"\ncon = sqlite3.connect(db_name)\ndf = pd.read_sql(\"select * from posts_cleaned\", con)\ncon.close()\n\ndf['text'] = df['title'] + ': ' + df['text']\n\n```", "```py\nfor col in nlp_columns:\n    df[col] = None\n\n```", "```py\nif spacy.prefer_gpu():\n    print(\"Working on GPU.\")\nelse:\n    print(\"No GPU found, working on CPU.\")\n\n```", "```py\nnlp = spacy.load('en_core_web_sm', disable=[])\nnlp.tokenizer = custom_tokenizer(nlp) # optional\n\n```", "```py\nbatch_size = 50\n\nfor i in range(0, len(df), batch_size):\n    docs = nlp.pipe(df['text'][i:i+batch_size])\n\n    for j, doc in enumerate(docs):\n        for col, values in extract_nlp(doc).items():\n            df[col].iloc[i+j] = values\n\n```", "```py\ncount_words(df, 'noun_phrases').head(10).plot(kind='barh').invert_yaxis()\n\n```", "```py\ndf[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items))\n\ncon = sqlite3.connect(db_name)\ndf.to_sql(\"posts_nlp\", con, index=False, if_exists=\"replace\")\ncon.close()\n\n```"]