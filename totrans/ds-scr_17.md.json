["```py\nxs = [[1.0] + row[:2] for row in data]  # [1, experience, salary]\nys = [row[2] for row in data]           # paid_account\n```", "```py\nfrom matplotlib import pyplot as plt\nfrom scratch.working_with_data import rescale\nfrom scratch.multiple_regression import least_squares_fit, predict\nfrom scratch.gradient_descent import gradient_step\n\nlearning_rate = 0.001\nrescaled_xs = rescale(xs)\nbeta = least_squares_fit(rescaled_xs, ys, learning_rate, 1000, 1)\n# [0.26, 0.43, -0.43]\npredictions = [predict(x_i, beta) for x_i in rescaled_xs]\n\nplt.scatter(predictions, ys)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt.show()\n```", "```py\ndef logistic(x: float) -> float:\n    return 1.0 / (1 + math.exp(-x))\n```", "```py\ndef logistic_prime(x: float) -> float:\n    y = logistic(x)\n    return y * (1 - y)\n```", "```py\nimport math\nfrom scratch.linear_algebra import Vector, dot\n\ndef _negative_log_likelihood(x: Vector, y: float, beta: Vector) -> float:\n    \"\"\"The negative log likelihood for one data point\"\"\"\n    if y == 1:\n        return -math.log(logistic(dot(x, beta)))\n    else:\n        return -math.log(1 - logistic(dot(x, beta)))\n```", "```py\nfrom typing import List\n\ndef negative_log_likelihood(xs: List[Vector],\n                            ys: List[float],\n                            beta: Vector) -> float:\n    return sum(_negative_log_likelihood(x, y, beta)\n               for x, y in zip(xs, ys))\n```", "```py\nfrom scratch.linear_algebra import vector_sum\n\ndef _negative_log_partial_j(x: Vector, y: float, beta: Vector, j: int) -> float:\n    \"\"\"\n The jth partial derivative for one data point.\n Here i is the index of the data point.\n \"\"\"\n    return -(y - logistic(dot(x, beta))) * x[j]\n\ndef _negative_log_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n    \"\"\"\n The gradient for one data point.\n \"\"\"\n    return [_negative_log_partial_j(x, y, beta, j)\n            for j in range(len(beta))]\n\ndef negative_log_gradient(xs: List[Vector],\n                          ys: List[float],\n                          beta: Vector) -> Vector:\n    return vector_sum([_negative_log_gradient(x, y, beta)\n                       for x, y in zip(xs, ys)])\n```", "```py\nfrom scratch.machine_learning import train_test_split\nimport random\nimport tqdm\n\nrandom.seed(0)\nx_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n\nlearning_rate = 0.01\n\n# pick a random starting point\nbeta = [random.random() for _ in range(3)]\n\nwith tqdm.trange(5000) as t:\n    for epoch in t:\n        gradient = negative_log_gradient(x_train, y_train, beta)\n        beta = gradient_step(beta, gradient, -learning_rate)\n        loss = negative_log_likelihood(x_train, y_train, beta)\n        t.set_description(f\"loss: {loss:.3f} beta: {beta}\")\n```", "```py\n[-2.0, 4.7, -4.5]\n```", "```py\nfrom scratch.working_with_data import scale\n\nmeans, stdevs = scale(xs)\nbeta_unscaled = [(beta[0]\n                  - beta[1] * means[1] / stdevs[1]\n                  - beta[2] * means[2] / stdevs[2]),\n                 beta[1] / stdevs[1],\n                 beta[2] / stdevs[2]]\n# [8.9, 1.6, -0.000288]\n```", "```py\ntrue_positives = false_positives = true_negatives = false_negatives = 0\n\nfor x_i, y_i in zip(x_test, y_test):\n    prediction = logistic(dot(beta, x_i))\n\n    if y_i == 1 and prediction >= 0.5:  # TP: paid and we predict paid\n        true_positives += 1\n    elif y_i == 1:                      # FN: paid and we predict unpaid\n        false_negatives += 1\n    elif prediction >= 0.5:             # FP: unpaid and we predict paid\n        false_positives += 1\n    else:                               # TN: unpaid and we predict unpaid\n        true_negatives += 1\n\nprecision = true_positives / (true_positives + false_positives)\nrecall = true_positives / (true_positives + false_negatives)\n```", "```py\npredictions = [logistic(dot(beta, x_i)) for x_i in x_test]\nplt.scatter(predictions, y_test, marker='+')\nplt.xlabel(\"predicted probability\")\nplt.ylabel(\"actual outcome\")\nplt.title(\"Logistic Regression Predicted vs. Actual\")\nplt.show()\n```"]