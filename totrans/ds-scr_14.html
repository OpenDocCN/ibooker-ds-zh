<html><head></head><body><section data-pdf-bookmark="Chapter 13. Naive Bayes" data-type="chapter" epub:type="chapter"><div class="chapter" id="naive_bayes">&#13;
<h1><span class="label">Chapter 13. </span>Naive Bayes</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>It is well for the heart to be naive and for the mind not to be.</p>&#13;
    <p data-type="attribution">Anatole France</p>&#13;
</blockquote>&#13;
&#13;
<p>A<a data-primary="Naive Bayes" data-secondary="spam filter examples" data-type="indexterm" id="NBspam13"/><a data-primary="spam filter example" data-type="indexterm" id="spam13"/> social network isn’t much good if people can’t network.  Accordingly, DataSciencester has a popular feature that allows members to send messages to other members.  And while most members are responsible citizens who send only well-received “how’s it going?” messages, a few miscreants persistently spam other members about get-rich schemes, no-prescription-required pharmaceuticals, and for-profit data science credentialing programs. Your users have begun to complain,&#13;
and so the VP of Messaging has asked you to use data science to figure out a way to filter out these spam messages.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="A Really Dumb Spam Filter" data-type="sect1"><div class="sect1" id="idm45635741332680">&#13;
<h1>A Really Dumb Spam Filter</h1>&#13;
&#13;
<p>Imagine a “universe” that consists of receiving a message chosen randomly from all possible messages.  Let <em>S</em> be the event “the message is spam” and <em>B</em> be the event “the message contains the word <em>bitcoin</em>.” Bayes’s theorem tells us that the probability that the message is spam conditional on containing the word <em>bitcoin</em> is:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis upper S vertical-bar upper B right-parenthesis equals left-bracket upper P left-parenthesis upper B vertical-bar upper S right-parenthesis upper P left-parenthesis upper S right-parenthesis right-bracket slash left-bracket upper P left-parenthesis upper B vertical-bar upper S right-parenthesis upper P left-parenthesis upper S right-parenthesis plus upper P left-parenthesis upper B vertical-bar normal not-sign upper S right-parenthesis upper P left-parenthesis normal not-sign upper S right-parenthesis right-bracket" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>|</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mo>[</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>]</mo>&#13;
    <mo>/</mo>&#13;
    <mo>[</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>|</mo>&#13;
    <mo>¬</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mo>¬</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>]</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The numerator is the probability that a message is spam <em>and</em> contains <em>bitcoin</em>, while the denominator is just the probability that a message contains <em>bitcoin</em>. Hence, you can think of this calculation as simply representing the proportion of <em>bitcoin</em> messages that are spam.</p>&#13;
&#13;
<p>If we have a large collection of messages we know are spam, and a large collection of messages we know are not spam, then we can easily estimate <em>P</em>(<em>B</em>|<em>S</em>) and <em>P</em>(<em>B</em>|<em>¬S</em>). If we further assume that any message is equally likely to be spam or not spam (so that <em>P</em>(<em>S</em>) = <em>P</em>(<em>¬S</em>) = 0.5), then:</p>&#13;
<div data-type="equation">&#13;
<math alttext="left-bracket upper P left-parenthesis upper S vertical-bar upper B right-parenthesis equals upper P left-parenthesis upper B vertical-bar upper S right-parenthesis slash left-bracket upper P left-parenthesis upper B vertical-bar upper S right-parenthesis plus upper P left-parenthesis upper B vertical-bar normal not-sign upper S right-parenthesis right-bracket right-bracket" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>|</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>/</mo>&#13;
    <mo>[</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mo>|</mo>&#13;
    <mo>¬</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>]</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>For example, if 50% of spam messages have the word <em>bitcoin</em>, but only 1% of nonspam messages do, then the probability that any given <em>bitcoin</em>-containing email is spam is:</p>&#13;
<div data-type="equation">&#13;
<math alttext="0.5 slash left-parenthesis 0.5 plus 0.01 right-parenthesis equals 98 percent-sign" display="block">&#13;
  <mrow>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>5</mn>&#13;
    <mo>/</mo>&#13;
    <mo>(</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>5</mn>&#13;
    <mo>+</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>01</mn>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>98</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="A More Sophisticated Spam Filter" data-type="sect1"><div class="sect1" id="idm45635741218744">&#13;
<h1>A More Sophisticated Spam Filter</h1>&#13;
&#13;
<p>Imagine now that we have a vocabulary of many words, <em>w</em><sub>1</sub> ..., <em>w</em><sub>n</sub>. To move this into the realm of probability theory, we’ll write <em>X</em><sub>i</sub> for the event “a message contains the word&#13;
<em>w</em><sub>i</sub>.”&#13;
Also imagine that (through some unspecified-at-this-point process) we’ve come up with an estimate <em>P</em>(<em>X</em><sub>i</sub>|<em>S</em>) for the probability that a spam message contains the <em>i</em>th word, and a similar estimate <em>P</em>(<em>X</em><sub>i</sub>|¬<em>S</em>) for the probability that a nonspam message contains the <em>i</em>th word.</p>&#13;
&#13;
<p>The key to Naive Bayes is making the (big) assumption that the presences (or absences) of each word are independent of one another, conditional on a message being spam or not. Intuitively, this assumption means that knowing whether a certain spam message contains the word <em>bitcoin</em> gives you no information about whether that same message contains the word <em>rolex</em>. In math terms, this means that:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis upper X 1 equals x 1 comma period period period comma upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar upper S right-parenthesis equals upper P left-parenthesis upper X 1 equals x 1 vertical-bar upper S right-parenthesis times ellipsis times upper P left-parenthesis upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar upper S right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mn>1</mn> </msub>&#13;
      <mo>=</mo>&#13;
      <msub><mi>x</mi> <mn>1</mn> </msub>&#13;
      <mo>,</mo>&#13;
      <mo>.</mo>&#13;
      <mo>.</mo>&#13;
      <mo>.</mo>&#13;
      <mo>,</mo>&#13;
      <msub><mi>X</mi> <mi>n</mi> </msub>&#13;
      <mo>=</mo>&#13;
      <msub><mi>x</mi> <mi>n</mi> </msub>&#13;
      <mo>|</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mn>1</mn> </msub>&#13;
      <mo>=</mo>&#13;
      <msub><mi>x</mi> <mn>1</mn> </msub>&#13;
      <mo>|</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>×</mo>&#13;
    <mo>⋯</mo>&#13;
    <mo>×</mo>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mi>n</mi> </msub>&#13;
      <mo>=</mo>&#13;
      <msub><mi>x</mi> <mi>n</mi> </msub>&#13;
      <mo>|</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This is an extreme assumption.  (There’s a reason the technique has <em>naive</em> in its name.) Imagine that our vocabulary consists <em>only</em> of the words <em>bitcoin</em> and <em>rolex</em>, and that half of all spam messages are for “earn bitcoin” and that the other half are for “authentic rolex.”  In this case, the Naive Bayes estimate that a spam message contains both <em>bitcoin</em> and <em>rolex</em> is:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis upper X 1 equals 1 comma upper X 2 equals 1 vertical-bar upper S right-parenthesis equals upper P left-parenthesis upper X 1 equals 1 vertical-bar upper S right-parenthesis upper P left-parenthesis upper X 2 equals 1 vertical-bar upper S right-parenthesis equals .5 times .5 equals .25" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mn>1</mn> </msub>&#13;
      <mo>=</mo>&#13;
      <mn>1</mn>&#13;
      <mo>,</mo>&#13;
      <msub><mi>X</mi> <mn>2</mn> </msub>&#13;
      <mo>=</mo>&#13;
      <mn>1</mn>&#13;
      <mo>|</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mn>1</mn> </msub>&#13;
      <mo>=</mo>&#13;
      <mn>1</mn>&#13;
      <mo>|</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mn>2</mn> </msub>&#13;
      <mo>=</mo>&#13;
      <mn>1</mn>&#13;
      <mo>|</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo>.</mo>&#13;
    <mn>5</mn>&#13;
    <mo>×</mo>&#13;
    <mo>.</mo>&#13;
    <mn>5</mn>&#13;
    <mo>=</mo>&#13;
    <mo>.</mo>&#13;
    <mn>25</mn>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>since we’ve assumed away the knowledge that <em>bitcoin</em> and <em>rolex</em> actually never occur together.  Despite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters.</p>&#13;
&#13;
<p>The same Bayes’s theorem reasoning we used for our “bitcoin-only” spam filter tells us that we can calculate the probability a message is spam using the equation:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis upper S vertical-bar upper X equals x right-parenthesis equals upper P left-parenthesis upper X equals x vertical-bar upper S right-parenthesis slash left-bracket upper P left-parenthesis upper X equals x vertical-bar upper S right-parenthesis plus upper P left-parenthesis upper X equals x vertical-bar normal not-sign upper S right-parenthesis right-bracket" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>|</mo>&#13;
    <mi>X</mi>&#13;
    <mo>=</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>X</mi>&#13;
    <mo>=</mo>&#13;
    <mi>x</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>/</mo>&#13;
    <mo>[</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>X</mi>&#13;
    <mo>=</mo>&#13;
    <mi>x</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>X</mi>&#13;
    <mo>=</mo>&#13;
    <mi>x</mi>&#13;
    <mo>|</mo>&#13;
    <mo>¬</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>]</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The Naive Bayes assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimates for each vocabulary word.</p>&#13;
&#13;
<p>In practice, you usually want to avoid multiplying lots of probabilities together, to prevent<a data-primary="underflow" data-type="indexterm" id="idm45635741130712"/> a problem called <em>underflow</em>, in which computers don’t deal well with<a data-primary="floating-point numbers" data-type="indexterm" id="idm45635741129400"/> floating-point numbers that are too close to 0.&#13;
Recalling from algebra that <math alttext="log left-parenthesis a b right-parenthesis equals log a plus log b">&#13;
  <mrow>&#13;
    <mo form="prefix">log</mo>&#13;
    <mo>(</mo>&#13;
    <mi>a</mi>&#13;
    <mi>b</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mo form="prefix">log</mo>&#13;
    <mi>a</mi>&#13;
    <mo>+</mo>&#13;
    <mo form="prefix">log</mo>&#13;
    <mi>b</mi>&#13;
  </mrow>&#13;
</math>&#13;
and that <math alttext="exp left-parenthesis log x right-parenthesis equals x">&#13;
  <mrow>&#13;
    <mo form="prefix">exp</mo><mo>(</mo>&#13;
    <mo form="prefix">log</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mi>x</mi>&#13;
  </mrow>&#13;
</math>,&#13;
we usually compute <math alttext="p 1 asterisk ellipsis asterisk p Subscript n">&#13;
  <mrow>&#13;
    <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
    <mo>*</mo>&#13;
    <mo>⋯</mo>&#13;
    <mo>*</mo>&#13;
    <msub><mi>p</mi> <mi>n</mi> </msub>&#13;
  </mrow>&#13;
</math> as the equivalent (but floating-point-friendlier):</p>&#13;
<div data-type="equation">&#13;
<math alttext="exp left-parenthesis log left-parenthesis p 1 right-parenthesis plus ellipsis plus log left-parenthesis p Subscript n Baseline right-parenthesis right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mo form="prefix">exp</mo><mo>(</mo>&#13;
    <mo form="prefix">log</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <mo>⋯</mo>&#13;
    <mo>+</mo>&#13;
    <mo form="prefix">log</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>p</mi> <mi>n</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The only challenge left is coming up with estimates for <math>&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>X</mi><mi>i</mi></msub>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> and <math>&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>X</mi><mi>i</mi></msub>&#13;
    <mo>|</mo>&#13;
    <mo>¬</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, the probabilities that a spam message (or nonspam message) contains the word <math>&#13;
  <msub><mi>w</mi> <mi>i</mi> </msub>&#13;
</math>.  If we have a fair number of “training” messages labeled as spam and not spam, an obvious first try is to estimate&#13;
<math>&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>X</mi> <mi>i</mi> </msub>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> simply as the fraction of spam messages containing the word &#13;
<math>&#13;
  <msub><mi>w</mi> <mi>i</mi> </msub>&#13;
</math>.</p>&#13;
&#13;
<p>This causes a big problem, though.  Imagine that in our training set the vocabulary word <em>data</em> only occurs in nonspam messages.&#13;
Then we’d estimate <math alttext="upper P left-parenthesis quotation-mark data quotation-mark vertical-bar upper S right-parenthesis equals 0">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mtext>data</mtext>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math>.&#13;
The result is that our Naive Bayes classifier would always assign spam probability 0 to <em>any</em> message containing the word <em>data</em>, even a message like “data on free bitcoin and authentic rolex watches.”&#13;
To avoid this problem, we usually use some kind of smoothing.</p>&#13;
&#13;
<p>In<a data-primary="pseudocounts" data-type="indexterm" id="idm45635741075368"/> particular, we’ll choose a <em>pseudocount</em>—<em>k</em>—and estimate the probability of seeing the <em>i</em>th word in a spam message as:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis upper X Subscript i Baseline vertical-bar upper S right-parenthesis equals left-parenthesis k plus number of spams containing w Subscript i Baseline right-parenthesis slash left-parenthesis 2 k plus number of spams right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mi>i</mi> </msub>&#13;
      <mo>|</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>k</mi>&#13;
      <mo>+</mo>&#13;
      <mtext>number</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>of</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>spams</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>containing</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <msub><mi>w</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>/</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>2</mn>&#13;
      <mi>k</mi>&#13;
      <mo>+</mo>&#13;
      <mtext>number</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>of</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>spams</mtext>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>We do similarly for <math>&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>X</mi> <mi>i</mi> </msub>&#13;
    <mo>|</mo>&#13;
    <mo>¬</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>. That is, when computing the spam probabilities for the <em>i</em>th word, we assume we also saw <em>k</em> additional nonspams containing the word and <em>k</em> additional nonspams not containing the word.</p>&#13;
&#13;
<p>For example, if <em>data</em> occurs in 0/98 spam messages, and if <em>k</em> is 1, we estimate <em>P</em>(data|<em>S</em>) as 1/100 = 0.01, which allows our classifier to still assign some nonzero spam probability to messages that contain the word <em>data</em>.<a data-primary="" data-startref="NBspam13" data-type="indexterm" id="idm45635741043096"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Implementation" data-type="sect1"><div class="sect1" id="idm45635741218152">&#13;
<h1>Implementation</h1>&#13;
&#13;
<p>Now<a data-primary="Naive Bayes" data-secondary="spam filter implementation" data-type="indexterm" id="idm45635741040584"/> we have all the pieces we need to build our classifier.  First, let’s create a simple function to tokenize messages into distinct words.&#13;
We’ll first convert each message to lowercase, then use <code>re.findall</code> to extract “words” consisting of letters, numbers, and apostrophes. Finally, we’ll use <code>set</code> to get just the distinct words:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Set</code>&#13;
<code class="kn">import</code> <code class="nn">re</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Set</code><code class="p">[</code><code class="nb">str</code><code class="p">]:</code>&#13;
    <code class="n">text</code> <code class="o">=</code> <code class="n">text</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code>                         <code class="c1"># Convert to lowercase,</code>&#13;
    <code class="n">all_words</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s2">"[a-z0-9']+"</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>  <code class="c1"># extract the words, and</code>&#13;
    <code class="k">return</code> <code class="nb">set</code><code class="p">(</code><code class="n">all_words</code><code class="p">)</code>                       <code class="c1"># remove duplicates.</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">tokenize</code><code class="p">(</code><code class="s2">"Data Science is science"</code><code class="p">)</code> <code class="o">==</code> <code class="p">{</code><code class="s2">"data"</code><code class="p">,</code> <code class="s2">"science"</code><code class="p">,</code> <code class="s2">"is"</code><code class="p">}</code></pre>&#13;
&#13;
<p>We’ll also define a type for our training data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">NamedTuple</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Message</code><code class="p">(</code><code class="n">NamedTuple</code><code class="p">):</code>&#13;
    <code class="n">text</code><code class="p">:</code> <code class="nb">str</code>&#13;
    <code class="n">is_spam</code><code class="p">:</code> <code class="nb">bool</code></pre>&#13;
&#13;
<p>As our classifier needs to keep track of tokens, counts, and labels&#13;
from the training data, we’ll make it a class. Following convention,&#13;
we refer to nonspam emails as <em>ham</em> emails.</p>&#13;
&#13;
<p>The constructor will take just one parameter, the pseudocount to use when&#13;
computing probabilities. It also initializes an empty set of tokens,&#13;
counters to track how often each token is seen in spam messages and ham messages,&#13;
and counts of how many spam and ham messages it was trained on:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code><code class="p">,</code> <code class="n">Tuple</code><code class="p">,</code> <code class="n">Dict</code><code class="p">,</code> <code class="n">Iterable</code>&#13;
<code class="kn">import</code> <code class="nn">math</code>&#13;
<code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">defaultdict</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">NaiveBayesClassifier</code><code class="p">:</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">k</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">k</code> <code class="o">=</code> <code class="n">k</code>  <code class="c1"># smoothing factor</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">tokens</code><code class="p">:</code> <code class="n">Set</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="nb">set</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">token_spam_counts</code><code class="p">:</code> <code class="n">Dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="n">defaultdict</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">token_ham_counts</code><code class="p">:</code> <code class="n">Dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="n">defaultdict</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">spam_messages</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">ham_messages</code> <code class="o">=</code> <code class="mi">0</code></pre>&#13;
&#13;
<p>Next, we’ll give it a method to train it on a bunch of messages.&#13;
First, we increment the <code>spam_messages</code> and <code>ham_messages</code> counts.&#13;
Then we tokenize each message text, and for each token we increment&#13;
the <code>token_spam_counts</code> or <code>token_ham_counts</code> based on the message type:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">messages</code><code class="p">:</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Message</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="k">for</code> <code class="n">message</code> <code class="ow">in</code> <code class="n">messages</code><code class="p">:</code>&#13;
            <code class="c1"># Increment message counts</code>&#13;
            <code class="k">if</code> <code class="n">message</code><code class="o">.</code><code class="n">is_spam</code><code class="p">:</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">spam_messages</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
            <code class="k">else</code><code class="p">:</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">ham_messages</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
            <code class="c1"># Increment word counts</code>&#13;
            <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">message</code><code class="o">.</code><code class="n">text</code><code class="p">):</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">tokens</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">token</code><code class="p">)</code>&#13;
                <code class="k">if</code> <code class="n">message</code><code class="o">.</code><code class="n">is_spam</code><code class="p">:</code>&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">token_spam_counts</code><code class="p">[</code><code class="n">token</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
                <code class="k">else</code><code class="p">:</code>&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">token_ham_counts</code><code class="p">[</code><code class="n">token</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code></pre>&#13;
&#13;
<p>Ultimately we’ll want to predict <em>P</em>(spam | token).&#13;
As we saw earlier, to apply Bayes’s theorem we need to know&#13;
<em>P</em>(token | spam) and <em>P</em>(token | ham) for each token&#13;
in the vocabulary. So we’ll create a “private” helper function&#13;
to compute those:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">_probabilities</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">token</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">float</code><code class="p">,</code> <code class="nb">float</code><code class="p">]:</code>&#13;
        <code class="sd">"""returns P(token | spam) and P(token | ham)"""</code>&#13;
        <code class="n">spam</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">token_spam_counts</code><code class="p">[</code><code class="n">token</code><code class="p">]</code>&#13;
        <code class="n">ham</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">token_ham_counts</code><code class="p">[</code><code class="n">token</code><code class="p">]</code>&#13;
&#13;
        <code class="n">p_token_spam</code> <code class="o">=</code> <code class="p">(</code><code class="n">spam</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">spam_messages</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">)</code>&#13;
        <code class="n">p_token_ham</code> <code class="o">=</code> <code class="p">(</code><code class="n">ham</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">ham_messages</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">)</code>&#13;
&#13;
        <code class="k">return</code> <code class="n">p_token_spam</code><code class="p">,</code> <code class="n">p_token_ham</code></pre>&#13;
&#13;
<p>Finally, we’re ready to write our <code>predict</code> method. As mentioned earlier, rather than multiplying together lots of small probabilities, we’ll instead sum up the log probabilities:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
        <code class="n">text_tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>&#13;
        <code class="n">log_prob_if_spam</code> <code class="o">=</code> <code class="n">log_prob_if_ham</code> <code class="o">=</code> <code class="mf">0.0</code>&#13;
&#13;
        <code class="c1"># Iterate through each word in our vocabulary</code>&#13;
        <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">tokens</code><code class="p">:</code>&#13;
            <code class="n">prob_if_spam</code><code class="p">,</code> <code class="n">prob_if_ham</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_probabilities</code><code class="p">(</code><code class="n">token</code><code class="p">)</code>&#13;
&#13;
            <code class="c1"># If *token* appears in the message,</code>&#13;
            <code class="c1"># add the log probability of seeing it</code>&#13;
            <code class="k">if</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">text_tokens</code><code class="p">:</code>&#13;
                <code class="n">log_prob_if_spam</code> <code class="o">+=</code> <code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">prob_if_spam</code><code class="p">)</code>&#13;
                <code class="n">log_prob_if_ham</code> <code class="o">+=</code> <code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">prob_if_ham</code><code class="p">)</code>&#13;
&#13;
            <code class="c1"># Otherwise add the log probability of _not_ seeing it,</code>&#13;
            <code class="c1"># which is log(1 - probability of seeing it)</code>&#13;
            <code class="k">else</code><code class="p">:</code>&#13;
                <code class="n">log_prob_if_spam</code> <code class="o">+=</code> <code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="mf">1.0</code> <code class="o">-</code> <code class="n">prob_if_spam</code><code class="p">)</code>&#13;
                <code class="n">log_prob_if_ham</code> <code class="o">+=</code> <code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="mf">1.0</code> <code class="o">-</code> <code class="n">prob_if_ham</code><code class="p">)</code>&#13;
&#13;
        <code class="n">prob_if_spam</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">log_prob_if_spam</code><code class="p">)</code>&#13;
        <code class="n">prob_if_ham</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">log_prob_if_ham</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">prob_if_spam</code> <code class="o">/</code> <code class="p">(</code><code class="n">prob_if_spam</code> <code class="o">+</code> <code class="n">prob_if_ham</code><code class="p">)</code></pre>&#13;
&#13;
<p>And now we have a classifier.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Testing Our Model" data-type="sect1"><div class="sect1" id="idm45635741041624">&#13;
<h1>Testing Our Model</h1>&#13;
&#13;
<p>Let’s make<a data-primary="Naive Bayes" data-secondary="model testing" data-type="indexterm" id="idm45635740546600"/><a data-primary="unit tests" data-type="indexterm" id="idm45635740418376"/> sure our model works by writing some unit tests for it.</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">messages</code> <code class="o">=</code> <code class="p">[</code><code class="n">Message</code><code class="p">(</code><code class="s2">"spam rules"</code><code class="p">,</code> <code class="n">is_spam</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>&#13;
            <code class="n">Message</code><code class="p">(</code><code class="s2">"ham rules"</code><code class="p">,</code> <code class="n">is_spam</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>&#13;
            <code class="n">Message</code><code class="p">(</code><code class="s2">"hello ham"</code><code class="p">,</code> <code class="n">is_spam</code><code class="o">=</code><code class="bp">False</code><code class="p">)]</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">NaiveBayesClassifier</code><code class="p">(</code><code class="n">k</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">messages</code><code class="p">)</code></pre>&#13;
&#13;
<p>First, let’s check that it got the counts right:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">assert</code> <code class="n">model</code><code class="o">.</code><code class="n">tokens</code> <code class="o">==</code> <code class="p">{</code><code class="s2">"spam"</code><code class="p">,</code> <code class="s2">"ham"</code><code class="p">,</code> <code class="s2">"rules"</code><code class="p">,</code> <code class="s2">"hello"</code><code class="p">}</code>&#13;
<code class="k">assert</code> <code class="n">model</code><code class="o">.</code><code class="n">spam_messages</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">model</code><code class="o">.</code><code class="n">ham_messages</code> <code class="o">==</code> <code class="mi">2</code>&#13;
<code class="k">assert</code> <code class="n">model</code><code class="o">.</code><code class="n">token_spam_counts</code> <code class="o">==</code> <code class="p">{</code><code class="s2">"spam"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"rules"</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code>&#13;
<code class="k">assert</code> <code class="n">model</code><code class="o">.</code><code class="n">token_ham_counts</code> <code class="o">==</code> <code class="p">{</code><code class="s2">"ham"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"rules"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"hello"</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code></pre>&#13;
&#13;
<p>Now let’s make a prediction. We’ll also (laboriously) go through&#13;
our Naive Bayes logic by hand, and make sure that we get the same result:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">text</code> <code class="o">=</code> <code class="s2">"hello spam"</code>&#13;
&#13;
<code class="n">probs_if_spam</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">),</code>      <code class="c1"># "spam"  (present)</code>&#13;
    <code class="mi">1</code> <code class="o">-</code> <code class="p">(</code><code class="mi">0</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">),</code>  <code class="c1"># "ham"   (not present)</code>&#13;
    <code class="mi">1</code> <code class="o">-</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">),</code>  <code class="c1"># "rules" (not present)</code>&#13;
    <code class="p">(</code><code class="mi">0</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">)</code>       <code class="c1"># "hello" (present)</code>&#13;
<code class="p">]</code>&#13;
&#13;
<code class="n">probs_if_ham</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="p">(</code><code class="mi">0</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">),</code>      <code class="c1"># "spam"  (present)</code>&#13;
    <code class="mi">1</code> <code class="o">-</code> <code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">),</code>  <code class="c1"># "ham"   (not present)</code>&#13;
    <code class="mi">1</code> <code class="o">-</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">),</code>  <code class="c1"># "rules" (not present)</code>&#13;
    <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="mf">0.5</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">*</code> <code class="mf">0.5</code><code class="p">),</code>      <code class="c1"># "hello" (present)</code>&#13;
<code class="p">]</code>&#13;
&#13;
<code class="n">p_if_spam</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="nb">sum</code><code class="p">(</code><code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">p</code><code class="p">)</code> <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">probs_if_spam</code><code class="p">))</code>&#13;
<code class="n">p_if_ham</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="nb">sum</code><code class="p">(</code><code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">p</code><code class="p">)</code> <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">probs_if_ham</code><code class="p">))</code>&#13;
&#13;
<code class="c1"># Should be about 0.83</code>&#13;
<code class="k">assert</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">text</code><code class="p">)</code> <code class="o">==</code> <code class="n">p_if_spam</code> <code class="o">/</code> <code class="p">(</code><code class="n">p_if_spam</code> <code class="o">+</code> <code class="n">p_if_ham</code><code class="p">)</code></pre>&#13;
&#13;
<p>This test passes, so it seems like our model is doing what we think it is.&#13;
If you look at the actual probabilities, the two big drivers are that our&#13;
message contains <em>spam</em> (which our lone training spam message did)&#13;
and that it doesn’t contain <em>ham</em> (which both our training ham messages did).</p>&#13;
&#13;
<p>Now let’s try it on some real data.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Our Model" data-type="sect1"><div class="sect1" id="idm45635740223176">&#13;
<h1>Using Our Model</h1>&#13;
&#13;
<p>A<a data-primary="Naive Bayes" data-secondary="model use" data-type="indexterm" id="idm45635739952712"/><a data-primary="SpamAssassin public corpus" data-type="indexterm" id="idm45635739951704"/> popular (if somewhat old) dataset is the <a href="https://spamassassin.apache.org/old/publiccorpus/">SpamAssassin public corpus</a>.  We’ll look at the files prefixed with <em>20021010</em>.</p>&#13;
&#13;
<p>Here is a script that will download and unpack them to the directory of your choice (or you can do it manually):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">BytesIO</code>  <code class="c1"># So we can treat bytes as a file.</code>&#13;
<code class="kn">import</code> <code class="nn">requests</code>         <code class="c1"># To download the files, which</code>&#13;
<code class="kn">import</code> <code class="nn">tarfile</code>          <code class="c1"># are in .tar.bz format.</code>&#13;
&#13;
<code class="n">BASE_URL</code> <code class="o">=</code> <code class="s2">"https://spamassassin.apache.org/old/publiccorpus"</code>&#13;
<code class="n">FILES</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"20021010_easy_ham.tar.bz2"</code><code class="p">,</code>&#13;
         <code class="s2">"20021010_hard_ham.tar.bz2"</code><code class="p">,</code>&#13;
         <code class="s2">"20021010_spam.tar.bz2"</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># This is where the data will end up,</code>&#13;
<code class="c1"># in /spam, /easy_ham, and /hard_ham subdirectories.</code>&#13;
<code class="c1"># Change this to where you want the data.</code>&#13;
<code class="n">OUTPUT_DIR</code> <code class="o">=</code> <code class="s1">'spam_data'</code>&#13;
&#13;
<code class="k">for</code> <code class="n">filename</code> <code class="ow">in</code> <code class="n">FILES</code><code class="p">:</code>&#13;
    <code class="c1"># Use requests to get the file contents at each URL.</code>&#13;
    <code class="n">content</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">f</code><code class="s2">"{BASE_URL}/{filename}"</code><code class="p">)</code><code class="o">.</code><code class="n">content</code>&#13;
&#13;
    <code class="c1"># Wrap the in-memory bytes so we can use them as a "file."</code>&#13;
    <code class="n">fin</code> <code class="o">=</code> <code class="n">BytesIO</code><code class="p">(</code><code class="n">content</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># And extract all the files to the specified output dir.</code>&#13;
    <code class="k">with</code> <code class="n">tarfile</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">fileobj</code><code class="o">=</code><code class="n">fin</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s1">'r:bz2'</code><code class="p">)</code> <code class="k">as</code> <code class="n">tf</code><code class="p">:</code>&#13;
        <code class="n">tf</code><code class="o">.</code><code class="n">extractall</code><code class="p">(</code><code class="n">OUTPUT_DIR</code><code class="p">)</code></pre>&#13;
&#13;
<p>It’s possible the location of the files will change (this happened between the first and second editions of this book), in which case adjust the script accordingly.</p>&#13;
&#13;
<p>After downloading the data you should have three folders: <em>spam</em>, <em>easy_ham</em>, and <em>hard_ham</em>.  Each folder contains many emails, each contained in a single file.  To keep things <em>really</em> simple, we’ll just look at the subject lines of each email.</p>&#13;
&#13;
<p>How do we identify the subject line?  When we look through the files, they all seem to start with “Subject:”.  So we’ll look for that:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">glob</code><code class="o">,</code> <code class="nn">re</code>&#13;
&#13;
<code class="c1"># modify the path to wherever you've put the files</code>&#13;
<code class="n">path</code> <code class="o">=</code> <code class="s1">'spam_data/*/*'</code>&#13;
&#13;
<code class="n">data</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Message</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
<code class="c1"># glob.glob returns every filename that matches the wildcarded path</code>&#13;
<code class="k">for</code> <code class="n">filename</code> <code class="ow">in</code> <code class="n">glob</code><code class="o">.</code><code class="n">glob</code><code class="p">(</code><code class="n">path</code><code class="p">):</code>&#13;
    <code class="n">is_spam</code> <code class="o">=</code> <code class="s2">"ham"</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">filename</code>&#13;
&#13;
    <code class="c1"># There are some garbage characters in the emails; the errors='ignore'</code>&#13;
    <code class="c1"># skips them instead of raising an exception.</code>&#13;
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="n">errors</code><code class="o">=</code><code class="s1">'ignore'</code><code class="p">)</code> <code class="k">as</code> <code class="n">email_file</code><code class="p">:</code>&#13;
        <code class="k">for</code> <code class="n">line</code> <code class="ow">in</code> <code class="n">email_file</code><code class="p">:</code>&#13;
            <code class="k">if</code> <code class="n">line</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s2">"Subject:"</code><code class="p">):</code>&#13;
                <code class="n">subject</code> <code class="o">=</code> <code class="n">line</code><code class="o">.</code><code class="n">lstrip</code><code class="p">(</code><code class="s2">"Subject: "</code><code class="p">)</code>&#13;
                <code class="n">data</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">Message</code><code class="p">(</code><code class="n">subject</code><code class="p">,</code> <code class="n">is_spam</code><code class="p">))</code>&#13;
                <code class="k">break</code>  <code class="c1"># done with this file</code></pre>&#13;
&#13;
<p>Now we can split the data into training data and test data,&#13;
and then we’re ready to build a classifier:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.machine_learning</code> <code class="kn">import</code> <code class="n">split_data</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>      <code class="c1"># just so you get the same answers as me</code>&#13;
<code class="n">train_messages</code><code class="p">,</code> <code class="n">test_messages</code> <code class="o">=</code> <code class="n">split_data</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">)</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">NaiveBayesClassifier</code><code class="p">()</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">train_messages</code><code class="p">)</code></pre>&#13;
&#13;
<p>Let’s generate some predictions and check how our model does:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>&#13;
&#13;
<code class="n">predictions</code> <code class="o">=</code> <code class="p">[(</code><code class="n">message</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">message</code><code class="o">.</code><code class="n">text</code><code class="p">))</code>&#13;
               <code class="k">for</code> <code class="n">message</code> <code class="ow">in</code> <code class="n">test_messages</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Assume that spam_probability &gt; 0.5 corresponds to spam prediction</code>&#13;
<code class="c1"># and count the combinations of (actual is_spam, predicted is_spam)</code>&#13;
<code class="n">confusion_matrix</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">((</code><code class="n">message</code><code class="o">.</code><code class="n">is_spam</code><code class="p">,</code> <code class="n">spam_probability</code> <code class="o">&gt;</code> <code class="mf">0.5</code><code class="p">)</code>&#13;
                           <code class="k">for</code> <code class="n">message</code><code class="p">,</code> <code class="n">spam_probability</code> <code class="ow">in</code> <code class="n">predictions</code><code class="p">)</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="n">confusion_matrix</code><code class="p">)</code></pre>&#13;
&#13;
<p>This gives 84 true positives (spam classified as “spam”), 25 false positives (ham classified as “spam”), 703 true negatives (ham classified as “ham”), and 44 false negatives (spam classified as “ham”).  This means our precision is 84 / (84 + 25) = 77%, and our recall is 84 / (84 + 44) = 65%, which are not bad numbers for such a simple model. (Presumably we’d do better if we looked at more than the subject lines.)</p>&#13;
&#13;
<p>We can also inspect the model’s innards to see which words are least and most indicative of spam:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">p_spam_given_token</code><code class="p">(</code><code class="n">token</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">model</code><code class="p">:</code> <code class="n">NaiveBayesClassifier</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="c1"># We probably shouldn't call private methods, but it's for a good cause.</code>&#13;
    <code class="n">prob_if_spam</code><code class="p">,</code> <code class="n">prob_if_ham</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">_probabilities</code><code class="p">(</code><code class="n">token</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">prob_if_spam</code> <code class="o">/</code> <code class="p">(</code><code class="n">prob_if_spam</code> <code class="o">+</code> <code class="n">prob_if_ham</code><code class="p">)</code>&#13;
&#13;
<code class="n">words</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">tokens</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">t</code><code class="p">:</code> <code class="n">p_spam_given_token</code><code class="p">(</code><code class="n">t</code><code class="p">,</code> <code class="n">model</code><code class="p">))</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"spammiest_words"</code><code class="p">,</code> <code class="n">words</code><code class="p">[</code><code class="o">-</code><code class="mi">10</code><code class="p">:])</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"hammiest_words"</code><code class="p">,</code> <code class="n">words</code><code class="p">[:</code><code class="mi">10</code><code class="p">])</code></pre>&#13;
&#13;
<p>The spammiest words include things like <em>sale</em>, <em>mortgage</em>, <em>money</em>, and <em>rates</em>, whereas the hammiest words include things like&#13;
<em>spambayes</em>, <em>users</em>, <em>apt</em>, and <em>perl</em>. So that also gives us some intuitive confidence that our model is basically doing the right thing.</p>&#13;
&#13;
<p>How could we get better performance?&#13;
One obvious way would be to get more data to train on.&#13;
There are a number of ways to improve the model as well.  Here are some possibilities that you might try:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Look at the message content, not just the subject line.  You’ll have to be careful&#13;
how you deal with the message headers.</p>&#13;
</li>&#13;
<li>&#13;
<p>Our classifier takes into account every word that appears in the training set, even words that appear only once.  Modify the classifier to accept an optional <code>min_count</code> threshold and ignore tokens that don’t appear at least that many times.</p>&#13;
</li>&#13;
<li>&#13;
<p>The tokenizer has no notion of similar words (e.g., <em>cheap</em> and <em>cheapest</em>). Modify the classifier to take an optional <code>stemmer</code> function that converts words<a data-primary="equivalence classes" data-type="indexterm" id="idm45635739430568"/> to <em>equivalence classes</em> of words. For example, a really simple<a data-primary="stemmer functions" data-type="indexterm" id="idm45635739429320"/> stemmer function might be:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">drop_final_s</code><code class="p">(</code><code class="n">word</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s2">"s$"</code><code class="p">,</code> <code class="s2">""</code><code class="p">,</code> <code class="n">word</code><code class="p">)</code></pre>&#13;
&#13;
<p>Creating<a data-primary="Porter Stemmer" data-type="indexterm" id="idm45635739420216"/> a good stemmer function is hard. People frequently use the <a href="http://tartarus.org/martin/PorterStemmer/">Porter stemmer</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Although our features are all of the form “message contains word <math>&#13;
  <msub><mi>w</mi> <mi>i</mi> </msub>&#13;
</math>,” there’s no reason why this has to be the case. In our implementation, we could add extra features like “message contains a number” by creating phony tokens like <em>contains:number</em> and modifying the <code>tokenizer</code> to emit them when appropriate.<a data-primary="" data-startref="spam13" data-type="indexterm" id="idm45635739403048"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635740222584">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Paul Graham’s<a data-primary="Naive Bayes" data-secondary="resources for learning about" data-type="indexterm" id="idm45635739399880"/> articles <a href="http://www.paulgraham.com/spam.html">“A Plan for Spam”</a> and <a href="http://www.paulgraham.com/better.html">“Better Bayesian Filtering”</a> are interesting and give more insight into the ideas behind building spam filters.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://scikit-learn.org/stable/modules/naive_bayes.html">scikit-learn</a> contains a <code>BernoulliNB</code> model<a data-primary="Naive Bayes" data-secondary="tools for" data-type="indexterm" id="idm45635739395528"/><a data-primary="scikit-learn" data-type="indexterm" id="idm45635739394520"/><a data-primary="BernoulliNB model" data-type="indexterm" id="idm45635739393848"/> that implements the same Naive Bayes algorithm we implemented here, as well as other variations on the model.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>