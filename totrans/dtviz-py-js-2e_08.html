<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 5. Getting Data Off the Web with Python" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_getting_data">
<h1><span class="label">Chapter 5. </span>Getting Data Off the Web <span class="keep-together">with Python</span></h1>
<p>A fundamental part of the data visualizer’s skill set is getting the right dataset in as clean a form as possible.<a data-primary="getting data off the web using Python" data-type="indexterm" id="ix_getPy"/> Sometimes you will be given a nice, clean dataset to analyze but often you will be tasked with either finding the data and/or cleaning the data supplied.</p>
<p>And more often than not these days, getting data involves getting it off the web. There are various ways you can do this, and Python provides some great libraries that make sucking up the data easy.</p>
<p>The main ways to get data off the web are:</p>
<ul>
<li>
<p>Get a raw data file in a recognized data format (e.g., JSON or CSV) over HTTP.</p>
</li>
<li>
<p>Use a dedicated API to get the data.</p>
</li>
<li>
<p>Scrape the data by getting web pages via HTTP and parsing them locally for the required data.</p>
</li>
</ul>
<p>This chapter will deal with these ways in turn, but first let’s get acquainted with the best Python HTTP library out there: Requests.</p>
<section data-pdf-bookmark="Getting Web Data with the Requests Library" data-type="sect1"><div class="sect1" id="http_requests">
<h1>Getting Web Data with the Requests Library</h1>
<p>As we saw in <a data-type="xref" href="ch04.xhtml#chapter_webdev101">Chapter 4</a>, the files that are used by web browsers to construct web pages are communicated via the Hypertext Transfer Protocol (HTTP), first developed by <a href="https://oreil.ly/uKF5f">Tim Berners-Lee</a>. Getting web content in order to parse it for data involves making HTTP requests.<a data-primary="getting data off the web using Python" data-secondary="getting data with Requests library" data-type="indexterm" id="ix_getPyReq"/><a data-primary="HTTP" data-secondary="requests" data-seealso="Requests library" data-type="indexterm" id="idm45607789808832"/><a data-primary="Requests library" data-type="indexterm" id="ix_Reqlib"/></p>
<p>Negotiating HTTP requests is a vital part of any general-purpose language, but getting web pages with Python used to be a rather irksome affair. The venerable urllib2 library was hardly user-friendly, with a very clunky API. <a href="https://oreil.ly/6VkKZ"><em>Requests</em></a>, courtesy of Kenneth Reitz, changed that, making HTTP a relative breeze and fast establishing itself as the go-to Python HTTP library.</p>
<p>Requests is not part of the Python standard library<sup><a data-type="noteref" href="ch05.xhtml#idm45607789542496" id="idm45607789542496-marker">1</a></sup> but is part of the <a href="https://oreil.ly/LD0ee">Anaconda package</a> (see <a data-type="xref" href="ch01.xhtml#chapter_install">Chapter 1</a>). If<a data-primary="Anaconda" data-secondary="Requests library" data-type="indexterm" id="idm45607789539888"/><a data-primary="pip installer" data-secondary="installing Requests library" data-type="indexterm" id="idm45607789539040"/> you’re not using Anaconda, the following <code>pip</code> command should do the job:</p>
<pre data-code-language="bash" data-type="programlisting">$ pip install requests
Downloading/unpacking requests
...
Cleaning up...</pre>
<p>If you’re using a Python version prior to 2.7.9 (I strongly recommend using Python 3+ wherever possible), then using Requests may generate some <a href="https://oreil.ly/8D08s">Secure Sockets Layer (SSL) warnings</a>.<a data-primary="Requests library" data-secondary="installing" data-type="indexterm" id="idm45607789529616"/> Upgrading to newer SSL libraries should fix this:<sup><a data-type="noteref" href="ch05.xhtml#idm45607789528976" id="idm45607789528976-marker">2</a></sup></p>
<pre data-code-language="bash" data-type="programlisting">$ pip install --upgrade ndg-httpsclient</pre>
<p>Now that you have Requests installed, you’re ready to perform the first task mentioned at the beginning of this chapter and grab some raw data files off the web.</p>
</div></section>
<section data-pdf-bookmark="Getting Data Files with Requests" data-type="sect1"><div class="sect1" id="idm45607789534096">
<h1>Getting Data Files with Requests</h1>
<p>A Python interpreter session is a good way <a data-primary="getting data off the web using Python" data-secondary="getting data with Requests library" data-tertiary="data files" data-type="indexterm" id="ix_getPyReqdafile"/><a data-primary="Requests library" data-secondary="getting data files with" data-type="indexterm" id="ix_Reqlibdafile"/>to put Requests through its paces, so find a friendly local command line, fire up IPython, and import <code>requests</code>:</p>
<pre data-type="programlisting">$ ipython
Python 3.8.9 (default, Apr  3 2021, 01:02:10)
...

In [1]: import requests</pre>
<p>To demonstrate, let’s use the library to download a Wikipedia page. We use the  Requests library’s <code>get</code> method to get the page and, by convention, assign the result to a <code>response</code> object:<a data-primary="responses (HTTP)" data-type="indexterm" id="idm45607789522656"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>\
<code class="s2">"https://en.wikipedia.org/wiki/Nobel_Prize"</code><code class="p">)</code></pre>
<p>Let’s use Python’s <a href="https://oreil.ly/CrJ8h"><code>dir</code></a> method to get a list of the <code>response</code> object’s attributes:</p>
<pre class="pagebreak-before" data-code-language="python" data-type="programlisting"><code class="nb">dir</code><code class="p">(</code><code class="n">response</code><code class="p">)</code></pre>
<pre data-type="programlisting">Out:
...
 ['content',
 'cookies',
 'elapsed',
 'encoding',
 'headers',
 ...
 'iter_content',
 'iter_lines',
 'json',
 'links',
 ...
 'status_code',
 'text',
 'url']</pre>
<p>Most of these attributes are self-explanatory and together provide a lot of information about the HTTP response generated.<a data-primary="HTTP" data-secondary="responses" data-type="indexterm" id="idm45607789474784"/> You’ll use a small subset of these attributes generally. Firstly, let’s check the status of the response:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">response</code><code class="o">.</code><code class="n">status_code</code>
<code class="n">Out</code><code class="p">:</code> <code class="mi">200</code></pre>
<p>As all good minimal web developers know, 200 is the <a href="https://oreil.ly/ucEoo">HTTP status code</a> for OK, indicating a successful transaction. <a data-primary="HTTP" data-secondary="status codes" data-type="indexterm" id="idm45607789442960"/><a data-primary="status codes (HTTP)" data-type="indexterm" id="idm45607789442080"/>Other than 200, the most common codes are:</p>
<dl>
<dt>401 (Unauthorized)</dt>
<dd>
<p>Attempting unauthorized access</p>
</dd>
<dt>400 (Bad Request)</dt>
<dd>
<p>Trying to access the web server incorrectly</p>
</dd>
<dt>403 (Forbidden)</dt>
<dd>
<p>Similar to 401 but no login opportunity was available</p>
</dd>
<dt>404 (Not Found)</dt>
<dd>
<p>Trying to access a web page that doesn’t exist</p>
</dd>
<dt>500 (Internal Server Error)</dt>
<dd>
<p>A general-purpose, catchall error</p>
</dd>
</dl>
<p>So, for example, if we made a spelling mistake with our request, asking to see the <code>SNoble_Prize</code> page, we’d get a 404 (Not Found) error:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">not_found_response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>\
<code class="s2">"http://en.wikipedia.org/wiki/SNobel_Prize"</code><code class="p">)</code>
<code class="n">not_found_response</code><code class="o">.</code><code class="n">status_code</code>
<code class="n">Out</code><code class="p">:</code> <code class="mi">404</code></pre>
<p>With our 200 OK response, from the correctly spelled request, let’s look at some of the info returned.<a data-primary="responses (HTTP)" data-secondary="information from example 200 OK response" data-type="indexterm" id="idm45607789388976"/><a data-primary="headers property (response object)" data-type="indexterm" id="idm45607789371520"/> A quick overview can be had with the <code>headers</code> property:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">response</code><code class="o">.</code><code class="n">headers</code></pre>
<pre data-type="programlisting">Out: {
  'date': 'Sat, 23 Oct 2021 23:58:49 GMT',
  'server': 'mw1435.eqiad.wmnet',
  'content-encoding': 'gzip', ...
  'last-modified': 'Sat, 23 Oct 2021 17:14:09 GMT', ...
  'content-type': 'text/html; charset=UTF-8'...
  'content-length': '88959'
  }</pre>
<p>This shows, among other things, that the page returned was gzip-encoded and 87 KB in size with <code>content-type</code> of <code>text/html</code>, encoded with Unicode UTF-8.<a data-primary="content-type of text/html" data-type="indexterm" id="idm45607789367424"/><a data-primary="HTML" data-secondary="returned from HTTP request" data-type="indexterm" id="idm45607789366816"/></p>
<p>Since we know text has been returned, we can use the <code>text</code> property of <a data-primary="text" data-secondary="returned from HTTP request" data-type="indexterm" id="idm45607789365200"/>the response to see what it is:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">response</code><code class="o">.</code><code class="n">text</code>
<code class="c1">#Out: u'&lt;!DOCTYPE html&gt;\n&lt;html lang="en"</code>
<code class="c1">#dir="ltr" class="client-nojs"&gt;\n&lt;head&gt;\n&lt;meta charset="UTF-8"</code>
<code class="c1">#/&gt;\n&lt;title&gt;Nobel Prize - Wikipedia, the free</code>
<code class="c1">#encyclopedia&lt;/title&gt;\n&lt;script&gt;document.documentElement... =</code></pre>
<p>This shows that we do indeed have our Wikipedia HTML page, with some inline JavaScript. As we’ll see in <a data-type="xref" href="#get_data_scraping">“Scraping Data”</a>, in order to make sense of this content, we’ll need a parser to read the HTML and provide the content blocks.<a data-primary="parsers" data-secondary="reading HTML text returned from HTTP response" data-type="indexterm" id="idm45607789334640"/></p>
<p>Now that we’ve grabbed a raw page off the web, let’s see how to use Requests to consume a web data API.<a data-primary="Requests library" data-secondary="getting data files with" data-startref="ix_Reqlibdafile" data-type="indexterm" id="idm45607789333280"/><a data-primary="getting data off the web using Python" data-secondary="getting data with Requests library" data-startref="ix_getPyReqdafile" data-tertiary="data files" data-type="indexterm" id="idm45607789318160"/><a data-primary="getting data off the web using Python" data-secondary="getting data with Requests library" data-startref="ix_getPyReq" data-type="indexterm" id="idm45607789316736"/><a data-primary="Requests library" data-startref="ix_Reqlib" data-type="indexterm" id="idm45607789315584"/></p>
</div></section>
<section data-pdf-bookmark="Using Python to Consume Data from a Web API" data-type="sect1"><div class="sect1" id="getting_data_webapis">
<h1>Using Python to Consume Data from a Web API</h1>
<p>If the data file you need isn’t on the web, there may well be an Application Programming Interface (API) serving the data you need.<a data-primary="getting data off the web using Python" data-secondary="consuming data from web APIs using Python" data-type="indexterm" id="ix_getPyAPI"/><a data-primary="web APIs" data-secondary="using Python to consume data from" data-type="indexterm" id="ix_webAPIPy"/> Using this will involve making a request to the appropriate server to retrieve your data in a fixed format or one you get to specify in the request.</p>
<p>The most popular data formats for web APIs are JSON and XML, though a number of esoteric formats exist. <a data-primary="data formats" data-secondary="for web APIs" data-secondary-sortas="web" data-type="indexterm" id="idm45607789288208"/><a data-primary="JSON" data-secondary="data format for web APIs" data-type="indexterm" id="idm45607789287056"/><a data-primary="XML" data-type="indexterm" id="idm45607789286144"/>For the purposes of the JavaScripting data visualizer, JavaScript Object Notation (JSON) is obviously preferred (see <a data-type="xref" href="ch04.xhtml#sect_data">“Data”</a>). Lucky for us, it is also starting to predominate.</p>
<p>There are different approaches to creating a web API, and for a few years there was a little war of the architectures among the three main types of APIs inhabiting the<a data-primary="RESTful APIs" data-type="indexterm" id="idm45607789284032"/> web:</p>
<dl>
<dt><a href="https://oreil.ly/ujgdJ">REST</a></dt>
<dd>
<p>Short for REpresentational State Transfer, using a combination of HTTP verbs (GET, POST, etc.) and Uniform Resource Identifiers (URIs; e.g., <em>/user/kyran</em>) to access, create, and adapt data.</p>
</dd>
<dt><a href="https://oreil.ly/ZMQvW">XML-RPC</a></dt>
<dd>
<p>A remote procedure call (RPC) protocol using XML encoding and HTTP transport.<a data-primary="XML-RPC" data-type="indexterm" id="idm45607789278816"/></p>
</dd>
<dt><a href="https://oreil.ly/l5LVL">SOAP</a></dt>
<dd>
<p>Short for Simple Object Access Protocol, using XML and HTTP.<a data-primary="SOAP APIs" data-type="indexterm" id="idm45607789276432"/></p>
</dd>
</dl>
<p>This battle seems to be resolving in a victory for <a href="https://oreil.ly/apc1l">RESTful APIs</a>, and this is a very good thing. Quite apart from RESTful APIs being more elegant, and easier to use and implement (see <a data-type="xref" href="ch13.xhtml#chapter_delivery_restful">Chapter 13</a>), some standardization here makes it much more likely that you will recognize and quickly adapt to a new API that comes your way. Ideally, you will be able to reuse existing code. There is a new player on the scene in the form of <a href="https://oreil.ly/JUGVS">GraphQL</a>, which bills itself as a better REST, but as a datavizzer you’re far more likely to be consuming conventional RESTful APIs.<a data-primary="GraphQL" data-type="indexterm" id="idm45607789273072"/></p>
<p>Most access and manipulation of remote data can be summed up by the acronym CRUD (create, retrieve, update, delete), originally coined to describe all the major functions implemented in relational databases.<a data-primary="POST method (HTTP)" data-type="indexterm" id="idm45607789271984"/><a data-primary="GET method (HTTP)" data-type="indexterm" id="idm45607789271280"/><a data-primary="PUT method (HTTP)" data-type="indexterm" id="idm45607789270608"/><a data-primary="DELETE method (HTTP)" data-type="indexterm" id="idm45607789269936"/><a data-primary="CRUD (create, retrieve, update, delete)" data-type="indexterm" id="idm45607789269264"/><a data-primary="HTTP" data-secondary="POST, GET, PUT, and DELETE methods" data-type="indexterm" id="idm45607789268624"/> HTTP provides CRUD counterparts with the POST, GET, PUT, and DELETE verbs and the REST abstraction builds on this use of these verbs, acting on a <a href="https://oreil.ly/xmX1k">Universal Resource Identifier (URI)</a>.</p>
<p>Discussions about what is and isn’t a proper RESTful interface can get quite involved,
but essentially the URI (e.g., <em>https://example.com/api/items/2</em>) should contain all the information required in order to perform a CRUD operation. The particular operation (e.g., GET or DELETE) is specified by the HTTP verb. This excludes architectures such as SOAP, which place stateful information in metadata on the requests header. Imagine the URI as the virtual address of the data and CRUD as all the operations you can perform on it.</p>
<p>As data visualizers keen to lay our hands on some interesting datasets, we are avid consumers here, so our HTTP verb of choice is GET, and the examples that follow will focus on the fetching of data with various well-known web APIs. Hopefully, some patterns will emerge.</p>
<p>Although the two constraints of stateless URIs and the use of the CRUD verbs is a nice constraint on the shape of RESTful APIs, there still manage to be many variants on the theme.</p>
<section data-pdf-bookmark="Consuming a RESTful Web API with Requests" data-type="sect2"><div class="sect2" id="idm45607789264912">
<h2>Consuming a RESTful Web API with Requests</h2>
<p>Requests has a fair number of bells and whistles based around the main HTTP request verbs. <a data-primary="web APIs" data-secondary="using Python to consume data from" data-tertiary="consuming RESTful API using Requests" data-type="indexterm" id="ix_webAPIPyREST"/><a data-primary="RESTful APIs" data-secondary="consuming using Requests library" data-type="indexterm" id="ix_RESTAPIReq"/><a data-primary="Requests library" data-secondary="consuming RESTful APIs with" data-type="indexterm" id="ix_ReqlibAPI"/>For a good overview, see <a href="https://oreil.ly/Bp8VG">the Requests quickstart</a>. For the purposes of getting data, you’ll use GET and POST pretty much exclusively, with GET being by a long way the most used verb. <a data-primary="POST method (HTTP)" data-type="indexterm" id="idm45607789258160"/><a data-primary="GET method (HTTP)" data-type="indexterm" id="idm45607789257456"/>POST allows you to emulate web forms, including login details, field values, etc. in the request. For those occasions where you find yourself driving a web form with, for example, lots of options selectors, Requests makes automation with POST easy. GET covers pretty much everything else, including the ubiquitous
RESTful APIs, which provide an increasing amount of the well-formed data available on the web.</p>
<p>Let’s look at a more complicated use of Requests, getting a URL with arguments.<a data-primary="Requests library" data-secondary="consuming RESTful APIs with" data-tertiary="getting URL with arguments, OECD API" data-type="indexterm" id="idm45607789256048"/> The <a href="https://oreil.ly/QAj3A">Organisation for Economic Cooperation and Development (OECD)</a> provides some <a href="https://data.oecd.org">useful datasets on its site</a>. <a data-primary="URL, getting with arguments using Request" data-type="indexterm" id="idm45607789253264"/><a data-primary="OECD (Organisation for Economic Cooperation and Development) web API" data-type="indexterm" id="idm45607789252592"/>These datasets provide mainly economic measures and statistics for the member countries of the OECD, and such data can form the basis of many interesting visualizations. The OECD provides a few of its own, such as one <a href="https://oreil.ly/aFmUv">allowing you to compare your country</a> with others in the OECD.</p>
<p>The OECD web API is described <a href="https://oreil.ly/f5VDc">in this documentation</a>, and queries are constructed with the dataset name (dsname) and some dot-separated dimensions, each of which can be a number of <code>+</code> separated values. The URL can also take standard HTTP parameters initiated by a <code>?</code> and separated by <code>&amp;</code>:</p>
<pre data-type="programlisting">&lt;root_url&gt;/&lt;dsname&gt;/&lt;dim 1&gt;.&lt;dim 2&gt;...&lt;dim n&gt;
/all?param1=foo&amp;param2=baa..
&lt;dim 1&gt; = 'AUS'+'AUT'+'BEL'...</pre>
<p>So the following is a valid URL:</p>
<pre data-type="programlisting">http://stats.oecd.org/sdmx-json/data/QNA   <a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a>
    /AUS+AUT.GDP+B1_GE.CUR+VOBARSA.Q       <a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-2" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a>
    /all?startTime=2009-Q2&amp;endTime=2011-Q4 <a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-3" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Specifies the QNA (Quarterly National Accounts) dataset.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-2" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Four dimensions, by location, subject, measure, and frequency.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-3" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Data from the second quarter of 2009 to the fourth quarter of 2011.</p></dd>
</dl>
<p>Let’s construct a little Python function to query the OECD’s API (<a data-type="xref" href="#oecd_request">Example 5-1</a>).</p>
<div class="pagebreak-before less_space" data-type="example" id="oecd_request">
<h5><span class="label">Example 5-1. </span>Making a URL for the OECD API</h5>
<pre class="less_space" data-code-language="python" data-type="programlisting"><code class="n">OECD_ROOT_URL</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">http://stats.oecd.org/sdmx-json/data</code><code class="s1">'</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">make_OECD_request</code><code class="p">(</code><code class="n">dsname</code><code class="p">,</code><code> </code><code class="n">dimensions</code><code class="p">,</code><code> </code><code class="n">params</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code><code> </code><code>\
</code><code class="n">root_dir</code><code class="o">=</code><code class="n">OECD_ROOT_URL</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">""" Make a URL for the OECD API and return a response """</code><code>
</code><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">params</code><code class="p">:</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>        </code><code class="n">params</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="p">}</code><code>
</code><code>
</code><code>    </code><code class="n">dim_args</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">+</code><code class="s1">'</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">d</code><code class="p">)</code><code> </code><code class="k">for</code><code> </code><code class="n">d</code><code> </code><code class="ow">in</code><code> </code><code class="n">dimensions</code><code class="p">]</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-2" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>    </code><code class="n">dim_str</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">.</code><code class="s1">'</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">dim_args</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">url</code><code> </code><code class="o">=</code><code> </code><code class="n">root_dir</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="s1">/</code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="n">dsname</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="s1">/</code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="n">dim_str</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="s1">/all</code><code class="s1">'</code><code>
</code><code>    </code><code class="nb">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">Requesting URL: </code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="n">url</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">,</code><code> </code><code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">)</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-3" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>You shouldn’t use mutable values, such as <code>{}</code>, for Python function defaults. See <a href="https://oreil.ly/Yv6bX">this Python guide</a> for an explanation of this gotcha.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-2" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>We first use a Python list comprehension and the <code>join</code> method to  create a list of dimensions, with members concatenated with plus signs (e.g., [<em>USA+AUS</em>, …​ ]).  <code>join</code> is then used again to concatenate the members of <code>dim_str</code> with periods.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-3" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Note that <code>requests</code>’ <code>get</code> can take a parameter dictionary as its second argument, using it to make the URL query string.</p></dd>
</dl></div>
<p>We can use this function like so, to grab economic data for the USA and Australia from 2009 to 2010:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">response</code> <code class="o">=</code> <code class="n">make_OECD_request</code><code class="p">(</code><code class="s1">'QNA'</code><code class="p">,</code>
    <code class="p">((</code><code class="s1">'USA'</code><code class="p">,</code> <code class="s1">'AUS'</code><code class="p">),(</code><code class="s1">'GDP'</code><code class="p">,</code> <code class="s1">'B1_GE'</code><code class="p">),(</code><code class="s1">'CUR'</code><code class="p">,</code> <code class="s1">'VOBARSA'</code><code class="p">),</code> <code class="p">(</code><code class="s1">'Q'</code><code class="p">)),</code>
    <code class="p">{</code><code class="s1">'startTime'</code><code class="p">:</code><code class="s1">'2009-Q1'</code><code class="p">,</code> <code class="s1">'endTime'</code><code class="p">:</code><code class="s1">'2010-Q1'</code><code class="p">})</code></pre>
<pre data-type="programlisting">Requesting URL: http://stats.oecd.org/sdmx-json/data/QNA/
    USA+AUS.GDP+B1_GE.CUR+VOBARSA.Q/all</pre>
<p>Now, to look at the data, we just check that the response is OK and have a look at the dictionary keys:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">if</code> <code class="n">response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">==</code> <code class="mi">200</code><code class="p">:</code>
   <code class="n">json</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()</code>
   <code class="n">json</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code>
<code class="n">Out</code><code class="p">:</code> <code class="p">[</code><code class="sa">u</code><code class="s1">'header'</code><code class="p">,</code> <code class="sa">u</code><code class="s1">'dataSets'</code><code class="p">,</code> <code class="sa">u</code><code class="s1">'structure'</code><code class="p">]</code></pre>
<p>The resulting JSON data is in the <a href="https://oreil.ly/HeE7G">SDMX format</a>, designed to facilitate the communication of statistical data.<a data-primary="SDMX format" data-type="indexterm" id="idm45607788941232"/> It’s not the most intuitive format around, but it’s often the case that datasets have a less than ideal structure. The good news is that Python is a great language for knocking data into shape. For Python’s <a href="https://pandas.pydata.org">pandas library</a> (see <a data-type="xref" href="ch08.xhtml#chapter_intro_to_pandas">Chapter 8</a>), there is <a href="https://oreil.ly/2PKxZ">pandaSDMX</a>, which currently handles the XML-based format.<a data-primary="pandas" data-type="indexterm" id="idm45607788938352"/></p>
<p>The OECD API is essentially RESTful with all of the query being contained in the URL and the HTTP verb GET specifying a fetch operation. If a specialized Python library isn’t available to use the API (e.g., Tweepy for Twitter), then you’ll probably end up writing something like <a data-type="xref" href="#oecd_request">Example 5-1</a>. Requests is a very friendly, well-designed library and can cope with pretty much all the manipulations required to use a web API.<a data-primary="web APIs" data-secondary="using Python to consume data from" data-startref="ix_webAPIPyREST" data-tertiary="consuming RESTful API using Requests" data-type="indexterm" id="idm45607788936560"/><a data-primary="RESTful APIs" data-secondary="consuming using Requests library" data-startref="ix_RESTAPIReq" data-type="indexterm" id="idm45607788929616"/><a data-primary="Requests library" data-secondary="consuming RESTful API with" data-startref="ix_ReqlibAPI" data-type="indexterm" id="idm45607788928528"/></p>
</div></section>
<section data-pdf-bookmark="Getting Country Data for the Nobel Dataviz" data-type="sect2"><div class="sect2" id="country_data">
<h2>Getting Country Data for the Nobel Dataviz</h2>
<p>There are some national statistics that will come in handy for the Nobel Prize visualization we’re using our toolchain to build.<a data-primary="Requests library" data-secondary="consuming RESTful APIs with" data-tertiary="getting country data from OECD API" data-type="indexterm" id="idm45607788925632"/> Population sizes, three-letter international codes (e.g., GDR, USA), and geographic centers are potentially useful when you are visualizing an international prize and its distribution. <a href="https://restcountries.com">REST countries</a> is a handy RESTful web resource with various international stats. <a data-primary="REST countries" data-type="indexterm" id="idm45607788923792"/>Let’s use it to grab some data.</p>
<p>Requests to REST countries take the following form:</p>
<pre data-type="programlisting">https://restcountries.com/v3.1/&lt;field&gt;/&lt;name&gt;?&lt;params&gt;</pre>
<p>As with the OECD API (see <a data-type="xref" href="#oecd_request">Example 5-1</a>), we can make a simple calling function to allow easy access to the API’s data, like so:</p>
<pre class="less_space" data-code-language="python" data-type="programlisting"><code class="n">REST_EU_ROOT_URL</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">https://restcountries.com/v3.1</code><code class="s2">"</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">REST_country_request</code><code class="p">(</code><code class="n">field</code><code class="o">=</code><code class="s1">'</code><code class="s1">all</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">name</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code><code> </code><code class="n">params</code><code class="o">=</code><code class="kc">None</code><code class="p">)</code><code class="p">:</code><code>
</code><code>
</code><code>    </code><code class="n">headers</code><code class="o">=</code><code class="p">{</code><code class="s1">'</code><code class="s1">User-Agent</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Mozilla/5.0</code><code class="s1">'</code><code class="p">}</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">params</code><code class="p">:</code><code>
</code><code>        </code><code class="n">params</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="p">}</code><code>
</code><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="n">field</code><code> </code><code class="o">==</code><code> </code><code class="s1">'</code><code class="s1">all</code><code class="s1">'</code><code class="p">:</code><code>
</code><code>         </code><code class="n">response</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">REST_EU_ROOT_URL</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="s1">/all</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>         </code><code class="k">return</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">url</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="si">%s</code><code class="s1">/</code><code class="si">%s</code><code class="s1">/</code><code class="si">%s</code><code class="s1">'</code><code class="o">%</code><code class="p">(</code><code class="n">REST_EU_ROOT_URL</code><code class="p">,</code><code> </code><code class="n">field</code><code class="p">,</code><code> </code><code class="n">name</code><code class="p">)</code><code>
</code><code>    </code><code class="nb">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">Requesting URL: </code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="n">url</code><code class="p">)</code><code>
</code><code>    </code><code class="n">response</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">,</code><code> </code><code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">,</code><code> </code><code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code> </code><code class="o">==</code><code> </code><code class="mi">200</code><code class="p">:</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-2" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>        </code><code class="k">raise</code><code> </code><code class="ne">Exception</code><code class="p">(</code><code class="s1">'</code><code class="s1">Request failed with status code </code><code class="s1">'</code><code> </code><code>\
</code><code>        </code><code class="o">+</code><code> </code><code class="nb">str</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code>     </code><code class="k">return</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="p">)</code><code> </code><code class="c1"># JSON encoded data</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>It’s usually a good idea to specify a valid <code>User-Agent</code> in the header of your request.<a data-primary="User-Agent in request headers" data-type="indexterm" id="idm45607788715376"/> Some sites will reject the request <span class="keep-together">otherwise</span>.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-2" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Before returning the response, make sure it has an OK (200) HTTP code; otherwise, raise an exception with a helpful <span class="keep-together">message</span>.</p></dd>
</dl>
<p>With the <code>REST_country_request</code> function in hand, let’s get a list of all the countries using the US dollar as currency:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">response</code> <code class="o">=</code> <code class="n">REST_country_request</code><code class="p">(</code><code class="s1">'currency'</code><code class="p">,</code> <code class="s1">'usd'</code><code class="p">)</code>
<code class="n">response</code>
<code class="n">Out</code><code class="p">:</code>
<code class="p">[{</code><code class="sa">u</code><code class="s1">'alpha2Code'</code><code class="p">:</code> <code class="sa">u</code><code class="s1">'AS'</code><code class="p">,</code>
  <code class="sa">u</code><code class="s1">'alpha3Code'</code><code class="p">:</code> <code class="sa">u</code><code class="s1">'ASM'</code><code class="p">,</code>
  <code class="sa">u</code><code class="s1">'altSpellings'</code><code class="p">:</code> <code class="p">[</code><code class="sa">u</code><code class="s1">'AS'</code><code class="p">,</code>
  <code class="o">...</code>
  <code class="sa">u</code><code class="s1">'capital'</code><code class="p">:</code> <code class="sa">u</code><code class="s1">'Pago Pago'</code><code class="p">,</code>
  <code class="sa">u</code><code class="s1">'currencies'</code><code class="p">:</code> <code class="p">[</code><code class="sa">u</code><code class="s1">'USD'</code><code class="p">],</code>
  <code class="sa">u</code><code class="s1">'demonym'</code><code class="p">:</code> <code class="sa">u</code><code class="s1">'American Samoan'</code><code class="p">,</code>
  <code class="o">...</code>
  <code class="sa">u</code><code class="s1">'latlng'</code><code class="p">:</code> <code class="p">[</code><code class="mf">12.15</code><code class="p">,</code> <code class="o">-</code><code class="mf">68.266667</code><code class="p">],</code>
  <code class="sa">u</code><code class="s1">'name'</code><code class="p">:</code> <code class="sa">u</code><code class="s1">'Bonaire'</code><code class="p">,</code>
  <code class="o">...</code>
  <code class="sa">u</code><code class="s1">'name'</code><code class="p">:</code> <code class="sa">u</code><code class="s1">'British Indian Ocean Territory'</code><code class="p">,</code>
  <code class="o">...</code>
  <code class="sa">u</code><code class="s1">'name'</code><code class="p">:</code> <code class="sa">u</code><code class="s1">'United States Minor Outlying Islands'</code><code class="p">,</code>
  <code class="o">...</code> <code class="p">]}]</code></pre>
<p>The full dataset at REST countries is pretty small, so for convenience we’ll store a copy as a JSON file. We’ll be using this in later chapters in both exploratory and presentational dataviz:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>

<code class="n">country_data</code> <code class="o">=</code> <code class="n">REST_country_request</code><code class="p">()</code> <code class="c1"># all world data</code>

<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s1">'data/world_country_data.json'</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">)</code> <code class="k">as</code> <code class="n">json_file</code><code class="p">:</code>
    <code class="n">json</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="n">country_data</code><code class="p">,</code> <code class="n">json_file</code><code class="p">)</code></pre>
<p>Now that we’ve rolled a couple of our own API consumers, let’s take a look at some dedicated libraries that wrap some of the larger web APIs in an easy-to-use form.<a data-primary="getting data off the web using Python" data-secondary="consuming data from web APIs using Python" data-startref="ix_getPyAPI" data-type="indexterm" id="idm45607788550880"/><a data-primary="web APIs" data-secondary="using Python to consume data from" data-startref="ix_webAPIPy" data-type="indexterm" id="idm45607788518720"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Using Libraries to Access Web APIs" data-type="sect1"><div class="sect1" id="idm45607789314512">
<h1>Using Libraries to Access Web APIs</h1>
<p>Requests is capable of negotiating with pretty much <a data-primary="web APIs" data-secondary="using libraries to access" data-type="indexterm" id="ix_webAPIlib"/><a data-primary="getting data off the web using Python" data-secondary="using libraries to access web APIs" data-type="indexterm" id="ix_getPyAPIlib"/><a data-primary="wrapper libraries" data-type="indexterm" id="idm45607788514064"/>all web APIs, but as the APIs start adding authentication and the data structures become more complicated, a good wrapper library can save a lot of hassle and reduce the tedious bookkeeping. In this section, I’ll cover a couple of the more popular <a href="https://oreil.ly/DBrZ8">wrapper libraries</a> to give you a feel for the workflow and some useful starting points.</p>
<section data-pdf-bookmark="Using Google Spreadsheets" data-type="sect2"><div class="sect2" id="idm45607788512480">
<h2>Using Google Spreadsheets</h2>
<p>It’s becoming more common these days to have live datasets <em>in the cloud</em>. <a data-primary="Google spreadsheets" data-type="indexterm" id="ix_Goosprd"/><a data-primary="web APIs" data-secondary="using libraries to access" data-tertiary="Google spreadsheets" data-type="indexterm" id="ix_webAPIlibGs"/>So, for example, you might find yourself required to visualize aspects of a Google spreadsheet that is the shared data pool for a group. My preference is to get this data out of the Google-plex and into pandas to start exploring it (see <a data-type="xref" href="ch11.xhtml#chapter_pandas_exploring">Chapter 11</a>), but a good library will let you access and adapt the data <em>in place</em>, negotiating the web traffic as required.</p>
<p><a href="https://oreil.ly/DNKYT"><em>gspread</em></a> is the best known Python library for accessing Google spreadsheets and makes doing so a relative breeze.<a data-primary="gspread library" data-type="indexterm" id="ix_gsprd"/></p>
<p>You’ll need  <a href="https://oreil.ly/z3u6y">OAuth 2.0</a> credentials to use the API.<sup><a data-type="noteref" href="ch05.xhtml#idm45607788534000" id="idm45607788534000-marker">3</a></sup> The most up-to-date guide can be found on the <a href="https://oreil.ly/tnO3b">Google Developers site</a>. Following those instructions should provide a JSON file containing your private key.</p>
<p>You’ll need to install <em>gspread</em> and the latest <em>google-auth</em> client library. Here’s how to do it with <code>pip</code>:</p>
<pre data-type="programlisting">$ pip install gspread
$ pip install --upgrade google-auth</pre>
<p>Depending on your system, you may also need pyOpenSSL:</p>
<pre data-type="programlisting">$ pip install PyOpenSSL</pre>
<p>Read <a href="https://oreil.ly/1xAPm">the docs</a> for more details and troubleshooting.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Google’s API assumes that the spreadsheets you are trying to access are owned or shared by your API account, not your personal one. The email address to share the spreadsheet with is available at your <a href="https://oreil.ly/z5KyM">Google developers console</a> and in the JSON credentials key needed to use the API. It should look something like <em>account-1@My Project…​iam.gserviceaccount.com</em>.</p>
</div>
<p>With those libraries installed, you should be able to access any of your spreadsheets with just  a few lines. I’m using the <a href="https://oreil.ly/AAj9X">Microbe-scope spreadsheet</a>. <a data-type="xref" href="#gspread_access">Example 5-2</a> shows how to load the spreadsheet.</p>
<div class="pagebreak-before less_space" data-type="example" id="gspread_access">
<h5><span class="label">Example 5-2. </span>Opening a Google spreadsheet</h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code><code> </code><code class="nn">gspread</code><code>
</code><code>
</code><code class="n">gc</code><code> </code><code class="o">=</code><code> </code><code class="n">gspread</code><code class="o">.</code><code class="n">service_account</code><code class="p">(</code><code>\
</code><code>                   </code><code class="n">filename</code><code class="o">=</code><code class="s1">'</code><code class="s1">data/google_credentials.json</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code class="n">ss</code><code> </code><code class="o">=</code><code> </code><code class="n">gc</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">Microbe-scope</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-2" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>The JSON credentials file is the one provided by Google services, usually of the form <em>My Project-b8ab5e38fd68.json</em>.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-2" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Here we’re opening the spreadsheet by name. Alternatives are <code>open_by_url</code> or <code>open_by_id</code>. See <a href="https://oreil.ly/sa4sa">the <code>gspread</code> documentation</a> for details.</p></dd>
</dl></div>
<p>Now that we’ve got our spreadsheet, we can see the worksheets it contains:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">ss</code><code class="o">.</code><code class="n">worksheets</code><code class="p">()</code>
<code class="n">Out</code><code class="p">:</code>
<code class="p">[</code><code class="o">&lt;</code><code class="n">Worksheet</code> <code class="s1">'bugs'</code> <code class="nb">id</code><code class="p">:</code><code class="mi">0</code><code class="o">&gt;</code><code class="p">,</code>
 <code class="o">&lt;</code><code class="n">Worksheet</code> <code class="s1">'outrageous facts'</code> <code class="nb">id</code><code class="p">:</code><code class="mi">430583748</code><code class="o">&gt;</code><code class="p">,</code>
 <code class="o">&lt;</code><code class="n">Worksheet</code> <code class="s1">'physicians per 1,000'</code> <code class="nb">id</code><code class="p">:</code><code class="mi">1268911119</code><code class="o">&gt;</code><code class="p">,</code>
 <code class="o">&lt;</code><code class="n">Worksheet</code> <code class="s1">'amends'</code> <code class="nb">id</code><code class="p">:</code><code class="mi">1001992659</code><code class="o">&gt;</code><code class="p">]</code>

<code class="n">ws</code> <code class="o">=</code> <code class="n">ss</code><code class="o">.</code><code class="n">worksheet</code><code class="p">(</code><code class="s1">'bugs'</code><code class="p">)</code></pre>
<p>With the worksheet <code>bugs</code> selected from the spreadsheet, <code>gspread</code> allows you to access and change  column, row, and cell values (assuming the sheet isn’t read-only). So we can get the values in the second column with the <code>col_values</code> command:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">ws</code><code class="o">.</code><code class="n">col_values</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">Out</code><code class="p">:</code> <code class="p">[</code><code class="kc">None</code><code class="p">,</code>
 <code class="s1">'grey = not plotted'</code><code class="p">,</code>
 <code class="s1">'Anthrax (untreated)'</code><code class="p">,</code>
 <code class="s1">'Bird Flu (H5N1)'</code><code class="p">,</code>
 <code class="s1">'Bubonic Plague (untreated)'</code><code class="p">,</code>
 <code class="s1">'C.Difficile'</code><code class="p">,</code>
 <code class="s1">'Campylobacter'</code><code class="p">,</code>
 <code class="s1">'Chicken Pox'</code><code class="p">,</code>
 <code class="s1">'Cholera'</code><code class="p">,</code><code class="o">...</code><code class="p">]</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If you get a <code>BadStatusLine</code> error while accessing a Google spreadsheet with <code>gspread</code>, it is probably because the session has expired. Reopening the spreadsheet should get things working again. This <a href="https://oreil.ly/xTGg9">outstanding <code>gspread</code> issue</a> provides more information.</p>
</div>
<p>Although you can use <em>gspread</em>’s API to plot directly, using a plot library like Matplotlib, I prefer to send the whole sheet to pandas, Python’s powerhouse programmatic spreadsheet. <a data-primary="pandas" data-secondary="using with gspread data" data-type="indexterm" id="idm45607788304880"/>This is easily achieved with <code>gspread</code>’s <code>get_all_records</code>, which returns a list of item dictionaries. This list can be used directly to initialize a pandas DataFrame (see <a data-type="xref" href="ch08.xhtml#pandas_objects">“The DataFrame”</a>):</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">ws</code><code class="o">.</code><code class="n">get_all_records</code><code class="p">(</code><code class="n">expected_headers</code><code class="o">=</code><code class="p">[]))</code>
<code class="n">df</code><code class="o">.</code><code class="n">info</code><code class="p">()</code>
<code class="n">Out</code><code class="p">:</code>
<code class="o">&lt;</code><code class="k">class</code> <code class="err">'</code><code class="nc">pandas</code><code class="o">.</code><code class="n">core</code><code class="o">.</code><code class="n">frame</code><code class="o">.</code><code class="n">DataFrame</code><code class="s1">'&gt;</code>
<code class="n">Int64Index</code><code class="p">:</code> <code class="mi">41</code> <code class="n">entries</code><code class="p">,</code> <code class="mi">0</code> <code class="n">to</code> <code class="mi">40</code>
<code class="n">Data</code> <code class="n">columns</code> <code class="p">(</code><code class="n">total</code> <code class="mi">23</code> <code class="n">columns</code><code class="p">):</code>
                                          <code class="mi">41</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="n">average</code> <code class="n">basic</code> <code class="n">reproductive</code> <code class="n">rate</code>           <code class="mi">41</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="k">case</code> <code class="n">fatality</code> <code class="n">rate</code>                        <code class="mi">41</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="n">infectious</code> <code class="n">dose</code>                           <code class="mi">41</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="o">...</code>
<code class="n">upper</code> <code class="n">R0</code>                                  <code class="mi">41</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="n">viral</code> <code class="n">load</code> <code class="ow">in</code> <code class="n">acute</code> <code class="n">stage</code>                 <code class="mi">41</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="n">yearly</code> <code class="n">fatalities</code>                         <code class="mi">41</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="n">dtypes</code><code class="p">:</code> <code class="nb">object</code><code class="p">(</code><code class="mi">23</code><code class="p">)</code>
<code class="n">memory</code> <code class="n">usage</code><code class="p">:</code> <code class="mf">7.5</code><code class="o">+</code> <code class="n">KB</code></pre>
<p>In <a data-type="xref" href="ch11.xhtml#chapter_pandas_exploring">Chapter 11</a>, we’ll see how to interactively explore a DataFrame’s data.</p>
</div></section>
<section data-pdf-bookmark="Using the Twitter API with Tweepy" data-type="sect2"><div class="sect2" id="sect_tweepy">
<h2>Using the Twitter API with Tweepy</h2>
<p>The advent of social media has generated a <a data-primary="gspread library" data-startref="ix_gsprd" data-type="indexterm" id="idm45607788260448"/><a data-primary="web APIs" data-secondary="using libraries to access" data-startref="ix_webAPIlibGs" data-tertiary="Google spreadsheets" data-type="indexterm" id="idm45607788109152"/><a data-primary="Google spreadsheets" data-startref="ix_Goosprd" data-type="indexterm" id="idm45607788107824"/>lot of data and an interest in visualizing the social networks, trending hashtags, and media storms contained in them.<a data-primary="Tweepy library, using Twitter API with" data-type="indexterm" id="ix_Twpy"/><a data-primary="web APIs" data-secondary="using libraries to access" data-tertiary="using Twitter API with Tweepy" data-type="indexterm" id="ix_webAPIlibTwit"/> Twitter’s broadcast network is probably the richest source of cool data visualizations, and its API provides tweets<sup><a data-type="noteref" href="ch05.xhtml#idm45607788104048" id="idm45607788104048-marker">4</a></sup> filtered by user, hashtag, date, and the like.</p>
<p>Python’s Tweepy is an easy-to-use Twitter library that provides a number of useful features, such as a <code>StreamListener</code> class for streaming live Twitter updates. To start using it, you’ll need a Twitter access token, which you can acquire by following the instructions  <a href="https://oreil.ly/ZkWNf">at the Twitter docs</a> to create your Twitter application. Once this application is created, you can get the keys and access tokens for your app by clicking on the link <a href="https://apps.twitter.com">at your Twitter app page</a>.</p>
<p>Tweepy typically requires the four authorization elements shown here:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># The user credential variables to access Twitter API</code>
<code class="n">access_token</code> <code class="o">=</code> <code class="s2">"2677230157-Ze3bWuBAw4kwoj4via2dEntU86...TD7z"</code>
<code class="n">access_token_secret</code> <code class="o">=</code> <code class="s2">"DxwKAvVzMFLq7WnQGnty49jgJ39Acu...paR8ZH"</code>
<code class="n">consumer_key</code> <code class="o">=</code> <code class="s2">"pIorGFGQHShuYQtIxzYWk1jMD"</code>
<code class="n">consumer_secret</code> <code class="o">=</code> <code class="s2">"yLc4Hw82G0Zn4vTi4q8pSBcNyHkn35BfIe...oVa4P7R"</code></pre>
<p>With those defined, accessing tweets could hardly be easier. Here we create an OAuth <code>auth</code> object using our tokens and keys and use it to start an API session. We can then grab the latest tweets from our timeline:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">0</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">tweepy</code>

        <code class="n">auth</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">OAuthHandler</code><code class="p">(</code><code class="n">consumer_key</code><code class="p">,</code>\
                                   <code class="n">consumer_secret</code><code class="p">)</code>
        <code class="n">auth</code><code class="o">.</code><code class="n">set_access_token</code><code class="p">(</code><code class="n">access_token</code><code class="p">,</code> <code class="n">access_token_secret</code><code class="p">)</code>

        <code class="n">api</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">API</code><code class="p">(</code><code class="n">auth</code><code class="p">)</code>

        <code class="n">public_tweets</code> <code class="o">=</code> <code class="n">api</code><code class="o">.</code><code class="n">home_timeline</code><code class="p">()</code>
        <code class="k">for</code> <code class="n">tweet</code> <code class="ow">in</code> <code class="n">public_tweets</code><code class="p">:</code>
            <code class="nb">print</code><code class="p">(</code><code class="n">tweet</code><code class="o">.</code><code class="n">text</code><code class="p">)</code></pre>
<pre data-type="programlisting">RT @Glinner: Read these tweets https://t.co/QqzJPsDxUD
Volodymyr Bilyachat https://t.co/VIyOHlje6b +1 bmeyer
#javascript
RT @bbcworldservice: If scientists edit genes to
make people healthier does it change what it means to be
human? https://t.co/Vciuyu6BCx h…
RT @ForrestTheWoods:
Launching something pretty cool tomorrow. I'm excited. Keep
...</pre>
<p>Tweepy’s <code>API</code> class offers a lot of convenience methods, which you can check out <a href="https://oreil.ly/2FTRw">in the Tweepy docs</a>.  A common visualization is using a network graph to show patterns of friends and followers among Twitter subpopulations. The Tweepy method <code>followers_ids</code> (get all users following) and <code>friends_ids</code> (get all users being followed) can be used to construct such a network:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">my_follower_ids</code><code> </code><code class="o">=</code><code> </code><code class="n">api</code><code class="o">.</code><code class="n">get_follower_ids</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code class="n">followers_tree</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="s1">'</code><code class="s1">followers</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="p">[</code><code class="p">]</code><code class="p">}</code><code>
</code><code class="k">for</code><code> </code><code class="nb">id</code><code> </code><code class="ow">in</code><code> </code><code class="n">my_follower_ids</code><code class="p">:</code><code>
</code><code>    </code><code class="c1"># get the followers of your followers</code><code>
</code><code>    </code><code class="k">try</code><code class="p">:</code><code>
</code><code>         </code><code class="n">follower_ids</code><code> </code><code class="o">=</code><code> </code><code class="n">api</code><code class="o">.</code><code class="n">get_follower_ids</code><code class="p">(</code><code class="n">user_id</code><code class="o">=</code><code class="nb">id</code><code class="p">)</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-2" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>    </code><code class="k">except</code><code> </code><code class="n">tweepy</code><code class="o">.</code><code class="n">errors</code><code class="o">.</code><code class="n">Unauthorized</code><code class="p">:</code><code>
</code><code>         </code><code class="nb">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Unauthorized to access user </code><code class="si">%d</code><code class="s2">'</code><code class="s2">s followers</code><code class="s2">"</code><code>\
</code><code>               </code><code class="o">%</code><code class="p">(</code><code class="nb">id</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">followers_tree</code><code class="p">[</code><code class="s1">'</code><code class="s1">followers</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code>\
</code><code>        </code><code class="p">{</code><code class="s1">'</code><code class="s1">id</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="nb">id</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">follower_ids</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">follower_ids</code><code class="p">}</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Gets a list of your followers’ IDs (e.g., <code>[1191701545, 1554134420, …​]</code>).</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-2" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>The first argument to <code>follower_ids</code> can be a user ID or screen name.</p></dd>
</dl>
<p>Note that you will probably run into rate-limit errors if you try and construct a network for anyone with more than a hundred followers (see <a href="https://oreil.ly/1KDH2">this Stack Overflow thread</a> for an explanation). To overcome this you will need to implement some basic rate limiting to reduce your request count to 180 per 15 minutes. Alternatively, you can pay Twitter for a premium account.</p>
<p>By mapping followers of followers, you can create a network of connections that might just reveal something interesting about groups and subgroups clustered about a particular individual or subject. There’s a nice example of just such a Twitter analysis on <a href="https://oreil.ly/sWH99">Gabe Sawhney’s blog</a>.</p>
<p>One of the coolest features of Tweepy is its <code>StreamListener</code> class, which makes it easy to collect and process filtered tweets in real time.<a data-primary="StreamListener class (Tweepy)" data-type="indexterm" id="idm45607787881136"/><a data-primary="streams (Twitter)" data-type="indexterm" id="idm45607787880464"/> Live updates of Twitter streams have been used by many memorable visualizations (see these examples from <a href="https://oreil.ly/mNOYX">FlowingData</a> and <a href="https://oreil.ly/ZpmLq">DensityDesign</a> for some inspiration). Let’s set up a little stream to record tweets mentioning Python, JavaScript, and dataviz. We’ll just print the results to the screen (in <code>on_data</code>) here, but you would normally cache them in a file or database (or do both with SQLite):</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>


<code class="k">class</code> <code class="nc">MyStream</code><code class="p">(</code><code class="n">tweepy</code><code class="o">.</code><code class="n">Stream</code><code class="p">):</code>
    <code class="sd">""" Customized tweet stream """</code>

    <code class="k">def</code> <code class="nf">on_data</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">tweet</code><code class="p">):</code>
        <code class="sd">"""Do something with the tweet data..."""</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">tweet</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">on_error</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">status</code><code class="p">):</code>
        <code class="k">return</code> <code class="kc">True</code> <code class="c1"># keep stream open</code>

<code class="n">stream</code> <code class="o">=</code> <code class="n">MyStream</code><code class="p">(</code><code class="n">consumer_key</code><code class="p">,</code> <code class="n">consumer_secret</code><code class="p">,</code>\
                     <code class="n">access_token</code><code class="p">,</code> <code class="n">access_token_secret</code><code class="p">)</code>
<code class="c1"># Start the stream with track list of keywords</code>
<code class="n">stream</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">track</code><code class="o">=</code><code class="p">[</code><code class="s1">'python'</code><code class="p">,</code> <code class="s1">'javascript'</code><code class="p">,</code> <code class="s1">'dataviz'</code><code class="p">])</code></pre>
<p>Now that we’ve had a taste of the kind of APIs you might run into during your search for interesting data, let’s look at the primary technique you’ll use if, as is often the case, no one is providing the data you want in a neat, user-friendly form: scraping data with Python.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Scraping Data" data-type="sect1"><div class="sect1" id="get_data_scraping">
<h1>Scraping Data</h1>
<p>Scraping is the chief metaphor used for the practice of getting data that wasn’t designed to be programmatically consumed off the web.<a data-primary="Tweepy library, using Twitter API with" data-startref="ix_Twpy" data-type="indexterm" id="idm45607787787984"/><a data-primary="web APIs" data-secondary="using libraries to access" data-startref="ix_webAPIlibTwit" data-tertiary="using Twitter API with Tweepy" data-type="indexterm" id="idm45607787787040"/><a data-primary="getting data off the web using Python" data-secondary="using libraries to access web APIs" data-startref="ix_getPyAPIlib" data-type="indexterm" id="idm45607787785616"/><a data-primary="web APIs" data-secondary="using libraries to access" data-startref="ix_webAPIlib" data-type="indexterm" id="idm45607787784464"/><a data-primary="scraping data" data-type="indexterm" id="ix_scrp"/><a data-primary="getting data off the web using Python" data-secondary="scraping data" data-type="indexterm" id="ix_getPyscrp"/> It is a pretty good metaphor because scraping is often about getting the balance right between removing too much and too little. Creating procedures that extract just the right data, as cleanly as possible, from web pages is a craft skill and often a fairly messy one at that. But the payoff is access to visualizable data that often cannot be acquired in any other way. Approached in the right way, scraping can even have an intrinsic satisfaction.</p>
<section data-pdf-bookmark="Why We Need to Scrape" data-type="sect2"><div class="sect2" id="idm45607787780848">
<h2>Why We Need to Scrape</h2>
<p>In an ideal virtual world, online data would be organized in a library, with everything cataloged through a sophisticated Dewey decimal system for the web page. <a data-primary="scraping data" data-secondary="reasons for" data-type="indexterm" id="idm45607787727872"/>Unfortunately for the keen data hunter, the web has grown organically, often unconstrained by considerations of easy data access for the budding data visualizer. So, in reality, the web resembles a big mound of data, some of it clean and usable (and thankfully this percentage is increasing) but much of it poorly formed and designed for human consumption. And humans are able to parse the kind of messy, poorly formed data that our relatively dumb computers have problems with.<sup><a data-type="noteref" href="ch05.xhtml#idm45607787727008" id="idm45607787727008-marker">5</a></sup></p>
<p>Scraping is about fashioning selection patterns that grab the data we want and leave the rest behind. If we’re lucky, the web pages containing the data will have helpful pointers, like named tables, specific identities in preference to generic classes, and so on. If we’re unlucky, then these pointers will be missing and we will have to resort to using other patterns or, in the worst case, ordinal specifiers such as <em>third table in the main div</em>. These are obviously pretty fragile, and will break if somebody adds a table above the third.</p>
<p>In this section, we’ll tackle a little scraping task, to get the same Nobel Prize winners data. We’ll use Python’s best-of-breed Beautiful Soup for this lightweight scraping foray, saving the heavy guns of Scrapy for the next chapter.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The fact that data and images are on the web does not mean that they are necessarily free to use. For our scraping examples we’ll be using Wikipedia, which allows full reuse under the <a href="https://oreil.ly/jBTaC">Creative Commons license</a>. <a data-primary="Wikipedia, using data from" data-type="indexterm" id="idm45607787723744"/><a data-primary="Creative Commons license" data-type="indexterm" id="idm45607787723136"/>It’s a good idea to make sure anything you scrape is available and, if in doubt, contact the site maintainer. You may be required to at least cite the original author.</p>
</div>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Beautiful Soup and lxml" data-type="sect2"><div class="sect2" id="idm45607787722016">
<h2>Beautiful Soup and lxml</h2>
<p>Python’s key lightweight scraping tools are Beautiful Soup and lxml. <a data-primary="scraping data" data-secondary="Beautiful Soup and lxml scraping tools" data-type="indexterm" id="idm45607787720288"/><a data-primary="Beautiful Soup (scraping tool)" data-type="indexterm" id="idm45607787719296"/><a data-primary="lxml (scraping tool)" data-type="indexterm" id="idm45607787718608"/>Their primary selection syntax is different but, confusingly, each can use the other’s parsers. The consensus seems to be that lxml’s parser is considerably faster, but Beautiful Soup’s might be more robust when dealing with poorly formed HTML. Personally, I’ve found lxml to be robust enough and its syntax, based on <a href="https://oreil.ly/A43cY">xpaths</a>, more powerful and often more intuitive. I think for someone coming from web development, familiar with CSS and jQuery, selection based on CSS selectors <a data-primary="xpath" data-type="indexterm" id="idm45607787717152"/><a data-primary="CSS" data-secondary="selectors, accessing with Beautiful Soup" data-type="indexterm" id="idm45607787716448"/>is much more natural. Depending on your system, lxml is usually the default parser for Beautiful Soup. We’ll be using it in the following sections.</p>
<p>Beautiful Soup is part of the Anaconda packages (see <a data-type="xref" href="ch01.xhtml#chapter_install">Chapter 1</a>) and easily installed with <code>pip</code>:</p>
<pre data-type="programlisting">$ pip install beautifulsoup4
$ pip install lxml</pre>
</div></section>
<section data-pdf-bookmark="A First Scraping Foray" data-type="sect2"><div class="sect2" id="idm45607787712608">
<h2>A First Scraping Foray</h2>
<p>Armed with Requests and Beautiful Soup, let’s give ourselves a little task to get the names, years, categories, and nationalities of all the Nobel Prize winners. <a data-primary="scraping data" data-secondary="scraping foray, getting data on Nobel prize winners" data-type="indexterm" id="idm45607787711280"/>We’ll start at the <a href="https://oreil.ly/cSFFW">main Wikipedia Nobel Prize page</a>. Scrolling down shows a table with all the laureates by year and category, which is a good start to our minimal data requirements.</p>
<p>Some kind of HTML explorer is pretty much a must for web scraping, and the best I know is Chrome’s web developer’s Elements tab (see <a data-type="xref" href="ch04.xhtml#chrome_elements">“The Elements Tab”</a>). <a data-type="xref" href="#wp_nobel">Figure 5-1</a> shows the key elements involved in quizzing a web page’s structure. We need to know how to select the data of interest, in this case a Wikipedia table, while avoiding other elements on the page. Crafting good selector patterns is the key to effective scraping, and highlighting the DOM element using the element inspector gives us both the CSS pattern and, with a right-click, the xpath. The latter is a particularly powerful syntax for DOM element selection and the basis of our industrial-strength scraping solution, Scrapy.</p>
<figure><div class="figure" id="wp_nobel">
<img alt="dpj2 0501" height="1222" src="assets/dpj2_0501.png" width="1442"/>
<h6><span class="label">Figure 5-1. </span>Wikipedia’s main Nobel Prize page: A and B show the wikitable’s CSS selector. Right-clicking and selecting C (Copy XPath) gives the table’s xpath (<code>//*[@id="mw-content-text"]/table[1]</code>). D shows a <code>thead</code> tag generated by jQuery.</h6>
</div></figure>
</div></section>
</div></section>
<section data-pdf-bookmark="Getting the Soup" data-type="sect1"><div class="sect1" id="idm45607787703888">
<h1>Getting the Soup</h1>
<p>The first thing you need to do before <a data-primary="getting data off the web using Python" data-secondary="scraping data" data-startref="ix_getPyscrp" data-type="indexterm" id="idm45607787702160"/><a data-primary="scraping data" data-startref="ix_scrp" data-type="indexterm" id="idm45607787700848"/><a data-primary="scraping data" data-secondary="using Beautiful Soup" data-type="indexterm" id="idm45607787699904"/><a data-primary="Beautiful Soup (scraping tool)" data-secondary="getting the soup" data-type="indexterm" id="idm45607787698960"/><a data-primary="getting data off the web using Python" data-secondary="parsing scraped data into the soup" data-type="indexterm" id="idm45607787698000"/>scraping the web page of interest is to parse it with Beautiful Soup, converting the HTML into a tag tree hierarchy or soup:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">bs4</code><code> </code><code class="kn">import</code><code> </code><code class="n">BeautifulSoup</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">requests</code><code>
</code><code>
</code><code class="n">BASE_URL</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">http://en.wikipedia.org</code><code class="s1">'</code><code>
</code><code class="c1"># Wikipedia will reject our request unless we add</code><code>
</code><code class="c1"># a 'User-Agent' attribute to our http header.</code><code>
</code><code class="n">HEADERS</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="s1">'</code><code class="s1">User-Agent</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Mozilla/5.0</code><code class="s1">'</code><code class="p">}</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">get_Nobel_soup</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">""" Return a parsed tag tree of our Nobel prize page """</code><code>
</code><code>    </code><code class="c1"># Make a request to the Nobel page, setting valid headers</code><code>
</code><code>    </code><code class="n">response</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code>
</code><code>        </code><code class="n">BASE_URL</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="s1">/wiki/List_of_Nobel_laureates</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>        </code><code class="n">headers</code><code class="o">=</code><code class="n">HEADERS</code><code class="p">)</code><code>
</code><code>    </code><code class="c1"># Return the content of the response parsed by Beautiful Soup</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">lxml</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO6-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO6-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>The second argument specifies the parser we want to use, namely lxml’s.</p></dd>
</dl>
<p>With our soup in hand, let’s see how to find our target tags.</p>
</div></section>
<section data-pdf-bookmark="Selecting Tags" data-type="sect1"><div class="sect1" id="idm45607787616096">
<h1>Selecting Tags</h1>
<p>Beautiful Soup offers a few ways to select tags from the parsed soup, with subtle differences that can be confusing. <a data-primary="getting data off the web using Python" data-secondary="selecting tags from data scraped by Beautiful Soup" data-type="indexterm" id="ix_getPyseltag"/><a data-primary="selecting tags (Beautiful Soup)" data-type="indexterm" id="ix_seltag"/><a data-primary="Beautiful Soup (scraping tool)" data-secondary="selecting tags from parsed soup" data-type="indexterm" id="ix_BeSoseltag"/>Before demonstrating the selection methods, let’s get the soup of our Nobel Prize page:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">soup</code> <code class="o">=</code> <code class="n">get_Nobel_soup</code><code class="p">()</code></pre>
<p>Our target table (see <a data-type="xref" href="#wp_nobel">Figure 5-1</a>) has two defining classes, <code>wikitable</code> and <code>sortable</code> (there are some unsortable tables on the page). <a data-primary="CSS" data-secondary="finding tags by CSS classes using Beautiful Soup" data-type="indexterm" id="idm45607790541184"/>We can use Beautiful Soup’s <code>find</code> method to find the first table tag with those classes. <code>find</code> takes a tag name as its first argument and a dictionary with class, ID, and other identifiers as its second:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code><code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'table'</code><code class="p">,</code> <code class="p">{</code><code class="s1">'class'</code><code class="p">:</code><code class="s1">'wikitable sortable'</code><code class="p">})</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">3</code><code class="p">]:</code>
<code class="o">&lt;</code><code class="n">table</code> <code class="n">class</code><code class="o">=</code><code class="s2">"wikitable sortable"</code><code class="o">&gt;</code>
<code class="o">&lt;</code><code class="n">tr</code><code class="o">&gt;</code>
<code class="o">&lt;</code><code class="n">th</code><code class="o">&gt;</code><code class="n">Year</code><code class="o">&lt;/</code><code class="n">th</code><code class="o">&gt;</code>
<code class="o">...</code></pre>
<p>Although we have successfully found our table by its classes, this method is not very robust. Let’s see what happens when we change the order of our CSS classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'table'</code><code class="p">,</code> <code class="p">{</code><code class="s1">'class'</code><code class="p">:</code><code class="s1">'sortable wikitable'</code><code class="p">})</code>
<code class="c1"># nothing returned</code></pre>
<p>So <code>find</code> cares about the order of the classes, using the class string to find the tag. If the classes were specified in a different order—​something that might well happen during an HTML edit, then the <code>find</code> fails. <a data-primary="lxml (scraping tool)" data-secondary="CSS selectors" data-type="indexterm" id="idm45607787441008"/>This fragility makes it difficult to recommend the Beautiful Soup selectors, such as <code>find</code> and <code>find_all</code>. When doing quick hacking, I find lxml’s <a href="https://lxml.de/cssselect.xhtml">CSS selectors</a> easier and more intuitive.</p>
<p>Using the soup’s <code>select</code> method (available if you specified the lxml parser when creating it), you can specify an HTML element using its CSS class, ID, and so on. This CSS selector is converted into the xpath syntax lxml uses internally.<sup><a data-type="noteref" href="ch05.xhtml#idm45607787437712" id="idm45607787437712-marker">6</a></sup></p>
<p>To get our wikitable, we just select a table in the soup, using the dot notation to indicate its classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'table.sortable.wikitable'</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code>
<code class="p">[</code><code class="o">&lt;</code><code class="n">table</code> <code class="n">class</code><code class="o">=</code><code class="s2">"wikitable sortable"</code><code class="o">&gt;</code>
 <code class="o">&lt;</code><code class="n">tr</code><code class="o">&gt;</code>
 <code class="o">&lt;</code><code class="n">th</code><code class="o">&gt;</code><code class="n">Year</code><code class="o">&lt;/</code><code class="n">th</code><code class="o">&gt;</code>
 <code class="o">...</code>
<code class="p">]</code></pre>
<p>Note that <code>select</code> returns an array of results, finding all the matching tags in the soup. lxml provides the <code>select_one</code> convenience method if you are selecting just one HTML element. Let’s grab our Nobel table and see what headers it has:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code><code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">table</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s1">'table.sortable.wikitable'</code><code class="p">)</code>

<code class="n">In</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">table</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'th'</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code>
<code class="p">[</code><code class="o">&lt;</code><code class="n">th</code><code class="o">&gt;</code><code class="n">Year</code><code class="o">&lt;/</code><code class="n">th</code><code class="o">&gt;</code><code class="p">,</code>
 <code class="o">&lt;</code><code class="n">th</code> <code class="n">width</code><code class="o">=</code><code class="s2">"18%"</code><code class="o">&gt;&lt;</code><code class="n">a</code> <code class="n">href</code><code class="o">=</code><code class="s2">"/wiki/..._in_Physics..&lt;/a&gt;&lt;/th&gt;,</code>
 <code class="o">&lt;</code><code class="n">th</code> <code class="n">width</code><code class="o">=</code><code class="s2">"16%"</code><code class="o">&gt;&lt;</code><code class="n">a</code> <code class="n">href</code><code class="o">=</code><code class="s2">"/wiki/..._in_Chemis..&lt;/a&gt;&lt;/th&gt;,</code>
 <code class="o">...</code>
<code class="p">]</code></pre>
<p>As a shorthand for <code>select</code>, you can call the tag directly on the soup; so these two are equivalent:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">table</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'th'</code><code class="p">)</code>
<code class="n">table</code><code class="p">(</code><code class="s1">'th'</code><code class="p">)</code></pre>
<p>With lxml’s parser, Beautiful Soup provides a <a data-primary="lxml (scraping tool)" data-secondary="filters for finding tags in Beautiful Soup" data-type="indexterm" id="idm45607787277168"/>number of different filters for finding tags, including the simple string name we’ve just used, searching by <a href="https://oreil.ly/GeU8Q">regular expression</a>, using a list of tag names, and more. <a data-primary="regular expressions" data-secondary="using in lxml parser of Beautiful Soup" data-type="indexterm" id="idm45607787275440"/>See this <a href="https://oreil.ly/iBQwc">comprehensive list</a> for more details.</p>
<p>As well as lxml’s <code>select</code> and <code>select_one</code>, there are 10 BeautfulSoup convenience methods for searching the parsed tree. These are essentially variants on <code>find</code> and <code>find_all</code> that specify which parts of the tree they search. For example, <code>find_parent</code> and <code>find_parents</code>, rather than looking for descendants down the tree, look for parent tags of the tag being searched.  All 10 methods are available in the Beautiful Soup <a href="https://oreil.ly/oPrQl">official docs</a>.</p>
<p>Now that we know how to select our Wikipedia table and are armed with lxml’s selection methods, let’s see how to craft some selection patterns to get the data we want.</p>
<section class="pagebreak-before less_space" data-pdf-bookmark="Crafting Selection Patterns" data-type="sect2"><div class="sect2" id="idm45607787269024">
<h2>Crafting Selection Patterns</h2>
<p>Having successfully selected our data table, we now want to craft some selection patterns to scrape the required data.<a data-primary="selecting tags (Beautiful Soup)" data-secondary="creating selection patterns" data-type="indexterm" id="ix_seltagpatt"/> Using the HTML explorer, you can see that the individual winners are contained in <code>&lt;td&gt;</code> cells, with an href <code>&lt;a&gt;</code> link to Wikipedia’s bio pages (in the case of individuals). Here’s a typical target row with CSS classes that we can use as targets to get the data in the <code>&lt;td&gt;</code> cells:</p>
<pre data-code-language="html" data-type="programlisting"> <code class="p">&lt;</code><code class="nt">tr</code><code class="p">&gt;</code>
  <code class="p">&lt;</code><code class="nt">td</code> <code class="na">align</code><code class="o">=</code><code class="s">"center"</code><code class="p">&gt;</code>
   1901
  <code class="p">&lt;/</code><code class="nt">td</code><code class="p">&gt;</code>
  <code class="p">&lt;</code><code class="nt">td</code><code class="p">&gt;</code>
   <code class="p">&lt;</code><code class="nt">span</code> <code class="na">class</code><code class="o">=</code><code class="s">"sortkey"</code><code class="p">&gt;</code>
    Röntgen, Wilhelm
   <code class="p">&lt;/</code><code class="nt">span</code><code class="p">&gt;</code>
   <code class="p">&lt;</code><code class="nt">span</code> <code class="na">class</code><code class="o">=</code><code class="s">"vcard"</code><code class="p">&gt;</code>
    <code class="p">&lt;</code><code class="nt">span</code> <code class="na">class</code><code class="o">=</code><code class="s">"fn"</code><code class="p">&gt;</code>
     <code class="p">&lt;</code><code class="nt">a</code> <code class="na">href</code><code class="o">=</code><code class="s">"/wiki/Wilhelm_R%C3%B6ntgen"</code> <code class="err">\</code>
        <code class="na">title</code><code class="o">=</code><code class="s">"Wilhelm Röntgen"</code><code class="p">&gt;</code>
      Wilhelm Röntgen
     <code class="p">&lt;/</code><code class="nt">a</code><code class="p">&gt;</code>
    <code class="p">&lt;/</code><code class="nt">span</code><code class="p">&gt;</code>
   <code class="p">&lt;/</code><code class="nt">span</code><code class="p">&gt;</code>
  <code class="p">&lt;/</code><code class="nt">td</code><code class="p">&gt;</code>
  <code class="p">&lt;</code><code class="nt">td</code><code class="p">&gt;</code>
  ...
<code class="p">&lt;/</code><code class="nt">tr</code><code class="p">&gt;</code></pre>
<p>If we loop through these data cells, keeping track of their row (year) and column (category), then we should be able to create a list of winners with all the data we specified except nationality.</p>
<p>The following <code>get_column_titles</code> function scrapes our table for the Nobel category column headers, ignoring the first Year column. Often the header cell in a Wikipedia table contains a web-linked <code>'a'</code> tag; all the Nobel categories fit this model, pointing to their respective Wikipedia pages. If the header is not clickable, we store its text and a null href:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code><code> </code><code class="nf">get_column_titles</code><code class="p">(</code><code class="n">table</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">""" Get the Nobel categories from the table header """</code><code>
</code><code>    </code><code class="n">cols</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>    </code><code class="k">for</code><code> </code><code class="n">th</code><code> </code><code class="ow">in</code><code> </code><code class="n">table</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s1">'</code><code class="s1">tr</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'</code><code class="s1">th</code><code class="s1">'</code><code class="p">)</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="p">]</code><code class="p">:</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO7-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>        </code><code class="n">link</code><code> </code><code class="o">=</code><code> </code><code class="n">th</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s1">'</code><code class="s1">a</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>        </code><code class="c1"># Store the category name and any Wikipedia link it has</code><code>
</code><code>        </code><code class="k">if</code><code> </code><code class="n">link</code><code class="p">:</code><code>
</code><code>            </code><code class="n">cols</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="n">link</code><code class="o">.</code><code class="n">text</code><code class="p">,</code><code>\
</code><code>                         </code><code class="s1">'</code><code class="s1">href</code><code class="s1">'</code><code class="p">:</code><code class="n">link</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'</code><code class="s1">href</code><code class="s1">'</code><code class="p">]</code><code class="p">}</code><code class="p">)</code><code>
</code><code>        </code><code class="k">else</code><code class="p">:</code><code>
</code><code>            </code><code class="n">cols</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="n">th</code><code class="o">.</code><code class="n">text</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">href</code><code class="s1">'</code><code class="p">:</code><code class="kc">None</code><code class="p">}</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">cols</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO7-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>We loop through the table head, ignoring the first Year column ([1:]). This selects the column headers shown in <a data-type="xref" href="#scraping_nobel_table">Figure 5-2</a>.</p></dd>
</dl>
<figure><div class="figure" id="scraping_nobel_table">
<img alt="dpj2 0502" height="943" src="assets/dpj2_0502.png" width="1446"/>
<h6><span class="label">Figure 5-2. </span>Wikipedia’s table of Nobel Prize winners</h6>
</div></figure>
<p>Let’s make sure <code>get_column_titles</code> is giving us what we want:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">get_column_titles</code><code class="p">(</code><code class="n">table</code><code class="p">)</code><code>
</code><code class="n">Out</code><code class="p">:</code><code>
</code><code class="p">[</code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Physics</code><code class="s1">'</code><code class="p">,</code><code> </code><code>\
</code><code>  </code><code class="s1">'</code><code class="s1">href</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">/wiki/List_of_Nobel_laureates_in_Physics</code><code class="s1">'</code><code class="p">}</code><code class="p">,</code><code>
</code><code> </code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Chemistry</code><code class="s1">'</code><code class="p">,</code><code>\
</code><code>  </code><code class="s1">'</code><code class="s1">href</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">/wiki/List_of_Nobel_laureates_in_Chemistry</code><code class="s1">'</code><code class="p">}</code><code class="p">,</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="p">]</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">get_Nobel_winners</code><code class="p">(</code><code class="n">table</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">cols</code><code> </code><code class="o">=</code><code> </code><code class="n">get_column_titles</code><code class="p">(</code><code class="n">table</code><code class="p">)</code><code>
</code><code>    </code><code class="n">winners</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>    </code><code class="k">for</code><code> </code><code class="n">row</code><code> </code><code class="ow">in</code><code> </code><code class="n">table</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'</code><code class="s1">tr</code><code class="s1">'</code><code class="p">)</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="p">:</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>        </code><code class="n">year</code><code> </code><code class="o">=</code><code> </code><code class="nb">int</code><code class="p">(</code><code class="n">row</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s1">'</code><code class="s1">td</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">text</code><code class="p">)</code><code> </code><code class="c1"># Gets 1st &lt;td&gt;</code><code>
</code><code>        </code><code class="k">for</code><code> </code><code class="n">i</code><code class="p">,</code><code> </code><code class="n">td</code><code> </code><code class="ow">in</code><code> </code><code class="nb">enumerate</code><code class="p">(</code><code class="n">row</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'</code><code class="s1">td</code><code class="s1">'</code><code class="p">)</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="p">]</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-2" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>            </code><code class="k">for</code><code> </code><code class="n">winner</code><code> </code><code class="ow">in</code><code> </code><code class="n">td</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'</code><code class="s1">a</code><code class="s1">'</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                </code><code class="n">href</code><code> </code><code class="o">=</code><code> </code><code class="n">winner</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'</code><code class="s1">href</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>                </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">href</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s1">'</code><code class="s1">#endnote</code><code class="s1">'</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                    </code><code class="n">winners</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">{</code><code>
</code><code>                        </code><code class="s1">'</code><code class="s1">year</code><code class="s1">'</code><code class="p">:</code><code class="n">year</code><code class="p">,</code><code>
</code><code>                        </code><code class="s1">'</code><code class="s1">category</code><code class="s1">'</code><code class="p">:</code><code class="n">cols</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="p">[</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                        </code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="n">winner</code><code class="o">.</code><code class="n">text</code><code class="p">,</code><code>
</code><code>                        </code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">:</code><code class="n">winner</code><code class="o">.</code><code class="n">attrs</code><code class="p">[</code><code class="s1">'</code><code class="s1">href</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>                    </code><code class="p">}</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">winners</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Gets all the Year rows, starting from the second, corresponding to the rows in <a data-type="xref" href="#scraping_nobel_table">Figure 5-2</a>.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-2" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Finds the <code>&lt;td&gt;</code> data cells shown in <a data-type="xref" href="#scraping_nobel_table">Figure 5-2</a>.</p></dd>
</dl>
<p>Iterating through the Year rows, we take the first Year column and then iterate over the remaining columns, using <code>enumerate</code> to keep track of our index, which will map to the category column names. We know that all the winner names are contained in an <code>&lt;a&gt;</code> tag but that there are occasional extra <code>&lt;a&gt;</code> tags beginning with <code>#endnote</code>, which we filter for. Finally we append a year, category, name, and link dictionary to our data array. Note that the winner selector has an <code>attrs</code> dictionary containing, among other things, the <code>&lt;a&gt;</code> tag’s href.</p>
<p>Let’s confirm that <code>get_Nobel_winners</code> delivers a list of Nobel Prize winner <span class="keep-together">dictionaries</span>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">get_Nobel_winners</code><code class="p">(</code><code class="n">table</code><code class="p">)</code></pre>
<pre data-type="programlisting">[{'year': 1901,
  'category': 'Physics',
  'name': 'Wilhelm Röntgen',
  'link': '/wiki/Wilhelm_R%C3%B6ntgen'},
 {'year': 1901,
  'category': 'Chemistry',
  'name': "Jacobus Henricus van 't Hoff",
  'link': '/wiki/Jacobus_Henricus_van_%27t_Hoff'},
 {'year': 1901,
  'category': 'Physiologyor Medicine',
  'name': 'Emil Adolf von Behring',
  'link': '/wiki/Emil_Adolf_von_Behring'},
 {'year': 1901,
 ...}]</pre>
<p>Now that we have the full list of Nobel Prize winners and links to their Wikipedia pages, we can use these links to scrape data from the individuals’ biographies. This will involve making a largish number of requests, and it’s not something we really want to do more than once. The sensible and respectful<sup><a data-type="noteref" href="ch05.xhtml#idm45607786692608" id="idm45607786692608-marker">7</a></sup> thing is to cache the data we scrape, allowing us to try out various scraping experiments without returning to Wikipedia.<a data-primary="selecting tags (Beautiful Soup)" data-secondary="creating selection patterns" data-startref="ix_seltagpatt" data-type="indexterm" id="idm45607786691984"/></p>
</div></section>
<section data-pdf-bookmark="Caching the Web Pages" data-type="sect2"><div class="sect2" id="idm45607787248336">
<h2>Caching the Web Pages</h2>
<p>It’s easy enough to rustle up a quick cacher in Python, but as often as not it’s easier still to find a better solution written by someone else and kindly donated to the open source community.<a data-primary="web pages" data-secondary="caching while scraping for data" data-type="indexterm" id="idm45607786689248"/><a data-primary="selecting tags (Beautiful Soup)" data-secondary="caching the web pages" data-type="indexterm" id="idm45607786686000"/><a data-primary="caching" data-secondary="of web pages being scraped" data-secondary-sortas="web" data-type="indexterm" id="idm45607786685088"/><a data-primary="Requests library" data-secondary="requests-cache plug-in" data-type="indexterm" id="idm45607786683904"/> Requests has a nice plug-in called <code>requests-cache</code> that, with a few lines of configuration, will take care of all your basic caching needs.</p>
<p>First, we install the plug-in using <code>pip</code>:</p>
<pre data-type="programlisting">$ pip install --upgrade requests-cache</pre>
<p><code>requests-cache</code> uses <a href="https://oreil.ly/8IklZ">monkey patching</a> to dynamically replace parts of the <code>requests</code> API at runtime. This means it can work transparently.<a data-primary="monkey patching" data-type="indexterm" id="idm45607786678720"/> You just have to install its cache and then use <code>requests</code> as usual, with all the caching being taken care of.  Here’s the simplest way to use <code>requests-cache</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">import</code> <code class="nn">requests_cache</code>

<code class="n">requests_cache</code><code class="o">.</code><code class="n">install_cache</code><code class="p">()</code>
<code class="c1"># use requests as usual...</code></pre>
<p>The <code>install_cache</code> method has a number of useful options, including allowing you to specify the cache <code>backend</code> (<code>sqlite</code>, <code>memory</code>, <code>mongdb</code>, or <code>redis</code>) or set an expiry time (<code>expiry_after</code>) in seconds on the caching. So the following creates a cache named <code>nobel_pages</code> with an <code>sqlite</code> backend and pages that expire in two hours (7,200 s):</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">requests_cache</code><code class="o">.</code><code class="n">install_cache</code><code class="p">(</code><code class="s1">'nobel_pages'</code><code class="p">,</code>\
                         <code class="n">backend</code><code class="o">=</code><code class="s1">'sqlite'</code><code class="p">,</code> <code class="n">expire_after</code><code class="o">=</code><code class="mi">7200</code><code class="p">)</code></pre>
<p><code>requests-cache</code> will serve most of your caching needs and couldn’t be much easier to use. For more details, see <a href="https://oreil.ly/d67bK">the official docs</a> where you’ll also find a little example of request throttling, which is a useful technique when doing bulk scraping.</p>
</div></section>
<section data-pdf-bookmark="Scraping the Winners’ Nationalities" data-type="sect2"><div class="sect2" id="idm45607786600896">
<h2>Scraping the Winners’ Nationalities</h2>
<p>With caching in place, let’s try getting the winners’ nationalities, using the first 50 for our experiment.<a data-primary="selecting tags (Beautiful Soup)" data-secondary="scraping Nobel Prize winners' nationalities" data-type="indexterm" id="idm45607786599104"/><a data-primary="scraping data" data-secondary="scraping Nobel Prize winners' nationalities" data-type="indexterm" id="idm45607786598192"/> A little <code>get_winner_nationality()</code> function will use the winner links we stored earlier to scrape their page and then use the info-box shown in <a data-type="xref" href="#country_bio">Figure 5-3</a> to get the <code>Nationality</code> attribute.</p>
<figure><div class="figure" id="country_bio">
<img alt="dpj2 0503" height="844" src="assets/dpj2_0503.png" width="1406"/>
<h6><span class="label">Figure 5-3. </span>Scraping a winner’s nationality</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When scraping, you are looking for reliable patterns and repeating elements with useful data. As we’ll see, the Wikipedia info-boxes for individuals are not such a reliable source, but clicking on a few random links certainly gives that impression. Depending on the size of the dataset, it’s good to perform a few experimental sanity checks. You can do this manually but, as mentioned at the start of the chapter, this won’t scale or improve your craft skills.</p>
</div>
<p><a data-type="xref" href="#get_winner_nationality">Example 5-3</a> takes one of the winner dictionaries we scraped earlier and returns a name-labeled dictionary with a <code>Nationality</code> key if one is found. Let’s run it on the first 50 winners and see how often a <code>Nationality</code> attribute is missing.</p>
<div data-type="example" id="get_winner_nationality">
<h5><span class="label">Example 5-3. </span>Scraping the winner’s country from their biography page</h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">HEADERS</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="s1">'</code><code class="s1">User-Agent</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Mozilla/5.0</code><code class="s1">'</code><code class="p">}</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">get_winner_nationality</code><code class="p">(</code><code class="n">w</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">""" scrape biographic data from the winner's wikipedia page """</code><code>
</code><code>    </code><code class="n">response</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'</code><code class="s1">http://en.wikipedia.org</code><code class="s1">'</code><code> </code><code>\
</code><code>                                </code><code class="o">+</code><code> </code><code class="n">w</code><code class="p">[</code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">headers</code><code class="o">=</code><code class="n">HEADERS</code><code class="p">)</code><code>
</code><code>    </code><code class="n">content</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s1">'</code><code class="s1">utf-8</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>    </code><code class="n">soup</code><code> </code><code class="o">=</code><code> </code><code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">content</code><code class="p">)</code><code>
</code><code>    </code><code class="n">person_data</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">w</code><code class="p">[</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">]</code><code class="p">}</code><code>
</code><code>    </code><code class="n">attr_rows</code><code> </code><code class="o">=</code><code> </code><code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'</code><code class="s1">table.infobox tr</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-1" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>    </code><code class="k">for</code><code> </code><code class="n">tr</code><code> </code><code class="ow">in</code><code> </code><code class="n">attr_rows</code><code class="p">:</code><code>                        </code><a class="co" href="#callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-2" id="co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>        </code><code class="k">try</code><code class="p">:</code><code>
</code><code>            </code><code class="n">attribute</code><code> </code><code class="o">=</code><code> </code><code class="n">tr</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s1">'</code><code class="s1">th</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">text</code><code>
</code><code>            </code><code class="k">if</code><code> </code><code class="n">attribute</code><code> </code><code class="o">==</code><code> </code><code class="s1">'</code><code class="s1">Nationality</code><code class="s1">'</code><code class="p">:</code><code>
</code><code>                </code><code class="n">person_data</code><code class="p">[</code><code class="n">attribute</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">tr</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s1">'</code><code class="s1">td</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">text</code><code>
</code><code>        </code><code class="k">except</code><code> </code><code class="ne">AttributeError</code><code class="p">:</code><code>
</code><code>            </code><code class="k">pass</code><code>
</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">person_data</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-1" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>We use a CSS selector to find all the <code>&lt;tr&gt;</code> rows of the table with class <code>infobox</code>.</p></dd>
<dt><a class="co" href="#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-2" id="callout_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Cycles through the rows looking for a Nationality field.</p></dd>
</dl></div>
<p><a data-type="xref" href="#nationality_test">Example 5-4</a> shows that 14 of the 50 first winners failed our attempt to scrape their nationality. In the case of the Institut de Droit International, national affiliation may well be moot, but Theodore Roosevelt is about as American as they come. Clicking on a few of the names shows the problem (see <a data-type="xref" href="#missing_nationality">Figure 5-4</a>). The lack of a standardized biography format means synonyms for <em>Nationality</em> are often employed, as in Marie Curie’s <em>Citizenship</em>; sometimes no reference is made, as with Niels Finsen; and Randall Cremer has nothing but a photograph in his info-box. We can discard the info-boxes as a reliable source of winners’ nationalities but, as they appeared to be the only regular source of potted data, this sends us back to the drawing board. In the next chapter, we’ll see a successful approach using Scrapy and a different start page.</p>
<div data-type="example" id="nationality_test">
<h5><span class="label">Example 5-4. </span>Testing for scraped nationalities</h5>
<pre data-code-language="python" data-type="programlisting"><code class="n">wdata</code> <code class="o">=</code> <code class="p">[]</code>
<code class="c1"># test first 50 winners</code>
<code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">winners</code><code class="p">[:</code><code class="mi">50</code><code class="p">]:</code>
    <code class="n">wdata</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">get_winner_nationality</code><code class="p">(</code><code class="n">w</code><code class="p">))</code>
<code class="n">missing_nationality</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">wdata</code><code class="p">:</code>
    <code class="c1"># if missing 'Nationality' add to list</code>
    <code class="k">if</code> <code class="ow">not</code> <code class="n">w</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'Nationality'</code><code class="p">):</code>
        <code class="n">missing_nationality</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">w</code><code class="p">)</code>
<code class="c1"># output list</code>
<code class="n">missing_nationality</code>

<code class="p">[{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Theodor Mommsen'</code><code class="p">},</code>
 <code class="p">{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Élie Ducommun'</code><code class="p">},</code>
 <code class="p">{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Charles Albert Gobat'</code><code class="p">},</code>
 <code class="p">{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Pierre Curie'</code><code class="p">},</code>
 <code class="p">{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Marie Curie'</code><code class="p">},</code>
 <code class="p">{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Niels Ryberg Finsen'</code><code class="p">},</code>
 <code class="o">...</code>
 <code class="p">{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Theodore Roosevelt'</code><code class="p">},</code> <code class="o">...</code> <code class="p">]</code></pre></div>
<figure><div class="figure" id="missing_nationality">
<img alt="dpj2 0504" height="954" src="assets/dpj2_0504.png" width="1440"/>
<h6><span class="label">Figure 5-4. </span>Winners without a recorded <em>nationality</em></h6>
</div></figure>
<p>Although Wikipedia is a relative free-for-all, production-wise, where data is designed for human consumption, you can expect a lack of rigor. Many sites have similar gotchas and as the datasets get bigger, more tests may be needed to find the flaws in a collection pattern.</p>
<p>Although our first scraping exercise was a little artificial in order to introduce the tools, I hope it captured something of the slightly messy spirit of web scraping. The ultimately abortive pursuit of a reliable Nationality field for our Nobel dataset could have been forestalled by a bit of web browsing and manual HTML-source trawling. However, if the dataset were significantly larger and the failure rate a bit smaller, then programmatic detection, which gets easier and easier as you become acquainted with the scraping modules, really starts to deliver.</p>
<p>This little scraping test was designed to introduce Beautiful Soup, and  shows that collecting the data we seek requires a little more thought, which is often the case with scraping. In the next chapter, we’ll wheel out the big gun, Scrapy, and, with what we’ve learned in this section, harvest the data we need for our Nobel Prize visualization.<a data-primary="getting data off the web using Python" data-secondary="selecting tags from data scraped by Beautiful Soup" data-startref="ix_getPyseltag" data-type="indexterm" id="idm45607786260448"/><a data-primary="Beautiful Soup (scraping tool)" data-secondary="selecting tags from parsed soup" data-startref="ix_BeSoseltag" data-type="indexterm" id="idm45607786259264"/><a data-primary="selecting tags (Beautiful Soup)" data-startref="ix_seltag" data-type="indexterm" id="idm45607786235680"/></p>
</div></section>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45607787615280">
<h1>Summary</h1>
<p>In this chapter, we’ve seen examples of the most common ways in which data can be sucked out of the web and into Python containers, databases, or pandas datasets. Python’s Requests library is the true workhorse of HTTP negotiation and a fundamental tool in our dataviz toolchain. For simpler, RESTful APIs, consuming data with Requests is a few lines of Python away. For the more awkward APIs, such as those with potentially complicated authorization, a wrapper library like Tweepy (for Twitter) can save a lot of hassle. Decent wrappers can also keep track of access rates and, where necessary, throttle your requests. This is a key consideration, particularly when there is the possibility of blacklisting unfriendly consumers.</p>
<p>We also started our first forays into data scraping, which is often a necessary fallback where no API exists and the data is for human consumption. In the next chapter, we’ll get all the Nobel Prize data needed for the book’s visualization using Python’s Scrapy, an industrial-strength<a data-primary="getting data off the web using Python" data-startref="ix_getPy" data-type="indexterm" id="idm45607786233552"/> scraping library.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45607789542496"><sup><a href="ch05.xhtml#idm45607789542496-marker">1</a></sup> This is actually a <a href="https://oreil.ly/WOjdB">deliberate policy</a> of the developers.</p><p data-type="footnote" id="idm45607789528976"><sup><a href="ch05.xhtml#idm45607789528976-marker">2</a></sup> There are some platform dependencies that might still generate errors. This <a href="https://oreil.ly/Zm082">Stack Overflow thread</a> is a good starting point if you still have problems.</p><p data-type="footnote" id="idm45607788534000"><sup><a href="ch05.xhtml#idm45607788534000-marker">3</a></sup> OAuth1 access has been deprecated recently.</p><p data-type="footnote" id="idm45607788104048"><sup><a href="ch05.xhtml#idm45607788104048-marker">4</a></sup> The free API is currently limited to around <a href="https://oreil.ly/LKzJX">350 requests per hour</a>.</p><p data-type="footnote" id="idm45607787727008"><sup><a href="ch05.xhtml#idm45607787727008-marker">5</a></sup> Much of modern machine learning and artificial intelligence (AI) research is dedicated to creating computer software that can cope with messy, noisy, fuzzy, informal data but, as of this book’s publication, there’s no off-the-shelf solution I know of.</p><p data-type="footnote" id="idm45607787437712"><sup><a href="ch05.xhtml#idm45607787437712-marker">6</a></sup> This CSS selection syntax should be familiar to anyone who’s used JavaScript’s <a href="https://jquery.com">jQuery library</a> and is also similar to that used by <a href="https://d3js.org">D3</a>.</p><p data-type="footnote" id="idm45607786692608"><sup><a href="ch05.xhtml#idm45607786692608-marker">7</a></sup> When scraping, you’re using other people’s web bandwidth, which ultimately costs them money. It’s just good manners to try to limit your number of requests.</p></div></div></section></div></body></html>