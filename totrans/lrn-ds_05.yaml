- en: Chapter 4\. Modeling with Summary Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw in [Chapter 2](ch02.html#ch-data-scope) the importance of data scope
    and in [Chapter 3](ch03.html#ch-theory-datadesign) the importance of data generation
    mechanisms, such as one that can be represented by an urn model. Urn models address
    one aspect of modeling: they describe chance variation and ensure that the data
    are representative of the target. Good scope and representative data lay the groundwork
    for extracting useful information from data, which is the other part of modeling.
    This information is often referred to as the *signal* in the data. We use models
    to approximate the signal, with the simplest of these being the constant model,
    where the signal is approximated by a single number, like the mean or median.
    Other, more complex models summarize relationships between features in the data,
    such as humidity and particulate matter in air quality ([Chapter 12](ch12.html#ch-pa)),
    upward mobility and commute time in communities ([Chapter 15](ch15.html#ch-linear)),
    and height and weight of animals ([Chapter 18](ch18.html#ch-donkey)). These more
    complex models are also approximations built from data. When a model fits the
    data well, it can provide a useful approximation to the world or simply a helpful
    description of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we introduce the basics of model fitting through a *loss* formulation.
    We demonstrate how to model patterns in the data by considering the loss that
    arises from using a simple summary to describe the data, the constant model. We
    delve deeper into the connections between the urn model and the fitted model in
    [Chapter 16](ch16.html#ch-risk), where we examine the balance between signal and
    noise when fitting models, and in [Chapter 17](ch17.html#ch-inf-pred-theory),
    where we tackle the topics of inference, prediction, and hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: The constant model lets us introduce model fitting from the perspective of *loss
    minimization* in a simple context, and it helps us connect summary statistics,
    like the mean and median, to more complex modeling scenarios in later chapters.
    We begin with an example that uses data about the late arrival of a bus to introduce
    the constant model.
  prefs: []
  type: TYPE_NORMAL
- en: The Constant Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A transit rider, Jake, often takes the northbound C bus at the 3rd & Pike bus
    stop in downtown Seattle.^([1](ch04.html#id780)) The bus is supposed to arrive
    every 10 minutes, but Jake notices that he sometimes waits a long time for the
    bus. He wants to know how late the bus usually is. Jake was able to acquire the
    scheduled arrival and actual arrival times for his bus from the Washington State
    Transportation Center. From these data, he can calculate the number of minutes
    that each bus is late to arrive at his stop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|   | route | direction | scheduled | actual | minutes_late |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | C | northbound | 2016-03-26 06:30:28 | 2016-03-26 06:26:04 | -4.40
    |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | C | northbound | 2016-03-26 01:05:25 | 2016-03-26 01:10:15 | 4.83
    |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | C | northbound | 2016-03-26 21:00:25 | 2016-03-26 21:05:00 | 4.58
    |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **1431** | C | northbound | 2016-04-10 06:15:28 | 2016-04-10 06:11:37 | -3.85
    |'
  prefs: []
  type: TYPE_TB
- en: '| **1432** | C | northbound | 2016-04-10 17:00:28 | 2016-04-10 16:56:54 | -3.57
    |'
  prefs: []
  type: TYPE_TB
- en: '| **1433** | C | northbound | 2016-04-10 20:15:25 | 2016-04-10 20:18:21 | 2.93
    |'
  prefs: []
  type: TYPE_TB
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `minutes_late` column in the data table records how late each bus was.
    Notice that some of the times are negative, meaning that the bus arrived early.
    Let’s examine a histogram of the number of minutes each bus is late:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_04in01.png)'
  prefs: []
  type: TYPE_IMG
- en: We can already see some interesting patterns in the data. For example, many
    buses arrive earlier than scheduled, but some are well over 20 minutes late. We
    also see a clear mode (high point) at 0, meaning many buses arrive roughly on
    time.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how late a bus on this route typically is, we’d like to summarize
    lateness by a constant—this is a statistic, a single number, like the mean, median,
    or mode. Let’s find each of these summary statistics for the `minutes_late` column
    in the data table.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the histogram, we estimate the mode of the data to be 0, and we use Python
    to compute the mean and median:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Naturally, we want to know which of these numbers best represents a summary
    of lateness. Rather than relying on rules of thumb, we take a more formal approach.
    We make a constant model for bus lateness. Let’s call this constant <math><mi>θ</mi></math>
    (in modeling, <math><mi>θ</mi></math> is often referred to as a *parameter*).
    For example, if we consider <math><mi>θ</mi> <mo>=</mo> <mn>5</mn></math> , then
    our model approximates the bus to typically be five minutes late.
  prefs: []
  type: TYPE_NORMAL
- en: Now, <math><mi>θ</mi> <mo>=</mo> <mn>5</mn></math> isn’t a particularly good
    guess. From the histogram of minutes late, we saw that there are many more points
    closer to 0 than 5\. But it isn’t clear that <math><mi>θ</mi> <mo>=</mo> <mn>0</mn></math>
    (the mode) is a better choice than <math><mi>θ</mi> <mo>=</mo> <mn>0.74</mn></math>
    (the median), <math><mi>θ</mi> <mo>=</mo> <mn>1.92</mn></math> (the mean), or
    something else entirely. To make choices between different values of <math><mi>θ</mi></math>
    , we would like to assign any value of <math><mi>θ</mi></math> a score that measures
    how well that constant fits the data. That is, we want to assess the loss involved
    in approximating the data by a constant, like <math><mi>θ</mi> <mo>=</mo> <mn>5</mn></math>
    . And ideally, we want to pick the constant that best fits our data, meaning the
    constant that has the smallest loss. In the next section, we describe more formally
    what we mean by loss and show how to use it to fit a model.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to model how late the northbound C bus is by a constant, which we call
    <math><mi>θ</mi></math> , and we want to use the data of actual number of minutes
    each bus is late to figure out a good value for <math><mi>θ</mi></math> . To do
    this, we use a *loss function*—a function that measures how far away our constant,
    <math><mi>θ</mi></math> , is from the actual data.
  prefs: []
  type: TYPE_NORMAL
- en: A loss function is a mathematical function that takes in <math><mi>θ</mi></math>
    and a data value <math><mi>y</mi></math> . It outputs a single number, the *loss*,
    that measures how far away <math><mi>θ</mi></math> is from <math><mi>y</mi></math>
    . We write the loss function as <math><mrow><mi mathvariant="script">l</mi></mrow>
    <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'By convention, the loss function outputs lower values for better values of
    <math><mi>θ</mi></math> and larger values for worse <math><mi>θ</mi></math> .
    To fit a constant to our data, we select the particular <math><mi>θ</mi></math>
    that produces the lowest average loss across all choices for <math><mi>θ</mi></math>
    . In other words, we find the <math><mi>θ</mi></math> that *minimizes the average
    loss* for our data, <math><msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>y</mi> <mi>n</mi></msub></math> . More formally, we write
    the average loss as <math><mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo>
    <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>y</mi> <mi>n</mi></msub> <mo stretchy="false">)</mo></math>
    , where:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>,</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>y</mi> <mi>n</mi></msub>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mtext>mean</mtext> <mrow><mo>{</mo>
    <mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi>θ</mi>
    <mo>,</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo> <mo>,</mo>
    <mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi>θ</mi>
    <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo stretchy="false">)</mo> <mo>,</mo>
    <mo>…</mo> <mo>,</mo> <mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>,</mo> <msub><mi>y</mi> <mi>n</mi></msub> <mo stretchy="false">)</mo>
    <mo>}</mo></mrow></mtd></mtr> <mtr><mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi>θ</mi>
    <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'As a shorthand, we often use the vector <math><mrow><mi mathvariant="bold">y</mi></mrow>
    <mo>=</mo> <mo stretchy="false">[</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>y</mi>
    <mi>n</mi></msub> <mo stretchy="false">]</mo></math> . Then we can write the average
    loss as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo>
    <mrow><mi mathvariant="bold">y</mi></mrow> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover> <mrow><mi mathvariant="script">l</mi></mrow>
    <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mrow><msub><mi>y</mi> <mi>i</mi></msub></mrow>
    <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that <math><mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo></math> tells us the
    model’s loss for a single data point while <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow> <mo stretchy="false">)</mo></math>
    gives the model’s average loss for all the data points. The capital <math><mi>L</mi></math>
    helps us remember that the average loss combines multiple smaller <math><mi mathvariant="script">l</mi></math>
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Once we define a loss function, we can find the value of <math><mi>θ</mi></math>
    that produces the smallest average loss. We call this minimizing value <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> . In other words, of all the
    possible <math><mi>θ</mi></math> values, <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    is the one that produces the smallest average loss for our data. We call this
    optimization process *model fitting*; it finds the best constant model for our
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we look at two particular loss functions: absolute error and squared
    error. Our goal is to fit the model and find <math><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow></math> for each of these loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean Absolute Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start with the *absolute error* loss function. Here’s the idea behind absolute
    loss. For some value of <math><mi>θ</mi></math> and data value <math><mi>y</mi></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: Find the error, <math><mi>y</mi> <mo>−</mo> <mi>θ</mi></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the absolute value of the error, <math><mo stretchy="false">|</mo> <mi>y</mi>
    <mo>−</mo> <mi>θ</mi> <mo stretchy="false">|</mo></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So the loss function is <math><mrow><mi mathvariant="script">l</mi></mrow> <mo
    stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo>
    <mo>=</mo> <mrow><mo stretchy="false">|</mo></mrow> <mi>y</mi> <mo>−</mo> <mi>θ</mi>
    <mo stretchy="false">|</mo></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Taking the absolute value of the error is a simple way to convert negative errors
    into positive ones. For instance, the point <math><mi>y</mi> <mo>=</mo> <mn>4</mn></math>
    is equally far away from <math><mi>θ</mi> <mo>=</mo> <mn>2</mn></math> and <math><mi>θ</mi>
    <mo>=</mo> <mn>6</mn></math> , so the errors are equally “bad.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The average of the absolute errors is called the *mean absolute error* (MAE).
    The MAE is the average of each of the individual absolute errors:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo>
    <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover> <mrow><mo stretchy="false">|</mo></mrow>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mo stretchy="false">|</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the name MAE tells you how to compute it: take the Mean of the
    Absolute value of the Errors, <math><mo fence="false" stretchy="false">{</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mo fence="false" stretchy="false">}</mo></math>
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write a simple Python function to compute this loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how this loss function behaves when we have just five data points
    <math><mo stretchy="false">[</mo> <mrow><mo>–</mo></mrow> <mn>1</mn> <mo>,</mo>
    <mn>0</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>5</mn> <mo>,</mo> <mn>10</mn>
    <mo stretchy="false">]</mo></math> . We can try different values of <math><mi>θ</mi></math>
    and see what the MAE outputs for each value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_04in02.png)'
  prefs: []
  type: TYPE_IMG
- en: We suggest verifying some of these loss values by hand to check that you understand
    how the MAE is computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of the values of <math><mi>θ</mi></math> that we tried, we found that <math><mi>θ</mi>
    <mo>=</mo> <mn>2</mn></math> has the lowest mean absolute error. For this simple
    example, 2 is the median of the data values. This isn’t a coincidence. Let’s now
    check what the average loss is for the original dataset of bus late times. We
    find the MAE when we set <math><mi>θ</mi></math> to the mode, median, and mean
    of the minutes late, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_04in03.png)'
  prefs: []
  type: TYPE_IMG
- en: We see again that the median (middle plot) gives a smaller loss than the mode
    and mean (left and right plots). In fact, for absolute loss, the minimizing <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> is the <math><mtext>median</mtext>
    <mo fence="false" stretchy="false">{</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>y</mi>
    <mi>n</mi></msub> <mo fence="false" stretchy="false">}</mo></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have found the best value of <math><mi>θ</mi></math> by simply trying
    out a few values and then picking the one with the smallest loss. To get a better
    sense of the MAE as a function of <math><mi>θ</mi></math> , we can try many more
    values of <math><mi>θ</mi></math> and plot a curve that shows how <math><mi>L</mi>
    <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo></math> changes as <math><mi>θ</mi></math> changes.
    We draw the curve for the preceding example with the five data values <math><mo
    stretchy="false">[</mo> <mrow><mo>–</mo></mrow> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>5</mn> <mo>,</mo> <mn>10</mn> <mo stretchy="false">]</mo></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_04in04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding plot shows that in fact, <math><mi>θ</mi> <mo>=</mo> <mn>2</mn></math>
    is the best choice for this small dataset of five values. Notice the shape of
    the curve. It is piecewise linear, where the line segments connect at the location
    of the data values (–1, 0, 2, and 5). This is a property of the absolute value
    function. With a lot of data, the flat pieces are less obvious. Our bus data have
    over 1,400 points and the MAE curve appears smoother:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_04in05.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use this plot to help confirm that the median of the data is the minimizing
    value; in other words, <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo>=</mo> <mn>0.74</mn></math> . This plot is not really a proof, but hopefully
    it’s convincing enough for you.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at another loss function that squares error.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Squared Error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have fitted a constant model to our data and found that with mean absolute
    error, the minimizer is the median. Now we’ll keep our model the same but switch
    to a different loss function: squared error. Instead of taking the absolute difference
    between each data value <math><mi>y</mi></math> and the constant <math><mi>θ</mi></math>
    , we’ll square the error. That is, for some value of <math><mi>θ</mi></math> and
    data value <math><mi>y</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: Find the error, <math><mi>y</mi> <mo>−</mo> <mi>θ</mi></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the square of the error, <math><mo stretchy="false">(</mo> <mi>y</mi> <mo>−</mo>
    <mi>θ</mi> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This gives the loss function <math><mrow><mi mathvariant="script">l</mi></mrow>
    <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo>
    <mo>=</mo> <mo stretchy="false">(</mo> <mi>y</mi> <mo>−</mo> <mi>θ</mi> <msup><mo
    stretchy="false">)</mo> <mn>2</mn></msup></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we want to use all of our data to find the best <math><mi>θ</mi></math>
    , so we compute the mean squared error, or MSE for short:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo>
    <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo>
    <mo>=</mo> <mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>y</mi> <mi>n</mi></msub> <mo stretchy="false">)</mo> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write a simple Python function to compute the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s again try the mean, median, and mode as potential minimizers of the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_04in06.png)'
  prefs: []
  type: TYPE_IMG
- en: Now when we fit the constant model using MSE loss, we find that the mean (right
    plot) has a smaller loss than the mode and the median (left and middle plots).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the MSE curve for different values of <math><mi>θ</mi></math> given
    our data. The curve shows that the minimizing value <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> is close to 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_04in07.png)'
  prefs: []
  type: TYPE_IMG
- en: One feature of this curve that is quite noticeable is how rapidly the MSE grows
    compared to the MAE (note the range on the vertical axis). This growth has to
    do with the nature of squaring errors; it places a much higher loss on data values
    further away from <math><mi>θ</mi></math> . If <math><mi>θ</mi> <mo>=</mo> <mn>10</mn></math>
    and <math><mi>y</mi> <mo>=</mo> <mn>110</mn></math> , the squared loss is <math><mo
    stretchy="false">(</mo> <mn>10</mn> <mo>−</mo> <mn>110</mn> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup> <mo>=</mo> <mn>10</mn> <mo>,</mo> <mn>000</mn></math> whereas
    the absolute loss is <math><mo stretchy="false">|</mo> <mn>10</mn> <mo>−</mo>
    <mn>110</mn> <mrow><mo stretchy="false">|</mo></mrow> <mo>=</mo> <mn>100</mn></math>
    . For this reason, the MSE is more sensitive to unusually large data values than
    the MAE.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the MSE curve, it appears that the minimizing <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> is the mean of <math><mrow><mi
    mathvariant="bold">y</mi></mrow></math> . Again, this is no mere coincidence;
    the mean of the data always coincides with <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    for squared error. We show how this comes about from the quadratic nature of the
    MSE. Along the way, we demonstrate a common representation of squared loss as
    a sum of variance and bias terms, which is at the heart of model fitting with
    squared loss. To begin, we add and subtract <math><mrow><mover><mi>y</mi> <mo
    stretchy="false">¯</mo></mover></mrow></math> in the loss function and expand
    the square as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow> <mo stretchy="false">)</mo></mtd>
    <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>−</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo> <mo>+</mo> <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo>−</mo> <mi>θ</mi> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mo>=</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">[</mo>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>
    <mo>+</mo> <mn>2</mn> <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>−</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo> <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo>−</mo> <mi>θ</mi> <mo stretchy="false">)</mo>
    <mo>+</mo> <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup> <mo
    stretchy="false">]</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we split the MSE into the sum of these three terms and note that the
    middle term is 0, due to the simple property of the average: <math><mo>∑</mo>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo stretchy="false">)</mo> <mo>=</mo>
    <mn>0</mn></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover></mtd>
    <mtd><mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo>
    <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup> <mo>+</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mn>2</mn> <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo>
    <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow> <mo stretchy="false">)</mo>
    <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <mi>θ</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>
    <mo>+</mo> <mn>2</mn> <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi> <mo
    stretchy="false">¯</mo></mover></mrow> <mo>−</mo> <mi>θ</mi> <mo stretchy="false">)</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo> <mo>+</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd></mtr>
    <mtr><mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>
    <mo>+</mo> <mo stretchy="false">(</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Of the remaining two terms, the first does not involve <math><mi>θ</mi></math>
    . You probably recognize it as the variance of the data. The second term is always
    non-negative. It is called the *bias squared*. This second term, the bias squared,
    is 0 when <math><mi>θ</mi></math> is <math><mrow><mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow></mrow></math>
    , so <math><mrow><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></mrow>
    <mo>=</mo> <mrow><mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow></mrow></math>
    gives the smallest MSE for any dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that for absolute loss, the best constant model is the median,
    but for squared error, it’s the mean. The choice of the loss function is an important
    aspect of model fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing Loss Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve worked with two loss functions, we can return to our original
    question: how do we choose whether to use the median, mean, or mode? Since these
    statistics minimize different loss functions,^([2](ch04.html#id794)) we can equivalently
    ask: what is the most appropriate loss function for our problem? To answer this
    question, we look at the context of our problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the MAE, the MSE gives especially large losses when the bus is much
    later (or earlier) than expected. A bus rider who wants to understand the typical
    late times would use the MAE and the median (0.74 minutes late), but a rider who
    despises unexpected large late times might summarize the data using the MSE and
    the mean (1.92 minutes late).
  prefs: []
  type: TYPE_NORMAL
- en: If we want to refine the model even more, we can use a more specialized loss
    function. For example, suppose that when a bus arrives early, it waits at the
    stop until the scheduled time of departure; then we might want to assign an early
    arrival 0 loss. And if a really late bus is a larger aggravation than a moderately
    late one, we might choose an *asymmetric loss function* that gives a larger penalty
    to super-late arrivals.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, context matters when choosing a loss function. By thinking carefully
    about how we plan to use the model, we can pick a loss function that helps us
    make good data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We introduced the constant model: a model that summarizes the data by a single
    value. To fit the constant model, we chose a loss function that measured how well
    a given constant fits a data value, and we computed the average loss over all
    of the data values. We saw that depending on the choice of loss function, we get
    a different minimizing value: we found that the mean minimizes the average squared
    error (MSE), and the median minimizes the average absolute error (MAE). We also
    discussed how we can incorporate context and knowledge of our problem to pick
    a loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of fitting models through loss minimization ties simple summary statistics—like
    the mean, median, and mode—to more complex modeling situations. The steps we took
    to model our data apply to many modeling scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the form of a model (such as the constant model).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a loss function (such as absolute error).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model by minimizing the loss over all the data (such as average loss).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the rest of this book, our modeling techniques expand upon one or more of
    these steps. We introduce new models, new loss functions, and new techniques for
    minimizing loss. [Chapter 5](ch05.html#ch-bus) revisits the study of a bus arriving
    late at its stop. This time, we present the problem as a case study and visit
    all stages of the data science lifecycle. By going through these stages, we make
    some unusual discoveries; when we augment our analysis by considering data scope
    and using an urn to simulate a rider arriving at the bus stop, we find that modeling
    bus lateness is not the same as modeling the rider’s experience waiting for a
    bus.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.html#id780-marker)) We (the authors) first learned of the bus arrival
    time data from an analysis by a data scientist named Jake VanderPlas. We’ve named
    the protagonist of this section in his honor.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch04.html#id794-marker)) The mode minimizes a loss function called 0-1
    loss. Although we haven’t covered this specific loss, the procedure is identical:
    pick the loss function, then find what minimizes the loss.'
  prefs: []
  type: TYPE_NORMAL
