<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 38. Introducing Scikit-Learn" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0502-introducing-scikit-learn">
<h1><span class="label">Chapter 38. </span>Introducing Scikit-Learn</h1>
<p>Several <a data-primary="machine learning" data-secondary="Scikit-Learn basics" data-type="indexterm" id="ix_ch38-asciidoc0"/><a data-primary="Scikit-Learn package" data-type="indexterm" id="ix_ch38-asciidoc1"/><a data-primary="Scikit-Learn package" data-secondary="basics" data-type="indexterm" id="ix_ch38-asciidoc2"/> Python libraries provide solid implementations of
a range of machine learning algorithms. One of the best known is
<a href="http://scikit-learn.org">Scikit-Learn</a>, a package that provides efficient
versions of a large number of common algorithms. Scikit-Learn is
characterized by a clean, uniform, and streamlined API, as well as by
very useful and complete documentation. A benefit of this
uniformity is that once you understand the basic use and syntax of
Scikit-Learn for one type of model, switching to a new model or
algorithm is straightforward.</p>
<p>This chapter provides an overview of the Scikit-Learn API. A solid
understanding of these API elements will form the foundation for
understanding the deeper practical discussion of machine learning
algorithms and approaches in the following chapters.</p>
<p>We will start by covering data representation in Scikit-Learn, then
delve into the Estimator API, and finally go through a more interesting
example of using these tools for exploring a set of images of
handwritten digits.</p>
<section data-pdf-bookmark="Data Representation in Scikit-Learn" data-type="sect1"><div class="sect1" id="ch_0502-introducing-scikit-learn_data-representation-in-scikit-learn">
<h1>Data Representation in Scikit-Learn</h1>
<p><a data-primary="data representation (Scikit-Learn package)" data-secondary="data as table" data-type="indexterm" id="idm45858744600416"/><a data-primary="Scikit-Learn package" data-secondary="data as table" data-type="indexterm" id="idm45858744599408"/><a data-primary="table, data as" data-type="indexterm" id="idm45858744598464"/>Machine <a data-primary="data representation (Scikit-Learn package)" data-type="indexterm" id="ix_ch38-asciidoc3"/><a data-primary="Scikit-Learn package" data-secondary="data representation in" data-type="indexterm" id="ix_ch38-asciidoc4"/>learning is about creating models from data; for that reason,
we’ll start by discussing how data can be represented. The
best way to think about data within Scikit-Learn is in terms of
<em>tables</em>.</p>
<p>A basic table is a two-dimensional grid of data, in which the rows
represent individual elements of the dataset, and the columns represent
quantities related to each of these elements. <a data-primary="Iris dataset" data-secondary="as table" data-secondary-sortas="table" data-type="indexterm" id="idm45858744594480"/>For example, consider the
<a href="https://oreil.ly/TeWYs">Iris dataset</a>,
famously analyzed by Ronald Fisher in 1936. We can download this dataset
in the form of a Pandas <code>DataFrame</code> using the
<a href="http://seaborn.pydata.org">Seaborn library</a>, and take a look at the
first few items:</p>
<pre class="pagebreak-before less_space" data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
        <code class="n">iris</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">load_dataset</code><code class="p">(</code><code class="s1">'iris'</code><code class="p">)</code>
        <code class="n">iris</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">1</code><code class="p">]:</code>    <code class="n">sepal_length</code>  <code class="n">sepal_width</code>  <code class="n">petal_length</code>  <code class="n">petal_width</code> <code class="n">species</code>
        <code class="mi">0</code>           <code class="mf">5.1</code>          <code class="mf">3.5</code>           <code class="mf">1.4</code>          <code class="mf">0.2</code>  <code class="n">setosa</code>
        <code class="mi">1</code>           <code class="mf">4.9</code>          <code class="mf">3.0</code>           <code class="mf">1.4</code>          <code class="mf">0.2</code>  <code class="n">setosa</code>
        <code class="mi">2</code>           <code class="mf">4.7</code>          <code class="mf">3.2</code>           <code class="mf">1.3</code>          <code class="mf">0.2</code>  <code class="n">setosa</code>
        <code class="mi">3</code>           <code class="mf">4.6</code>          <code class="mf">3.1</code>           <code class="mf">1.5</code>          <code class="mf">0.2</code>  <code class="n">setosa</code>
        <code class="mi">4</code>           <code class="mf">5.0</code>          <code class="mf">3.6</code>           <code class="mf">1.4</code>          <code class="mf">0.2</code>  <code class="n">setosa</code></pre>
<p>Here each row of the data refers to a single observed flower, and the
number of rows is the total number of flowers in the dataset. In
general, we will refer to the rows of the matrix as <em>samples</em>, and the
number of rows as <code>n_samples</code>.</p>
<p>Likewise, each column of the data refers to a particular quantitative
piece of information that describes each sample. In general, we will
refer to the columns of the matrix as <em>features</em>, and the number of
columns as <code>n_features</code>.</p>
<section data-pdf-bookmark="The Features Matrix" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_the-features-matrix">
<h2>The Features Matrix</h2>
<p><a data-primary="data representation (Scikit-Learn package)" data-secondary="features matrix" data-type="indexterm" id="idm45858744541088"/><a data-primary="features matrix" data-type="indexterm" id="idm45858744539920"/><a data-primary="Scikit-Learn package" data-secondary="features matrix" data-type="indexterm" id="idm45858744539248"/>The table layout makes clear that the information can be thought of as a
two-dimensional numerical array or matrix, which we will call the
<em>features matrix</em>. By convention, this matrix is often stored in a
variable named <code>X</code>. The features matrix is assumed to be
two-dimensional, with shape <code>[n_samples, n_features]</code>, and is most often
contained in a NumPy array or a Pandas <code>DataFrame</code>, though some
Scikit-Learn models also accept SciPy sparse matrices.</p>
<p>The samples (i.e., rows) always refer to the individual objects
described by the dataset. For example, a sample might represent a
flower, a person, a document, an image, a sound file, a video, an
astronomical object, or anything else you can describe with a set of
quantitative measurements.</p>
<p>The features (i.e., columns) always refer to the distinct observations
that describe each sample in a quantitative manner. Features are often
real-valued, but may be Boolean or discrete-valued in some cases.</p>
</div></section>
<section data-pdf-bookmark="The Target Array" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_the-target-array">
<h2>The Target Array</h2>
<p><a data-primary="data representation (Scikit-Learn package)" data-secondary="target array" data-type="indexterm" id="ix_ch38-asciidoc5"/><a data-primary="Scikit-Learn package" data-secondary="target array" data-type="indexterm" id="ix_ch38-asciidoc6"/><a data-primary="target array" data-type="indexterm" id="ix_ch38-asciidoc7"/>In addition to the feature matrix <code>X</code>, we also generally work with a
<em>label</em> or <em>target</em> array, which by convention we will usually call <code>y</code>.
The target array is usually one-dimensional, with length <code>n_samples</code>,
and is generally contained in a NumPy array or Pandas <code>Series</code>. The
target array may have continuous numerical values, or discrete
classes/labels. While some Scikit-Learn estimators do handle multiple
target values in the form of a two-dimensional, <code>[n_samples, n_targets]</code>
target array, we will primarily be working with the common case of a
one-dimensional target array.</p>
<p>A common point of confusion is how the target array differs from the
other feature columns. The distinguishing characteristic of the target
array is that it is usually the quantity we want to <em>predict from the
features</em>: in statistical terms, it is the dependent variable. For
example, given the preceding data we may wish to construct a model that
can predict the species of flower based on the other measurements; in
this case, the <code>species</code> column would be considered the target array.</p>
<p><a data-primary="Iris dataset" data-secondary="visualization of" data-type="indexterm" id="idm45858744462304"/>With this target array in mind, we can use Seaborn (discussed in
<a data-type="xref" href="ch36.xhtml#section-0414-visualization-with-seaborn">Chapter 36</a>)
to conveniently visualize the data (see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_9_0">Figure 38-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
        <code class="n">sns</code><code class="o">.</code><code class="n">pairplot</code><code class="p">(</code><code class="n">iris</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="s1">'species'</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mf">1.5</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_9_0">
<img alt="output 9 0" height="398" src="assets/output_9_0.png" width="600"/>
<h6><span class="label">Figure 38-1. </span>A visualization of the Iris dataset<sup><a data-type="noteref" href="ch38.xhtml#idm45858744412448" id="idm45858744412448-marker">1</a></sup></h6>
</div></figure>
<p class="pagebreak-before less_space">For use in Scikit-Learn, we will extract the features matrix and target
array from the <code>DataFrame</code>, which we can do using some of the Pandas
<code>DataFrame</code> operations discussed in
<a data-type="xref" href="part03.xhtml#section-0300-introduction-to-pandas">Part III</a>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">X_iris</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s1">'species'</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">X_iris</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">y_iris</code> <code class="o">=</code> <code class="n">iris</code><code class="p">[</code><code class="s1">'species'</code><code class="p">]</code>
        <code class="n">y_iris</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="p">(</code><code class="mi">150</code><code class="p">,)</code></pre>
<p>To summarize, the expected layout of features and target values is
visualized in <a data-type="xref" href="#fig_images_in_0502-samples-features">Figure 38-2</a>.<a data-startref="ix_ch38-asciidoc7" data-type="indexterm" id="idm45858744319600"/><a data-startref="ix_ch38-asciidoc6" data-type="indexterm" id="idm45858744318992"/><a data-startref="ix_ch38-asciidoc5" data-type="indexterm" id="idm45858744318320"/></p>
<figure><div class="figure" id="fig_images_in_0502-samples-features">
<img alt="05.02 samples features" height="588" src="assets/05.02-samples-features.png" width="600"/>
<h6><span class="label">Figure 38-2. </span>Scikit-Learn’s data layout<sup><a data-type="noteref" href="ch38.xhtml#idm45858744315904" id="idm45858744315904-marker">2</a></sup></h6>
</div></figure>
<p>With this data properly formatted, we can move on to consider
Scikit-Learn’s Estimator API.<a data-startref="ix_ch38-asciidoc4" data-type="indexterm" id="idm45858744314000"/><a data-startref="ix_ch38-asciidoc3" data-type="indexterm" id="idm45858744313296"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="The Estimator API" data-type="sect1"><div class="sect1" id="ch_0502-introducing-scikit-learn_the-estimator-api">
<h1>The Estimator API</h1>
<p><a data-primary="Estimator API" data-type="indexterm" id="ix_ch38-asciidoc8"/><a data-primary="Scikit-Learn package" data-secondary="Estimator API" data-type="indexterm" id="ix_ch38-asciidoc9"/>The Scikit-Learn API is designed with the following guiding principles
in mind, as outlined in the <a href="http://arxiv.org/abs/1309.0238">Scikit-Learn
API paper</a>:</p>
<dl>
<dt><em>Consistency</em></dt>
<dd>
<p>All objects share a common interface drawn from a
limited set of methods, with consistent documentation.</p>
</dd>
<dt><em>Inspection</em></dt>
<dd>
<p>All specified parameter values are exposed as public
attributes.</p>
</dd>
<dt><em>Limited object hierarchy</em></dt>
<dd>
<p>Only algorithms are represented by Python
classes; datasets are represented in standard formats (NumPy arrays,
Pandas <code>DataFrame</code> objects, SciPy sparse matrices) and parameter names
use standard Python strings.</p>
</dd>
<dt><em>Composition</em></dt>
<dd>
<p>Many machine learning tasks can be expressed as
sequences of more fundamental algorithms, and Scikit-Learn makes use of
this wherever possible.</p>
</dd>
<dt><em>Sensible defaults</em></dt>
<dd>
<p>When models require user-specified parameters,
the library defines an appropriate default value.</p>
</dd>
</dl>
<p>In practice, these principles make Scikit-Learn very easy to use, once
the basic principles are understood. Every machine learning algorithm in
Scikit-Learn is implemented via the Estimator API, which provides a
consistent interface for a wide range of machine learning applications.</p>
<section data-pdf-bookmark="Basics of the API" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_basics-of-the-api">
<h2>Basics of the API</h2>
<p><a data-primary="Estimator API" data-secondary="basics" data-type="indexterm" id="idm45858744278992"/>Most commonly, the steps in using the Scikit-Learn Estimator API are as
follows:</p>
<ol>
<li>
<p>Choose a class of model by importing the appropriate estimator class
from Scikit-Learn.</p>
</li>
<li>
<p>Choose model hyperparameters by instantiating
this class with desired values.</p>
</li>
<li>
<p>Arrange data into a features
matrix and target vector, as outlined earlier in this chapter.</p>
</li>
<li>
<p>Fit the model to your data by calling the <code>fit</code> method of the model
instance.</p>
</li>
<li>
<p>Apply the model to new data:</p>
<ul>
<li>
<p>For supervised
learning, often we predict labels for unknown data using the <code>predict</code>
method.</p>
</li>
<li>
<p>For unsupervised learning, we often transform or infer
properties of the data using the <code>transform</code> or <code>predict</code> method.</p>
</li>
</ul>
</li>
</ol>
<p>We will now step through several simple examples of applying supervised
and unsupervised learning methods.</p>
</div></section>
<section data-pdf-bookmark="Supervised Learning Example: Simple Linear Regression" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_supervised-learning-example-simple-linear-regression">
<h2>Supervised Learning Example: Simple Linear Regression</h2>
<p><a data-primary="Estimator API" data-secondary="simple linear regression example" data-type="indexterm" id="ix_ch38-asciidoc10"/>As an example of this process, let’s consider a simple
linear regression—that is, the common case of fitting a line to
<math alttext="left-parenthesis x comma y right-parenthesis">
<mrow>
<mo>(</mo>
<mi>x</mi>
<mo>,</mo>
<mi>y</mi>
<mo>)</mo>
</mrow>
</math> data. We will use the following simple data for our
regression example (see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_20_0">Figure 38-3</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

        <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="mi">10</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code>
        <code class="n">y</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x</code> <code class="o">-</code> <code class="mi">1</code> <code class="o">+</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_20_0">
<img alt="output 20 0" height="383" src="assets/output_20_0.png" width="600"/>
<h6><span class="label">Figure 38-3. </span>Data for linear regression</h6>
</div></figure>
<p>With this data in place, we can use the recipe outlined earlier.
We’ll walk through the process in the following sections.</p>
<section data-pdf-bookmark="1. Choose a class of model" data-type="sect3"><div class="sect3" id="ch_0502-introducing-scikit-learn_1.-choose-a-class-of-model">
<h3>1. Choose a class of model</h3>
<p>In Scikit-Learn, every class of model is represented by a Python class.
So, for example, if we would like to compute a simple <code>LinearRegression</code>
model, we can import the linear regression class:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code></pre>
<p>Note that other more general linear regression models exist as well; you
can read more about them in the
<a href="https://oreil.ly/YVOFd"><code>sklearn.linear_model</code>
module documentation</a>.</p>
</div></section>
<section data-pdf-bookmark="2. Choose model hyperparameters" data-type="sect3"><div class="sect3" id="ch_0502-introducing-scikit-learn_2.-choose-model-hyperparameters">
<h3>2. Choose model hyperparameters</h3>
<p><a data-primary="hyperparameters" data-seealso="model validation" data-type="indexterm" id="idm45858744150288"/>An important point is that <em>a class of model is not the same as an
instance of a model</em>.</p>
<p>Once we have decided on our model class, there are still some options
open to us. Depending on the model class we are working with, we might
need to answer one or more questions like the following:</p>
<ul>
<li>
<p>Would we like to fit for the offset (i.e., <em>y</em>-intercept)?</p>
</li>
<li>
<p>Would we like the model to be normalized?</p>
</li>
<li>
<p>Would we like to preprocess our features to add model flexibility?</p>
</li>
<li>
<p>What degree of regularization would we like to use in our model?</p>
</li>
<li>
<p>How many model components would we like to use?</p>
</li>
</ul>
<p>These are examples of the important choices that must be made <em>once the
model class is selected</em>. These choices are often represented as
<em>hyperparameters</em>, or parameters that must be set before the model is
fit to data. In Scikit-Learn, hyperparameters are chosen by passing
values at model instantiation. We will explore how you can
quantitatively choose hyperparameters in
<a data-type="xref" href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">Chapter 39</a>.</p>
<p>For our linear regression example, we can instantiate the
<code>LinearRegression</code> class and specify that we’d like to fit the
intercept using the <code>fit_intercept</code> hyperparameter:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">(</code><code class="n">fit_intercept</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="n">model</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">LinearRegression</code><code class="p">()</code></pre>
<p>Keep in mind that when the model is instantiated, the only action is the
storing of these hyperparameter values. In particular, we have not yet
applied the model to any data: the Scikit-Learn API makes very clear the
distinction between <em>choice of model</em> and <em>application of model to
data</em>.</p>
</div></section>
<section data-pdf-bookmark="3. Arrange data into a features matrix and target vector" data-type="sect3"><div class="sect3" id="ch_0502-introducing-scikit-learn_3.-arrange-data-into-a-features-matrix-and-target-vector">
<h3>3. Arrange data into a features matrix and target vector</h3>
<p>Previously we examined the Scikit-Learn data representation, which
requires a two-dimensional features matrix and a one-dimensional target
array. Here our target variable <code>y</code> is already in the correct form (a
length-<code>n_samples</code> array), but we need to massage the data <code>x</code> to make
it a matrix of size <code>[n_samples, n_features]</code>.</p>
<p>In this case, this
amounts to a simple reshaping of the one-dimensional array:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">X</code> <code class="o">=</code> <code class="n">x</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]</code>
        <code class="n">X</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="4. Fit the model to the data" data-type="sect3"><div class="sect3" id="ch_0502-introducing-scikit-learn_4.-fit-the-model-to-the-data">
<h3>4. Fit the model to the data</h3>
<p>Now it is time to apply our model to the data. This can be done with the
<code>fit</code> method of the model:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">LinearRegression</code><code class="p">()</code></pre>
<p>This <code>fit</code> command causes a number of model-dependent internal
computations to take place, and the results of these computations are
stored in model-specific attributes that the user can explore. In
Scikit-Learn, by convention all model parameters that were learned
during the <code>fit</code> process have trailing underscores; for example in this
linear model, we have the following:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="n">model</code><code class="o">.</code><code class="n">coef_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code><code class="mf">1.9776566</code><code class="p">])</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="n">model</code><code class="o">.</code><code class="n">intercept_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="o">-</code><code class="mf">0.9033107255311146</code></pre>
<p>These two parameters represent the slope and intercept of the simple
linear fit to the data. Comparing the results to the data definition, we
see that they are close to the values used to generate the data: a slope
of 2 and intercept of –1.</p>
<p>One question that frequently comes up regards the uncertainty in such
internal model parameters. In general, Scikit-Learn does not provide
tools to draw conclusions from internal model parameters themselves:
interpreting model parameters is much more a <em>statistical modeling</em>
question than a <em>machine learning</em> question. Machine learning instead
focuses on what the model <em>predicts</em>. If you would like to dive into the
meaning of fit parameters within the model, other tools are available,
including the <a href="https://oreil.ly/adDFZ"><code>statsmodels</code> Python
package</a>.</p>
</div></section>
<section data-pdf-bookmark="5. Predict labels for unknown data" data-type="sect3"><div class="sect3" id="ch_0502-introducing-scikit-learn_5.-predict-labels-for-unknown-data">
<h3>5. Predict labels for unknown data</h3>
<p>Once the model is trained, the main task of supervised machine learning
is to evaluate it based on what it says about new data that was not part
of the training set. In Scikit-Learn, this can be done using the
<code>predict</code> method. For the sake of this example, our “new data” will be
a grid of <em>x</em> values, and we will ask what <em>y</em> values the model
predicts:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">xfit</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">11</code><code class="p">)</code></pre>
<p>As before, we need to coerce these <em>x</em> values into a
<code>[n_samples, n_features]</code> features matrix, after which we can feed it to
the model:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="n">Xfit</code> <code class="o">=</code> <code class="n">xfit</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]</code>
         <code class="n">yfit</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">Xfit</code><code class="p">)</code></pre>
<p>Finally, let’s visualize the results by plotting first the
raw data, and then this model fit (see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_41_0">Figure 38-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">yfit</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_41_0">
<img alt="output 41 0" height="394" src="assets/output_41_0.png" width="600"/>
<h6><span class="label">Figure 38-4. </span>A simple linear regression fit to the data</h6>
</div></figure>
<p>Typically the efficacy of the model is evaluated by comparing its
results to some known baseline, as we will see in the next example.<a data-startref="ix_ch38-asciidoc10" data-type="indexterm" id="idm45858743785424"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Supervised Learning Example: Iris Classification" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_supervised-learning-example-iris-classification">
<h2>Supervised Learning Example: Iris Classification</h2>
<p><a data-primary="Estimator API" data-secondary="Iris classification example" data-type="indexterm" id="idm45858743783056"/><a data-primary="Iris dataset" data-secondary="classification" data-type="indexterm" id="idm45858743782112"/>Let’s take a look at another example of this process, using
the Iris dataset we discussed earlier. Our question will be this: given
a model trained on a portion of the Iris data, how well can we predict
the remaining labels?</p>
<p><a data-primary="Gaussian naive Bayes classification" data-type="indexterm" id="idm45858743780784"/>For this task, we will use a simple generative model known as <em>Gaussian
naive Bayes</em>, which proceeds by assuming each class is drawn from an
axis-aligned Gaussian distribution (see <a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a> for more details). Because it is so
fast and has no hyperparameters to choose, Gaussian naive Bayes is often
a good model to use as a baseline classification, before exploring
whether improvements can be found through more sophisticated models.</p>
<p>We would like to evaluate the model on data it has not seen before, so
we will split the data into a <em>training set</em> and a <em>testing set</em>. This
could be done by hand, but it is more convenient to use the
<code>train_test_split</code> utility function:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
         <code class="n">Xtrain</code><code class="p">,</code> <code class="n">Xtest</code><code class="p">,</code> <code class="n">ytrain</code><code class="p">,</code> <code class="n">ytest</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X_iris</code><code class="p">,</code> <code class="n">y_iris</code><code class="p">,</code>
                                                         <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
<p class="pagebreak-before less_space">With the data arranged, we can follow our recipe to predict the labels:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code> <code class="c1"># 1. choose model class</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">GaussianNB</code><code class="p">()</code>                       <code class="c1"># 2. instantiate model</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">Xtrain</code><code class="p">,</code> <code class="n">ytrain</code><code class="p">)</code>                  <code class="c1"># 3. fit model to data</code>
         <code class="n">y_model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">Xtest</code><code class="p">)</code>             <code class="c1"># 4. predict on new data</code></pre>
<p>Finally, we can use the <code>accuracy_score</code> utility to see the fraction of
predicted labels that match their true values:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
         <code class="n">accuracy_score</code><code class="p">(</code><code class="n">ytest</code><code class="p">,</code> <code class="n">y_model</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="mf">0.9736842105263158</code></pre>
<p>With an accuracy topping 97%, we see that even this very naive
classification algorithm is effective for this particular dataset!</p>
</div></section>
<section data-pdf-bookmark="Unsupervised Learning Example: Iris Dimensionality" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_unsupervised-learning-example-iris-dimensionality">
<h2>Unsupervised Learning Example: Iris Dimensionality</h2>
<p><a data-primary="Estimator API" data-secondary="Iris dimensionality example" data-type="indexterm" id="idm45858743613840"/><a data-primary="Iris dataset" data-secondary="dimensionality" data-type="indexterm" id="idm45858743612896"/><a data-primary="unsupervised learning" data-secondary="dimensionality reduction" data-type="indexterm" id="idm45858743611952"/>As an example of an unsupervised learning problem, let’s
take a look at reducing the dimensionality of the Iris data so as to
more easily visualize it. Recall that the Iris data is four-dimensional:
there are four features recorded for each sample.</p>
<p>The task of dimensionality reduction centers around determining whether
there is a suitable lower-dimensional representation that retains the
essential features of the data. Often dimensionality reduction is used
as an aid to visualizing data: after all, it is much easier to plot data
in two dimensions than in four dimensions or more!</p>
<p>Here we will use <em>principal component analysis</em> (PCA; see
<a data-type="xref" href="ch45.xhtml#section-0509-principal-component-analysis">Chapter 45</a>), which is a fast linear dimensionality reduction
technique. We will ask the model to return two components—that is, a
two-dimensional representation of the data.</p>
<p>Following the sequence of steps outlined earlier, we have:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>  <code class="c1"># 1. choose model class</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>            <code class="c1"># 2. instantiate model</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_iris</code><code class="p">)</code>                      <code class="c1"># 3. fit model to data</code>
         <code class="n">X_2D</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_iris</code><code class="p">)</code>         <code class="c1"># 4. transform the data</code></pre>
<p>Now let’s plot the results. A quick way to do this is to
insert the results into the original Iris <code>DataFrame</code>, and use
Seaborn’s <code>lmplot</code> to show the results (see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_53_0">Figure 38-5</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="n">iris</code><code class="p">[</code><code class="s1">'PCA1'</code><code class="p">]</code> <code class="o">=</code> <code class="n">X_2D</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code>
         <code class="n">iris</code><code class="p">[</code><code class="s1">'PCA2'</code><code class="p">]</code> <code class="o">=</code> <code class="n">X_2D</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code>
         <code class="n">sns</code><code class="o">.</code><code class="n">lmplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s2">"PCA1"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"PCA2"</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="s1">'species'</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">iris</code><code class="p">,</code> <code class="n">fit_reg</code><code class="o">=</code><code class="kc">False</code><code class="p">);</code></pre>
<p>We see that in the two-dimensional representation, the species are
fairly well separated, even though the PCA algorithm had no knowledge of
the species labels! This suggests to us that a relatively
straightforward classification will probably be effective on the
dataset, as we saw before.</p>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_53_0">
<img alt="output 53 0" height="399" src="assets/output_53_0.png" width="600"/>
<h6><span class="label">Figure 38-5. </span>The Iris data projected to two dimensions<sup><a data-type="noteref" href="ch38.xhtml#idm45858743482640" id="idm45858743482640-marker">3</a></sup></h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Unsupervised Learning Example: Iris Clustering" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_unsupervised-learning-example-iris-clustering">
<h2>Unsupervised Learning Example: Iris Clustering</h2>
<p><a data-primary="Estimator API" data-secondary="Iris clustering example" data-type="indexterm" id="idm45858743479616"/><a data-primary="Iris dataset" data-secondary="clustering" data-type="indexterm" id="idm45858743478416"/><a data-primary="unsupervised learning" data-secondary="clustering" data-type="indexterm" id="idm45858743477472"/>Let’s next look at applying clustering to the Iris data. A
clustering algorithm attempts to find distinct groups of data without
reference to any labels. <a data-primary="clustering" data-secondary="Gaussian mixture models" data-type="indexterm" id="idm45858743476400"/><a data-primary="Gaussian mixture models (GMMs)" data-secondary="clustering with" data-type="indexterm" id="idm45858743475456"/>Here we will use a powerful clustering method
called a <em>Gaussian mixture model</em> (GMM), discussed in more detail in
<a data-type="xref" href="ch48.xhtml#section-0512-gaussian-mixtures">Chapter 48</a>. A
GMM attempts to model the data as a collection of Gaussian blobs.</p>
<p>We can fit the Gaussian mixture model as follows:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.mixture</code> <code class="kn">import</code> <code class="n">GaussianMixture</code>      <code class="c1"># 1. choose model class</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                                 <code class="n">covariance_type</code><code class="o">=</code><code class="s1">'full'</code><code class="p">)</code>  <code class="c1"># 2. instantiate model</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_iris</code><code class="p">)</code>                                <code class="c1"># 3. fit model to data</code>
         <code class="n">y_gmm</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_iris</code><code class="p">)</code>                    <code class="c1"># 4. determine labels</code></pre>
<p>As before, we will add the cluster label to the Iris <code>DataFrame</code> and use
Seaborn to plot the results (see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_58_0">Figure 38-6</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="n">iris</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">]</code> <code class="o">=</code> <code class="n">y_gmm</code>
         <code class="n">sns</code><code class="o">.</code><code class="n">lmplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s2">"PCA1"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"PCA2"</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">iris</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="s1">'species'</code><code class="p">,</code>
                    <code class="n">col</code><code class="o">=</code><code class="s1">'cluster'</code><code class="p">,</code> <code class="n">fit_reg</code><code class="o">=</code><code class="kc">False</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_58_0">
<img alt="output 58 0" height="409" src="assets/output_58_0.png" width="600"/>
<h6><span class="label">Figure 38-6. </span><span class="roman">k</span>-means clusters within the Iris data<sup><a data-type="noteref" href="ch38.xhtml#idm45858743295632" id="idm45858743295632-marker">4</a></sup></h6>
</div></figure>
<p>By splitting the data by cluster number, we see exactly how well the GMM
algorithm has recovered the underlying labels: the <em>setosa</em> species is
separated perfectly within cluster 0, while there remains a small amount
of mixing between <em>versicolor</em> and <em>virginica</em>. This means that even
without an expert to tell us the species labels of the individual
flowers, the measurements of these flowers are distinct enough that we
could <em>automatically</em> identify the presence of these different groups of
species with a simple clustering algorithm! This sort of algorithm might
further give experts in the field clues as to the relationships between
the samples they are observing.<a data-startref="ix_ch38-asciidoc9" data-type="indexterm" id="idm45858743291840"/><a data-startref="ix_ch38-asciidoc8" data-type="indexterm" id="idm45858743291136"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Application: Exploring Handwritten Digits" data-type="sect1"><div class="sect1" id="ch_0502-introducing-scikit-learn_application-exploring-handwritten-digits">
<h1>Application: Exploring Handwritten Digits</h1>
<p><a data-primary="optical character recognition" data-secondary="Scikit-Learn application" data-type="indexterm" id="ix_ch38-asciidoc11"/><a data-primary="Scikit-Learn package" data-secondary="handwritten digit application" data-type="indexterm" id="ix_ch38-asciidoc12"/>To demonstrate these principles on a more interesting problem,
let’s consider one piece of the optical character
recognition problem: the identification of handwritten digits. In the
wild, this problem involves both locating and identifying characters in
an image. Here we’ll take a shortcut and use
Scikit-Learn’s set of preformatted digits, which is built
into the library.</p>
<section data-pdf-bookmark="Loading and Visualizing the Digits Data" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_loading-and-visualizing-the-digits-data">
<h2>Loading and Visualizing the Digits Data</h2>
<p><a data-primary="optical character recognition" data-secondary="loading/visualizing digits data" data-type="indexterm" id="ix_ch38-asciidoc13"/>We can use Scikit-Learn’s data access interface to take a
look at this data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
         <code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>
         <code class="n">digits</code><code class="o">.</code><code class="n">images</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code></pre>
<p>The images data is a three-dimensional array: 1,797 samples each
consisting of an 8 × 8 grid of pixels. Let’s visualize the
first hundred of these (see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_65_0">Figure 38-7</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

         <code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">),</code>
                                  <code class="n">subplot_kw</code><code class="o">=</code><code class="p">{</code><code class="s1">'xticks'</code><code class="p">:[],</code> <code class="s1">'yticks'</code><code class="p">:[]},</code>
                                  <code class="n">gridspec_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">hspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>

         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axes</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'binary'</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'nearest'</code><code class="p">)</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mf">0.05</code><code class="p">,</code> <code class="mf">0.05</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">[</code><code class="n">i</code><code class="p">]),</code>
                     <code class="n">transform</code><code class="o">=</code><code class="n">ax</code><code class="o">.</code><code class="n">transAxes</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'green'</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_65_0">
<img alt="output 65 0" height="585" src="assets/output_65_0.png" width="600"/>
<h6><span class="label">Figure 38-7. </span>The handwritten digits data; each sample is represented by one 8 × 8 grid of pixels</h6>
</div></figure>
<p>In order to work with this data within Scikit-Learn, we need a
two-dimensional, <code>[n_samples, n_features]</code> representation. We can
accomplish this by treating each pixel in the image as a feature: that
is, by flattening out the pixel arrays so that we have a length-64 array
of pixel values representing each digit. Additionally, we need the
target array, which gives the previously determined label for each
digit. These two quantities are built into the digits dataset under the
<code>data</code> and <code>target</code> attributes, respectively:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="n">X</code> <code class="o">=</code> <code class="n">digits</code><code class="o">.</code><code class="n">data</code>
         <code class="n">X</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">64</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="n">y</code> <code class="o">=</code> <code class="n">digits</code><code class="o">.</code><code class="n">target</code>
         <code class="n">y</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,)</code></pre>
<p>We see here that there are 1,797 samples and 64 features.<a data-startref="ix_ch38-asciidoc13" data-type="indexterm" id="idm45858743006768"/></p>
</div></section>
<section data-pdf-bookmark="Unsupervised Learning Example: Dimensionality Reduction" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_unsupervised-learning-example-dimensionality-reduction">
<h2>Unsupervised Learning Example: Dimensionality Reduction</h2>
<p><a data-primary="unsupervised learning" data-secondary="dimensionality reduction" data-type="indexterm" id="idm45858743004608"/>We’d like to visualize our points within the 64-dimensional
parameter space, but it’s difficult to effectively visualize
points in such a high-dimensional space. Instead, we’ll
reduce the number of dimensions, using an unsupervised method. <a data-primary="Isomap" data-secondary="dimensionality reduction" data-type="indexterm" id="idm45858743003536"/>Here,
we’ll make use of a manifold learning algorithm called
Isomap (see <a data-type="xref" href="ch46.xhtml#section-0510-manifold-learning">Chapter 46</a>) and transform the data to two dimensions:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">26</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">Isomap</code>
         <code class="n">iso</code> <code class="o">=</code> <code class="n">Isomap</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
         <code class="n">iso</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">data_projected</code> <code class="o">=</code> <code class="n">iso</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">data_projected</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">26</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code></pre>
<p>We see that the projected data is now two-dimensional. Let’s
plot this data to see if we can learn anything from its structure (see
<a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_73_0">Figure 38-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">27</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">data_projected</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">data_projected</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">,</code>
                     <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code>
                     <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s1">'viridis'</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">colorbar</code><code class="p">(</code><code class="n">label</code><code class="o">=</code><code class="s1">'digit label'</code><code class="p">,</code> <code class="n">ticks</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">clim</code><code class="p">(</code><code class="o">-</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">9.5</code><code class="p">);</code></pre>
<p>This plot gives us some good intuition into how well various numbers are
separated in the larger 64-dimensional space. For example, zeros and
ones have very little overlap in the parameter space. Intuitively, this
makes sense: a zero is empty in the middle of the image, while a one
will generally have ink in the middle. On the other hand, there seems to
be a more or less continuous spectrum between ones and fours: we can
understand this by realizing that some people draw ones with “hats” on
them, which causes them to look similar to fours.</p>
<p>Overall, however, despite some mixing at the edges, the different groups
appear to be fairly well localized in the parameter space: this suggests
that even a very straightforward supervised classification algorithm
should perform suitably on the full high-dimensional dataset.
Let’s give it a try.</p>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_73_0">
<img alt="output 73 0" height="392" src="assets/output_73_0.png" width="600"/>
<h6><span class="label">Figure 38-8. </span>An Isomap embedding of the digits data</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Classification on Digits" data-type="sect2"><div class="sect2" id="ch_0502-introducing-scikit-learn_classification-on-digits">
<h2>Classification on Digits</h2>
<p><a data-primary="optical character recognition" data-secondary="digit classification" data-type="indexterm" id="ix_ch38-asciidoc14"/>Let’s apply a classification algorithm to the digits data.
<a data-primary="Gaussian naive Bayes classification" data-type="indexterm" id="idm45858742848240"/>As we did with the Iris data previously, we will split the data into
training and testing sets and fit a Gaussian naive Bayes model:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">28</code><code class="p">]:</code> <code class="n">Xtrain</code><code class="p">,</code> <code class="n">Xtest</code><code class="p">,</code> <code class="n">ytrain</code><code class="p">,</code> <code class="n">ytest</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">29</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">GaussianNB</code><code class="p">()</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">Xtrain</code><code class="p">,</code> <code class="n">ytrain</code><code class="p">)</code>
         <code class="n">y_model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">Xtest</code><code class="p">)</code></pre>
<p>Now that we have the model’s predictions, we can gauge its
accuracy by comparing the true values of the test set to the
predictions:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">30</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
         <code class="n">accuracy_score</code><code class="p">(</code><code class="n">ytest</code><code class="p">,</code> <code class="n">y_model</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">30</code><code class="p">]:</code> <code class="mf">0.8333333333333334</code></pre>
<p>With even this very simple model, we find about 83% accuracy for
classification of the digits! However, this single number
doesn’t tell us where we’ve gone wrong. <a data-primary="confusion matrix" data-type="indexterm" id="idm45858742663760"/>One nice
way to do this is to use the <em>confusion matrix</em>, which we can compute
with Scikit-Learn and plot with Seaborn (see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_81_0">Figure 38-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">31</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>

         <code class="n">mat</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">ytest</code><code class="p">,</code> <code class="n">y_model</code><code class="p">)</code>

         <code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">mat</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">cbar</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'Blues'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'predicted value'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'true value'</code><code class="p">);</code></pre>
<p>This shows us where the mislabeled points tend to be: for example, many
of the twos here are misclassified as either ones or eights.</p>
<figure class="width-60"><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_81_0">
<img alt="output 81 0" height="600" src="assets/output_81_0.png" width="600"/>
<h6><span class="label">Figure 38-9. </span>A confusion matrix showing the frequency of misclassifications by our 
<span class="keep-together">classifier</span></h6>
</div></figure>
<p>Another way to gain intuition into the characteristics of the model is
to plot the inputs again, with their predicted labels. We’ll
use green for correct labels and red for incorrect labels; see <a data-type="xref" href="#fig_0502-introducing-scikit-learn_files_in_output_83_0">Figure 38-10</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">32</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">),</code>
                                  <code class="n">subplot_kw</code><code class="o">=</code><code class="p">{</code><code class="s1">'xticks'</code><code class="p">:[],</code> <code class="s1">'yticks'</code><code class="p">:[]},</code>
                                  <code class="n">gridspec_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">hspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>

         <code class="n">test_images</code> <code class="o">=</code> <code class="n">Xtest</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code>

         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axes</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">test_images</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'binary'</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'nearest'</code><code class="p">)</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mf">0.05</code><code class="p">,</code> <code class="mf">0.05</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">y_model</code><code class="p">[</code><code class="n">i</code><code class="p">]),</code>
                     <code class="n">transform</code><code class="o">=</code><code class="n">ax</code><code class="o">.</code><code class="n">transAxes</code><code class="p">,</code>
                     <code class="n">color</code><code class="o">=</code><code class="s1">'green'</code> <code class="k">if</code> <code class="p">(</code><code class="n">ytest</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">==</code> <code class="n">y_model</code><code class="p">[</code><code class="n">i</code><code class="p">])</code> <code class="k">else</code> <code class="s1">'red'</code><code class="p">)</code></pre>
<p>Examining this subset of the data can give us some insight into where
the algorithm might be not performing optimally. To go beyond our 83%
classification success rate, we might switch to a more sophisticated
algorithm such as support vector machines (see
<a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>), random forests (see
<a data-type="xref" href="ch44.xhtml#section-0508-random-forests">Chapter 44</a>), or another classification approach<a data-startref="ix_ch38-asciidoc14" data-type="indexterm" id="idm45858742420656"/>.<a data-startref="ix_ch38-asciidoc12" data-type="indexterm" id="idm45858742419888"/><a data-startref="ix_ch38-asciidoc11" data-type="indexterm" id="idm45858742419184"/></p>
<figure><div class="figure" id="fig_0502-introducing-scikit-learn_files_in_output_83_0">
<img alt="output 83 0" height="585" src="assets/output_83_0.png" width="600"/>
<h6><span class="label">Figure 38-10. </span>Data showing correct (green) and incorrect (red) labels; for a color version of this plot, see the <a href="https://oreil.ly/PDSH_GitHub">online version of the book</a></h6>
</div></figure>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch_0502-introducing-scikit-learn_summary">
<h1>Summary</h1>
<p>In this chapter we covered the essential features of the Scikit-Learn
data representation and the Estimator API. Regardless of the type of
estimator used, the same import/instantiate/fit/predict pattern holds.
Armed with this information, you can explore the
Scikit-Learn documentation and try out various models on your
data.<a data-startref="ix_ch38-asciidoc2" data-type="indexterm" id="idm45858742414432"/><a data-startref="ix_ch38-asciidoc1" data-type="indexterm" id="idm45858742385040"/><a data-startref="ix_ch38-asciidoc0" data-type="indexterm" id="idm45858742384432"/></p>
<p>In the next chapter, we will explore perhaps the most important topic in
machine learning: how to select and validate your model.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858744412448"><sup><a href="ch38.xhtml#idm45858744412448-marker">1</a></sup> A full-size, full-color version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p><p data-type="footnote" id="idm45858744315904"><sup><a href="ch38.xhtml#idm45858744315904-marker">2</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/J8V6U">online appendix</a>.</p><p data-type="footnote" id="idm45858743482640"><sup><a href="ch38.xhtml#idm45858743482640-marker">3</a></sup> A full-color version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p><p data-type="footnote" id="idm45858743295632"><sup><a href="ch38.xhtml#idm45858743295632-marker">4</a></sup> A full-size, full-color version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p></div></div></section></div></body></html>