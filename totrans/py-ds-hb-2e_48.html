<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 43. In Depth: Support Vector Machines" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0507-support-vector-machines">
<h1><span class="label">Chapter 43. </span>In Depth: Support Vector Machines</h1>
<p><a data-primary="machine learning" data-secondary="support vector machines" data-type="indexterm" id="ix_ch43-asciidoc0"/><a data-primary="support vector machines (SVMs)" data-type="indexterm" id="ix_ch43-asciidoc1"/>Support vector machines (SVMs) are a particularly powerful and flexible
class of supervised algorithms for both classification and regression.
In this chapter, we will explore the intuition behind SVMs and their use
in classification problems.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>
        <code class="kn">from</code> <code class="nn">scipy</code> <code class="kn">import</code> <code class="n">stats</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Full-size, full-color figures are available in the <a href="https://oreil.ly/PDSH_GitHub">supplemental materials on GitHub</a>.</p>
</div>
<section data-pdf-bookmark="Motivating Support Vector Machines" data-type="sect1"><div class="sect1" id="ch_0507-support-vector-machines_motivating-support-vector-machines">
<h1>Motivating Support Vector Machines</h1>
<p><a data-primary="support vector machines (SVMs)" data-secondary="motivating" data-type="indexterm" id="ix_ch43-asciidoc2"/>As part of our discussion of Bayesian classification (see
<a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a>), we
learned about a simple kind of model that describes the distribution of
each underlying class, and experimented with using it to
probabilistically determine labels for new points. That was an example
of <em>generative classification</em>; here we will consider instead
<a data-primary="discriminative classification" data-type="indexterm" id="ix_ch43-asciidoc3"/><em>discriminative classification</em>. That is, rather than modeling each
class, we will simply find a line or curve (in two dimensions) or
manifold (in multiple dimensions) that divides the classes from each
other.</p>
<p class="pagebreak-before less_space">As an example of this, consider the simple case of a classification task
in which the two classes of points are well separated (see <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_5_0">Figure 43-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>
        <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                          <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">cluster_std</code><code class="o">=</code><code class="mf">0.60</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">);</code></pre>
<figure class="width-80"><div class="figure" id="fig_0507-support-vector-machines_files_in_output_5_0">
<img alt="output 5 0" height="392" src="assets/output_5_0.png" width="600"/>
<h6><span class="label">Figure 43-1. </span>Simple data for classification</h6>
</div></figure>
<p>A linear discriminative classifier would attempt to draw a straight line
separating the two sets of data, and thereby create a model for
classification. For two-dimensional data like that shown here, this is a
task we could do by hand. But immediately we see a problem: there is
more than one possible dividing line that can perfectly discriminate
between the two classes!</p>
<p>We can draw some of them as follows; <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_7_0">Figure 43-2</a> shows the
result:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">xfit</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mf">3.5</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="mf">0.6</code><code class="p">],</code> <code class="p">[</code><code class="mf">2.1</code><code class="p">],</code> <code class="s1">'x'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code> <code class="n">markeredgewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">markersize</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>

        <code class="k">for</code> <code class="n">m</code><code class="p">,</code> <code class="n">b</code> <code class="ow">in</code> <code class="p">[(</code><code class="mi">1</code><code class="p">,</code> <code class="mf">0.65</code><code class="p">),</code> <code class="p">(</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">1.6</code><code class="p">),</code> <code class="p">(</code><code class="o">-</code><code class="mf">0.2</code><code class="p">,</code> <code class="mf">2.9</code><code class="p">)]:</code>
            <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">m</code> <code class="o">*</code> <code class="n">xfit</code> <code class="o">+</code> <code class="n">b</code><code class="p">,</code> <code class="s1">'-k'</code><code class="p">)</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mf">3.5</code><code class="p">);</code></pre>
<figure class="width-80"><div class="figure" id="fig_0507-support-vector-machines_files_in_output_7_0">
<img alt="output 7 0" height="391" src="assets/output_7_0.png" width="600"/>
<h6><span class="label">Figure 43-2. </span>Three perfect linear discriminative classifiers for our data</h6>
</div></figure>
<p>These are three <em>very</em> different separators which, nevertheless,
perfectly discriminate between these samples. Depending on which you
choose, a new data point (e.g., the one marked by the “X” in this
plot) will be assigned a different label! Evidently our simple intuition
of “drawing a line between classes” is not good enough, and we need to
think a bit more deeply.<a data-startref="ix_ch43-asciidoc3" data-type="indexterm" id="idm45858733603248"/></p>
</div></section>
<section data-pdf-bookmark="Support Vector Machines: Maximizing the Margin" data-type="sect1"><div class="sect1" id="ch_0507-support-vector-machines_support-vector-machines-maximizing-the-margin">
<h1>Support Vector Machines: Maximizing the Margin</h1>
<p><a data-primary="margins, maximizing" data-type="indexterm" id="ix_ch43-asciidoc4"/><a data-primary="support vector machines (SVMs)" data-secondary="maximizing the margin" data-type="indexterm" id="ix_ch43-asciidoc5"/>Support vector machines offer one way to improve on this. The intuition
is this: rather than simply drawing a zero-width line between the
classes, we can draw around each line a <em>margin</em> of some width, up to
the nearest point. Here is an example of how this might look (<a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_10_0">Figure 43-3</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">xfit</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mf">3.5</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>

        <code class="k">for</code> <code class="n">m</code><code class="p">,</code> <code class="n">b</code><code class="p">,</code> <code class="n">d</code> <code class="ow">in</code> <code class="p">[(</code><code class="mi">1</code><code class="p">,</code> <code class="mf">0.65</code><code class="p">,</code> <code class="mf">0.33</code><code class="p">),</code> <code class="p">(</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">1.6</code><code class="p">,</code> <code class="mf">0.55</code><code class="p">),</code> <code class="p">(</code><code class="o">-</code><code class="mf">0.2</code><code class="p">,</code> <code class="mf">2.9</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">)]:</code>
            <code class="n">yfit</code> <code class="o">=</code> <code class="n">m</code> <code class="o">*</code> <code class="n">xfit</code> <code class="o">+</code> <code class="n">b</code>
            <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">yfit</code><code class="p">,</code> <code class="s1">'-k'</code><code class="p">)</code>
            <code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">yfit</code> <code class="o">-</code> <code class="n">d</code><code class="p">,</code> <code class="n">yfit</code> <code class="o">+</code> <code class="n">d</code><code class="p">,</code> <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code>
                             <code class="n">color</code><code class="o">=</code><code class="s1">'lightgray'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mf">3.5</code><code class="p">);</code></pre>
<p>The line that maximizes this margin is the one we will choose as the
optimal model.</p>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_10_0">
<img alt="output 10 0" height="398" src="assets/output_10_0.png" width="600"/>
<h6><span class="label">Figure 43-3. </span>Visualization of “margins” within discriminative classifiers</h6>
</div></figure>
<section data-pdf-bookmark="Fitting a Support Vector Machine" data-type="sect2"><div class="sect2" id="ch_0507-support-vector-machines_fitting-a-support-vector-machine">
<h2>Fitting a Support Vector Machine</h2>
<p><a data-primary="Scikit-Learn package" data-secondary="support vector classifier" data-type="indexterm" id="ix_ch43-asciidoc6"/><a data-primary="support vector classifier" data-type="indexterm" id="ix_ch43-asciidoc7"/><a data-primary="support vector machines (SVMs)" data-secondary="fitting" data-type="indexterm" id="ix_ch43-asciidoc8"/>Let’s see the result of an actual fit to this data: we will
use Scikit-Learn’s support vector classifier (<code>SVC</code>) to
train an SVM model on this data. For the time being, we will use a
linear kernel and set the <code>C</code> parameter to a very large number
(we’ll discuss the meaning of these in more depth
momentarily):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code> <code class="c1"># "Support vector classifier"</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">1E10</code><code class="p">)</code>
        <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">SVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mf">10000000000.0</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">)</code></pre>
<p>To better visualize what’s happening here, let’s
create a quick convenience function that will plot SVM decision
boundaries for us (<a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_16_0">Figure 43-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">plot_svc_decision_function</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">plot_support</code><code class="o">=</code><code class="kc">True</code><code class="p">):</code>
            <code class="sd">"""Plot the decision function for a 2D SVC"""</code>
            <code class="k">if</code> <code class="n">ax</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
                <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>
            <code class="n">xlim</code> <code class="o">=</code> <code class="n">ax</code><code class="o">.</code><code class="n">get_xlim</code><code class="p">()</code>
            <code class="n">ylim</code> <code class="o">=</code> <code class="n">ax</code><code class="o">.</code><code class="n">get_ylim</code><code class="p">()</code>

            <code class="c1"># create grid to evaluate model</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="n">xlim</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">xlim</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="mi">30</code><code class="p">)</code>
            <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="n">ylim</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">ylim</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="mi">30</code><code class="p">)</code>
            <code class="n">Y</code><code class="p">,</code> <code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">meshgrid</code><code class="p">(</code><code class="n">y</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code>
            <code class="n">xy</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">([</code><code class="n">X</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">Y</code><code class="o">.</code><code class="n">ravel</code><code class="p">()])</code><code class="o">.</code><code class="n">T</code>
            <code class="n">P</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">xy</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>

            <code class="c1"># plot decision boundary and margins</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">contour</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">Y</code><code class="p">,</code> <code class="n">P</code><code class="p">,</code> <code class="n">colors</code><code class="o">=</code><code class="s1">'k'</code><code class="p">,</code>
                       <code class="n">levels</code><code class="o">=</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code>
                       <code class="n">linestyles</code><code class="o">=</code><code class="p">[</code><code class="s1">'--'</code><code class="p">,</code> <code class="s1">'-'</code><code class="p">,</code> <code class="s1">'--'</code><code class="p">])</code>

            <code class="c1"># plot support vectors</code>
            <code class="k">if</code> <code class="n">plot_support</code><code class="p">:</code>
                <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">support_vectors_</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code>
                           <code class="n">model</code><code class="o">.</code><code class="n">support_vectors_</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code>
                           <code class="n">s</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">edgecolors</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code>
                           <code class="n">facecolors</code><code class="o">=</code><code class="s1">'none'</code><code class="p">);</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="n">xlim</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="n">ylim</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>
        <code class="n">plot_svc_decision_function</code><code class="p">(</code><code class="n">model</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_16_0">
<img alt="output 16 0" height="398" src="assets/output_16_0.png" width="600"/>
<h6><span class="label">Figure 43-4. </span>A support vector machine classifier fit to the data, with margins (dashed lines) and support vectors (circles) shown</h6>
</div></figure>
<p>This is the dividing line that maximizes the margin between the two sets
of points. Notice that a few of the training points just touch the
margin: they are circled in <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_20_0">Figure 43-5</a>. <a data-primary="support vector (defined)" data-type="indexterm" id="idm45858732945392"/>These points are the
pivotal elements of this fit; they are known as the <em>support vectors</em>,
and give the algorithm its name. In Scikit-Learn, the identities of
these points are stored in the <code>support_vectors_</code> attribute of the
classifier:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">model</code><code class="o">.</code><code class="n">support_vectors_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">array</code><code class="p">([[</code><code class="mf">0.44359863</code><code class="p">,</code> <code class="mf">3.11530945</code><code class="p">],</code>
               <code class="p">[</code><code class="mf">2.33812285</code><code class="p">,</code> <code class="mf">3.43116792</code><code class="p">],</code>
               <code class="p">[</code><code class="mf">2.06156753</code><code class="p">,</code> <code class="mf">1.96918596</code><code class="p">]])</code></pre>
<p>A key to this classifier’s success is that for the fit, only
the positions of the support vectors matter; any points further from the
margin that are on the correct side do not modify the fit. Technically,
this is because these points do not contribute to the loss function used
to fit the model, so their position and number do not matter so long as
they do not cross the margin.</p>
<p>We can see this, for example, if we plot the model learned from the
first 60 points and first 120 points of this dataset (<a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_20_0">Figure 43-5</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">plot_svm</code><code class="p">(</code><code class="n">N</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
            <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                              <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">cluster_std</code><code class="o">=</code><code class="mf">0.60</code><code class="p">)</code>
            <code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="p">[:</code><code class="n">N</code><code class="p">]</code>
            <code class="n">y</code> <code class="o">=</code> <code class="n">y</code><code class="p">[:</code><code class="n">N</code><code class="p">]</code>
            <code class="n">model</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">1E10</code><code class="p">)</code>
            <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

            <code class="n">ax</code> <code class="o">=</code> <code class="n">ax</code> <code class="ow">or</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">6</code><code class="p">)</code>
            <code class="n">plot_svc_decision_function</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">ax</code><code class="p">)</code>

        <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
        <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">left</code><code class="o">=</code><code class="mf">0.0625</code><code class="p">,</code> <code class="n">right</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">axi</code><code class="p">,</code> <code class="n">N</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">ax</code><code class="p">,</code> <code class="p">[</code><code class="mi">60</code><code class="p">,</code> <code class="mi">120</code><code class="p">]):</code>
            <code class="n">plot_svm</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="n">axi</code><code class="p">)</code>
            <code class="n">axi</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'N = </code><code class="si">{0}</code><code class="s1">'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">N</code><code class="p">))</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_20_0">
<img alt="output 20 0" height="383" src="assets/output_20_0.png" width="600"/>
<h6><span class="label">Figure 43-5. </span>The influence of new training points on the SVM model</h6>
</div></figure>
<p>In the left panel, we see the model and the support vectors for 60
training points. In the right panel, we have doubled the number of
training points, but the model has not changed: the three support
vectors in the left panel are the same as the support vectors in the
right panel. This insensitivity to the exact behavior of distant points
is one of the strengths of the SVM model.</p>
<p>If you are running this notebook live, you can use IPython’s
interactive widgets to view this feature of the SVM model interactively:<a data-startref="ix_ch43-asciidoc8" data-type="indexterm" id="idm45858732591328"/><a data-startref="ix_ch43-asciidoc7" data-type="indexterm" id="idm45858732647184"/><a data-startref="ix_ch43-asciidoc6" data-type="indexterm" id="idm45858732646512"/></p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">ipywidgets</code> <code class="kn">import</code> <code class="n">interact</code><code class="p">,</code> <code class="n">fixed</code>
         <code class="n">interact</code><code class="p">(</code><code class="n">plot_svm</code><code class="p">,</code> <code class="n">N</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">200</code><code class="p">),</code> <code class="n">ax</code><code class="o">=</code><code class="n">fixed</code><code class="p">(</code><code class="kc">None</code><code class="p">));</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="n">interactive</code><code class="p">(</code><code class="n">children</code><code class="o">=</code><code class="p">(</code><code class="n">IntSlider</code><code class="p">(</code><code class="n">value</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s1">'N'</code><code class="p">,</code> <code class="nb">max</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="nb">min</code><code class="o">=</code><code class="mi">10</code><code class="p">),</code>
          <code class="o">&gt;</code> <code class="n">Output</code><code class="p">()),</code> <code class="n">_dom_classes</code><code class="o">=</code><code class="p">(</code><code class="s1">'widget-...</code></pre>
</div></section>
<section data-pdf-bookmark="Beyond Linear Boundaries: Kernel SVM" data-type="sect2"><div class="sect2" id="ch_0507-support-vector-machines_beyond-linear-boundaries-kernel-svm">
<h2>Beyond Linear Boundaries: Kernel SVM</h2>
<p><a data-primary="kernel SVM" data-type="indexterm" id="ix_ch43-asciidoc9"/><a data-primary="support vector machines (SVMs)" data-secondary="kernels and" data-type="indexterm" id="ix_ch43-asciidoc10"/>Where SVM can become quite powerful is when it is combined with
<em>kernels</em>. We have seen a version of kernels before, in the basis
function regressions of <a data-type="xref" href="ch42.xhtml#section-0506-linear-regression">Chapter 42</a>. There we projected our data into a
higher-dimensional space defined by polynomials and Gaussian basis
functions, and thereby were able to fit for nonlinear relationships with
a linear classifier.</p>
<p>In SVM models, we can use a version of the same idea. To motivate the
need for kernels, let’s look at some data that is not
linearly separable (<a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_25_0">Figure 43-6</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_circles</code>
         <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_circles</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">factor</code><code class="o">=</code><code class="mf">.1</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">.1</code><code class="p">)</code>

         <code class="n">clf</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>
         <code class="n">plot_svc_decision_function</code><code class="p">(</code><code class="n">clf</code><code class="p">,</code> <code class="n">plot_support</code><code class="o">=</code><code class="kc">False</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_25_0">
<img alt="output 25 0" height="438" src="assets/output_25_0.png" width="600"/>
<h6><span class="label">Figure 43-6. </span>A linear classifier performs poorly for nonlinear boundaries</h6>
</div></figure>
<p>It is clear that no linear discrimination will <em>ever</em> be able to
separate this data. But we can draw a lesson from the basis function
regressions in <a data-type="xref" href="ch42.xhtml#section-0506-linear-regression">Chapter 42</a>, and think about how we might project the data into a higher
dimension such that a linear separator <em>would</em> be sufficient. <a data-primary="radial basis function (RBF)" data-type="indexterm" id="idm45858732402752"/><a data-primary="RBF (radial basis function)" data-type="indexterm" id="idm45858732402048"/>For
example, one simple projection we could use would be to compute a
<em>radial basis function</em> (RBF) centered on the middle clump:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">r</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="p">(</code><code class="n">X</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code></pre>
<p>We can visualize this extra data dimension using a three-dimensional
plot, as seen in <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_29_0">Figure 43-7</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">mpl_toolkits</code> <code class="kn">import</code> <code class="n">mplot3d</code>

         <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="n">projection</code><code class="o">=</code><code class="s1">'3d'</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">scatter3D</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">r</code><code class="p">,</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">view_init</code><code class="p">(</code><code class="n">elev</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">azim</code><code class="o">=</code><code class="mi">30</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'x'</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'y'</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">set_zlabel</code><code class="p">(</code><code class="s1">'r'</code><code class="p">);</code></pre>
<figure class="width-70"><div class="figure" id="fig_0507-support-vector-machines_files_in_output_29_0">
<img alt="output 29 0" height="565" src="assets/output_29_0.png" width="600"/>
<h6><span class="label">Figure 43-7. </span>A third dimension added to the data allows for linear separation</h6>
</div></figure>
<p>We can see that with this additional dimension, the data becomes
trivially linearly separable, by drawing a separating plane at, say,
<em>r</em>=0.7.</p>
<p>In this case we had to choose and carefully tune our projection: if we
had not centered our radial basis function in the right location, we
would not have seen such clean, linearly separable results. In general,
the need to make such a choice is a problem: we would like to somehow
automatically find the best basis functions to use.</p>
<p>One strategy to this end is to compute a basis function centered at
<em>every</em> point in the dataset, and let the SVM algorithm sift through the
results. <a data-primary="kernel transformation" data-type="indexterm" id="idm45858732260784"/>This type of basis function transformation is known as a
<em>kernel transformation</em>, as it is based on a similarity relationship (or
kernel) between each pair of points.</p>
<p>A potential problem with this strategy—projecting <math alttext="upper N">
<mi>N</mi>
</math> points
into <math alttext="upper N">
<mi>N</mi>
</math> dimensions—is that it might become very
computationally intensive as <math alttext="upper N">
<mi>N</mi>
</math> grows large. <a data-primary="kernel trick" data-type="indexterm" id="idm45858732208256"/>However,
because of a neat little procedure known as the
<a href="https://oreil.ly/h7PBj"><em>kernel trick</em></a>, a fit on
kernel-transformed data can be done implicitly—that is, without ever
building the full <math alttext="upper N">
<mi>N</mi>
</math>-dimensional representation of the
kernel projection. This kernel trick is built into the SVM, and is one
of the reasons the method is so powerful.</p>
<p>In Scikit-Learn, we can apply kernelized SVM simply by changing our
linear kernel to an RBF kernel, using the <code>kernel</code> model hyperparameter:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">clf</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'rbf'</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">1E6</code><code class="p">)</code>
         <code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">SVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mf">1000000.0</code><code class="p">)</code></pre>
<p>Let’s use our previously defined function to visualize the
fit and identify the support vectors (<a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_33_0">Figure 43-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>
         <code class="n">plot_svc_decision_function</code><code class="p">(</code><code class="n">clf</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">clf</code><code class="o">.</code><code class="n">support_vectors_</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">clf</code><code class="o">.</code><code class="n">support_vectors_</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code>
                     <code class="n">s</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">facecolors</code><code class="o">=</code><code class="s1">'none'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_33_0">
<img alt="output 33 0" height="392" src="assets/output_33_0.png" width="600"/>
<h6><span class="label">Figure 43-8. </span>Kernel SVM fit to the data</h6>
</div></figure>
<p>Using this kernelized support vector machine, we learn a suitable
nonlinear decision boundary. This kernel transformation strategy is used
often in machine learning to turn fast linear methods into fast
nonlinear methods, especially for models in which the kernel trick can
be used.<a data-startref="ix_ch43-asciidoc10" data-type="indexterm" id="idm45858732090672"/><a data-startref="ix_ch43-asciidoc9" data-type="indexterm" id="idm45858732089968"/></p>
</div></section>
<section data-pdf-bookmark="Tuning the SVM: Softening Margins" data-type="sect2"><div class="sect2" id="ch_0507-support-vector-machines_tuning-the-svm-softening-margins">
<h2>Tuning the SVM: Softening Margins</h2>
<p><a data-primary="support vector machines (SVMs)" data-secondary="softening margins" data-type="indexterm" id="ix_ch43-asciidoc11"/>Our discussion thus far has centered around very clean datasets, in
which a perfect decision boundary exists. But what if your data has some
amount of overlap? For example, you may have data like this (see <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_36_0">Figure 43-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                           <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">cluster_std</code><code class="o">=</code><code class="mf">1.2</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_36_0">
<img alt="output 36 0" height="381" src="assets/output_36_0.png" width="600"/>
<h6><span class="label">Figure 43-9. </span>Data with some level of overlap</h6>
</div></figure>
<p>To handle this case, the SVM implementation has a bit of a fudge factor
that “softens” the margin: that is, it allows some of the points to
creep into the margin if that allows a better fit. The hardness of the
margin is controlled by a tuning parameter, most often known as <code>C</code>. For
a very large <code>C</code>, the margin is hard, and points cannot lie in it. For a
smaller <code>C</code>, the margin is softer and can grow to encompass some points.</p>
<p>The plot shown in <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_38_0">Figure 43-10</a> gives a visual picture of how a
changing <code>C</code> affects the final fit via the softening of the margin:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                           <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">cluster_std</code><code class="o">=</code><code class="mf">0.8</code><code class="p">)</code>

         <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
         <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">left</code><code class="o">=</code><code class="mf">0.0625</code><code class="p">,</code> <code class="n">right</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>

         <code class="k">for</code> <code class="n">axi</code><code class="p">,</code> <code class="n">C</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">ax</code><code class="p">,</code> <code class="p">[</code><code class="mf">10.0</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">]):</code>
             <code class="n">model</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="n">C</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'autumn'</code><code class="p">)</code>
             <code class="n">plot_svc_decision_function</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">axi</code><code class="p">)</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">support_vectors_</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code>
                         <code class="n">model</code><code class="o">.</code><code class="n">support_vectors_</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code>
                         <code class="n">s</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">facecolors</code><code class="o">=</code><code class="s1">'none'</code><code class="p">);</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'C = </code><code class="si">{0:.1f}</code><code class="s1">'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">C</code><code class="p">),</code> <code class="n">size</code><code class="o">=</code><code class="mi">14</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_38_0">
<img alt="output 38 0" height="210" src="assets/output_38_0.png" width="600"/>
<h6><span class="label">Figure 43-10. </span>The effect of the <code>C</code> parameter on the support vector fit</h6>
</div></figure>
<p>The optimal value of <code>C</code> will depend on your dataset, and you should
tune this parameter using cross-validation or a similar procedure (refer
back to
<a data-type="xref" href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">Chapter 39</a>)<a data-startref="ix_ch43-asciidoc11" data-type="indexterm" id="idm45858731746592"/>.<a data-startref="ix_ch43-asciidoc4" data-type="indexterm" id="idm45858731745760"/><a data-startref="ix_ch43-asciidoc2" data-type="indexterm" id="idm45858731745056"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Example: Face Recognition" data-type="sect1"><div class="sect1" id="ch_0507-support-vector-machines_example-face-recognition">
<h1>Example: Face Recognition</h1>
<p><a data-primary="face recognition" data-secondary="support vector machines" data-type="indexterm" id="ix_ch43-asciidoc12"/><a data-primary="support vector machines (SVMs)" data-secondary="face recognition example" data-type="indexterm" id="ix_ch43-asciidoc13"/>As an example of support vector machines in action, let’s
take a look at the facial recognition problem. We will use the Labeled
Faces in the Wild dataset, which consists of several thousand collated
photos of various public figures. A fetcher for the dataset is built
into Scikit-Learn:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_lfw_people</code>
         <code class="n">faces</code> <code class="o">=</code> <code class="n">fetch_lfw_people</code><code class="p">(</code><code class="n">min_faces_per_person</code><code class="o">=</code><code class="mi">60</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">target_names</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">images</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="p">[</code><code class="s1">'Ariel Sharon'</code> <code class="s1">'Colin Powell'</code> <code class="s1">'Donald Rumsfeld'</code> <code class="s1">'George W Bush'</code>
          <code class="s1">'Gerhard Schroeder'</code> <code class="s1">'Hugo Chavez'</code> <code class="s1">'Junichiro Koizumi'</code> <code class="s1">'Tony Blair'</code><code class="p">]</code>
         <code class="p">(</code><code class="mi">1348</code><code class="p">,</code> <code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">)</code></pre>
<p>Let’s plot a few of these faces to see what
we’re working with (see <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_43_0">Figure 43-11</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">axi</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'bone'</code><code class="p">)</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[],</code>
                     <code class="n">xlabel</code><code class="o">=</code><code class="n">faces</code><code class="o">.</code><code class="n">target_names</code><code class="p">[</code><code class="n">faces</code><code class="o">.</code><code class="n">target</code><code class="p">[</code><code class="n">i</code><code class="p">]])</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_43_0">
<img alt="output 43 0" height="595" src="assets/output_43_0.png" width="600"/>
<h6><span class="label">Figure 43-11. </span>Examples from the Labeled Faces in the Wild dataset</h6>
</div></figure>
<p>Each image contains 62 × 47, or around 3,000, pixels. We could proceed
by simply using each pixel value as a feature, but often it is more
effective to use some sort of preprocessor to extract more meaningful
features; here we will use principal component analysis (see
<a data-type="xref" href="ch45.xhtml#section-0509-principal-component-analysis">Chapter 45</a>) to extract 150 fundamental components to feed into
our support vector machine classifier. We can do this most
straightforwardly by packaging the preprocessor and the classifier into
a single pipeline:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>
         <code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>
         <code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>

         <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code> <code class="n">whiten</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                   <code class="n">svd_solver</code><code class="o">=</code><code class="s1">'randomized'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
         <code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'rbf'</code><code class="p">,</code> <code class="n">class_weight</code><code class="o">=</code><code class="s1">'balanced'</code><code class="p">)</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">pca</code><code class="p">,</code> <code class="n">svc</code><code class="p">)</code></pre>
<p class="pagebreak-before less_space">For the sake of testing our classifier output, we will split the data
into a training set and a testing set:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
         <code class="n">Xtrain</code><code class="p">,</code> <code class="n">Xtest</code><code class="p">,</code> <code class="n">ytrain</code><code class="p">,</code> <code class="n">ytest</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">faces</code><code class="o">.</code><code class="n">target</code><code class="p">,</code>
                                                         <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>
<p>Finally, we can use grid search cross-validation to explore combinations
of parameters. Here we will adjust <code>C</code> (which controls the margin
hardness) and <code>gamma</code> (which controls the size of the radial basis
function kernel), and determine the best model:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>
         <code class="n">param_grid</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'svc__C'</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">50</code><code class="p">],</code>
                       <code class="s1">'svc__gamma'</code><code class="p">:</code> <code class="p">[</code><code class="mf">0.0001</code><code class="p">,</code> <code class="mf">0.0005</code><code class="p">,</code> <code class="mf">0.001</code><code class="p">,</code> <code class="mf">0.005</code><code class="p">]}</code>
         <code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">param_grid</code><code class="p">)</code>

         <code class="o">%</code><code class="k">time</code> grid.fit(Xtrain, ytrain)
         <code class="nb">print</code><code class="p">(</code><code class="n">grid</code><code class="o">.</code><code class="n">best_params_</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="n">CPU</code> <code class="n">times</code><code class="p">:</code> <code class="n">user</code> <code class="mi">1</code><code class="nb">min</code> <code class="mi">19</code><code class="n">s</code><code class="p">,</code> <code class="n">sys</code><code class="p">:</code> <code class="mf">8.56</code> <code class="n">s</code><code class="p">,</code> <code class="n">total</code><code class="p">:</code> <code class="mi">1</code><code class="nb">min</code> <code class="mi">27</code><code class="n">s</code>
         <code class="n">Wall</code> <code class="n">time</code><code class="p">:</code> <code class="mf">36.2</code> <code class="n">s</code>
         <code class="p">{</code><code class="s1">'svc__C'</code><code class="p">:</code> <code class="mi">10</code><code class="p">,</code> <code class="s1">'svc__gamma'</code><code class="p">:</code> <code class="mf">0.001</code><code class="p">}</code></pre>
<p>The optimal values fall toward the middle of our grid; if they fell at
the edges, we would want to expand the grid to make sure we have found
the true optimum.</p>
<p>Now with this cross-validated model we can predict the labels for the
test data, which the model has not yet seen:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">best_estimator_</code>
         <code class="n">yfit</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">Xtest</code><code class="p">)</code></pre>
<p>Let’s take a look at a few of the test images along with
their predicted values (see <a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_53_0">Figure 43-12</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">6</code><code class="p">)</code>
         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">axi</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">Xtest</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'bone'</code><code class="p">)</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[])</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">target_names</code><code class="p">[</code><code class="n">yfit</code><code class="p">[</code><code class="n">i</code><code class="p">]]</code><code class="o">.</code><code class="n">split</code><code class="p">()[</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code>
                            <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code> <code class="k">if</code> <code class="n">yfit</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">==</code> <code class="n">ytest</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">else</code> <code class="s1">'red'</code><code class="p">)</code>
         <code class="n">fig</code><code class="o">.</code><code class="n">suptitle</code><code class="p">(</code><code class="s1">'Predicted Names; Incorrect Labels in Red'</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">14</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_53_0">
<img alt="output 53 0" height="399" src="assets/output_53_0.png" width="600"/>
<h6><span class="label">Figure 43-12. </span>Labels predicted by our model</h6>
</div></figure>
<p>Out of this small sample, our optimal estimator mislabeled only a single
face (Bush’s face in the bottom row was mislabeled as Blair). We can get
a better sense of our estimator’s performance using the
classification report, which lists recovery statistics label by label:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">classification_report</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">ytest</code><code class="p">,</code> <code class="n">yfit</code><code class="p">,</code>
                                     <code class="n">target_names</code><code class="o">=</code><code class="n">faces</code><code class="o">.</code><code class="n">target_names</code><code class="p">))</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">25</code><code class="p">]:</code>                    <code class="n">precision</code>    <code class="n">recall</code>  <code class="n">f1</code><code class="o">-</code><code class="n">score</code>   <code class="n">support</code>

              <code class="n">Ariel</code> <code class="n">Sharon</code>       <code class="mf">0.65</code>      <code class="mf">0.73</code>      <code class="mf">0.69</code>        <code class="mi">15</code>
              <code class="n">Colin</code> <code class="n">Powell</code>       <code class="mf">0.80</code>      <code class="mf">0.87</code>      <code class="mf">0.83</code>        <code class="mi">68</code>
           <code class="n">Donald</code> <code class="n">Rumsfeld</code>       <code class="mf">0.74</code>      <code class="mf">0.84</code>      <code class="mf">0.79</code>        <code class="mi">31</code>
             <code class="n">George</code> <code class="n">W</code> <code class="n">Bush</code>       <code class="mf">0.92</code>      <code class="mf">0.83</code>      <code class="mf">0.88</code>       <code class="mi">126</code>
         <code class="n">Gerhard</code> <code class="n">Schroeder</code>       <code class="mf">0.86</code>      <code class="mf">0.83</code>      <code class="mf">0.84</code>        <code class="mi">23</code>
               <code class="n">Hugo</code> <code class="n">Chavez</code>       <code class="mf">0.93</code>      <code class="mf">0.70</code>      <code class="mf">0.80</code>        <code class="mi">20</code>
         <code class="n">Junichiro</code> <code class="n">Koizumi</code>       <code class="mf">0.92</code>      <code class="mf">1.00</code>      <code class="mf">0.96</code>        <code class="mi">12</code>
                <code class="n">Tony</code> <code class="n">Blair</code>       <code class="mf">0.85</code>      <code class="mf">0.95</code>      <code class="mf">0.90</code>        <code class="mi">42</code>

                  <code class="n">accuracy</code>                           <code class="mf">0.85</code>       <code class="mi">337</code>
                 <code class="n">macro</code> <code class="n">avg</code>       <code class="mf">0.83</code>      <code class="mf">0.84</code>      <code class="mf">0.84</code>       <code class="mi">337</code>
              <code class="n">weighted</code> <code class="n">avg</code>       <code class="mf">0.86</code>      <code class="mf">0.85</code>      <code class="mf">0.85</code>       <code class="mi">337</code></pre>
<p>We might also display the confusion matrix between these classes (see
<a data-type="xref" href="#fig_0507-support-vector-machines_files_in_output_57_0">Figure 43-13</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">26</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>
         <code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
         <code class="n">mat</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">ytest</code><code class="p">,</code> <code class="n">yfit</code><code class="p">)</code>
         <code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">mat</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">fmt</code><code class="o">=</code><code class="s1">'d'</code><code class="p">,</code>
                     <code class="n">cbar</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'Blues'</code><code class="p">,</code>
                     <code class="n">xticklabels</code><code class="o">=</code><code class="n">faces</code><code class="o">.</code><code class="n">target_names</code><code class="p">,</code>
                     <code class="n">yticklabels</code><code class="o">=</code><code class="n">faces</code><code class="o">.</code><code class="n">target_names</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'true label'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'predicted label'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0507-support-vector-machines_files_in_output_57_0">
<img alt="output 57 0" height="253" src="assets/output_57_0.png" width="600"/>
<h6><span class="label">Figure 43-13. </span>Confusion matrix for the faces data</h6>
</div></figure>
<p>This helps us get a sense of which labels are likely to be confused by
the estimator.</p>
<p>For a real-world facial recognition task, in which the photos do not
come pre-cropped into nice grids, the only difference in the facial
classification scheme is the feature selection: you would need to use a
more sophisticated algorithm to find the faces, and extract features
that are independent of the pixellation. For this kind of application,
one good option is to make use of <a href="http://opencv.org">OpenCV</a>, which,
among other things, includes pretrained implementations of
state-of-the-art feature extraction tools for images in general and
faces in particular.<a data-startref="ix_ch43-asciidoc13" data-type="indexterm" id="idm45858730777408"/><a data-startref="ix_ch43-asciidoc12" data-type="indexterm" id="idm45858730776704"/></p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch_0507-support-vector-machines_summary">
<h1>Summary</h1>
<p><a data-primary="support vector machines (SVMs)" data-secondary="advantages/disadvantages" data-type="indexterm" id="idm45858730774624"/>This has been a brief intuitive introduction to the principles behind
support vector machines. These models are a powerful classification
method for a number of 
<span class="keep-together">reasons</span>:</p>
<ul>
<li>
<p>Their dependence on relatively few support vectors means that they are
compact and take up very little memory.</p>
</li>
<li>
<p>Once the model is trained, the prediction phase is very fast.</p>
</li>
<li>
<p>Because they are affected only by points near the margin, they work
well with high-dimensional data—even data with more dimensions than
samples, which is challenging for other algorithms.</p>
</li>
<li>
<p>Their integration with kernel methods makes them very versatile, able
to adapt to many types of data.</p>
</li>
</ul>
<p>However, SVMs have several disadvantages as well:</p>
<ul>
<li>
<p>The scaling with the number of samples <math alttext="upper N">
<mi>N</mi>
</math> is
<math alttext="script upper O left-bracket upper N cubed right-bracket">
<mrow>
<mi>𝒪</mi>
<mo>[</mo>
<msup><mi>N</mi> <mn>3</mn> </msup>
<mo>]</mo>
</mrow>
</math> at worst, or
<math alttext="script upper O left-bracket upper N squared right-bracket">
<mrow>
<mi>𝒪</mi>
<mo>[</mo>
<msup><mi>N</mi> <mn>2</mn> </msup>
<mo>]</mo>
</mrow>
</math> for efficient implementations. For large
numbers of training samples, this computational cost can be prohibitive.</p>
</li>
<li>
<p>The results are strongly dependent on a suitable choice for the
softening parameter <code>C</code>. This must be carefully chosen via
cross-validation, which can be expensive as datasets grow in size.</p>
</li>
<li>
<p>The results do not have a direct probabilistic interpretation. This
can be estimated via an internal cross-validation (see the <code>probability</code>
parameter of <code>SVC</code>), but this extra estimation is costly.</p>
</li>
</ul>
<p>With those traits in mind, I generally only turn to SVMs once other
simpler, faster, and less tuning-intensive methods have been shown to be
insufficient for my needs. Nevertheless, if you have the CPU cycles to
commit to training and cross-validating an SVM on your data, the method
can lead to excellent results.<a data-startref="ix_ch43-asciidoc1" data-type="indexterm" id="idm45858730754992"/><a data-startref="ix_ch43-asciidoc0" data-type="indexterm" id="idm45858730754288"/></p>
</div></section>
</div></section></div></body></html>