- en: Chapter 5\. Record Blocking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#chapter_4), we introduced probabilistic matching techniques
    to allow us to combine exact equivalence on individual attributes into a weighted
    composite score. That score allowed us to calculate the overall probability that
    two records refer to the same entity.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have sought to resolve only small-scale datasets where we could exhaustively
    compare every record with every other to find all possible matches. However, in
    most entity resolution scenarios, we will be dealing with larger datasets where
    this approach isn’t practical or affordable.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will introduce record blocking to reduce the number of permutations
    we need to consider while minimizing the likelihood of missing a true positive
    match. We will leverage the Splink framework, introduced in the last chapter,
    to apply the Fellegi-Sunter model and use the expectation-maximization algorithm
    to estimate the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will consider how to measure our matching performance over this larger
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Sample Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we considered the challenge of resolving entities across
    two datasets containing information about members of the UK House of Commons.
    In this chapter, we extend this resolution challenge to a much larger dataset
    containing a list of the persons with significant control of registered UK companies.
  prefs: []
  type: TYPE_NORMAL
- en: In the UK, Companies House is an executive agency sponsored by the Department
    for Business and Trade. It incorporates and dissolves limited companies, registering
    company information and making it available to the public.
  prefs: []
  type: TYPE_NORMAL
- en: When registering a UK limited company, there is an obligation to declare who
    owns or controls a company. These entities are known as persons with significant
    control (PSCs); they’re sometimes called “beneficial owners.” Companies House
    provides a downloadable data snapshot containing the full list of PSCs.
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we will attempt to resolve the entities listed in this dataset
    with the list of members of Parliament we acquired from Wikipedia. This will show
    us which MPs may be PSC of UK companies.
  prefs: []
  type: TYPE_NORMAL
- en: Data Acquisition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will reuse the same Wikipedia source data on MPs returned
    at the 2019 UK general election that we examined in previous chapters. However,
    to allow us to match against a much larger dataset,  without generating an unmanageable
    number of false positives, we need to enrich our initial data with additional
    attributes. Specifically, we will seek to augment our dataset with date of birth
    information, extracted from the individual wiki page associated with each of the
    MPs, to help strengthen the quality of our matches.
  prefs: []
  type: TYPE_NORMAL
- en: We will also download the most recent snapshot of the PSC data published by
    Companies House and then normalize and filter that dataset down to the attributes
    we need for matching.
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create our enriched Wikipedia dataset, we select the MPs from the wiki page
    as we did in [Chapter 2](ch02.html#chapter_2); however, this time we also extract
    the Wikipedia link to each individual MP and append this as an additional column
    in our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now follow these links and extract the date of birth information, if
    present, from the web page Infobox. As before, we can use the Beautiful Soup `html
    parser` to find and extract the attribute we need or return a default null value.
    The `apply` method allows us to apply this function to each row in the Wikipedia
    dataset, creating a new column entitled `Birthday`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: UK Companies House Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Companies House publishes a snapshot of PSC data in JSON format. It is made
    available both as a single ZIP file and as multiple ZIP files for ease of downloading.
    Extracting each partial ZIP file in turn allows us to normalize the JSON structure
    that we concatenate into a composite DataFrame of the attributes we need for matching
    plus the associated unique company number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Data Standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the raw data we need, we standardize the attributes and column
    names across the two datasets. As we will be using the Splink framework, we also
    add a unique ID column.
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To standardize the date-enriched Wikipedia data, we convert the date column
    into month and year integers. As in [Chapter 2](ch02.html#chapter_2), we extract
    `Firstname` and `Lastname` attributes. We also add a unique ID column and a blank
    company number column to match the equivalent field in the Companies House data.
    Finally, we retain only the columns we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: UK Companies House Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To standardize the UK Companies House data, we first drop any rows with missing
    year or month date of birth columns as we won’t be able to match these records.
    As with the Wikipedia data, we standardize the column names, generate the unique
    ID, and retain the matching subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at a few rows (with `Firstname`s and `Lastname`s sanitized), as shown
    in [Figure 5-1](#fig-5-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Example rows of UK Companies House persons with significant control
    data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Record Blocking and Attribute Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have consistent data, we can configure our matching process. Before
    we do, it’s worth taking a look at the size of the challenge. We have 650 MP records
    and our standardized PSC data has more than 10 million records. If we were to
    consider all permutations, we would have approximately 6 billion comparisons to
    make.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performing a simple join on records with matching `Month` and `Year` values,
    we can see the size of the intersection is approximately 11 million records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple exact match on all four attributes yields 266 potential matches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A sanitized sample of these simple join matches is shown in [Figure 5-2](#fig-5-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Simple join on `Lastname`, `Firstname`, `Year`, and `Month`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Record Blocking with Splink
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To reduce the number of record combinations we need to consider, Splink allows
    us to configure blocking rules. These rules determine which record pairs are evaluated
    to determine whether they refer to the same entity. Clearly, considering only
    a subset of the population creates a risk of missing true matches, it’s important
    to select rules that minimize this while at the same time reducing the volume
    as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Splink allows us to create composite rules, essentially `OR` statements, where
    if any of the conditions are met, then the combination is selected for further
    comparison. However, in this example we’ll use only a single blocking rule that
    selects only records with matching year and month of birth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Attribute Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the record comparisons that are produced by the blocking rules, we will
    determine whether they refer to the same person by using a combination of approximate
    matches scores on first name and last name and exact matches on month and year.
    Because we are comparing names, we use the Jaro-Winkler algorithm from [Chapter 3](ch03.html#chapter_3).
  prefs: []
  type: TYPE_NORMAL
- en: We can configure Splink with a set of minimum threshold values that together
    segment the population; Splink will add an exact match segment and a default zero
    match segment for those attribute pairs that score beneath the minimum value provided.
    In this case, we will just use a single threshold of 0.9 to illustrate the process,
    giving us three segments for each component of the name. Each segment is evaluated
    as a separate attribute for the purposes of calculating the overall match probability
    of the record pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our settings, let’s instantiate our linker and profile the
    matching columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can see the results in [Figure 5-3](#fig-5-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. First name, last name, month, and year distributions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can see that we have some common first names and last names with a long
    tail of less frequent values. For month of birth, the values are fairly regularly
    distributed but for year, we see some years are more likely than others. We can
    take this frequency distribution into account in our matching process by setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Each year value will be considered separately for the purposes of calculating
    match probabilities; thus a match on an unpopular year will be weighted more highly
    than a match on a more frequently observed value.
  prefs: []
  type: TYPE_NORMAL
- en: As we did in [Chapter 4](ch04.html#chapter_4), we could use the expectation-maximization
    algorithm to determine the *m* and *u* values, that is, the match and not match
    probabilities, for each attribute segment. By default, these calculations consider
    the full population prior to applying the blocking rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate the *u* values, Splink takes a slightly different approach by taking
    random pairwise record comparisons, assuming they do not match, and computing
    how often these coincidences occur. Since the probability of two random records
    being a match (representing the same entity) is usually very low, this approach
    generates good estimates of the *u* values. An additional benefit of this approach
    is that if the *u* probabilities are correct, it “anchors” the EM estimation procedure
    and greatly improves the chance of it converging to a global, rather than a local,
    minimum. To apply this approach, we need to make sure our random population is
    sufficiently large to be representative of the full range of possible combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Splink allows us to set blocking rules for estimating the match probabilities.
    Here the attribute parameters for each segment are estimated on the subset of
    the population according to the first condition and then the process is repeated
    for the subset selected by the second condition. Since the attributes included
    in the blocking condition cannot themselves be estimated, it is essential that
    the conditions overlap, allowing each attribute to be evaluated under at least
    one condition.
  prefs: []
  type: TYPE_NORMAL
- en: Random Sample
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that the expectation-maximization method selects records at random, so
    you can expect some variation from the calculated parameters in this book if you
    are following along.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we block on equivalent last name and month, allowing us to
    estimate first name and year segment probabilities, and then we repeat with the
    opposite combination. This way each attribute segment is evaluated at least once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can examine the resulting match weights using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/hoer_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Model parameters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In [Figure 5-4](#fig-5-4), we can see a strongly negative prior (starting)
    match weight with positive weights for each attribute exact match and for approximate
    matches on `Firstname` and `Lastname`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In [Figure 5-5](#fig-5-5), we can see the proportion of matching and nonmatching
    record comparisons that the expectation maximization algorithm calculates for
    each segment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Proportion of record comparisons
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Match Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a trained model with optimized match parameters for each attribute,
    we can predict whether the record pairs that aren’t blocked refer to the same
    entity. In this example, we set an overall threshold match probability at 0.99:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We then join the prediction results to the PSC dataset by unique ID so that
    we can pick up the company number that the matched entity is associated with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we rename our output columns and retain only the ones we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This gives us 346 predicted matches, both exact and approximate, as shown in
    [Figure 5-6](#fig-5-6) (with the PSC first names and last names sanitized).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. Exact matches on `Lastname` and `Firstname`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If we remove the exact matches, we can examine the additional approximate matches
    to see how well our probabilistic approach has performed. This is shown in [Figure 5-7](#fig-5-7) (with
    the PSC first names and last names sanitized):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/hoer_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Approximate matches—nonexact `Firstname` or `Lastname`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examining the results, shown in [Table 5-1](#table-5-1), we can see several
    candidates that may be true positive matches.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Approximate matches—manual comparison
  prefs: []
  type: TYPE_NORMAL
- en: '| `match_weight` | `match_probability` | `Firstname_psc` | `Firstname_w` |
    `Lastname_psc` | `Lastname_w` | `company_​num⁠ber` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 13.51481459 | 0.999914572 | John | John | Mcdonnell | McDonnell | 5350064
    |'
  prefs: []
  type: TYPE_TB
- en: '| 11.66885836 | 0.999692963 | Stephen | Stephen | Mcpartland | McPartland |
    7572556 |'
  prefs: []
  type: TYPE_TB
- en: '| 11.50728191 | 0.999656589 | James | James | Heappey Mp | Heappey | 5074477
    |'
  prefs: []
  type: TYPE_TB
- en: '| 9.637598832 | 0.998746141 | Matt | Matthew | Hancock | Hancock | 14571407
    |'
  prefs: []
  type: TYPE_TB
- en: '| 13.51481459 | 0.999914572 | John | John | Mcdonnell | McDonnell | 4662034
    |'
  prefs: []
  type: TYPE_TB
- en: '| 9.320995827 | 0.998438931 | Siobhan | Siobhan | Mcdonagh | McDonagh | 246884
    |'
  prefs: []
  type: TYPE_TB
- en: '| 11.46050878 | 0.999645277 | Alison | Alison | Mcgovern | McGovern | 10929919
    |'
  prefs: []
  type: TYPE_TB
- en: '| 9.57364719 | 0.998689384 | Jessica | Jess | Phillips | Phillips | 560074
    |'
  prefs: []
  type: TYPE_TB
- en: '| 12.14926274 | 0.999779904 | Grahame | Grahame | Morris Mp | Morris | 13523499
    |'
  prefs: []
  type: TYPE_TB
- en: '| 11.66885836 | 0.999692963 | Stephen | Stephen | Mcpartland | McPartland |
    9165947 |'
  prefs: []
  type: TYPE_TB
- en: '| 13.51481459 | 0.999914572 | John | John | Mcdonnell | McDonnell | 6496912
    |'
  prefs: []
  type: TYPE_TB
- en: '| 11.62463457 | 0.999683409 | Anna | Anna | Mcmorrin | McMorrin | 9965110 |'
  prefs: []
  type: TYPE_TB
- en: Despite our initial data standardization, we can see that we have inconsistent
    capitalization on last name, and we also have a couple of PSC records where the
    last name is appended with “Mp.” This is frequently the case with entity resolution
    problems—we often have to iterate several times, refining our data standardization
    as we learn more about our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we assume that all the exact matches and the approximate matches in [Table 5-1](#table-5-1)
    are true positive matches, then we can calculate our precision metrics as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper T r u e p o s i t i v e m a t c h e s left-parenthesis
    upper F upper P right-parenthesis equals 266 plus 12 equals 278"><mrow><mi>T</mi>
    <mi>r</mi> <mi>u</mi> <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi>
    <mi>i</mi> <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi>
    <mi>e</mi> <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo> <mn>266</mn>
    <mo>+</mo> <mn>12</mn> <mo>=</mo> <mn>278</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper F a l s e p o s i t i v e m a t c h e s left-parenthesis
    upper F upper P right-parenthesis equals 80 minus 12 equals 68"><mrow><mi>F</mi>
    <mi>a</mi> <mi>l</mi> <mi>s</mi> <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi>
    <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi>
    <mi>h</mi> <mi>e</mi> <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo>
    <mn>80</mn> <mo>-</mo> <mn>12</mn> <mo>=</mo> <mn>68</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P r e c i s i o n equals StartFraction upper T upper P
    Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction
    equals StartFraction 278 Over left-parenthesis 278 plus 68 right-parenthesis EndFraction
    almost-equals 80 percent-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>c</mi>
    <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>278</mn> <mrow><mo>(</mo><mn>278</mn><mo>+</mo><mn>68</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>80</mn> <mo>%</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Without manual verification, we don’t definitively know which of our `notmatch`
    population are true or false negatives, and therefore we cannot calculate recall
    or overall accuracy metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used approximate matching within a probabilistic framework 
    to identify members of Parliament who may have significant control over UK companies.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how blocking can be used to reduce the number of record pairs we need
    to evaluate to a practical size without unacceptably increasing the risk that
    we miss some important potential matches.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how important data standardization is to optimizing performance and how
    getting the best performance in entity resolution is often an iterative process.
  prefs: []
  type: TYPE_NORMAL
