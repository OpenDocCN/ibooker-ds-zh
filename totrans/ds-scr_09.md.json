["```py\nfrom scratch.linear_algebra import Vector, dot\n\ndef sum_of_squares(v: Vector) -> float:\n    \"\"\"Computes the sum of squared elements in v\"\"\"\n    return dot(v, v)\n```", "```py\nfrom typing import Callable\n\ndef difference_quotient(f: Callable[[float], float],\n                        x: float,\n                        h: float) -> float:\n    return (f(x + h) - f(x)) / h\n```", "```py\ndef square(x: float) -> float:\n    return x * x\n```", "```py\ndef derivative(x: float) -> float:\n    return 2 * x\n```", "```py\nxs = range(-10, 11)\nactuals = [derivative(x) for x in xs]\nestimates = [difference_quotient(square, x, h=0.001) for x in xs]\n\n# plot to show they're basically the same\nimport matplotlib.pyplot as plt\nplt.title(\"Actual Derivatives vs. Estimates\")\nplt.plot(xs, actuals, 'rx', label='Actual')       # red  x\nplt.plot(xs, estimates, 'b+', label='Estimate')   # blue +\nplt.legend(loc=9)\nplt.show()\n```", "```py\ndef partial_difference_quotient(f: Callable[[Vector], float],\n                                v: Vector,\n                                i: int,\n                                h: float) -> float:\n    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n    w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n         for j, v_j in enumerate(v)]\n\n    return (f(w) - f(v)) / h\n```", "```py\ndef estimate_gradient(f: Callable[[Vector], float],\n                      v: Vector,\n                      h: float = 0.0001):\n    return [partial_difference_quotient(f, v, i, h)\n            for i in range(len(v))]\n```", "```py\nimport random\nfrom scratch.linear_algebra import distance, add, scalar_multiply\n\ndef gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n    assert len(v) == len(gradient)\n    step = scalar_multiply(step_size, gradient)\n    return add(v, step)\n\ndef sum_of_squares_gradient(v: Vector) -> Vector:\n    return [2 * v_i for v_i in v]\n```", "```py\n# pick a random starting point\nv = [random.uniform(-10, 10) for i in range(3)]\n\nfor epoch in range(1000):\n    grad = sum_of_squares_gradient(v)    # compute the gradient at v\n    v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n    print(epoch, v)\n\nassert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0\n```", "```py\n# x ranges from -50 to 49, y is always 20 * x + 5\ninputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n```", "```py\ndef linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n    slope, intercept = theta\n    predicted = slope * x + intercept    # The prediction of the model.\n    error = (predicted - y)              # error is (predicted - actual).\n    squared_error = error ** 2           # We'll minimize squared error\n    grad = [2 * error * x, 2 * error]    # using its gradient.\n    return grad\n```", "```py\nfrom scratch.linear_algebra import vector_mean\n\n# Start with random values for slope and intercept\ntheta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n\nlearning_rate = 0.001\n\nfor epoch in range(5000):\n    # Compute the mean of the gradients\n    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n    # Take a step in that direction\n    theta = gradient_step(theta, grad, -learning_rate)\n    print(epoch, theta)\n\nslope, intercept = theta\nassert 19.9 < slope < 20.1,   \"slope should be about 20\"\nassert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n```", "```py\nfrom typing import TypeVar, List, Iterator\n\nT = TypeVar('T')  # this allows us to type \"generic\" functions\n\ndef minibatches(dataset: List[T],\n                batch_size: int,\n                shuffle: bool = True) -> Iterator[List[T]]:\n    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n    # start indexes 0, batch_size, 2 * batch_size, ...\n    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n\n    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n\n    for start in batch_starts:\n        end = start + batch_size\n        yield dataset[start:end]\n```", "```py\ntheta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n\nfor epoch in range(1000):\n    for batch in minibatches(inputs, batch_size=20):\n        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n        theta = gradient_step(theta, grad, -learning_rate)\n    print(epoch, theta)\n\nslope, intercept = theta\nassert 19.9 < slope < 20.1,   \"slope should be about 20\"\nassert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n```", "```py\ntheta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n\nfor epoch in range(100):\n    for x, y in inputs:\n        grad = linear_gradient(x, y, theta)\n        theta = gradient_step(theta, grad, -learning_rate)\n    print(epoch, theta)\n\nslope, intercept = theta\nassert 19.9 < slope < 20.1,   \"slope should be about 20\"\nassert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n```"]