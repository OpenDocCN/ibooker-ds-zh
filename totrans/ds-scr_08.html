<html><head></head><body><section data-pdf-bookmark="Chapter 7. Hypothesis and Inference" data-type="chapter" epub:type="chapter"><div class="chapter" id="hypothesis_and_inference">&#13;
<h1><span class="label">Chapter 7. </span>Hypothesis and Inference</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
<p>It is the mark of a truly intelligent person to be moved by statistics.</p>&#13;
<p data-type="attribution">George Bernard Shaw</p>&#13;
</blockquote>&#13;
&#13;
<p>What will we do with all this statistics and probability theory?&#13;
The <em>science</em> part of data science frequently involves&#13;
forming and testing <em>hypotheses</em>&#13;
about our data and the processes that generate it.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Statistical Hypothesis Testing" data-type="sect1"><div class="sect1" id="idm45635754935176">&#13;
<h1>Statistical Hypothesis Testing</h1>&#13;
&#13;
<p>Often, as<a data-primary="hypothesis and inference" data-secondary="statistical hypothesis testing" data-type="indexterm" id="idm45635754933832"/><a data-primary="inference" data-see="hypothesis and inference" data-type="indexterm" id="idm45635754932744"/> data scientists, we’ll want to test whether a certain hypothesis is likely to be true.  For our purposes, hypotheses are assertions like “this coin is fair” or “data scientists prefer Python to R” or “people are more likely to navigate away from the page without ever reading the content if we pop up an irritating interstitial advertisement with a tiny, hard-to-find close button” that can be translated into statistics about data. Under various assumptions, those statistics can be thought of as observations of random variables from known distributions, which allows us to make statements about how likely those assumptions are to hold.</p>&#13;
&#13;
<p>In the classical setup, we<a data-primary="null hypothesis" data-type="indexterm" id="idm45635754930376"/> have a <em>null hypothesis</em>, &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math>, that represents some default position, and some alternative hypothesis, &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>1</mn> </msub>&#13;
</math>, that we’d like to compare it with. We use statistics to decide whether we can reject &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> as false or not.  This will probably make more sense with an example.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Flipping a Coin" data-type="sect1"><div class="sect1" id="idm45635754923832">&#13;
<h1>Example: Flipping a Coin</h1>&#13;
&#13;
<p>Imagine<a data-primary="hypothesis and inference" data-secondary="coin flip example" data-type="indexterm" id="idm45635754921816"/> we have a coin and we want to test whether it’s fair.  We’ll make the assumption that the coin has some probability <em>p</em> of landing heads, and so our null hypothesis is that the coin is fair—that is, that <em>p</em> = 0.5.  We’ll test this against the alternative hypothesis <em>p</em>	≠ 0.5.</p>&#13;
&#13;
<p>In particular, our test will involve flipping the coin some number, <em>n</em>, times and counting the number of heads, <em>X</em>.  Each coin flip is a Bernoulli trial, which means that <em>X</em> is a Binomial(<em>n</em>,<em>p</em>) random variable, which (as we saw in <a data-type="xref" href="ch06.html#probability">Chapter 6</a>) we can approximate using the normal distribution:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Tuple</code>&#13;
<code class="kn">import</code> <code class="nn">math</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">normal_approximation_to_binomial</code><code class="p">(</code><code class="n">n</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">p</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">float</code><code class="p">,</code> <code class="nb">float</code><code class="p">]:</code>&#13;
    <code class="sd">"""Returns mu and sigma corresponding to a Binomial(n, p)"""</code>&#13;
    <code class="n">mu</code> <code class="o">=</code> <code class="n">p</code> <code class="o">*</code> <code class="n">n</code>&#13;
    <code class="n">sigma</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">p</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p</code><code class="p">)</code> <code class="o">*</code> <code class="n">n</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code></pre>&#13;
&#13;
<p>Whenever a random variable follows a normal distribution, we can use <code>normal_cdf</code> to figure out the probability that its realized value lies within or outside a particular interval:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.probability</code> <code class="kn">import</code> <code class="n">normal_cdf</code>&#13;
&#13;
<code class="c1"># The normal cdf _is_ the probability the variable is below a threshold</code>&#13;
<code class="n">normal_probability_below</code> <code class="o">=</code> <code class="n">normal_cdf</code>&#13;
&#13;
<code class="c1"># It's above the threshold if it's not below the threshold</code>&#13;
<code class="k">def</code> <code class="nf">normal_probability_above</code><code class="p">(</code><code class="n">lo</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                             <code class="n">mu</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>&#13;
                             <code class="n">sigma</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""The probability that an N(mu, sigma) is greater than lo."""</code>&#13;
    <code class="k">return</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">normal_cdf</code><code class="p">(</code><code class="n">lo</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># It's between if it's less than hi, but not less than lo</code>&#13;
<code class="k">def</code> <code class="nf">normal_probability_between</code><code class="p">(</code><code class="n">lo</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                               <code class="n">hi</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                               <code class="n">mu</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>&#13;
                               <code class="n">sigma</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""The probability that an N(mu, sigma) is between lo and hi."""</code>&#13;
    <code class="k">return</code> <code class="n">normal_cdf</code><code class="p">(</code><code class="n">hi</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code> <code class="o">-</code> <code class="n">normal_cdf</code><code class="p">(</code><code class="n">lo</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># It's outside if it's not between</code>&#13;
<code class="k">def</code> <code class="nf">normal_probability_outside</code><code class="p">(</code><code class="n">lo</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                               <code class="n">hi</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                               <code class="n">mu</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>&#13;
                               <code class="n">sigma</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""The probability that an N(mu, sigma) is not between lo and hi."""</code>&#13;
    <code class="k">return</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">normal_probability_between</code><code class="p">(</code><code class="n">lo</code><code class="p">,</code> <code class="n">hi</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code></pre>&#13;
&#13;
<p>We can also do the reverse—find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood.  For example, if we want to find an interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability (leaving 60%):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.probability</code> <code class="kn">import</code> <code class="n">inverse_normal_cdf</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">normal_upper_bound</code><code class="p">(</code><code class="n">probability</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                       <code class="n">mu</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>&#13;
                       <code class="n">sigma</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Returns the z for which P(Z &lt;= z) = probability"""</code>&#13;
    <code class="k">return</code> <code class="n">inverse_normal_cdf</code><code class="p">(</code><code class="n">probability</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">normal_lower_bound</code><code class="p">(</code><code class="n">probability</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                       <code class="n">mu</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>&#13;
                       <code class="n">sigma</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Returns the z for which P(Z &gt;= z) = probability"""</code>&#13;
    <code class="k">return</code> <code class="n">inverse_normal_cdf</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">probability</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">normal_two_sided_bounds</code><code class="p">(</code><code class="n">probability</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                            <code class="n">mu</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>&#13;
                            <code class="n">sigma</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">float</code><code class="p">,</code> <code class="nb">float</code><code class="p">]:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Returns the symmetric (about the mean) bounds</code>&#13;
<code class="sd">    that contain the specified probability</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="n">tail_probability</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">probability</code><code class="p">)</code> <code class="o">/</code> <code class="mi">2</code>&#13;
&#13;
    <code class="c1"># upper bound should have tail_probability above it</code>&#13;
    <code class="n">upper_bound</code> <code class="o">=</code> <code class="n">normal_lower_bound</code><code class="p">(</code><code class="n">tail_probability</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># lower bound should have tail_probability below it</code>&#13;
    <code class="n">lower_bound</code> <code class="o">=</code> <code class="n">normal_upper_bound</code><code class="p">(</code><code class="n">tail_probability</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">lower_bound</code><code class="p">,</code> <code class="n">upper_bound</code></pre>&#13;
&#13;
<p>In particular, let’s say that we choose to flip the coin <em>n</em> = 1,000 times.  If our hypothesis of fairness is true, <em>X</em> should be distributed approximately normally with mean 500 and standard deviation 15.8:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code> <code class="o">=</code> <code class="n">normal_approximation_to_binomial</code><code class="p">(</code><code class="mi">1000</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">)</code></pre>&#13;
&#13;
<p>We<a data-primary="significance" data-type="indexterm" id="idm45635754414904"/> need to make a decision about <em>significance</em>—how willing<a data-primary="type 1/type 2 errors" data-type="indexterm" id="idm45635754413784"/> we are to make a <em>type 1 error</em> (“false positive”), in which we reject &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> even though it’s true.  For reasons lost to the annals of history, this willingness is often set at 5% or 1%.  Let’s choose 5%.</p>&#13;
&#13;
<p>Consider the test that rejects &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> if <em>X</em> falls outside the bounds given by:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># (469, 531)</code>&#13;
<code class="n">lower_bound</code><code class="p">,</code> <code class="n">upper_bound</code> <code class="o">=</code> <code class="n">normal_two_sided_bounds</code><code class="p">(</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code><code class="p">)</code></pre>&#13;
&#13;
<p>Assuming <em>p</em> really equals 0.5 (i.e., &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> is true), there is just a 5% chance we observe an <em>X</em> that lies outside this interval, which is the exact significance we wanted.  Said differently, if &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> is true, then, approximately 19 times out of 20, this test will give the correct result.</p>&#13;
&#13;
<p>We<a data-primary="false negatives/false positives" data-type="indexterm" id="idm45635754391176"/> are also often interested in the <em>power</em> of a test, which is the probability of not making a <em>type 2 error</em>  (“false negative”), in which we fail to reject &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> even though it’s false.  In order to measure this, we have to specify what exactly &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> being false <em>means</em>.  (Knowing merely that <em>p</em> is <em>not</em> 0.5 doesn’t give us a ton of information about the distribution of <em>X</em>.)  In particular, let’s check what happens if <em>p</em> is really 0.55, so that the coin is slightly biased toward heads.</p>&#13;
&#13;
<p>In that case, we can calculate the power of the test with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># 95% bounds based on assumption p is 0.5</code>&#13;
<code class="n">lo</code><code class="p">,</code> <code class="n">hi</code> <code class="o">=</code> <code class="n">normal_two_sided_bounds</code><code class="p">(</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># actual mu and sigma based on p = 0.55</code>&#13;
<code class="n">mu_1</code><code class="p">,</code> <code class="n">sigma_1</code> <code class="o">=</code> <code class="n">normal_approximation_to_binomial</code><code class="p">(</code><code class="mi">1000</code><code class="p">,</code> <code class="mf">0.55</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># a type 2 error means we fail to reject the null hypothesis,</code>&#13;
<code class="c1"># which will happen when X is still in our original interval</code>&#13;
<code class="n">type_2_probability</code> <code class="o">=</code> <code class="n">normal_probability_between</code><code class="p">(</code><code class="n">lo</code><code class="p">,</code> <code class="n">hi</code><code class="p">,</code> <code class="n">mu_1</code><code class="p">,</code> <code class="n">sigma_1</code><code class="p">)</code>&#13;
<code class="n">power</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">type_2_probability</code>      <code class="c1"># 0.887</code></pre>&#13;
&#13;
<p>Imagine instead that our null hypothesis was that the coin is not biased toward heads, or that <math>&#13;
  <mrow>&#13;
    <mi>p</mi>&#13;
    <mo>≤</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>5</mn>&#13;
  </mrow>&#13;
</math>.  In that case we want a <em>one-sided test</em> that rejects the null hypothesis when <em>X</em> is much larger than 500 but not when <em>X</em> is smaller than 500.  So, a 5% significance test involves using <code>normal_probability_below</code> to find the cutoff below which 95% of the probability lies:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">hi</code> <code class="o">=</code> <code class="n">normal_upper_bound</code><code class="p">(</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code><code class="p">)</code>&#13;
<code class="c1"># is 526 (&lt; 531, since we need more probability in the upper tail)</code>&#13;
&#13;
<code class="n">type_2_probability</code> <code class="o">=</code> <code class="n">normal_probability_below</code><code class="p">(</code><code class="n">hi</code><code class="p">,</code> <code class="n">mu_1</code><code class="p">,</code> <code class="n">sigma_1</code><code class="p">)</code>&#13;
<code class="n">power</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">type_2_probability</code>      <code class="c1"># 0.936</code></pre>&#13;
&#13;
<p>This is a more powerful test, since it no longer rejects &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> when <em>X</em> is below 469 (which is very unlikely to happen if &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>1</mn> </msub>&#13;
</math> is true) and instead rejects &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> when <em>X</em> is between 526 and 531 (which is somewhat likely to happen if &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>1</mn> </msub>&#13;
</math> is true).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="p-Values" data-type="sect1"><div class="sect1" id="idm45635754922888">&#13;
<h1>p-Values</h1>&#13;
&#13;
<p>An<a data-primary="hypothesis and inference" data-secondary="p-values" data-type="indexterm" id="idm45635754184360"/><a data-primary="p-values" data-type="indexterm" id="idm45635754183336"/> alternative way of thinking about the preceding test involves <em>p-values</em>.  Instead of choosing bounds based on some probability cutoff, we compute the probability—assuming &#13;
<math>&#13;
  <msub><mi>H</mi> <mn>0</mn> </msub>&#13;
</math> is true—that we would see a value at least as extreme as the one we actually observed.</p>&#13;
&#13;
<p>For our two-sided test of whether the coin is fair, we compute:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">two_sided_p_value</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">mu</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code> <code class="n">sigma</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    How likely are we to see a value at least as extreme as x (in either</code>&#13;
<code class="sd">    direction) if our values are from an N(mu, sigma)?</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">if</code> <code class="n">x</code> <code class="o">&gt;=</code> <code class="n">mu</code><code class="p">:</code>&#13;
        <code class="c1"># x is greater than the mean, so the tail is everything greater than x</code>&#13;
        <code class="k">return</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">normal_probability_above</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="c1"># x is less than the mean, so the tail is everything less than x</code>&#13;
        <code class="k">return</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">normal_probability_below</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code></pre>&#13;
&#13;
<p>If we were to see 530 heads, we would compute:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">two_sided_p_value</code><code class="p">(</code><code class="mf">529.5</code><code class="p">,</code> <code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code><code class="p">)</code>   <code class="c1"># 0.062</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Why<a data-primary="continuity corrections" data-type="indexterm" id="idm45635754085064"/> did we use a value of <code>529.5</code> rather than using <code>530</code>? This is what’s called a <a href="http://en.wikipedia.org/wiki/Continuity_correction"><em>continuity correction</em></a>. It reflects the fact that <code>normal_probability_between(529.5, 530.5, mu_0, sigma_0)</code> is a better estimate of the probability of seeing 530 heads than&#13;
<code>normal_probability_between(530, 531, mu_0, sigma_0)</code> is.</p>&#13;
&#13;
<p>Correspondingly, <code>normal_probability_above(529.5, mu_0, sigma_0)</code> is a better estimate&#13;
of the probability of seeing at least 530 heads.  You may have noticed that we also used this in the code that produced <a data-type="xref" href="ch06.html#make_hist_result">Figure 6-4</a>.</p>&#13;
</div>&#13;
&#13;
<p>One way to convince yourself that this is a sensible estimate is with a simulation:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
&#13;
<code class="n">extreme_value_count</code> <code class="o">=</code> <code class="mi">0</code>&#13;
<code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">):</code>&#13;
    <code class="n">num_heads</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="mi">1</code> <code class="k">if</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="o">&lt;</code> <code class="mf">0.5</code> <code class="k">else</code> <code class="mi">0</code>    <code class="c1"># Count # of heads</code>&#13;
                    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">))</code>                <code class="c1"># in 1000 flips,</code>&#13;
    <code class="k">if</code> <code class="n">num_heads</code> <code class="o">&gt;=</code> <code class="mi">530</code> <code class="ow">or</code> <code class="n">num_heads</code> <code class="o">&lt;=</code> <code class="mi">470</code><code class="p">:</code>             <code class="c1"># and count how often</code>&#13;
        <code class="n">extreme_value_count</code> <code class="o">+=</code> <code class="mi">1</code>                         <code class="c1"># the # is 'extreme'</code>&#13;
&#13;
<code class="c1"># p-value was 0.062 =&gt; ~62 extreme values out of 1000</code>&#13;
<code class="k">assert</code> <code class="mi">59</code> <code class="o">&lt;</code> <code class="n">extreme_value_count</code> <code class="o">&lt;</code> <code class="mi">65</code><code class="p">,</code> <code class="n">f</code><code class="s2">"{extreme_value_count}"</code></pre>&#13;
&#13;
<p>Since the <em>p</em>-value is greater than our 5% significance, we don’t reject the null.  If we instead saw 532 heads, the <em>p</em>-value would be:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">two_sided_p_value</code><code class="p">(</code><code class="mf">531.5</code><code class="p">,</code> <code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code><code class="p">)</code>   <code class="c1"># 0.0463</code></pre>&#13;
&#13;
<p>which is smaller than the 5% significance, which means we would reject the null.  It’s the exact same test as before. It’s just a different way of approaching the statistics.</p>&#13;
&#13;
<p>Similarly, we would have:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">upper_p_value</code> <code class="o">=</code> <code class="n">normal_probability_above</code>&#13;
<code class="n">lower_p_value</code> <code class="o">=</code> <code class="n">normal_probability_below</code></pre>&#13;
&#13;
<p>For our one-sided test, if we saw 525 heads we would compute:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">upper_p_value</code><code class="p">(</code><code class="mf">524.5</code><code class="p">,</code> <code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code><code class="p">)</code> <code class="c1"># 0.061</code></pre>&#13;
&#13;
<p>which means we wouldn’t reject the null.  If we saw 527 heads, the computation would be:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">upper_p_value</code><code class="p">(</code><code class="mf">526.5</code><code class="p">,</code> <code class="n">mu_0</code><code class="p">,</code> <code class="n">sigma_0</code><code class="p">)</code> <code class="c1"># 0.047</code></pre>&#13;
&#13;
<p>and we would reject the null.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Make sure your data is roughly normally distributed before using <code>normal_probability_above</code> to compute <em>p</em>-values.  The annals of bad data science are filled with examples of people opining that the chance of some observed event occurring at random is one in a million, when what they really mean is “the chance, assuming the data is distributed normally,” which is fairly meaningless if the data isn’t.</p>&#13;
&#13;
<p>There are various statistical tests for normality, but even plotting the data is a good start.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Confidence Intervals" data-type="sect1"><div class="sect1" id="idm45635754185208">&#13;
<h1>Confidence Intervals</h1>&#13;
&#13;
<p>We’ve<a data-primary="hypothesis and inference" data-secondary="confidence intervals" data-type="indexterm" id="idm45635753929816"/><a data-primary="confidence intervals" data-type="indexterm" id="idm45635753928840"/> been testing hypotheses about the value of&#13;
the heads probability <em>p</em>, which is a <em>parameter</em>&#13;
of the unknown “heads” distribution. When this is the case,&#13;
a third approach is to construct a <em>confidence interval</em> around the observed value of the&#13;
parameter.</p>&#13;
&#13;
<p>For example, we can estimate the probability of the unfair coin by looking at the average value of the Bernoulli variables corresponding to each flip—1 if heads, 0 if tails.  If we observe 525 heads out of 1,000 flips, then we estimate <em>p</em> equals 0.525.</p>&#13;
&#13;
<p>How <em>confident</em> can we be about this estimate?  Well, if we knew the exact value of <em>p</em>, the central limit theorem (recall <a data-type="xref" href="ch06.html#central_limit_theorem">“The Central Limit Theorem”</a>) tells us that the average of those Bernoulli variables should be approximately normal, with mean <em>p</em> and standard deviation:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">p</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p</code><code class="p">)</code> <code class="o">/</code> <code class="mi">1000</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here we don’t know <em>p</em>, so instead we use our estimate:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">p_hat</code> <code class="o">=</code> <code class="mi">525</code> <code class="o">/</code> <code class="mi">1000</code>&#13;
<code class="n">mu</code> <code class="o">=</code> <code class="n">p_hat</code>&#13;
<code class="n">sigma</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">p_hat</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p_hat</code><code class="p">)</code> <code class="o">/</code> <code class="mi">1000</code><code class="p">)</code>   <code class="c1"># 0.0158</code></pre>&#13;
&#13;
<p>This is not entirely justified, but people seem to do it anyway. Using the normal approximation, we conclude that we are “95% confident” that the following interval contains the true parameter <em>p</em>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">normal_two_sided_bounds</code><code class="p">(</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code>        <code class="c1"># [0.4940, 0.5560]</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>This is a statement about the <em>interval</em>, not about <em>p</em>.&#13;
You should understand it as the assertion that if you were&#13;
to repeat the experiment many times, 95% of the time the&#13;
“true” parameter (which is the same every time)&#13;
would lie within the observed confidence interval&#13;
(which might be different every time).</p>&#13;
</div>&#13;
&#13;
<p>In particular, we do not conclude that the coin is unfair,&#13;
since 0.5 falls within our confidence interval.</p>&#13;
&#13;
<p>If instead we’d seen 540 heads, then we’d have:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">p_hat</code> <code class="o">=</code> <code class="mi">540</code> <code class="o">/</code> <code class="mi">1000</code>&#13;
<code class="n">mu</code> <code class="o">=</code> <code class="n">p_hat</code>&#13;
<code class="n">sigma</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">p_hat</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p_hat</code><code class="p">)</code> <code class="o">/</code> <code class="mi">1000</code><code class="p">)</code> <code class="c1"># 0.0158</code>&#13;
<code class="n">normal_two_sided_bounds</code><code class="p">(</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">mu</code><code class="p">,</code> <code class="n">sigma</code><code class="p">)</code> <code class="c1"># [0.5091, 0.5709]</code></pre>&#13;
&#13;
<p>Here, “fair coin” doesn’t lie in the confidence interval.&#13;
(The “fair coin” hypothesis doesn’t pass a test&#13;
that you’d expect it to pass 95% of the time if it were true.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="p-Hacking" data-type="sect1"><div class="sect1" id="idm45635753930920">&#13;
<h1>p-Hacking</h1>&#13;
&#13;
<p>A<a data-primary="hypothesis and inference" data-secondary="p-hacking" data-type="indexterm" id="idm45635753618664"/><a data-primary="p-hacking" data-type="indexterm" id="idm45635753617688"/> procedure that erroneously rejects the null hypothesis only 5% of the time will—by definition—5% of the time erroneously reject the null hypothesis:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">run_experiment</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">bool</code><code class="p">]:</code>&#13;
    <code class="sd">"""Flips a fair coin 1000 times, True = heads, False = tails"""</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="o">&lt;</code> <code class="mf">0.5</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">)]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">reject_fairness</code><code class="p">(</code><code class="n">experiment</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">bool</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="nb">bool</code><code class="p">:</code>&#13;
    <code class="sd">"""Using the 5% significance levels"""</code>&#13;
    <code class="n">num_heads</code> <code class="o">=</code> <code class="nb">len</code><code class="p">([</code><code class="n">flip</code> <code class="k">for</code> <code class="n">flip</code> <code class="ow">in</code> <code class="n">experiment</code> <code class="k">if</code> <code class="n">flip</code><code class="p">])</code>&#13;
    <code class="k">return</code> <code class="n">num_heads</code> <code class="o">&lt;</code> <code class="mi">469</code> <code class="ow">or</code> <code class="n">num_heads</code> <code class="o">&gt;</code> <code class="mi">531</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">experiments</code> <code class="o">=</code> <code class="p">[</code><code class="n">run_experiment</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">)]</code>&#13;
<code class="n">num_rejections</code> <code class="o">=</code> <code class="nb">len</code><code class="p">([</code><code class="n">experiment</code>&#13;
                      <code class="k">for</code> <code class="n">experiment</code> <code class="ow">in</code> <code class="n">experiments</code>&#13;
                      <code class="k">if</code> <code class="n">reject_fairness</code><code class="p">(</code><code class="n">experiment</code><code class="p">)])</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">num_rejections</code> <code class="o">==</code> <code class="mi">46</code></pre>&#13;
&#13;
<p>What this means is that if you’re setting out to find “significant” results, you usually can. Test enough hypotheses against your dataset, and one of them will almost certainly appear significant. Remove the right outliers, and you can probably get your <em>p</em>-value below 0.05. (We did something vaguely similar in <a data-type="xref" href="ch05.html#correlation">“Correlation”</a>; did you notice?)</p>&#13;
&#13;
<p>This is sometimes called <a href="https://www.nature.com/news/scientific-method-statistical-errors-1.14700"><em>p-hacking</em></a> and is in some ways a consequence of the “inference from <em>p</em>-values framework.” A good article criticizing this approach is <a href="http://www.iro.umontreal.ca/~dift3913/cours/papers/cohen1994_The_earth_is_round.pdf">“The Earth Is Round”</a>, by Jacob Cohen.</p>&#13;
&#13;
<p>If you want to do good <em>science</em>, you should determine your hypotheses before looking at the data, you should clean your data without the hypotheses in mind, and you should keep in mind that <em>p</em>-values are not substitutes for common sense.  (An alternative approach is discussed in <a data-type="xref" href="#bayesian_inference">“Bayesian Inference”</a>.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Running an A/B Test" data-type="sect1"><div class="sect1" id="idm45635753470328">&#13;
<h1>Example: Running an A/B Test</h1>&#13;
&#13;
<p>One<a data-primary="hypothesis and inference" data-secondary="A/B tests" data-type="indexterm" id="idm45635753468632"/><a data-primary="A/B tests" data-type="indexterm" id="idm45635753467656"/> of your primary responsibilities at DataSciencester is experience optimization, which is a euphemism for trying to get people to click on advertisements.  One of your advertisers has developed a new energy drink targeted at data scientists, and the VP of Advertisements wants your help choosing between advertisement A (“tastes great!”) and advertisement B (“less bias!”).</p>&#13;
&#13;
<p>Being a <em>scientist</em>, you decide to run an <em>experiment</em> by randomly showing site visitors one of the two&#13;
advertisements and tracking how many people click on each one.</p>&#13;
&#13;
<p>If 990 out of 1,000 A-viewers click their ad, while only 10 out of 1,000 B-viewers click their ad, you can be pretty confident that A is the better ad. But what if the differences are not so stark? Here’s where you’d use statistical inference.</p>&#13;
&#13;
<p>Let’s say that <math>&#13;
  <msub><mi>N</mi> <mn>A</mn> </msub>&#13;
</math> people see ad A, and that <math>&#13;
  <msub><mi>n</mi> <mn>A</mn> </msub>&#13;
</math> of them click it.&#13;
We can think of each ad view as a Bernoulli trial where <math>&#13;
  <msub><mi>p</mi> <mn>A</mn> </msub>&#13;
</math>&#13;
is the probability that someone clicks ad A.  Then (if <math>&#13;
  <msub><mi>N</mi> <mn>A</mn> </msub>&#13;
</math> is large, which it is here) we know that&#13;
<math>&#13;
  <mrow>&#13;
    <msub><mi>n</mi> <mi>A</mi> </msub>&#13;
    <mo>/</mo>&#13;
    <msub><mi>N</mi> <mi>A</mi> </msub>&#13;
  </mrow>&#13;
</math> is approximately a normal random variable with mean <math>&#13;
  <msub><mi>p</mi> <mi>A</mi> </msub>&#13;
</math> and standard deviation &#13;
<math>&#13;
  <mrow>&#13;
    <msub><mi>σ</mi> <mi>A</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <msqrt>&#13;
      <mrow>&#13;
        <msub><mi>p</mi> <mi>A</mi> </msub>&#13;
        <mrow>&#13;
          <mo>(</mo>&#13;
          <mn>1</mn>&#13;
          <mo>-</mo>&#13;
          <msub><mi>p</mi> <mi>A</mi> </msub>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
        <mo>/</mo>&#13;
        <msub><mi>N</mi> <mi>A</mi> </msub>&#13;
      </mrow>&#13;
    </msqrt>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>Similarly, <math>&#13;
  <mrow>&#13;
    <msub><mi>n</mi> <mi>B</mi> </msub>&#13;
    <mo>/</mo>&#13;
    <msub><mi>N</mi> <mi>B</mi> </msub>&#13;
  </mrow>&#13;
</math> is approximately a normal random variable with mean <math>&#13;
  <msub><mi>p</mi> <mi>B</mi> </msub>&#13;
</math> and standard deviation <math>&#13;
  <mrow>&#13;
    <msub><mi>σ</mi> <mi>B</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <msqrt>&#13;
      <mrow>&#13;
        <msub><mi>p</mi> <mi>B</mi> </msub>&#13;
        <mrow>&#13;
          <mo>(</mo>&#13;
          <mn>1</mn>&#13;
          <mo>-</mo>&#13;
          <msub><mi>p</mi> <mi>B</mi> </msub>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
        <mo>/</mo>&#13;
        <msub><mi>N</mi> <mi>B</mi> </msub>&#13;
      </mrow>&#13;
    </msqrt>&#13;
  </mrow>&#13;
</math>. We can express this in code as:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">estimated_parameters</code><code class="p">(</code><code class="n">N</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">n</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">float</code><code class="p">,</code> <code class="nb">float</code><code class="p">]:</code>&#13;
    <code class="n">p</code> <code class="o">=</code> <code class="n">n</code> <code class="o">/</code> <code class="n">N</code>&#13;
    <code class="n">sigma</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">p</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">p</code><code class="p">)</code> <code class="o">/</code> <code class="n">N</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">p</code><code class="p">,</code> <code class="n">sigma</code></pre>&#13;
&#13;
<p>If we assume those two normals are independent (which seems reasonable, since the individual Bernoulli trials ought to be), then their difference should also be normal with mean &#13;
<math>&#13;
  <mrow>&#13;
    <msub><mi>p</mi> <mi>B</mi> </msub>&#13;
    <mo>-</mo>&#13;
    <msub><mi>p</mi> <mi>A</mi> </msub>&#13;
  </mrow>&#13;
</math> and standard deviation <math>&#13;
  <msqrt>&#13;
    <mrow>&#13;
      <msubsup><mi>σ</mi> <mi>A</mi> <mn>2</mn> </msubsup>&#13;
      <mo>+</mo>&#13;
      <msubsup><mi>σ</mi> <mi>B</mi> <mn>2</mn> </msubsup>&#13;
    </mrow>&#13;
  </msqrt>&#13;
</math>.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>This is sort of cheating. The math only works out exactly like this if you <em>know</em> the standard deviations.  Here we’re estimating them from the data, which means that we really should be using a <em>t</em>-distribution.&#13;
But for large enough datasets, it’s close enough that it doesn’t make much of a difference.</p>&#13;
</div>&#13;
&#13;
<p>This means we can test the <em>null hypothesis</em> that <math>&#13;
  <msub><mi>p</mi> <mi>A</mi> </msub>&#13;
</math> and <math>&#13;
  <msub><mi>p</mi> <mi>B</mi> </msub>&#13;
</math> are the same&#13;
(that is, that <math>&#13;
  <mrow>&#13;
    <msub><mi>p</mi> <mi>A</mi> </msub>&#13;
    <mo>-</mo>&#13;
    <msub><mi>p</mi> <mi>B</mi> </msub>&#13;
  </mrow>&#13;
</math> is 0) by using the statistic:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">a_b_test_statistic</code><code class="p">(</code><code class="n">N_A</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">n_A</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">N_B</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">n_B</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="n">p_A</code><code class="p">,</code> <code class="n">sigma_A</code> <code class="o">=</code> <code class="n">estimated_parameters</code><code class="p">(</code><code class="n">N_A</code><code class="p">,</code> <code class="n">n_A</code><code class="p">)</code>&#13;
    <code class="n">p_B</code><code class="p">,</code> <code class="n">sigma_B</code> <code class="o">=</code> <code class="n">estimated_parameters</code><code class="p">(</code><code class="n">N_B</code><code class="p">,</code> <code class="n">n_B</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">(</code><code class="n">p_B</code> <code class="o">-</code> <code class="n">p_A</code><code class="p">)</code> <code class="o">/</code> <code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">sigma_A</code> <code class="o">**</code> <code class="mi">2</code> <code class="o">+</code> <code class="n">sigma_B</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code></pre>&#13;
&#13;
<p>which should approximately be a standard normal.</p>&#13;
&#13;
<p>For example, if “tastes great” gets 200 clicks out of 1,000 views and “less bias” gets 180 clicks out of 1,000 views, the statistic equals:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">z</code> <code class="o">=</code> <code class="n">a_b_test_statistic</code><code class="p">(</code><code class="mi">1000</code><code class="p">,</code> <code class="mi">200</code><code class="p">,</code> <code class="mi">1000</code><code class="p">,</code> <code class="mi">180</code><code class="p">)</code>    <code class="c1"># -1.14</code></pre>&#13;
&#13;
<p>The probability of seeing such a large difference&#13;
if the means were actually equal would be:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">two_sided_p_value</code><code class="p">(</code><code class="n">z</code><code class="p">)</code>                            <code class="c1"># 0.254</code></pre>&#13;
&#13;
<p>which is large enough that we can’t conclude there’s much of a difference.  On the other hand, if “less bias” only got 150 clicks, we’d have:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">z</code> <code class="o">=</code> <code class="n">a_b_test_statistic</code><code class="p">(</code><code class="mi">1000</code><code class="p">,</code> <code class="mi">200</code><code class="p">,</code> <code class="mi">1000</code><code class="p">,</code> <code class="mi">150</code><code class="p">)</code>    <code class="c1"># -2.94</code>&#13;
<code class="n">two_sided_p_value</code><code class="p">(</code><code class="n">z</code><code class="p">)</code>                            <code class="c1"># 0.003</code></pre>&#13;
&#13;
<p>which means there’s only a 0.003 probability we’d see such a large difference if the ads were equally effective.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Bayesian Inference" data-type="sect1"><div class="sect1" id="bayesian_inference">&#13;
<h1>Bayesian Inference</h1>&#13;
&#13;
<p>The<a data-primary="hypothesis and inference" data-secondary="Bayesian inference" data-type="indexterm" id="idm45635753141752"/><a data-primary="Bayesian inference" data-type="indexterm" id="idm45635753140776"/> procedures we’ve looked at have involved&#13;
making probability statements about our <em>tests</em>: e.g., “There’s only a 3% chance you’d observe such an extreme statistic&#13;
if our null hypothesis were true.”</p>&#13;
&#13;
<p>An<a data-primary="prior distributions" data-type="indexterm" id="idm45635753139048"/><a data-primary="posterior distributions" data-type="indexterm" id="idm45635753138312"/> alternative approach to inference involves treating the unknown parameters themselves as random variables. The analyst (that’s you) starts with a <em>prior distribution</em> for the parameters and then uses the observed data and Bayes’s theorem to get an updated <em>posterior distribution</em> for the parameters. Rather than making probability judgments about the tests, you make probability judgments about the parameters.</p>&#13;
&#13;
<p>For<a data-primary="Beta distributions" data-type="indexterm" id="idm45635753118328"/> example, when the unknown parameter is a probability (as in our coin-flipping example), we often use a prior from the <em>Beta distribution</em>, which puts all its probability between 0 and 1:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">B</code><code class="p">(</code><code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""A normalizing constant so that the total probability is 1"""</code>&#13;
    <code class="k">return</code> <code class="n">math</code><code class="o">.</code><code class="n">gamma</code><code class="p">(</code><code class="n">alpha</code><code class="p">)</code> <code class="o">*</code> <code class="n">math</code><code class="o">.</code><code class="n">gamma</code><code class="p">(</code><code class="n">beta</code><code class="p">)</code> <code class="o">/</code> <code class="n">math</code><code class="o">.</code><code class="n">gamma</code><code class="p">(</code><code class="n">alpha</code> <code class="o">+</code> <code class="n">beta</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">beta_pdf</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="n">x</code> <code class="o">&lt;=</code> <code class="mi">0</code> <code class="ow">or</code> <code class="n">x</code> <code class="o">&gt;=</code> <code class="mi">1</code><code class="p">:</code>          <code class="c1"># no weight outside of [0, 1]</code>&#13;
        <code class="k">return</code> <code class="mi">0</code>&#13;
    <code class="k">return</code> <code class="n">x</code> <code class="o">**</code> <code class="p">(</code><code class="n">alpha</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">x</code><code class="p">)</code> <code class="o">**</code> <code class="p">(</code><code class="n">beta</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code> <code class="o">/</code> <code class="n">B</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code></pre>&#13;
&#13;
<p>Generally speaking, this distribution centers its weight at:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">alpha</code> <code class="o">/</code> <code class="p">(</code><code class="n">alpha</code> <code class="o">+</code> <code class="n">beta</code><code class="p">)</code></pre>&#13;
&#13;
<p>and the larger <code>alpha</code> and <code>beta</code> are, the “tighter” the distribution is.</p>&#13;
&#13;
<p>For example, if <code>alpha</code> and <code>beta</code> are both 1, it’s just the uniform distribution (centered at 0.5, very dispersed).  If <code>alpha</code> is much larger than <code>beta</code>, most of the weight is near 1. And if <code>alpha</code> is much smaller than <code>beta</code>, most of the weight is near 0. <a data-type="xref" href="#beta_priors">Figure 7-1</a> shows several different Beta distributions.</p>&#13;
&#13;
<figure><div class="figure" id="beta_priors">&#13;
<img alt="Example Beta distributions." src="assets/dsf2_0701.png"/>&#13;
<h6><span class="label">Figure 7-1. </span>Example Beta distributions</h6>&#13;
</div></figure>&#13;
&#13;
<p>Say we assume a prior distribution on <em>p</em>.  Maybe we don’t want to take a stand on whether the coin is fair, and we choose <code>alpha</code> and <code>beta</code> to both equal 1.  Or maybe we have a strong belief that the coin lands heads 55% of the time, and we choose <code>alpha</code> equals 55, <code>beta</code> equals 45.</p>&#13;
&#13;
<p>Then we flip our coin a bunch of times and see <em>h</em> heads and <em>t</em> tails.  Bayes’s theorem (and some mathematics too tedious for us to go through here) tells us that the posterior distribution for <em>p</em> is again a Beta distribution, but with parameters <code>alpha + h</code> and <code>beta + t</code>.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It is no coincidence that the posterior distribution was again a Beta distribution.&#13;
The number of heads is given by a<a data-primary="Binomial distributions" data-type="indexterm" id="idm45635752967672"/> Binomial distribution,&#13;
and the Beta is the <a href="http://www.johndcook.com/blog/conjugate_prior_diagram/"><em>conjugate prior</em></a> to the Binomial distribution.&#13;
This means that whenever you update a Beta prior using observations&#13;
from the corresponding binomial, you will get back a Beta posterior.</p>&#13;
</div>&#13;
&#13;
<p>Let’s say you flip the coin 10 times and see only 3 heads. If you started with the uniform prior (in some sense refusing to take a stand about the coin’s fairness), your posterior distribution would be a Beta(4, 8), centered around 0.33.  Since you considered all probabilities equally likely, your best guess is close to the observed probability.</p>&#13;
&#13;
<p>If you started with a Beta(20, 20) (expressing a belief that the coin was roughly fair), your posterior distribution would be a Beta(23, 27), centered around 0.46, indicating a revised belief that maybe the coin is slightly biased toward tails.</p>&#13;
&#13;
<p>And if you started with a Beta(30, 10)&#13;
(expressing a belief that the coin was biased to flip 75% heads), your posterior distribution would be a Beta(33, 17), centered around 0.66.  In that case you’d still believe in a heads bias, but less strongly than you did initially. These three different posteriors are plotted in <a data-type="xref" href="#beta_posteriors">Figure 7-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="beta_posteriors">&#13;
<img alt="Posteriors arising from different priors." src="assets/dsf2_0702.png"/>&#13;
<h6><span class="label">Figure 7-2. </span>Posteriors arising from different priors</h6>&#13;
</div></figure>&#13;
&#13;
<p>If you flipped the coin more and more times, the prior would matter&#13;
less and less until eventually you’d have (nearly) the same posterior&#13;
distribution no matter which prior you started with.</p>&#13;
&#13;
<p>For example, no matter how biased you initially thought the coin was,&#13;
it would be hard to maintain that belief after seeing 1,000 heads out of 2,000 flips (unless you are a lunatic who picks something like a Beta(1000000,1) prior).</p>&#13;
&#13;
<p>What’s interesting is that this allows us to make probability statements&#13;
about hypotheses: “Based on the prior and the observed data, there is only a 5% likelihood the coin’s heads probability is between 49% and 51%.” This is philosophically very different from a statement like&#13;
“If the coin were fair, we would expect to observe data so extreme only 5% of the time.”</p>&#13;
&#13;
<p>Using Bayesian inference to test hypotheses is considered somewhat controversial—in part because the mathematics can get somewhat complicated, and in part because of the subjective nature of choosing a prior. We won’t use it any further in this book, but it’s good to know about.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635753142984">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We’ve barely<a data-primary="hypothesis and inference" data-secondary="resources for learning" data-type="indexterm" id="idm45635752935528"/> scratched the surface&#13;
of what you should know about statistical inference.&#13;
The books recommended at the end of <a data-type="xref" href="ch05.html#statistics">Chapter 5</a>&#13;
go into a lot more detail.</p>&#13;
</li>&#13;
<li>&#13;
<p>Coursera<a data-primary="Coursera" data-type="indexterm" id="idm45635752932664"/> offers a&#13;
<a href="https://www.coursera.org/course/statistics">Data Analysis and Statistical Inference</a>&#13;
course that covers many of these topics.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>