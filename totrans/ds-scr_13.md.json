["```py\nfrom typing import List\nfrom collections import Counter\n\ndef raw_majority_vote(labels: List[str]) -> str:\n    votes = Counter(labels)\n    winner, _ = votes.most_common(1)[0]\n    return winner\n\nassert raw_majority_vote(['a', 'b', 'c', 'b']) == 'b'\n```", "```py\ndef majority_vote(labels: List[str]) -> str:\n    \"\"\"Assumes that labels are ordered from nearest to farthest.\"\"\"\n    vote_counts = Counter(labels)\n    winner, winner_count = vote_counts.most_common(1)[0]\n    num_winners = len([count\n                       for count in vote_counts.values()\n                       if count == winner_count])\n\n    if num_winners == 1:\n        return winner                     # unique winner, so return it\n    else:\n        return majority_vote(labels[:-1]) # try again without the farthest\n\n# Tie, so look at first 4, then 'b'\nassert majority_vote(['a', 'b', 'c', 'b', 'a']) == 'b'\n```", "```py\nfrom typing import NamedTuple\nfrom scratch.linear_algebra import Vector, distance\n\nclass LabeledPoint(NamedTuple):\n    point: Vector\n    label: str\n\ndef knn_classify(k: int,\n                 labeled_points: List[LabeledPoint],\n                 new_point: Vector) -> str:\n\n    # Order the labeled points from nearest to farthest.\n    by_distance = sorted(labeled_points,\n                         key=lambda lp: distance(lp.point, new_point))\n\n    # Find the labels for the k closest\n    k_nearest_labels = [lp.label for lp in by_distance[:k]]\n\n    # and let them vote.\n    return majority_vote(k_nearest_labels)\n```", "```py\nimport requests\n\ndata = requests.get(\n  \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n)\n\nwith open('iris.dat', 'w') as f:\n    f.write(data.text)\n```", "```py\nsepal_length, sepal_width, petal_length, petal_width, class\n```", "```py\n5.1,3.5,1.4,0.2,Iris-setosa\n```", "```py\nfrom typing import Dict\nimport csv\nfrom collections import defaultdict\n\ndef parse_iris_row(row: List[str]) -> LabeledPoint:\n    \"\"\"\n sepal_length, sepal_width, petal_length, petal_width, class\n \"\"\"\n    measurements = [float(value) for value in row[:-1]]\n    # class is e.g. \"Iris-virginica\"; we just want \"virginica\"\n    label = row[-1].split(\"-\")[-1]\n\n    return LabeledPoint(measurements, label)\n\nwith open('iris.data') as f:\n    reader = csv.reader(f)\n    iris_data = [parse_iris_row(row) for row in reader]\n\n# We'll also group just the points by species/label so we can plot them\npoints_by_species: Dict[str, List[Vector]] = defaultdict(list)\nfor iris in iris_data:\n    points_by_species[iris.label].append(iris.point)\n```", "```py\nfrom matplotlib import pyplot as plt\nmetrics = ['sepal length', 'sepal width', 'petal length', 'petal width']\npairs = [(i, j) for i in range(4) for j in range(4) if i < j]\nmarks = ['+', '.', 'x']  # we have 3 classes, so 3 markers\n\nfig, ax = plt.subplots(2, 3)\n\nfor row in range(2):\n    for col in range(3):\n        i, j = pairs[3 * row + col]\n        ax[row][col].set_title(f\"{metrics[i]} vs {metrics[j]}\", fontsize=8)\n        ax[row][col].set_xticks([])\n        ax[row][col].set_yticks([])\n\n        for mark, (species, points) in zip(marks, points_by_species.items()):\n            xs = [point[i] for point in points]\n            ys = [point[j] for point in points]\n            ax[row][col].scatter(xs, ys, marker=mark, label=species)\n\nax[-1][-1].legend(loc='lower right', prop={'size': 6})\nplt.show()\n```", "```py\nimport random\nfrom scratch.machine_learning import split_data\n\nrandom.seed(12)\niris_train, iris_test = split_data(iris_data, 0.70)\nassert len(iris_train) == 0.7 * 150\nassert len(iris_test) == 0.3 * 150\n```", "```py\nfrom typing import Tuple\n\n# track how many times we see (predicted, actual)\nconfusion_matrix: Dict[Tuple[str, str], int] = defaultdict(int)\nnum_correct = 0\n\nfor iris in iris_test:\n    predicted = knn_classify(5, iris_train, iris.point)\n    actual = iris.label\n\n    if predicted == actual:\n        num_correct += 1\n\n    confusion_matrix[(predicted, actual)] += 1\n\npct_correct = num_correct / len(iris_test)\nprint(pct_correct, confusion_matrix)\n```", "```py\ndef random_point(dim: int) -> Vector:\n    return [random.random() for _ in range(dim)]\n```", "```py\ndef random_distances(dim: int, num_pairs: int) -> List[float]:\n    return [distance(random_point(dim), random_point(dim))\n            for _ in range(num_pairs)]\n```", "```py\nimport tqdm\ndimensions = range(1, 101)\n\navg_distances = []\nmin_distances = []\n\nrandom.seed(0)\nfor dim in tqdm.tqdm(dimensions, desc=\"Curse of Dimensionality\"):\n    distances = random_distances(dim, 10000)      # 10,000 random pairs\n    avg_distances.append(sum(distances) / 10000)  # track the average\n    min_distances.append(min(distances))          # track the minimum\n```", "```py\nmin_avg_ratio = [min_dist / avg_dist\n                 for min_dist, avg_dist in zip(min_distances, avg_distances)]\n```"]