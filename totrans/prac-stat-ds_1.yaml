- en: Chapter 1\. Exploratory Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter focuses on the first step in any data science project: exploring
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Classical statistics focused almost exclusively on *inference*, a sometimes
    complex set of procedures for drawing conclusions about large populations based
    on small samples. In 1962, [John W. Tukey](https://oreil.ly/LQw6q) ([Figure 1-1](#JWTukey))
    called for a reformation of statistics in his seminal paper “The Future of Data
    Analysis” [[Tukey-1962]](bibliography01.xhtml#Tukey-1962). He proposed a new scientific
    discipline called *data analysis* that included statistical inference as just
    one component. Tukey forged links to the engineering and computer science communities
    (he coined the terms *bit*, short for binary digit, and *software*), and his original
    tenets are surprisingly durable and form part of the foundation for data science.
    The field of exploratory data analysis was established with Tukey’s 1977 now-classic
    book *Exploratory Data Analysis* [[Tukey-1977]](bibliography01.xhtml#Tukey-1977).
    Tukey presented simple plots (e.g., boxplots, scatterplots) that, along with summary
    statistics (mean, median, quantiles, etc.), help paint a picture of a data set.
  prefs: []
  type: TYPE_NORMAL
- en: With the ready availability of computing power and expressive data analysis
    software, exploratory data analysis has evolved well beyond its original scope.
    Key drivers of this discipline have been the rapid development of new technology,
    access to more and bigger data, and the greater use of quantitative analysis in
    a variety of disciplines. David Donoho, professor of statistics at Stanford University
    and former undergraduate student of Tukey’s, authored an excellent article based
    on his presentation at the Tukey Centennial workshop in Princeton, New Jersey
    [[Donoho-2015]](bibliography01.xhtml#Donoho-2015). Donoho traces the genesis of
    data science back to Tukey’s pioneering work in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![John Tukey, the eminent statistician, whose ideas developed over fifty years
    ago form the foundation of data science.](Images/psd2_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. John Tukey, the eminent statistician whose ideas developed over
    50 years ago form the foundation of data science
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Elements of Structured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data comes from many sources: sensor measurements, events, text, images, and
    videos. The *Internet of Things* (IoT) is spewing out streams of information.
    Much of this data is unstructured: images are a collection of pixels, with each
    pixel containing RGB (red, green, blue) color information. Texts are sequences
    of words and nonword characters, often organized by sections, subsections, and
    so on. Clickstreams are sequences of actions by a user interacting with an app
    or a web page. In fact, a major challenge of data science is to harness this torrent
    of raw data into actionable information. To apply the statistical concepts covered
    in this book, unstructured raw data must be processed and manipulated into a structured
    form. One of the commonest forms of structured data is a table with rows and columns—as
    data might emerge from a relational database or be collected for a study.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two basic types of structured data: numeric and categorical. Numeric
    data comes in two forms: *continuous*, such as wind speed or time duration, and
    *discrete*, such as the count of the occurrence of an event. *Categorical* data
    takes only a fixed set of values, such as a type of TV screen (plasma, LCD, LED,
    etc.) or a state name (Alabama, Alaska, etc.). *Binary* data is an important special
    case of categorical data that takes on only one of two values, such as 0/1, yes/no,
    or true/false. Another useful type of categorical data is *ordinal* data in which
    the categories are ordered; an example of this is a numerical rating (1, 2, 3,
    4, or 5).'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we bother with a taxonomy of data types? It turns out that for the purposes
    of data analysis and predictive modeling, the data type is important to help determine
    the type of visual display, data analysis, or statistical model. In fact, data
    science software, such as *R* and *Python*, uses these data types to improve computational
    performance. More important, the data type for a variable determines how software
    will handle computations for that variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Software engineers and database programmers may wonder why we even need the
    notion of *categorical* and *ordinal* data for analytics. After all, categories
    are merely a collection of text (or numeric) values, and the underlying database
    automatically handles the internal representation. However, explicit identification
    of data as categorical, as distinct from text, does offer some advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that data is categorical can act as a signal telling software how statistical
    procedures, such as producing a chart or fitting a model, should behave. In particular,
    ordinal data can be represented as an `ordered.factor` in *R*, preserving a user-specified
    ordering in charts, tables, and models. In *Python*, `scikit-learn` supports ordinal
    data with the `sklearn.preprocessing.OrdinalEncoder`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage and indexing can be optimized (as in a relational database).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The possible values a given categorical variable can take are enforced in the
    software (like an enum).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The third “benefit” can lead to unintended or unexpected behavior: the default
    behavior of data import functions in *R* (e.g., `read.csv`) is to automatically
    convert a text column into a `factor`. Subsequent operations on that column will
    assume that the only allowable values for that column are the ones originally
    imported, and assigning a new text value will introduce a warning and produce
    an `NA` (missing value). The `pandas` package in *Python* will not make such a
    conversion automatically. However, you can specify a column as categorical explicitly
    in the `read_csv` function.'
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [`pandas` documentation](https://oreil.ly/UGX-4) describes the different
    data types and how they can be manipulated in *Python*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data types can be confusing, since types may overlap, and the taxonomy in one
    software may differ from that in another. The [R Tutorial website](https://oreil.ly/2YUoA)
    covers the taxonomy for *R*. The [`pandas` documentation](https://oreil.ly/UGX-4)
    describes the different data types and how they can be manipulated in *Python*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databases are more detailed in their classification of data types, incorporating
    considerations of precision levels, fixed- or variable-length fields, and more;
    see the [W3Schools guide to SQL](https://oreil.ly/cThTM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectangular Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The typical frame of reference for an analysis in data science is a *rectangular
    data* object, like a spreadsheet or database table.
  prefs: []
  type: TYPE_NORMAL
- en: '*Rectangular data* is the general term for a two-dimensional matrix with rows
    indicating records (cases) and columns indicating features (variables); *data
    frame* is the specific format in *R* and *Python*. The data doesn’t always start
    in this form: unstructured data (e.g., text) must be processed and manipulated
    so that it can be represented as a set of features in the rectangular data (see
    [“Elements of Structured Data”](#StructuredData)). Data in relational databases
    must be extracted and put into a single table for most data analysis and modeling
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-1\. A typical data frame format
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | currency | sellerRating | Duration | endDay | ClosePrice | OpenPrice
    | Competitive? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Music/Movie/Game | US | 3249 | 5 | Mon | 0.01 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Music/Movie/Game | US | 3249 | 5 | Mon | 0.01 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Automotive | US | 3115 | 7 | Tue | 0.01 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Automotive | US | 3115 | 7 | Tue | 0.01 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Automotive | US | 3115 | 7 | Tue | 0.01 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Automotive | US | 3115 | 7 | Tue | 0.01 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Automotive | US | 3115 | 7 | Tue | 0.01 | 0.01 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Automotive | US | 3115 | 7 | Tue | 0.01 | 0.01 | 1 |'
  prefs: []
  type: TYPE_TB
- en: In [Table 1-1](#dataframe), there is a mix of measured or counted data (e.g.,
    duration and price) and categorical data (e.g., category and currency). As mentioned
    earlier, a special form of categorical variable is a binary (yes/no or 0/1) variable,
    seen in the rightmost column in [Table 1-1](#dataframe)—an indicator variable
    showing whether an auction was competitive (had multiple bidders) or not. This
    indicator variable also happens to be an *outcome* variable, when the scenario
    is to predict whether an auction is competitive or not.
  prefs: []
  type: TYPE_NORMAL
- en: Data Frames and Indexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional database tables have one or more columns designated as an index,
    essentially a row number. This can vastly improve the efficiency of certain database
    queries. In *Python*, with the `pandas` library, the basic rectangular data structure
    is a `DataFrame` object. By default, an automatic integer index is created for
    a `DataFrame` based on the order of the rows. In `pandas`, it is also possible
    to set multilevel/hierarchical indexes to improve the efficiency of certain operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *R*, the basic rectangular data structure is a `data.frame` object. A `data.frame`
    also has an implicit integer index based on the row order. The native *R* `data.frame`
    does not support user-specified or multilevel indexes, though a custom key can
    be created through the `row.names` attribute. To overcome this deficiency, two
    new packages are gaining widespread use: `data.table` and `dplyr`. Both support
    multilevel indexes and offer significant speedups in working with a `data.frame`.'
  prefs: []
  type: TYPE_NORMAL
- en: Terminology Differences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Terminology for rectangular data can be confusing. Statisticians and data scientists
    use different terms for the same thing. For a statistician, *predictor variables*
    are used in a model to predict a *response* or *dependent variable*. For a data
    scientist, *features* are used to predict a *target*. One synonym is particularly
    confusing: computer scientists will use the term *sample* for a single row; a
    *sample* to a statistician means a collection of rows.'
  prefs: []
  type: TYPE_NORMAL
- en: Nonrectangular Data Structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are other data structures besides rectangular data.
  prefs: []
  type: TYPE_NORMAL
- en: Time series data records successive measurements of the same variable. It is
    the raw material for statistical forecasting methods, and it is also a key component
    of the data produced by devices—the Internet of Things.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial data structures, which are used in mapping and location analytics, are
    more complex and varied than rectangular data structures. In the *object* representation,
    the focus of the data is an object (e.g., a house) and its spatial coordinates.
    The *field* view, by contrast, focuses on small units of space and the value of
    a relevant metric (pixel brightness, for example).
  prefs: []
  type: TYPE_NORMAL
- en: Graph (or network) data structures are used to represent physical, social, and
    abstract relationships. For example, a graph of a social network, such as Facebook
    or LinkedIn, may represent connections between people on the network. Distribution
    hubs connected by roads are an example of a physical network. Graph structures
    are useful for certain types of problems, such as network optimization and recommender
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these data types has its specialized methodology in data science. The
    focus of this book is on rectangular data, the fundamental building block of predictive
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs in Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In computer science and information technology, the term *graph* typically refers
    to a depiction of the connections among entities, and to the underlying data structure.
    In statistics, *graph* is used to refer to a variety of plots and *visualizations*,
    not just of connections among entities, and the term applies only to the visualization,
    not to the data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Documentation on data frames in *R*](https://oreil.ly/NsONR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Documentation on data frames in *Python*](https://oreil.ly/oxDKQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimates of Location
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Variables with measured or count data might have thousands of distinct values.
    A basic step in exploring your data is getting a “typical value” for each feature
    (variable): an estimate of where most of the data is located (i.e., its central
    tendency).'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, summarizing data might seem fairly trivial: just take the
    *mean* of the data. In fact, while the mean is easy to compute and expedient to
    use, it may not always be the best measure for a central value. For this reason,
    statisticians have developed and promoted several alternative estimates to the
    mean.'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics and Estimates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Statisticians often use the term *estimate* for a value calculated from the
    data at hand, to draw a distinction between what we see from the data and the
    theoretical true or exact state of affairs. Data scientists and business analysts
    are more likely to refer to such a value as a *metric*. The difference reflects
    the approach of statistics versus that of data science: accounting for uncertainty
    lies at the heart of the discipline of statistics, whereas concrete business or
    organizational objectives are the focus of data science. Hence, statisticians
    estimate, and data scientists measure.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most basic estimate of location is the mean, or *average* value. The mean
    is the sum of all values divided by the number of values. Consider the following
    set of numbers: {3 5 1 2}. The mean is (3 + 5 + 1 + 2) / 4 = 11 / 4 = 2.75. You
    will encounter the symbol <math alttext="x overbar"><mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></math> (pronounced “x-bar”) being used to represent the mean
    of a sample from a population. The formula to compute the mean for a set of *n*
    values <math alttext="x 1 comma x 2 comma ellipsis comma x Subscript n Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math> is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>Mean</mtext> <mo>=</mo> <mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mi>i=1</mi> <mi>n</mi></msubsup>
    <msub><mi>x</mi> <mi>i</mi></msub></mrow> <mi>n</mi></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*N* (or *n*) refers to the total number of records or observations. In statistics
    it is capitalized if it is referring to a population, and lowercase if it refers
    to a sample from a population. In data science, that distinction is not vital,
    so you may see it both ways.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A variation of the mean is a *trimmed mean*, which you calculate by dropping
    a fixed number of sorted values at each end and then taking an average of the
    remaining values. Representing the sorted values by <math alttext="x Subscript
    left-parenthesis 1 right-parenthesis Baseline comma x Subscript left-parenthesis
    2 right-parenthesis Baseline comma ellipsis comma x Subscript left-parenthesis
    n right-parenthesis Baseline"><mrow><msub><mi>x</mi> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msub></mrow></math>
    where <math alttext="x Subscript left-parenthesis 1 right-parenthesis"><msub><mi>x</mi>
    <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msub></math> is the smallest value
    and <math alttext="x Subscript left-parenthesis n right-parenthesis"><msub><mi>x</mi>
    <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msub></math> the largest, the formula
    to compute the trimmed mean with <math alttext="p"><mi>p</mi></math> smallest
    and largest values omitted is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>Trimmed</mtext> <mtext>mean</mtext> <mo>=</mo>
    <mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mi>p</mi><mo>+</mo><mn>1</mn></mrow> <mrow><mi>n</mi><mo>-</mo><mi>p</mi></mrow></msubsup>
    <msub><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub></mrow> <mrow><mi>n</mi><mo>-</mo><mn>2</mn><mi>p</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: A trimmed mean eliminates the influence of extreme values. For example, in international
    diving the top score and bottom score from five judges are dropped, and [the final
    score is the average of the scores from the three remaining judges](https://oreil.ly/uV4P0).
    This makes it difficult for a single judge to manipulate the score, perhaps to
    favor their country’s contestant. Trimmed means are widely used, and in many cases
    are preferable to using the ordinary mean—see [“Median and Robust Estimates”](#Median)
    for further discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of mean is a *weighted mean*, which you calculate by multiplying
    each data value <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    by a user-specified weight <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    and dividing their sum by the sum of the weights. The formula for a weighted mean
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="Weighted mean x overbar Subscript w Baseline equals StartFraction
    sigma-summation Underscript i equals 1 Overscript n Endscripts w Subscript i Baseline
    x Subscript i Baseline Over sigma-summation Underscript i equals 1 Overscript
    n Endscripts w Subscript i Baseline EndFraction" display="block"><mrow><mi>Weighted</mi>
    <mi>mean</mi> <mo>=</mo> <msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover>
    <mi>w</mi></msub> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msub><mi>w</mi> <mi>i</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msub><mi>w</mi> <mi>i</mi></msub></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main motivations for using a weighted mean:'
  prefs: []
  type: TYPE_NORMAL
- en: Some values are intrinsically more variable than others, and highly variable
    observations are given a lower weight. For example, if we are taking the average
    from multiple sensors and one of the sensors is less accurate, then we might downweight
    the data from that sensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data collected does not equally represent the different groups that we are
    interested in measuring. For example, because of the way an online experiment
    was conducted, we may not have a set of data that accurately reflects all groups
    in the user base. To correct that, we can give a higher weight to the values from
    the groups that were underrepresented.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Median and Robust Estimates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *median* is the middle number on a sorted list of the data. If there is
    an even number of data values, the middle value is one that is not actually in
    the data set, but rather the average of the two values that divide the sorted
    data into upper and lower halves. Compared to the mean, which uses all observations,
    the median depends only on the values in the center of the sorted data. While
    this might seem to be a disadvantage, since the mean is much more sensitive to
    the data, there are many instances in which the median is a better metric for
    location. Let’s say we want to look at typical household incomes in neighborhoods
    around Lake Washington in Seattle. In comparing the Medina neighborhood to the
    Windermere neighborhood, using the mean would produce very different results because
    Bill Gates lives in Medina. If we use the median, it won’t matter how rich Bill
    Gates is—the position of the middle observation will remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: For the same reasons that one uses a weighted mean, it is also possible to compute
    a *weighted median*. As with the median, we first sort the data, although each
    data value has an associated weight. Instead of the middle number, the weighted
    median is a value such that the sum of the weights is equal for the lower and
    upper halves of the sorted list. Like the median, the weighted median is robust
    to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The median is referred to as a *robust* estimate of location since it is not
    influenced by *outliers* (extreme cases) that could skew the results. An outlier
    is any value that is very distant from the other values in a data set. The exact
    definition of an outlier is somewhat subjective, although certain conventions
    are used in various data summaries and plots (see [“Percentiles and Boxplots”](#Boxplots)).
    Being an outlier in itself does not make a data value invalid or erroneous (as
    in the previous example with Bill Gates). Still, outliers are often the result
    of data errors such as mixing data of different units (kilometers versus meters)
    or bad readings from a sensor. When outliers are the result of bad data, the mean
    will result in a poor estimate of location, while the median will still be valid.
    In any case, outliers should be identified and are usually worthy of further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to typical data analysis, where outliers are sometimes informative
    and sometimes a nuisance, in *anomaly detection* the points of interest are the
    outliers, and the greater mass of data serves primarily to define the “normal”
    against which anomalies are measured.
  prefs: []
  type: TYPE_NORMAL
- en: 'The median is not the only robust estimate of location. In fact, a trimmed
    mean is widely used to avoid the influence of outliers. For example, trimming
    the bottom and top 10% (a common choice) of the data will provide protection against
    outliers in all but the smallest data sets. The trimmed mean can be thought of
    as a compromise between the median and the mean: it is robust to extreme values
    in the data, but uses more data to calculate the estimate for location.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Robust Metrics for Location
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statisticians have developed a plethora of other estimators for location, primarily
    with the goal of developing an estimator more robust than the mean and also more
    efficient (i.e., better able to discern small location differences between data
    sets). While these methods are potentially useful for small data sets, they are
    not likely to provide added benefit for large or even moderately sized data sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Location Estimates of Population and Murder Rates'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Table 1-2](#state_table) shows the first few rows in the data set containing
    population and murder rates (in units of murders per 100,000 people per year)
    for each US state (2010 Census).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-2\. A few rows of the `data.frame` state of population and murder rate
    by state
  prefs: []
  type: TYPE_NORMAL
- en: '|  | State | Population | Murder rate | Abbreviation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Alabama | 4,779,736 | 5.7 | AL |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Alaska | 710,231 | 5.6 | AK |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Arizona | 6,392,017 | 4.7 | AZ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Arkansas | 2,915,918 | 5.6 | AR |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | California | 37,253,956 | 4.4 | CA |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Colorado | 5,029,196 | 2.8 | CO |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Connecticut | 3,574,097 | 2.4 | CT |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Delaware | 897,934 | 5.8 | DE |'
  prefs: []
  type: TYPE_TB
- en: 'Compute the mean, trimmed mean, and median for the population using *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To compute mean and median in *Python* we can use the `pandas` methods of the
    data frame. The trimmed mean requires the `trim_mean` function in `scipy.stats`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The mean is bigger than the trimmed mean, which is bigger than the median.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because the trimmed mean excludes the largest and smallest five states
    (`trim=0.1` drops 10% from each end). If we want to compute the average murder
    rate for the country, we need to use a weighted mean or median to account for
    different populations in the states. Since base *R* doesn’t have a function for
    weighted median, we need to install a package such as `matrixStats`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Weighted mean is available with `NumPy`. For weighted median, we can use the
    specialized package [wquantiles](https://oreil.ly/4SIPQ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the weighted mean and the weighted median are about the same.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Wikipedia article on [central tendency](https://oreil.ly/qUW2i) contains
    an extensive discussion of various measures of location.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: John Tukey’s 1977 classic *Exploratory Data Analysis* (Pearson) is still widely
    read.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimates of Variability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Location is just one dimension in summarizing a feature. A second dimension,
    *variability*, also referred to as *dispersion*, measures whether the data values
    are tightly clustered or spread out. At the heart of statistics lies variability:
    measuring it, reducing it, distinguishing random from real variability, identifying
    the various sources of real variability, and making decisions in the presence
    of it.'
  prefs: []
  type: TYPE_NORMAL
- en: Just as there are different ways to measure location (mean, median, etc.), there
    are also different ways to measure variability.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Deviation and Related Estimates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most widely used estimates of variation are based on the differences, or
    *deviations*, between the estimate of location and the observed data. For a set
    of data {1, 4, 4}, the mean is 3 and the median is 4. The deviations from the
    mean are the differences: 1 – 3 = –2, 4 – 3 = 1, 4 – 3 = 1. These deviations tell
    us how dispersed the data is around the central value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to measure variability is to estimate a typical value for these deviations.
    Averaging the deviations themselves would not tell us much—the negative deviations
    offset the positive ones. In fact, the sum of the deviations from the mean is
    precisely zero. Instead, a simple approach is to take the average of the absolute
    values of the deviations from the mean. In the preceding example, the absolute
    value of the deviations is {2 1 1}, and their average is (2 + 1 + 1) / 3 = 1.33.
    This is known as the *mean absolute deviation* and is computed with the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>Mean</mtext> <mtext>absolute</mtext> <mtext>deviation</mtext>
    <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mfenced separators="" open="|" close="|"><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mfenced></mrow>
    <mi>n</mi></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="x overbar"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>
    is the sample mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best-known estimates of variability are the *variance* and the *standard
    deviation*, which are based on squared deviations. The variance is an average
    of the squared deviations, and the standard deviation is the square root of the
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd><mtext>Variance</mtext></mtd>
    <mtd><mo>=</mo> <msup><mi>s</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><munderover><mo>∑</mo>
    <mrow class="MJX-TeXAtom-ORD"><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mi>n</mi></munderover>
    <msup><mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>−</mo> <mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi>
    <mo>¯</mo></mover></mrow> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><mi>n</mi>
    <mo>−</mo> <mn>1</mn></mrow></mfrac></mtd></mtr> <mtr><mtd><mtext>Standard deviation</mtext></mtd>
    <mtd><mo>=</mo> <mi>s</mi> <mo>=</mo> <msqrt><mtext>Variance</mtext></msqrt></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard deviation is much easier to interpret than the variance since
    it is on the same scale as the original data. Still, with its more complicated
    and less intuitive formula, it might seem peculiar that the standard deviation
    is preferred in statistics over the mean absolute deviation. It owes its preeminence
    to statistical theory: mathematically, working with squared values is much more
    convenient than absolute values, especially for statistical models.'
  prefs: []
  type: TYPE_NORMAL
- en: Neither the variance, the standard deviation, nor the mean absolute deviation
    is robust to outliers and extreme values (see [“Median and Robust Estimates”](#Median)
    for a discussion of robust estimates for location). The variance and standard
    deviation are especially sensitive to outliers since they are based on the squared
    deviations.
  prefs: []
  type: TYPE_NORMAL
- en: 'A robust estimate of variability is the *median absolute deviation from the
    median* or MAD:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>Median</mtext> <mtext>absolute</mtext> <mtext>deviation</mtext>
    <mo>=</mo> <mtext>Median</mtext> <mfenced separators="" open="(" close=")"><mfenced
    separators="" open="|" close="|"><msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo>
    <mi>m</mi></mfenced> <mo>,</mo> <mfenced separators="" open="|" close="|"><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>-</mo> <mi>m</mi></mfenced> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <mfenced separators="" open="|" close="|"><msub><mi>x</mi> <mi>N</mi></msub> <mo>-</mo>
    <mi>m</mi></mfenced></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where *m* is the median. Like the median, the MAD is not influenced by extreme
    values. It is also possible to compute a trimmed standard deviation analogous
    to the trimmed mean (see [“Mean”](#Mean)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The variance, the standard deviation, the mean absolute deviation, and the median
    absolute deviation from the median are not equivalent estimates, even in the case
    where the data comes from a normal distribution. In fact, the standard deviation
    is always greater than the mean absolute deviation, which itself is greater than
    the median absolute deviation. Sometimes, the median absolute deviation is multiplied
    by a constant scaling factor to put the MAD on the same scale as the standard
    deviation in the case of a normal distribution. The commonly used factor of 1.4826
    means that 50% of the normal distribution fall within the range <math alttext="plus-or-minus
    MAD"><mrow><mo>±</mo> <mi>MAD</mi></mrow></math> (see, e.g., [*https://oreil.ly/SfDk2*](https://oreil.ly/SfDk2)).
  prefs: []
  type: TYPE_NORMAL
- en: Estimates Based on Percentiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A different approach to estimating dispersion is based on looking at the spread
    of the sorted data. Statistics based on sorted (ranked) data are referred to as
    *order statistics*. The most basic measure is the *range*: the difference between
    the largest and smallest numbers. The minimum and maximum values themselves are
    useful to know and are helpful in identifying outliers, but the range is extremely
    sensitive to outliers and not very useful as a general measure of dispersion in
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the sensitivity to outliers, we can look at the range of the data after
    dropping values from each end. Formally, these types of estimates are based on
    differences between *percentiles*. In a data set, the *P*th percentile is a value
    such that at least *P* percent of the values take on this value or less and at
    least (100 – *P*) percent of the values take on this value or more. For example,
    to find the 80th percentile, sort the data. Then, starting with the smallest value,
    proceed 80 percent of the way to the largest value. Note that the median is the
    same thing as the 50th percentile. The percentile is essentially the same as a
    *quantile*, with quantiles indexed by fractions (so the .8 quantile is the same
    as the 80th percentile).
  prefs: []
  type: TYPE_NORMAL
- en: 'A common measurement of variability is the difference between the 25th percentile
    and the 75th percentile, called the *interquartile range* (or IQR). Here is a
    simple example: {3,1,5,3,6,7,2,9}. We sort these to get {1,2,3,3,5,6,7,9}. The
    25th percentile is at 2.5, and the 75th percentile is at 6.5, so the interquartile
    range is 6.5 – 2.5 = 4. Software can have slightly differing approaches that yield
    different answers (see the following tip); typically, these differences are smaller.'
  prefs: []
  type: TYPE_NORMAL
- en: For very large data sets, calculating exact percentiles can be computationally
    very expensive since it requires sorting all the data values. Machine learning
    and statistical software use special algorithms, such as [[Zhang-Wang-2007]](bibliography01.xhtml#Zhang-Wang-2007),
    to get an approximate percentile that can be calculated very quickly and is guaranteed
    to have a certain accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Percentile: Precise Definition'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we have an even number of data (*n* is even), then the percentile is ambiguous
    under the preceding definition. In fact, we could take on any value between the
    order statistics <math alttext="x Subscript left-parenthesis j right-parenthesis"><msub><mi>x</mi>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msub></math> and <math alttext="x
    Subscript left-parenthesis j plus 1 right-parenthesis"><msub><mi>x</mi> <mrow><mo>(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msub></math>
    where *j* satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mn>100</mn> <mo>*</mo> <mfrac><mi>j</mi> <mi>n</mi></mfrac>
    <mo>≤</mo> <mi>P</mi> <mo><</mo> <mn>100</mn> <mo>*</mo> <mfrac><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow>
    <mi>n</mi></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, the percentile is the weighted average:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>Percentile</mtext> <mrow><mo>(</mo> <mi>P</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfenced separators="" open="(" close=")"><mn>1</mn>
    <mo>-</mo> <mi>w</mi></mfenced> <msub><mi>x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <mi>w</mi> <msub><mi>x</mi> <mrow><mo>(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: for some weight *w* between 0 and 1. Statistical software has slightly differing
    approaches to choosing *w*. In fact, the *R* function `quantile` offers nine different
    alternatives to compute the quantile. Except for small data sets, you don’t usually
    need to worry about the precise way a percentile is calculated. At the time of
    this writing, *Python*’s `numpy.quantile` supports only one approach, linear interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Variability Estimates of State Population'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Table 1-3](#state) (repeated from [Table 1-2](#state_table) for convenience)
    shows the first few rows in the data set containing population and murder rates
    for each state.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-3\. A few rows of the `data.frame` state of population and murder rate
    by state
  prefs: []
  type: TYPE_NORMAL
- en: '|  | State | Population | Murder rate | Abbreviation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Alabama | 4,779,736 | 5.7 | AL |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Alaska | 710,231 | 5.6 | AK |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Arizona | 6,392,017 | 4.7 | AZ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Arkansas | 2,915,918 | 5.6 | AR |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | California | 37,253,956 | 4.4 | CA |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Colorado | 5,029,196 | 2.8 | CO |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Connecticut | 3,574,097 | 2.4 | CT |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Delaware | 897,934 | 5.8 | DE |'
  prefs: []
  type: TYPE_TB
- en: 'Using *R*’s built-in functions for the standard deviation, the interquartile
    range (IQR), and the median absolute deviation from the median (MAD), we can compute
    estimates of variability for the state population data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pandas` data frame provides methods for calculating standard deviation
    and quantiles. Using the quantiles, we can easily determine the IQR. For the robust
    MAD, we use the function `robust.scale.mad` from the `statsmodels` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The standard deviation is almost twice as large as the MAD (in *R*, by default,
    the scale of the MAD is adjusted to be on the same scale as the mean). This is
    not surprising since the standard deviation is sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: David Lane’s online statistics resource has a [section on percentiles](https://oreil.ly/o2fBI).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kevin Davenport has a [useful post on *R*-Bloggers](https://oreil.ly/E7zcG)
    about deviations from the median and their robust properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the Data Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each of the estimates we’ve covered sums up the data in a single number to describe
    the location or variability of the data. It is also useful to explore how the
    data is distributed overall.
  prefs: []
  type: TYPE_NORMAL
- en: Percentiles and Boxplots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [“Estimates Based on Percentiles”](#Percentiles), we explored how percentiles
    can be used to measure the spread of the data. Percentiles are also valuable for
    summarizing the entire distribution. It is common to report the quartiles (25th,
    50th, and 75th percentiles) and the deciles (the 10th, 20th, …, 90th percentiles).
    Percentiles are especially valuable for summarizing the *tails* (the outer range)
    of the distribution. Popular culture has coined the term *one-percenters* to refer
    to the people in the top 99th percentile of wealth.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 1-4](#PercentileTable) displays some percentiles of the murder rate
    by state. In *R*, this would be produced by the `quantile` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pandas` data frame method `quantile` provides it in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Table 1-4\. Percentiles of murder rate by state
  prefs: []
  type: TYPE_NORMAL
- en: '| 5% | 25% | 50% | 75% | 95% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1.60 | 2.42 | 4.00 | 5.55 | 6.51 |'
  prefs: []
  type: TYPE_TB
- en: 'The median is 4 murders per 100,000 people, although there is quite a bit of
    variability: the 5th percentile is only 1.6 and the 95th percentile is 6.51.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Boxplots*, introduced by Tukey [[Tukey-1977]](bibliography01.xhtml#Tukey-1977),
    are based on percentiles and give a quick way to visualize the distribution of
    data. [Figure 1-2](#BoxplotFigure) shows a boxplot of the population by state
    produced by *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`pandas` provides a number of basic exploratory plots for data frame; one of
    them is boxplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Boxplot of state populations](Images/psd2_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Boxplot of state populations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this boxplot we can immediately see that the median state population is
    about 5 million, half the states fall between about 2 million and about 7 million,
    and there are some high population outliers. The top and bottom of the box are
    the 75th and 25th percentiles, respectively. The median is shown by the horizontal
    line in the box. The dashed lines, referred to as *whiskers*, extend from the
    top and bottom of the box to indicate the range for the bulk of the data. There
    are many variations of a boxplot; see, for example, the documentation for the
    *R* function `boxplot` [[R-base-2015]](bibliography01.xhtml#R-base-2015). By default,
    the *R* function extends the whiskers to the furthest point beyond the box, except
    that it will not go beyond 1.5 times the IQR. *Matplotlib* uses the same implementation;
    other software may use a different rule.
  prefs: []
  type: TYPE_NORMAL
- en: Any data outside of the whiskers is plotted as single points or circles (often
    considered outliers).
  prefs: []
  type: TYPE_NORMAL
- en: Frequency Tables and Histograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A frequency table of a variable divides up the variable range into equally
    spaced segments and tells us how many values fall within each segment. [Table 1-5](#FreqTable)
    shows a frequency table of the population by state computed in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `pandas.cut` creates a series that maps the values into the segments.
    Using the method `value_counts`, we get the frequency table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Table 1-5\. A frequency table of population by state
  prefs: []
  type: TYPE_NORMAL
- en: '| BinNumber | BinRange | Count | States |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 563,626–4,232,658 | 24 | WY,VT,ND,AK,SD,DE,MT,RI,NH,ME,HI,ID,NE,WV,NM,NV,UT,KS,AR,MS,IA,CT,OK,OR
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4,232,659–7,901,691 | 14 | KY,LA,SC,AL,CO,MN,WI,MD,MO,TN,AZ,IN,MA,WA
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 7,901,692–11,570,724 | 6 | VA,NJ,NC,GA,MI,OH |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 11,570,725–15,239,757 | 2 | PA,IL |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 15,239,758–18,908,790 | 1 | FL |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 18,908,791–22,577,823 | 1 | NY |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 22,577,824–26,246,856 | 1 | TX |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 26,246,857–29,915,889 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 29,915,890–33,584,922 | 0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 33,584,923–37,253,956 | 1 | CA |'
  prefs: []
  type: TYPE_TB
- en: 'The least populous state is Wyoming, with 563,626 people, and the most populous
    is California, with 37,253,956 people. This gives us a range of 37,253,956 – 563,626
    = 36,690,330, which we must divide up into equal size bins—let’s say 10 bins.
    With 10 equal size bins, each bin will have a width of 3,669,033, so the first
    bin will span from 563,626 to 4,232,658. By contrast, the top bin, 33,584,923
    to 37,253,956, has only one state: California. The two bins immediately below
    California are empty, until we reach Texas. It is important to include the empty
    bins; the fact that there are no values in those bins is useful information. It
    can also be useful to experiment with different bin sizes. If they are too large,
    important features of the distribution can be obscured. If they are too small,
    the result is too granular, and the ability to see the bigger picture is lost.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both frequency tables and percentiles summarize the data by creating bins. In
    general, quartiles and deciles will have the same count in each bin (equal-count
    bins), but the bin sizes will be different. The frequency table, by contrast,
    will have different counts in the bins (equal-size bins), and the bin sizes will
    be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'A histogram is a way to visualize a frequency table, with bins on the x-axis
    and the data count on the y-axis. In [Figure 1-3](#HistogramFigure), for example,
    the bin centered at 10 million (1e+07) runs from roughly 8 million to 12 million,
    and there are six states in that bin. To create a histogram corresponding to [Table 1-5](#FreqTable)
    in *R*, use the `hist` function with the `breaks` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`pandas` supports histograms for data frames with the `DataFrame.plot.hist`
    method. Use the keyword argument `bins` to define the number of bins. The various
    plot methods return an axis object that allows further fine-tuning of the visualization
    using `Matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram is shown in [Figure 1-3](#HistogramFigure). In general, histograms
    are plotted such that:'
  prefs: []
  type: TYPE_NORMAL
- en: Empty bins are included in the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bins are of equal width.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of bins (or, equivalently, bin size) is up to the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bars are contiguous—no empty space shows between bars, unless there is an empty
    bin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Histogram of state populations](Images/psd2_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Histogram of state populations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Statistical Moments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In statistical theory, location and variability are referred to as the first
    and second *moments* of a distribution. The third and fourth moments are called
    *skewness* and *kurtosis*. Skewness refers to whether the data is skewed to larger
    or smaller values, and kurtosis indicates the propensity of the data to have extreme
    values. Generally, metrics are not used to measure skewness and kurtosis; instead,
    these are discovered through visual displays such as Figures [1-2](#BoxplotFigure)
    and [1-3](#HistogramFigure).
  prefs: []
  type: TYPE_NORMAL
- en: Density Plots and Estimates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Related to the histogram is a density plot, which shows the distribution of
    data values as a continuous line. A density plot can be thought of as a smoothed
    histogram, although it is typically computed directly from the data through a
    *kernel density estimate* (see [[Duong-2001]](bibliography01.xhtml#Duong-2001)
    for a short tutorial). [Figure 1-4](#DensityFigure) displays a density estimate
    superposed on a histogram. In *R*, you can compute a density estimate using the
    `density` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`pandas` provides the `density` method to create a density plot. Use the argument
    `bw_method` to control the smoothness of the density curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_exploratory_data_analysis_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Plot functions often take an optional axis (`ax`) argument, which will cause
    the plot to be added to the same graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'A key distinction from the histogram plotted in [Figure 1-3](#HistogramFigure)
    is the scale of the y-axis: a density plot corresponds to plotting the histogram
    as a proportion rather than counts (you specify this in *R* using the argument
    `freq=FALSE`). Note that the total area under the density curve = 1, and instead
    of counts in bins you calculate areas under the curve between any two points on
    the x-axis, which correspond to the proportion of the distribution lying between
    those two points.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Density of state murder rates](Images/psd2_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Density of state murder rates
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Density Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Density estimation is a rich topic with a long history in statistical literature.
    In fact, over 20 *R* packages have been published that offer functions for density
    estimation. [[Deng-Wickham-2011]](bibliography01.xhtml#Deng-Wickham-2011) give
    a comprehensive review of *R* packages, with a particular recommendation for `ASH`
    or `KernSmooth`. The density estimation methods in `pandas` and `scikit-learn`
    also offer good implementations. For many data science problems, there is no need
    to worry about the various types of density estimates; it suffices to use the
    base functions.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A SUNY Oswego professor provides a [step-by-step guide to creating a boxplot](https://oreil.ly/wTpnE).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density estimation in *R* is covered in [Henry Deng and Hadley Wickham’s paper
    of the same name](https://oreil.ly/TbWYS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R*-Bloggers has a [useful post on histograms in *R*](https://oreil.ly/Ynp-n),
    including customization elements, such as binning (breaks).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R*-Bloggers also has a [similar post on boxplots in *R*](https://oreil.ly/0DSb2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matthew Conlen published an [interactive presentation](https://oreil.ly/bC9nu)
    that demonstrates the effect of choosing different kernels and bandwidth on kernel
    density estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Binary and Categorical Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For categorical data, simple proportions or percentages tell the story of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting a summary of a binary variable or a categorical variable with a few
    categories is a fairly easy matter: we just figure out the proportion of 1s, or
    the proportions of the important categories. For example, [Table 1-6](#AirportDelays)
    shows the percentage of delayed flights by the cause of delay at Dallas/Fort Worth
    Airport since 2010. Delays are categorized as being due to factors under carrier
    control, air traffic control (ATC) system delays, weather, security, or a late
    inbound aircraft.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-6\. Percentage of delays by cause at Dallas/Fort Worth Airport
  prefs: []
  type: TYPE_NORMAL
- en: '| Carrier | ATC | Weather | Security | Inbound |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 23.02 | 30.40 | 4.03 | 0.12 | 42.43 |'
  prefs: []
  type: TYPE_TB
- en: 'Bar charts, seen often in the popular press, are a common visual tool for displaying
    a single categorical variable. Categories are listed on the x-axis, and frequencies
    or proportions on the y-axis. [Figure 1-5](#Barchart) shows the airport delays
    per year by cause for Dallas/Fort Worth (DFW), and it is produced with the *R*
    function `barplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`pandas` also supports bar charts for data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Bar chart of airline delays at DFW by cause.](Images/psd2_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. Bar chart of airline delays at DFW by cause
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that a bar chart resembles a histogram; in a bar chart the x-axis represents
    different categories of a factor variable, while in a histogram the x-axis represents
    values of a single variable on a numeric scale. In a histogram, the bars are typically
    shown touching each other, with gaps indicating values that did not occur in the
    data. In a bar chart, the bars are shown separate from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Pie charts are an alternative to bar charts, although statisticians and data
    visualization experts generally eschew pie charts as less visually informative
    (see [[Few-2007]](bibliography01.xhtml#Few-2007)).
  prefs: []
  type: TYPE_NORMAL
- en: Numerical Data as Categorical Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Frequency Tables and Histograms”](#Histograms), we looked at frequency
    tables based on binning the data. This implicitly converts the numeric data to
    an ordered factor. In this sense, histograms and bar charts are similar, except
    that the categories on the x-axis in the bar chart are not ordered. Converting
    numeric data to categorical data is an important and widely used step in data
    analysis since it reduces the complexity (and size) of the data. This aids in
    the discovery of relationships between features, particularly at the initial stages
    of an analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mode is the value—or values in case of a tie—that appears most often in
    the data. For example, the mode of the cause of delay at Dallas/Fort Worth airport
    is “Inbound.” As another example, in most parts of the United States, the mode
    for religious preference would be Christian. The mode is a simple summary statistic
    for categorical data, and it is generally not used for numeric data.
  prefs: []
  type: TYPE_NORMAL
- en: Expected Value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A special type of categorical data is data in which the categories represent
    or can be mapped to discrete values on the same scale. A marketer for a new cloud
    technology, for example, offers two levels of service, one priced at $300/month
    and another at $50/month. The marketer offers free webinars to generate leads,
    and the firm figures that 5% of the attendees will sign up for the $300 service,
    15% will sign up for the $50 service, and 80% will not sign up for anything. This
    data can be summed up, for financial purposes, in a single “expected value,” which
    is a form of weighted mean, in which the weights are probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected value is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiply each outcome by its probability of occurrence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum these values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the cloud service example, the expected value of a webinar attendee is thus
    $22.50 per month, calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>E</mi>
    <mi>V</mi></mrow></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mo>(</mo>
    <mn>0</mn> <mo>.</mo> <mn>05</mn> <mo>)</mo> <mo>(</mo> <mn>300</mn> <mo>)</mo>
    <mo>+</mo> <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>15</mn> <mo>)</mo> <mo>(</mo>
    <mn>50</mn> <mo>)</mo> <mo>+</mo> <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>80</mn>
    <mo>)</mo> <mo>(</mo> <mn>0</mn> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo></mtd>
    <mtd><mrow><mn>22</mn> <mo>.</mo> <mn>5</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected value is really a form of weighted mean: it adds the ideas of
    future expectations and probability weights, often based on subjective judgment.
    Expected value is a fundamental concept in business valuation and capital budgeting—for
    example, the expected value of five years of profits from a new acquisition, or
    the expected cost savings from new patient management software at a clinic.'
  prefs: []
  type: TYPE_NORMAL
- en: Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We referred above to the *probability* of a value occurring. Most people have
    an intuitive understanding of probability, encountering the concept frequently
    in weather forecasts (the chance of rain) or sports analysis (the probability
    of winning). Sports and games are more often expressed as odds, which are readily
    convertible to probabilities (if the odds that a team will win are 2 to 1, its
    probability of winning is 2/(2+1) = 2/3). Surprisingly, though, the concept of
    probability can be the source of deep philosophical discussion when it comes to
    defining it. Fortunately, we do not need a formal mathematical or philosophical
    definition here. For our purposes, the probability that an event will happen is
    the proportion of times it will occur if the situation could be repeated over
    and over, countless times. Most often this is an imaginary construction, but it
    is an adequate operational understanding of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No statistics course is complete without a [lesson on misleading graphs](https://oreil.ly/rDMuT),
    which often involves bar charts and pie charts.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory data analysis in many modeling projects (whether in data science
    or in research) involves examining correlation among predictors, and between predictors
    and a target variable. Variables X and Y (each with measured data) are said to
    be positively correlated if high values of X go with high values of Y, and low
    values of X go with low values of Y. If high values of X go with low values of
    Y, and vice versa, the variables are negatively correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider these two variables, perfectly correlated in the sense that each goes
    from low to high:'
  prefs: []
  type: TYPE_NORMAL
- en: 'v1: {1, 2, 3}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'v2: {4, 5, 6}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector sum of products is <math alttext="1 dot 4 plus 2 dot 5 plus 3 dot
    6 equals 32"><mrow><mn>1</mn> <mo>·</mo> <mn>4</mn> <mo>+</mo> <mn>2</mn> <mo>·</mo>
    <mn>5</mn> <mo>+</mo> <mn>3</mn> <mo>·</mo> <mn>6</mn> <mo>=</mo> <mn>32</mn></mrow></math>
    . Now try shuffling one of them and recalculating—the vector sum of products will
    never be higher than 32. So this sum of products could be used as a metric; that
    is, the observed sum of 32 could be compared to lots of random shufflings (in
    fact, this idea relates to a resampling-based estimate; see [“Permutation Test”](ch03.xhtml#Permutation)).
    Values produced by this metric, though, are not that meaningful, except by reference
    to the resampling distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'More useful is a standardized variant: the *correlation coefficient*, which
    gives an estimate of the correlation between two variables that always lies on
    the same scale. To compute *Pearson’s correlation coefficient*, we multiply deviations
    from the mean for variable 1 times those for variable 2, and divide by the product
    of the standard deviations:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="r equals StartFraction sigma-summation Underscript i equals 1
    Overscript n Endscripts left-parenthesis x Subscript i Baseline minus x overbar
    right-parenthesis left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis
    Over left-parenthesis n minus 1 right-parenthesis s Subscript x Baseline s Subscript
    y Baseline EndFraction" display="block"><mrow><mi>r</mi> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><mrow><mo>(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow><msub><mi>s</mi>
    <mi>x</mi></msub> <msub><mi>s</mi> <mi>y</mi></msub></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that we divide by *n* – 1 instead of *n*; see [“Degrees of Freedom, and
    *n* or *n* – 1?”](#Nminus1) for more details. The correlation coefficient always
    lies between +1 (perfect positive correlation) and –1 (perfect negative correlation);
    0 indicates no correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variables can have an association that is not linear, in which case the correlation
    coefficient may not be a useful metric. The relationship between tax rates and
    revenue raised is an example: as tax rates increase from zero, the revenue raised
    also increases. However, once tax rates reach a high level and approach 100%,
    tax avoidance increases and tax revenue actually declines.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 1-7](#CorrTable), called a *correlation matrix*, shows the correlation
    between the daily returns for telecommunication stocks from July 2012 through
    June 2015. From the table, you can see that Verizon (VZ) and ATT (T) have the
    highest correlation. Level 3 (LVLT), which is an infrastructure company, has the
    lowest correlation with the others. Note the diagonal of 1s (the correlation of
    a stock with itself is 1) and the redundancy of the information above and below
    the diagonal.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-7\. Correlation between telecommunication stock returns
  prefs: []
  type: TYPE_NORMAL
- en: '|  | T | CTL | FTR | VZ | LVLT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| T | 1.000 | 0.475 | 0.328 | 0.678 | 0.279 |'
  prefs: []
  type: TYPE_TB
- en: '| CTL | 0.475 | 1.000 | 0.420 | 0.417 | 0.287 |'
  prefs: []
  type: TYPE_TB
- en: '| FTR | 0.328 | 0.420 | 1.000 | 0.287 | 0.260 |'
  prefs: []
  type: TYPE_TB
- en: '| VZ | 0.678 | 0.417 | 0.287 | 1.000 | 0.242 |'
  prefs: []
  type: TYPE_TB
- en: '| LVLT | 0.279 | 0.287 | 0.260 | 0.242 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: 'A table of correlations like [Table 1-7](#CorrTable) is commonly plotted to
    visually display the relationship between multiple variables. [Figure 1-6](#CorrPlot)
    shows the correlation between the daily returns for major exchange-traded funds
    (ETFs). In *R*, we can easily create this using the package `corrplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to create the same graph in *Python*, but there is no implementation
    in the common packages. However, most support the visualization of correlation
    matrices using heatmaps. The following code demonstrates this using the `seaborn.heatmap`
    package. In the accompanying source code repository, we include *Python* code
    to generate the more comprehensive visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The ETFs for the S&P 500 (SPY) and the Dow Jones Index (DIA) have a high correlation.
    Similarly, the QQQ and the XLK, composed mostly of technology companies, are positively
    correlated. Defensive ETFs, such as those tracking gold prices (GLD), oil prices
    (USO), or market volatility (VXX), tend to be weakly or negatively correlated
    with the other ETFs. The orientation of the ellipse indicates whether two variables
    are positively correlated (ellipse is pointed to the top right) or negatively
    correlated (ellipse is pointed to the top left). The shading and width of the
    ellipse indicate the strength of the association: thinner and darker ellipses
    correspond to stronger relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation between ETF returns.](Images/psd2_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. Correlation between ETF returns
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like the mean and standard deviation, the correlation coefficient is sensitive
    to outliers in the data. Software packages offer robust alternatives to the classical
    correlation coefficient. For example, the *R* package [`robust`](https://oreil.ly/isORz)
    uses the function `covRob` to compute a robust estimate of correlation. The methods
    in the `scikit-learn` module [*sklearn.covariance*](https://oreil.ly/su7wi) implement
    a variety of approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Other Correlation Estimates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statisticians long ago proposed other types of correlation coefficients, such
    as *Spearman’s rho* or *Kendall’s tau*. These are correlation coefficients based
    on the rank of the data. Since they work with ranks rather than values, these
    estimates are robust to outliers and can handle certain types of nonlinearities.
    However, data scientists can generally stick to Pearson’s correlation coefficient,
    and its robust alternatives, for exploratory analysis. The appeal of rank-based
    estimates is mostly for smaller data sets and specific hypothesis tests.
  prefs: []
  type: TYPE_NORMAL
- en: Scatterplots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The standard way to visualize the relationship between two measured data variables
    is with a scatterplot. The x-axis represents one variable and the y-axis another,
    and each point on the graph is a record. See [Figure 1-7](#ScatterplotImage) for
    a plot of the correlation between the daily returns for ATT and Verizon. This
    is produced in *R* with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The same graph can be generated in *Python* using the `pandas` scatter method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The returns have a positive relationship: while they cluster around zero, on
    most days, the stocks go up or go down in tandem (upper-right and lower-left quadrants).
    There are fewer days where one stock goes down significantly while the other stock
    goes up, or vice versa (lower-right and upper-left quadrants).'
  prefs: []
  type: TYPE_NORMAL
- en: While the plot [Figure 1-7](#ScatterplotImage) displays only 754 data points,
    it’s already obvious how difficult it is to identify details in the middle of
    the plot. We will see later how adding transparency to the points, or using hexagonal
    binning and density plots, can help to find additional structure in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Scatterplot between returns for ATT and Verizon.](Images/psd2_0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. Scatterplot of correlation between returns for ATT and Verizon
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Statistics*, 4th ed., by David Freedman, Robert Pisani, and Roger Purves (W.
    W. Norton, 2007) has an excellent discussion of correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Two or More Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Familiar estimators like mean and variance look at variables one at a time (*univariate
    analysis*). Correlation analysis (see [“Correlation”](#Correlations)) is an important
    method that compares two variables (*bivariate analysis*). In this section we
    look at additional estimates and plots, and at more than two variables (*multivariate
    analysis*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Like univariate analysis, bivariate analysis involves both computing summary
    statistics and producing visual displays. The appropriate type of bivariate or
    multivariate analysis depends on the nature of the data: numeric versus categorical.'
  prefs: []
  type: TYPE_NORMAL
- en: Hexagonal Binning and Contours (Plotting Numeric Versus Numeric Data)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scatterplots are fine when there is a relatively small number of data values.
    The plot of stock returns in [Figure 1-7](#ScatterplotImage) involves only about
    750 points. For data sets with hundreds of thousands or millions of records, a
    scatterplot will be too dense, so we need a different way to visualize the relationship.
    To illustrate, consider the data set `kc_tax`, which contains the tax-assessed
    values for residential properties in King County, Washington. In order to focus
    on the main part of the data, we strip out very expensive and very small or large
    residences using the `subset` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In `pandas`, we filter the data set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 1-8](#HexagonalBinning) is a *hexagonal binning* plot of the relationship
    between the finished square feet and the tax-assessed value for homes in King
    County. Rather than plotting points, which would appear as a monolithic dark cloud,
    we grouped the records into hexagonal bins and plotted the hexagons with a color
    indicating the number of records in that bin. In this chart, the positive relationship
    between square feet and tax-assessed value is clear. An interesting feature is
    the hint of additional bands above the main (darkest) band at the bottom, indicating
    homes that have the same square footage as those in the main band but a higher
    tax-assessed value.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-8](#HexagonalBinning) was generated by the powerful *R* package `ggplot2`,
    developed by Hadley Wickham [[ggplot2]](bibliography01.xhtml#ggplot2). `ggplot2`
    is one of several new software libraries for advanced exploratory visual analysis
    of data; see [“Visualizing Multiple Variables”](#StatisticalGraphics):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, hexagonal binning plots are readily available using the `pandas`
    data frame method `hexbin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Hexagonal binning for tax-assessed value versus finished square feet.](Images/psd2_0108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. Hexagonal binning for tax-assessed value versus finished square
    feet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 1-9](#Contours) uses contours overlaid onto a scatterplot to visualize
    the relationship between two numeric variables. The contours are essentially a
    topographical map to two variables; each contour band represents a specific density
    of points, increasing as one nears a “peak.” This plot shows a similar story as
    [Figure 1-8](#HexagonalBinning): there is a secondary peak “north” of the main
    peak. This chart was also created using `ggplot2` with the built-in `geom_density2d`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `seaborn` `kdeplot` function in *Python* creates a contour plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![Contour plot of tax-assessed value versus finished square feet.](Images/psd2_0109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. Contour plot for tax-assessed value versus finished square feet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other types of charts are used to show the relationship between two numeric
    variables, including *heat maps*. Heat maps, hexagonal binning, and contour plots
    all give a visual representation of a two-dimensional density. In this way, they
    are natural analogs to histograms and density plots.
  prefs: []
  type: TYPE_NORMAL
- en: Two Categorical Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A useful way to summarize two categorical variables is a contingency table—a
    table of counts by category. [Table 1-8](#CrossTabs) shows the contingency table
    between the grade of a personal loan and the outcome of that loan. This is taken
    from data provided by Lending Club, a leader in the peer-to-peer lending business.
    The grade goes from A (high) to G (low). The outcome is either fully paid, current,
    late, or charged off (the balance of the loan is not expected to be collected).
    This table shows the count and row percentages. High-grade loans have a very low
    late/charge-off percentage as compared with lower-grade loans.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1-8\. Contingency table of loan grade and status
  prefs: []
  type: TYPE_NORMAL
- en: '| Grade | Charged off | Current | Fully paid | Late | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A | 1562 | 50051 | 20408 | 469 | 72490 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0.022 | 0.690 | 0.282 | 0.006 | 0.161 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 5302 | 93852 | 31160 | 2056 | 132370 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0.040 | 0.709 | 0.235 | 0.016 | 0.294 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 6023 | 88928 | 23147 | 2777 | 120875 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0.050 | 0.736 | 0.191 | 0.023 | 0.268 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 5007 | 53281 | 13681 | 2308 | 74277 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0.067 | 0.717 | 0.184 | 0.031 | 0.165 |'
  prefs: []
  type: TYPE_TB
- en: '| E | 2842 | 24639 | 5949 | 1374 | 34804 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0.082 | 0.708 | 0.171 | 0.039 | 0.077 |'
  prefs: []
  type: TYPE_TB
- en: '| F | 1526 | 8444 | 2328 | 606 | 12904 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0.118 | 0.654 | 0.180 | 0.047 | 0.029 |'
  prefs: []
  type: TYPE_TB
- en: '| G | 409 | 1990 | 643 | 199 | 3241 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0.126 | 0.614 | 0.198 | 0.061 | 0.007 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 22671 | 321185 | 97316 | 9789 | 450961 |'
  prefs: []
  type: TYPE_TB
- en: 'Contingency tables can look only at counts, or they can also include column
    and total percentages. Pivot tables in Excel are perhaps the most common tool
    used to create contingency tables. In *R*, the `CrossTable` function in the `descr`
    package produces contingency tables, and the following code was used to create
    [Table 1-8](#CrossTabs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pivot_table` method creates the pivot table in *Python*. The `aggfunc`
    argument allows us to get the counts. Calculating the percentages is a bit more
    involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_exploratory_data_analysis_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `margins` keyword argument will add the column and row sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_exploratory_data_analysis_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We create a copy of the pivot table, ignoring the column sums.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_exploratory_data_analysis_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We divide the rows with the row sum.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_exploratory_data_analysis_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We divide the `'All'` column by its sum.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical and Numeric Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Boxplots (see [“Percentiles and Boxplots”](#Boxplots)) are a simple way to
    visually compare the distributions of a numeric variable grouped according to
    a categorical variable. For example, we might want to compare how the percentage
    of flight delays varies across airlines. [Figure 1-10](#SideBySideBoxplots) shows
    the percentage of flights in a month that were delayed where the delay was within
    the carrier’s control:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pandas` `boxplot` method takes the `by` argument that splits the data
    set into groups and creates the individual boxplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Boxplot of percent of airline delays by carrier.](Images/psd2_0110.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-10\. Boxplot of percent of airline delays by carrier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Alaska stands out as having the fewest delays, while American has the most
    delays: the lower quartile for American is higher than the upper quartile for
    Alaska.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A *violin plot*, introduced by [[Hintze-Nelson-1998]](bibliography01.xhtml#Hintze-Nelson-1998),
    is an enhancement to the boxplot and plots the density estimate with the density
    on the y-axis. The density is mirrored and flipped over, and the resulting shape
    is filled in, creating an image resembling a violin. The advantage of a violin
    plot is that it can show nuances in the distribution that aren’t perceptible in
    a boxplot. On the other hand, the boxplot more clearly shows the outliers in the
    data. In `ggplot2`, the function `geom_violin` can be used to create a violin
    plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Violin plots are available with the `violinplot` method of the `seaborn` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding plot is shown in [Figure 1-11](#ViolinPlot). The violin plot
    shows a concentration in the distribution near zero for Alaska and, to a lesser
    extent, Delta. This phenomenon is not as obvious in the boxplot. You can combine
    a violin plot with a boxplot by adding `geom_boxplot` to the plot (although this
    works best when colors are used).
  prefs: []
  type: TYPE_NORMAL
- en: '![Violin plot of percent of airline delays by carrier.](Images/psd2_0111.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-11\. Violin plot of percent of airline delays by carrier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visualizing Multiple Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The types of charts used to compare two variables—scatterplots, hexagonal binning,
    and boxplots—are readily extended to more variables through the notion of *conditioning*.
    As an example, look back at [Figure 1-8](#HexagonalBinning), which showed the
    relationship between homes’ finished square feet and their tax-assessed values.
    We observed that there appears to be a cluster of homes that have higher tax-assessed
    value per square foot. Diving deeper, [Figure 1-12](#HouseByZip) accounts for
    the effect of location by plotting the data for a set of zip codes. Now the picture
    is much clearer: tax-assessed value is much higher in some zip codes (98105, 98126)
    than in others (98108, 98188). This disparity gives rise to the clusters observed
    in [Figure 1-8](#HexagonalBinning).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We created [Figure 1-12](#HouseByZip) using `ggplot2` and the idea of *facets*,
    or a conditioning variable (in this case, zip code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_exploratory_data_analysis_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `ggplot` functions `facet_wrap` and `facet_grid` to specify the conditioning
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tax assess value versus finished square feet by zip code.](Images/psd2_0112.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-12\. Tax-assessed value versus finished square feet by zip code
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Most *Python* packages base their visualizations on `Matplotlib`. While it
    is in principle possible to create faceted graphs using `Matplotlib`, the code
    can get complicated. Fortunately, `seaborn` has a relatively straightforward way
    of creating these graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_exploratory_data_analysis_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the arguments `col` and `row` to specify the conditioning variables. For
    a single conditioning variable, use `col` together with `col_wrap` to wrap the
    faceted graphs into multiple rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_exploratory_data_analysis_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `map` method calls the `hexbin` function with subsets of the original data
    set for the different zip codes. `extent` defines the limits of the x- and y-axes.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of conditioning variables in a graphics system was pioneered with
    *Trellis graphics*, developed by Rick Becker, Bill Cleveland, and others at Bell
    Labs [[Trellis-Graphics]](bibliography01.xhtml#Trellis-Graphics). This idea has
    propagated to various modern graphics systems, such as the `lattice` [[lattice]](bibliography01.xhtml#lattice)
    and `ggplot2` packages in *R* and the `seaborn` [[seaborn]](bibliography01.xhtml#seaborn)
    and `Bokeh` [[bokeh]](bibliography01.xhtml#bokeh) modules in *Python*. Conditioning
    variables are also integral to business intelligence platforms such as Tableau
    and Spotfire. With the advent of vast computing power, modern visualization platforms
    have moved well beyond the humble beginnings of exploratory data analysis. However,
    key concepts and tools developed a half century ago (e.g., simple boxplots) still
    form a foundation for these systems.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Modern Data Science with R* by Benjamin Baumer, Daniel Kaplan, and Nicholas
    Horton (Chapman & Hall/CRC Press, 2017) has an excellent presentation of “a grammar
    for graphics” (the “gg” in `ggplot`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ggplot2: Elegant Graphics for Data Analysis* by Hadley Wickham (Springer,
    2009) is an excellent resource from the creator of `ggplot2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Josef Fruehwald has a web-based tutorial on [`ggplot2`](https://oreil.ly/zB2Dz).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory data analysis (EDA), pioneered by John Tukey, set a foundation for
    the field of data science. The key idea of EDA is that the first and most important
    step in any project based on data is to *look at the data*. By summarizing and
    visualizing the data, you can gain valuable intuition and understanding of the
    project.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has reviewed concepts ranging from simple metrics, such as estimates
    of location and variability, to rich visual displays that explore the relationships
    between multiple variables, as in [Figure 1-12](#HouseByZip). The diverse set
    of tools and techniques being developed by the open source community, combined
    with the expressiveness of the *R* and *Python* languages, has created a plethora
    of ways to explore and analyze data. Exploratory analysis should be a cornerstone
    of any data science project.
  prefs: []
  type: TYPE_NORMAL
