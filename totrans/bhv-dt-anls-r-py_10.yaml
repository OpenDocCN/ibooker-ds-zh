- en: Chapter 7\. Measuring Uncertainty with the Bootstrap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With ideal data, you are now able to draw robust conclusions from behavioral
    data and measure the causal impact of a business/environment change on human behaviors.
    But how can you proceed if you have suboptimal data? In academic research, one
    can always fall back to the null hypothesis when faced with inconclusive data
    and refuse to pass judgment. But in applied research there is no null hypothesis,
    only alternative courses of action to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Small sample sizes, weirdly shaped variables, or situations that require advanced
    analytical tools (e.g., hierarchical modeling, which we’ll see later in the book)
    can all result in shaky conclusions. Certainly, a linear regression algorithm
    will spit out a coefficient under all but the most extreme cases, but should you
    trust it? Can you confidently advise your boss to stake millions of dollars on
    it?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I’ll introduce you to an extremely powerful and general simulation
    tool, the Bootstrap, which will allow us to draw robust conclusions from any data,
    however small or weird. It works by creating and analyzing slightly different
    versions of your data based on random numbers. A great feature of the Bootstrap
    is that you literally can never go wrong by applying it: in situations that are
    best-case scenarios for traditional statistical methods (e.g., running a basic
    linear regression on a large and well-behaved data set), the Bootstrap is slower
    and less accurate, but it is still in the ballpark. But as soon as you move away
    from such best-case scenarios, the Bootstrap quickly outperforms traditional statistical
    methods, often by a wide margin.^([1](ch07.xhtml#ch01fn10)) Therefore, we’ll rely
    on it extensively throughout the rest of the book. In particular, we’ll use it
    when designing and analyzing experiments in [Part IV](part04.xhtml#designing_and_analyzing_experiments),
    to build simulated equivalents of p-values that are more intuitive than the traditional,
    statistical, ones.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, we’ll focus on exploratory/descriptive data analysis,
    and we’ll see that the Bootstrap can already be of use at that stage. In the second
    section, we’ll use the Bootstrap in the context of regression. We’ll then broaden
    our perspective to discuss when to use the Bootstrap and what tools you can use
    to make your life easier with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intro to the Bootstrap: “Polling” Oneself Up'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While our ultimate goal is to use the Bootstrap for regression, we can start
    with the simpler example of descriptive statistics: getting the mean of a sample
    data set.'
  prefs: []
  type: TYPE_NORMAL
- en: Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll use the following packages in addition to the common
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The Business Problem: Small Data with an Outlier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C-Mart’s management is interested in understanding how long it takes its bakers
    to prepare made-to-order cakes, for the purpose of possibly revising its pricing
    structure. To that end, they have asked C-Mart’s industrial engineer to do a time
    study. As its name indicates, a time study (a.k.a. time-and-motion study) is the
    direct observation of a production process to measure the duration of the tasks
    involved. Given that the process is time-consuming (pun intended), the engineer
    has selected ten different stores that are somewhat representative of C-Mart’s
    business. In each store they observed one baker preparing one cake. They also
    recorded each baker’s work experience, measured in months on the job.
  prefs: []
  type: TYPE_NORMAL
- en: All together the engineer has 10 observations, which is not a very large sample
    size to begin with. Even if all of the data conformed very consistently to a clear
    relationship, the sample size alone would suggest using the Bootstrap. However,
    when exploring their data, the engineer observed the presence of an outlier ([Figure 7-1](#experience_and_preparation_time_by_bake)).
  prefs: []
  type: TYPE_NORMAL
- en: We have one extreme point in the upper left corner, corresponding to a new employee
    who spent most of a day on a complex cake for a corporate retreat. How should
    the engineer report the data from their study? They might be tempted to treat
    the largest observation as an outlier, which is the polite way of saying “discard
    it and pretend it didn’t happen.” But that observation, while unusual, is not
    an aberration per se. There was no measurement error, and those circumstances
    probably occur from time to time. An alternative would be to only report the overall
    mean duration, 56 minutes, but that would also be misleading because it would
    not convey the variability and uncertainty in the data. The traditional recommendation
    in that situation would be to use a confidence interval around the mean. Let’s
    calculate the normal 95% confidence interval through a regression. (Using a regression
    is overkill in this case—there are much simpler ways to calculate a mean—but it
    will serve as a gentle introduction to the process we’ll use later in the chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Experience and preparation time by baker](Images/BEDA_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Experience and preparation time by baker
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We first run the regression `times~1`, i.e., with only the intercept. We then
    extract the resulting estimate for the intercept coefficient that, in case you’re
    not familiar with that calculation, is equal to the mean of our dependent variable.
    We also extract the standard error for that coefficient. As you’ll learn in any
    stats class, the lower limit of a normal 95%-CI is equal to the mean minus 1.96
    times the standard error, and the upper limit is equal to the mean plus 1.96 times
    the standard error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, the 95%-CI in this case is [−23; 135], which is obviously nonsensical
    because duration times can’t be negative. This happened because traditional CIs
    assume that the variable at hand follows a normal distribution around its mean,
    which in this case is incorrect. We can imagine that the engineer’s audience would
    not take too kindly to negative durations, but that is one of the problems that
    the Bootstrap can solve.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap Confidence Interval for the Sample Mean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Bootstrap allows us to make full use of the data that we do have available
    and to draw reasonable conclusions regardless of sample size or data shape challenges.
    It does so by creating multiple imaginary data sets based on the data that we
    have available. Comparing these data sets with each other allows us to cut through
    noise and more accurately represent the importance of outlier values. It can also
    provide tighter confidence intervals, since it removes some of the uncertainty
    created by noise.
  prefs: []
  type: TYPE_NORMAL
- en: This is different from just choosing a narrower range from the start (e.g.,
    selecting the 80%-CI instead of the 95%-CI) because the Bootstrap’s generated
    data sets reflect true probability distributions given the available data. There
    will be no generated data set with a negative duration because the data does not
    reflect that possibility, but there will be data sets that reflect very long durations
    because the original data does include that as a possibility. So a confidence
    interval generated using the Bootstrap would be expected to remove more from the
    negative side of the range, but it might not remove as much (or might even add
    to) the positive side of the range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process to build a Bootstrap CI is conceptually simple:'
  prefs: []
  type: TYPE_NORMAL
- en: We simulate new samples of the same size by drawing with replacement from our
    observed sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, for each simulated sample we calculate our statistic of interest (here
    the mean, which is what our industrial engineer wants to measure).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we build our CI by looking at the percentiles of the values obtained
    in step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drawing with replacement means that each value has the same probability of being
    drawn each time, regardless of whether or not it has already been drawn.
  prefs: []
  type: TYPE_NORMAL
- en: For example, drawing with replacement from (A, B, C) is equally likely to yield
    (B, C, C) or (A, C, B) or (B, B, B), etc. Because there are three possibilities
    for each of the three positions, there are 3 x 3 x 3 = 27 possible simulated samples.
    If we drew without replacement, this would mean that a value cannot be drawn more
    than once, and the only possible combinations would be permutations of the original
    sample, such as (A, C, B) or (B, A, C). This would simply amount to shuffling
    the values around, which would be pointless because the mean (or any other statistic
    of interest) would remain exactly the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Drawing with replacement is very simple in both R and Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The beauty of generating new samples by drawing only from our observed sample
    is that it avoids making any distributional assumption about data outside of the
    sample we observed. To see what this means, let’s simulate B = 2,000 Bootstrap
    samples (to avoid confusion, I’ll always use B for the number of Bootstrap samples
    and N for the sample size) and calculate the mean of each. Our code proceeds as
    follows (the callout numbers are shared between R and Python):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker71)'
  prefs: []
  type: TYPE_NORMAL
- en: First I initialize an empty list for the results, as well as B and N.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker72)'
  prefs: []
  type: TYPE_NORMAL
- en: Then I use a `for` loop to generate the Bootstrap samples by drawing with replacement
    from the original data, each time calculating the mean and adding it to the list
    of results.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker73)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in R I reformat the list into a tibble, for ease of use with `ggplot2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-2](#distribution_of_the_means_of_twocommaz) shows the distribution
    of the means.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution of the means of 2,000 samples](Images/BEDA_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Distribution of the means of 2,000 samples
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, the histogram is very irregular: there is a big peak close
    to the mean of our original data set along with smaller peaks corresponding to
    certain patterns. Given how extreme our outlier is, each of the seven peaks corresponds
    to its number of repetitions in the Bootstrap sample, from zero to six. In other
    words, it doesn’t appear at all in the samples whose means are in the first (leftmost)
    peak, it appears exactly once in the samples whose means are in the second peak,
    and so on. It’s worth noting that even if we increased the number of Bootstrap
    samples, the irregularity of the histogram would not disappear (i.e., the “valleys”
    between the peaks would not get filled), because it reflects the roughness of
    our data and not the limitations of our random process. The range of values within
    our data is so extreme that the highest possible means when the outlier is excluded
    are still rarely high enough to meet the lowest possible means when the outlier
    is included. If the value of the outlier were cut in half, and thus were closer
    to the rest of the population, the histogram would appear to smooth out considerably
    because the edges of the outlier count peaks would overlap each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of Bootstrap samples matters nonetheless, but for a different reason:
    the higher that number, the more you’ll be able to see very unlikely samples and
    therefore extreme values. Here, the absolute highest possible value for a sample
    mean would be 413 if we drew the outlier 10 times. This has a probability of (0.1)^(10)
    (one-tenth to the power of 10), meaning it will happen about one time per 10 billion
    samples. With our mere 2,000 samples, we are barely seeing values around 200\.
    But the overall mean or median of our samples would remain the same plus or minus
    negligible sampling variations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some general guidelines for number of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 to 200 samples to get an accurate central estimate (e.g., a coefficient
    in a regression; it’s called “central” because it’s roughly speaking at the center
    of a CI, as opposed to the CI’s bounds or limits)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1,000 to 2,000 samples to get accurate 90%-CI bounds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5,000 samples to get accurate 99%-CI bounds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, start low, and if in doubt increase the number and try again. This
    is fundamentally different from, for example, running multiple analyses on your
    data until you get numbers you like (a.k.a. “p-value hacking” or “p-hacking”);
    it’s more like changing the resolution of your screen when looking at a figure.
    It entails no risk for your analyses; it simply takes more or less of your time,
    depending on the size of your data and the computational power of your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Given the data that we have, the only way we could increase the smoothness of
    the histogram would be to increase the sample size. However, we would have to
    increase the size of the original sample from the real world, not the size of
    the Bootstrap samples. Why can’t we increase the size of the Bootstrap samples
    (e.g., drawing 100 values with replacement from our sample of 10 values)? Because
    our goal is not to create new samples but to determine how far off our estimate
    of the mean could be when we make the assumption that the population is proportionately
    identical to our original sample. To do this we need to use all the information
    in the original sample—no less and no more. Creating larger samples from our 10
    original values would be “pretending” that we have more information than we actually
    have.
  prefs: []
  type: TYPE_NORMAL
- en: 'The engineer is ready to use the Bootstrap to determine the bounds of the CI
    for the duration of cake preparation. These bounds are determined from the *empirical*
    distribution of the preceding means. This means that instead of trying to fit
    a statistical distribution (e.g., normal), they can simply order the values from
    smallest to largest and then look at the 2.5% quantile and the 97.5% quantile
    to find the two-tailed 95%-CI. With 2,000 samples, the 2.5% quantile is equal
    to the value of the 50th smallest mean (because 2,000 * 0.025 = 50), and the 97.5%
    quantile is equal to the value of the 1950th mean from smaller to larger, or the
    50th largest mean (because both tails have the same number of values). Fortunately,
    we don’t have to calculate these by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The Bootstrap 95%-CI is [7.50; 140.80] (plus or minus some sampling difference),
    which is much more realistic. [Figure 7-3](#distribution_of_the_means_of_twocommaze)
    shows the same histogram as [Figure 7-2](#distribution_of_the_means_of_twocommaz)
    but adds the mean of the means, the normal CI bounds and the Bootstrap CI bounds.
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution of the means of 2,000 samples, with mean of the means (thick
    line), normal 95%-CI bounds (dotted lines), and Bootstrap CI bounds (dashed lines)](Images/BEDA_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Distribution of the means of 2,000 samples, with mean of the means
    (thick line), normal 95%-CI bounds (dotted lines), and Bootstrap CI bounds (dashed
    lines)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the Bootstrap lower bound being above zero, we can also note
    that the Bootstrap upper bound is slightly higher than the normal upper bound,
    which better reflects the asymmetry of the distribution toward the right.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap Confidence Intervals for Ad Hoc Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the Bootstrap has allowed us to build a reasonable CI when the traditional
    statistical approach was failing. We can also use it to build CIs in situations
    where there is no other way to do so. Let’s imagine, for example, that C-Mart’s
    management is considering instituting a time promise—“your cake in three hours
    or 50% off”—and wants to know how often a cake currently takes more than three
    hours to be baked. Our estimate would be the sample percentage: it happens in
    1 of the 10 observed cases, or 10%. But we can’t leave it at that, because there
    is significant uncertainty around that estimate, which we need to convey. Ten
    percent out of 10 observations is much more uncertain than 10% out of 100 or 1,000
    observations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So how can we build a CI around that 10% value? With the Bootstrap, of course.
    The process is exactly the same as earlier, except that instead of taking the
    mean of each simulated sample, we’ll measure the percentage of the values in the
    sample that are above 180 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The histogram of the results is shown in [Figure 7-4](#histogram_of_count_of_samples_with_a_gi).
    There’s “white space” between the bars again because we have only 10 data points,
    so percentages are multiples of 10%. That would not be the case with more data
    points; in general percentages will be multiples of 1/N with N the sample size
    (e.g., with 20 points, percentages would be multiples of 5%).
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of count of samples with a given proportion of durations above
    180 minutes](Images/BEDA_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Histogram of count of samples with a given proportion of durations
    above 180 minutes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In about 700 of the 2,000 simulated samples, there was no cake with a preparation
    time above 180 minutes. In about 750, there was exactly one such cake, and so
    on. The corresponding 95%-CI is [0; 0.3]: the 50th lowest value is 0 and the 50th
    highest value is 0.3.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, even with such limited data, we can quite confidently say that
    it’s very unlikely (although not impossible) that more than 30% of the cakes take
    more than three hours to prepare. That’s still a pretty large confidence interval,
    but not too shabby for only 10 observations and such a unique statistic!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If this is hard to wrap your mind around, you can reframe the preceding problem
    by calculating the confidence interval for a binomial distribution with 1 success
    in 10 observations. Approximation methods are available in R and Python to calculate
    CIs in that case. These tend to be more conservative (i.e., broader) than our
    Bootstrap CI, but not vastly so.
  prefs: []
  type: TYPE_NORMAL
- en: By using the Bootstrap, the engineer can sharpen the analyses they would regularly
    want to perform with their data. They’re able to use limited data to answer a
    variety of questions with a reasonable amount of certainty (and correspondingly
    tolerable uncertainty).
  prefs: []
  type: TYPE_NORMAL
- en: The Bootstrap for Regression Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While building a confidence interval around the mean can be useful, regression
    is really what this book is about, so let’s see how we can use the Bootstrap for
    that purpose. Our industrial engineer at C-Mart wants to determine the effect
    of experience on baking time using the same data about cake preparation. The corresponding
    CD is very simple ([Figure 7-5](#causal_diagram_for_our_relationship_of)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Causal diagram for our relationship of interest](Images/BEDA_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Causal diagram for our relationship of interest
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Running a regression on our data is straightforward, given that the causal
    diagram did not reveal any confounder. However, the resulting coefficient is not
    significant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Our estimated coefficient is −9.8, meaning that every additional month of experience
    is expected to remove 9.8 minutes of preparation time. However, the traditional
    CI based on the regression standard error would be [–22.2; 2.5]. From a traditional
    perspective, this would be game over: the CI includes zero, meaning that months
    of experience could have a positive, negative, or zero effect on baking time,
    so we would decline to draw any substantive conclusion. Let’s see instead what
    the Bootstrap tells us. The process is exactly the same as before: we simulate
    samples of 10 data points by drawing with replacement from our original sample
    a large number of times, then save the regression coefficient. Last time we used
    B = 2,000 samples. This time let’s use B = 4,000, as it makes the corresponding
    histogram look smoother ([Figure 7-6](#distribution_of_the_regression_coeffic)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Distribution of the regression coefficients of preparation time on experience,
    with their mean (thick line), Bootstrap CI bounds (thick dashed lines), and normal
    CI bounds (thin dotted lines) (B=4,000 Bootstrap samples)](Images/BEDA_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Distribution of the regression coefficients of preparation time
    on experience, with their mean (thick line), Bootstrap CI bounds (thick dashed
    lines), and normal CI bounds (thin dotted lines) (B = 4,000 Bootstrap samples)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Bootstrap CI is [–28; –0.2]. As you can see in [Figure 7-6](#distribution_of_the_regression_coeffic),
    it’s again asymmetric compared to the symmetric normal bounds, with a long tail
    to the left of the mean. The highly irregular shape of the distribution reflects
    the existence of two competing hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: The tall and narrow peak near zero is made of samples that don’t include the
    outlier, and as such it corresponds with the view that the outlier is a freak
    accident that won’t repeat itself. That’s the confidence interval you would get
    if you discarded the outlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The broad and flat hump to the left is made of samples that include the outlier
    one or several times. It reflects the hypothesis that the outlier is truly representative
    of our data and that its true frequency may be even higher than in our small sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can think of this as data-driven scenario analysis. What if this pattern
    didn’t exist? What if it dominated our data? Instead of having to choose between
    discarding the outlier or letting it drive our results, the Bootstrap allows us
    to consider all possibilities at once.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to building a CI, we can use the Bootstrap to determine the equivalent
    of a p-value. If you look at the output of our regression at the beginning of
    this section, you’ll see the value 0.16 for experience in the column for the p-values
    (i.e., the column with the label Pr(>|t|)). You have probably already been told
    that a coefficient is statistically significant (i.e., statistically significantly
    different from zero) if its p-value is less than 0.05, or 0.01 in more stringent
    cases. Mathematically speaking, the p-value is such that the (1 minus p-value)-CI
    has zero as one of its bounds. In the case of the normal regression, zero is the
    upper bound of the 84%-CI. Because 84% is less than 95% or 99%, the coefficient
    for experience would not be considered statistically significant. The exact same
    logic can be used with the Bootstrap; we just have to calculate the fraction of
    the Bootstrap sample whose coefficient is above zero, and multiply it by 2 because
    it’s a two-tailed test:^([2](ch07.xhtml#ch01fn11))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This means that our empirical Bootstrap p-value^([3](ch07.xhtml#ch01fn12)) is
    about 0.04, as opposed to the traditional p-value of 0.16 rooted in statistical
    assumptions. This is helpful because people are often familiar with statistical
    p-values, and Bootstrap p-values can be used instead. From a business perspective,
    we can now be confident that the regression coefficient is between null and strongly
    negative. In addition, we could easily calculate the equivalent of a p-value for
    any other threshold (e.g., if we wanted to use −1 instead of zero as a threshold),
    or for any interval, such as [−1; +1], if we wanted.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use the Bootstrap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hopefully, by now you’re convinced of the virtues of the Bootstrap for small
    and oddly shaped data sets. But what about large or evenly shaped data sets? Should
    you always use the Bootstrap? The short answer is that it is never wrong to use
    it, but it can be impractical or overkill. For experimental data, we’ll rely extensively
    on the Bootstrap, as we’ll see in [Part IV](part04.xhtml#designing_and_analyzing_experiments)
    of the book. For observational data analysis, which is the focus of this chapter,
    things are more complicated. [Figure 7-7](#decision_tree_to_use_the_bootstrap)
    presents the decision tree we’ll use. It may look a bit intimidating, but it can
    be broken down conceptually into three blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: If you only want a central estimate (e.g., a regression coefficient) and the
    conditions for the traditional estimate to be sufficient are fulfilled, you can
    use it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want a CI and the conditions for the traditional CI to be sufficient
    are fulfilled, you can use it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In any other case or when in doubt, use the Bootstrap CI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review these blocks in turn.
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree to use the Bootstrap](Images/BEDA_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Decision tree to use the Bootstrap
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conditions for the Traditional Central Estimate to Be Sufficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to keep in mind is that the Bootstrap yields a central estimate
    or coefficient that is very close to the one obtained by traditional methods (i.e.,
    whatever you would have done if you didn’t know about the Bootstrap). Therefore,
    it never makes sense to start directly with the Bootstrap, when a traditional
    estimate is one line of code away.
  prefs: []
  type: TYPE_NORMAL
- en: However, if your data is small (typically less than 100 rows) or in any regard
    weird (e.g., it has multiple peaks or is asymmetric), then that central estimate
    can be misleading. In that case, you should really use the Bootstrap to calculate
    a confidence interval instead, ideally displaying its results in the form of a
    histogram as we did in [Figure 7-6](#distribution_of_the_regression_coeffic).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if the coefficient is close to a boundary or threshold, and therefore
    not economically clear-cut, you’ll need to use a CI and the central estimate won’t
    be sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Even when things are as clean and clear-cut as they can be, you may still want
    to have a CI, for example because your boss or your business partner required
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Conditions for the Traditional CI to Be Sufficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to have a CI but your data is not so small or weird that the Bootstrap
    CI is required, the question becomes whether a traditional CI would be reliable
    and sufficient for your purpose. There are two tests you need to run in that situation:'
  prefs: []
  type: TYPE_NORMAL
- en: Check the presence or not of influential points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the normality of the regression residuals (only if the regression is linear).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only if your data is devoid of influential points and you don’t see any issue
    with the residuals can you use the traditional CI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Influential points are points whose deletion would substantially modify the
    regression, and there is a statistic, [Cook’s distance](https://oreil.ly/0OS4s),
    which measures precisely that. For our purposes here, it’s enough to know that
    a data point is considered influential if its Cook’s distance is more than one.
    R and Python have one-liners to calculate Cook’s distance for points with respect
    to a regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By definition, an influential point doesn’t follow the same pattern as the
    other points (otherwise deleting it would not drastically change the results of
    our regression). This means that an influential point is always an outlier, but
    an outlier is not always an influential point: an outlier lies far out from the
    cloud that the other points form, but it could still be close to the regression
    line calculated without it and have a small Cook’s distance. In our baking example,
    the outlier point is also an influential point.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have any influential point in your data, it suggests that the standard
    distributional assumptions are not met, and using the Bootstrap may be wiser.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t have influential points in your data, there is a second check
    you need to do in the case of a linear regression: you need to make sure that
    the regression residuals are approximately normal. This doesn’t apply to logistic
    regression because its residuals follow a Bernoulli distribution and not a normal
    distribution. This check answers both the questions of “How non-normal is non-normal?”
    and “How large is large?” because they are related: larger data dampens minor
    deviations from normality, so a degree of non-normality that would be problematic
    with a hundred points might be OK with a hundred thousand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s extract the regression residuals and visually assess their normality.
    In R, we obtain the residuals by applying the function `resid()` to our linear
    regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax in Python is also straightforward: we first get the residuals from
    the model, then draw a density plot from the Seaborn package, and draw the QQ-plot
    with the statsmodels package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 7-8](#density_plot_left_parenthesisleftright) displays the two plots
    we created in R, a density plot and a QQ-plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Density plot (left) and QQ-plot (right) of regression residuals](Images/BEDA_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. Density plot (left) and QQ-plot (right) of regression residuals
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s first look at the density plot on the left. For a normal density, we would
    expect to see a curve with a single peak centered on zero and with smoothly decreasing,
    symmetric left and right tails. This is clearly not the case here due to the presence
    of an outlier with a large residual, so we conclude that the residuals are not
    normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: The plot on the right is a QQ-plot (QQ stands for Quantile-Quantile), plotted
    with `geom_qq()` or `qqplot()`, which shows the values of our residuals on the
    x-axis and a theoretical normal distribution on the y-axis. For a normal density,
    we would expect all the points to be on the line or very close to it, which is
    again not the case here because of the outlier.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever the residuals of a linear regression are not normally distributed,
    the Bootstrap will give you better results for CIs and p-values than the traditional
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, it is never wrong to build Bootstrap CIs and you can always fall back
    on them. But when you only need the central estimate and you can safely rely on
    it, or when you can safely rely on a traditional CI, it can be overkill to jump
    to the Bootstrap.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s see in a bit more detail how you can determine the number of
    Bootstrap samples to use.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the Number of Bootstrap Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have decided to use the Bootstrap, you need to determine the number
    of samples to use in your simulation. If you just want to get a broad sense of
    the variability of an estimate, B = 25 to 200 gives reasonably robust results
    for main estimates, according to Efron, the “inventor” of the Bootstrap. Think
    of it as a 75%-CI. You wouldn’t bet the farm on it, but it tells you more than
    just a mean.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, let’s say you want a precise p-value or 95%-CI because there
    is uncertainty as to whether or not a critical threshold, usually zero, is in
    it or not. Then you’ll need a much larger B, because we’re typically looking at
    the 2.5% smallest or 2.5% highest values of the Bootstrap distribution. With B
    = 200, the lower bound of a two-tailed 95%-CI is equal to 200 * 2.5%, or the fifth-smallest
    value, and similarly the upper bound is equal to the fifth-largest value. Five
    is a pretty small number. You can pretty easily get unlucky and get five numbers
    that are smaller or larger than expected, and throw off your CI bound. Let’s visualize
    that by repeating the Bootstrap regression from the previous section with only
    200 samples. As you can see in [Figure 7-9](#distribution_of_the_regression_coeffici),
    the shape of the distribution is overall similar to [Figure 7-5](#causal_diagram_for_our_relationship_of),
    but now the upper bound for our CI is *above* zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution of the regression coefficients of preparation time on experience,
    with their mean (thick line), the Bootstrap CI bounds (thick dashed lines) and
    the normal CI bounds (thin dotted lines) (B=200 Bootstrap samples)](Images/BEDA_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. Distribution of the regression coefficients of preparation time
    on experience, with their mean (thick line), the Bootstrap CI bounds (thick dashed
    lines) and the normal CI bounds (thin dotted lines) (B = 200 Bootstrap samples)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Therefore, if a business decision hinges on where that bound is in relation
    to zero, you’ll need to make sure that you estimate it accurately by increasing
    B. Having 1,000 or even 2,000 samples is a generally accepted guideline in such
    circumstances. At B = 2,000 the 2.5% quantile is equal to the 50th value, so the
    odds are much more in your favor. In addition, with very small data sets such
    as the one used in this chapter, even simulating 4,000 samples takes no more than
    a few seconds, which is why I have used such a large B.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap when to use the Bootstrap with observational data, bringing together
    the test conditions and the number of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: Always start with your traditional regression model to get the main estimate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have less than 100 points in your data, always use the Bootstrap with
    B between 25 and 200 to assess the uncertainty around that estimate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With N > 100, check your data for signs of influential points (with Cook’s distance)
    or non-normality (with the density plot and QQ-plot of residuals). If anything
    looks fishy, use the Bootstrap, again with B between 25 and 200 for main estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of N, if you need a precise confidence interval or achieved significance
    level (a.k.a. p-value), do another Bootstrap simulation with B between 1,000 and
    2,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you’ve gotten a sense of how long it takes to run a Bootstrap simulation
    on your data with a small to medium B, and what the corresponding histogram or
    CI looks like, always feel free to push the dial on B. Feel free to run a simulation
    with B = 10,000 overnight to get a nicely dented graph and an exactingly precise
    CI bound.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the Bootstrap in R and Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have shown you how to apply the Bootstrap algorithm “by hand,” so that you
    can understand what it does, but there are packages that will do it in fewer lines
    of code and will run faster. They will also allow you to use improved versions
    of the Bootstrap that would be impractical to code manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'R: The boot Package'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `boot` package and its `boot()` function provide a one-stop shop for Bootstrap
    analyses. For all its simplicity, the way it generates Bootstrap samples is not
    intuitive, so it’s worth looking at that feature separately first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that in the earlier section on Bootstrap for regression analysis,
    I generated Bootstrap samples with the `slice_sample()` function before running
    our regression of interest on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative approach to generate Bootstrap samples is to take a list of
    indices, sample from it with replacement, then subset our data based on that list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the approach used in the `boot()` function. We must create a function
    taking as arguments our original data and a list of index J and returning our
    variable of interest (here, the regression estimate for experience). The `boot()`
    function will take care of generating that list for each iteration; we only need
    to subset our data with it within our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating that function, we pass it to the `boot()` function as the argument
    `statistic`, as well as our original data as `data`, and the number of Bootstrap
    samples as `R` (for replications). The `boot()` function returns an object that
    we then pass to the `boot.ci()` function to get our CI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `boot.ci()` function can return a variety of CIs, as determined by the parameter
    `type`. “norm” is the traditional CI based on a normal distribution. “perc” is
    the percentile, or quantile, Bootstrap that we calculated by hand previously.
    “bca” is the bias-corrected and accelerated percentile Bootstrap (BC[a]). The
    BC[a] Bootstrap refines the percentile Bootstrap by leveraging some of its statistical
    properties; these are beyond our scope here. You can learn more about them in
    any of the sources listed as references; suffice it to say that the BC[a] Bootstrap
    is considered best practice when using Bootstrap simulations. It can be pretty
    demanding in terms of computations however, so I would recommend using the percentile
    Bootstrap first, and once you have a reasonably final version of your code, try
    switching to the BC[a] Bootstrap.
  prefs: []
  type: TYPE_NORMAL
- en: In the present case, the normal and percentile CIs are very close to what we
    calculated by hand, as expected. The BC[a] CI shifts to the left, strengthening
    our original conclusions that the coefficient is most likely strongly negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you understand the intuition behind the use of the `boot` package,
    let’s create a reusable function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `boot_CI_fun()` function takes as arguments a data set and a metric function
    and returns a 90% confidence interval for that metric function on that data set,
    based on 100 Bootstrap samples and the percentile approach.
  prefs: []
  type: TYPE_NORMAL
- en: Python Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python offers very different trade-offs to the analyst compared to R: on the
    one hand, it has fewer statistical packages and there is no equivalent of the
    R `boot` package that would implement the Bootstrap algorithm directly. On the
    other hand, I find it to be more forgiving of beginners in terms of performance.
    This is especially true for bootstrapping, because `for` loops, which beginners
    tend to use a lot, are comparatively much less costly. Therefore, I expect Python
    users to get much more mileage out of the naive implementation we started with.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, should you need to add computational oomph to your Bootstrap implementation
    in Python, you can do so by going “full NumPy”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker771)'
  prefs: []
  type: TYPE_NORMAL
- en: We convert our original pandas dataframe to a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker772)'
  prefs: []
  type: TYPE_NORMAL
- en: We initialize the NumPy random number generator only once, outside of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker773)'
  prefs: []
  type: TYPE_NORMAL
- en: We create our bootstrapped data set by using the NumPy random number generator,
    which is significantly faster than the pandas `.sample()` method for dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#comarker774)'
  prefs: []
  type: TYPE_NORMAL
- en: We extract the predictor columns from our array and in the following line manually
    add a constant column for the intercept (whereas `statsmodel` was previously handling
    that under the hood for us).
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#comarker775)'
  prefs: []
  type: TYPE_NORMAL
- en: We extract the column for the dependent variable from our array.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#comarker776)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading the function calls from the right to the left: we fit the linear regression
    model with the `np.linalg.lstsq()` function to the predictor and dependent variable
    data. The `rcond=-1` parameter removes an unimportant warning. The value we want
    is in the `[0][0]` cell for this particular model; you can find the specific cell
    you need by running `np.linalg.lstsq(X, Y, rcond=-1)` once and inspecting its
    output. Finally, we append the value to our result list.'
  prefs: []
  type: TYPE_NORMAL
- en: Going full NumPy can significantly improve your performance, to the order of
    fifty times faster or so for larger data sets. However, our original code did
    just fine for our small data set, and it was more readable and less error-prone.
    Moreover, if you go beyond straightforward linear or logistic regressions, you’ll
    have to search on the Internet for a NumPy implementation of the algorithm you
    want. But should you need to improve the performance of your Bootstrap code in
    Python, you now know how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Behavioral data analyses often have to deal with smaller or weirder data. Fortunately,
    the advent of computer simulations has given us in the Bootstrap a great tool
    to deal with such situations. Bootstrap confidence intervals allow us to correctly
    assess the uncertainty in our estimates without relying on potentially faulty
    statistical assumptions about the distribution of our data. With observational
    data, the Bootstrap is most useful when our data exhibits signs of influential
    points or non-normality; otherwise, it is often overkill. With experimental data,
    however, the heavy reliance on p-values to make decisions means that we’ll use
    it extensively, as we’ll see in the next part of the book.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.xhtml#ch01fn10-marker)) See Wilcox (2010), which shows the danger
    of assuming normality as a matter of course.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.xhtml#ch01fn11-marker)) To see why, note that if you have a 90%-CI,
    you’ll have 5% of values remaining on each side out of it because (1 − 0.9) /
    2 = 0.05\. Conversely, if you see that you have 5% of values on one side out of
    a CI, then it’s the 90%-CI, because (1 − 2 * 0.05) = 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.xhtml#ch01fn12-marker)) To be fully accurate, our Bootstrap p-value
    would better be called the Bootstrap achieved significance level (ASL).
  prefs: []
  type: TYPE_NORMAL
