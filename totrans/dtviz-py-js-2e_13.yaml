- en: Chapter 9\. Cleaning Data with pandas
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。使用pandas清洗数据
- en: The previous two chapters introduced pandas and NumPy, the Numeric Python library
    it extends. Armed with basic pandas know-how, we’re ready to start the cleaning
    stage of our toolchain, aiming to find and eliminate the dirty data in our scraped
    dataset (see [Chapter 6](ch06.xhtml#chapter_heavy_scraping)). This chapter will
    also extend your pandas knowledge, introducing new methods in a working context.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前两章介绍了pandas和NumPy，它扩展了Numeric Python库。具备基本的pandas知识，我们现在可以开始我们工具链的清洗阶段，旨在查找并消除我们抓取数据集中的脏数据（参见[第6章](ch06.xhtml#chapter_heavy_scraping)）。本章还将在工作环境中介绍新方法，扩展你的pandas知识。
- en: 'In [Chapter 8](ch08.xhtml#chapter_intro_to_pandas), we covered the core components
    of pandas: the DataFrame, a programmatic spreadsheet capable of dealing with the
    many different datatypes found in the real world, and its building block, the
    Series, a heterogeneous extension of NumPy’s homogeneous `ndarray`. We also covered
    how to read from and write to different datastores, including JSON, CSV files,
    MongoDB, and SQL databases. Now we’ll start to put pandas through its paces, showing
    how it can be used to clean dirty data. I’ll introduce the key elements of data
    cleaning using our dirty Nobel Prize dataset as an example.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.xhtml#chapter_intro_to_pandas)中，我们介绍了pandas的核心组件：DataFrame，一种可以处理现实世界中多种不同数据类型的程序化电子表格，以及其构建块Series，是NumPy均匀ndarray的异构扩展。我们还介绍了如何从不同的数据存储中读取和写入，包括JSON、CSV文件、MongoDB和SQL数据库。现在我们将开始展示pandas的实际运用，展示如何使用它来清洗肮脏的数据。我将以我们肮脏的诺贝尔奖数据集为例，介绍数据清洗的关键要素。
- en: I’ll take it slowly, introducing key pandas concepts in a working environment.
    Let’s first establish why cleaning data is such an important part of a data visualizer’s
    work.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我会慢慢介绍关键的pandas概念，让你在实际工作环境中了解。让我们首先弄清楚为什么数据清洗是数据可视化工作中如此重要的一部分。
- en: Coming Clean About Dirty Data
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 洗清肮脏的数据
- en: I think it’s fair to say that most people entering the field of data visualization
    underestimate, often by a fairly large factor, the amount of time they’re going
    to spend trying to make their data presentable. The fact is that getting clean
    datasets that are a pleasure to transform into cool visualizations could well
    take over half your time. Data in the wild is very rarely pristine, often bearing
    the sticky paw prints of mistaken manual data entry, missing whole fields due
    to oversight or parsing errors and/or mixed datetime formats.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为可以说，大多数进入数据可视化领域的人都低估了他们将花费在使数据呈现可用状态上的时间，通常低估的程度相当大。事实上，要获得干净的数据集，可以轻松转化为酷炫可视化效果，可能会占用你一半以上的时间。野外的数据很少是完美的，往往带有错误的手动数据输入的粘滞痕迹，由于疏忽或解析错误而丢失整个字段和/或混合的日期时间格式。
- en: For this book, and to pose a properly meaty challenge, our Nobel Prize dataset
    has been scraped from Wikipedia, a manually edited website with fairly informal
    guidelines. In this sense, the data is bound to be dirty—​humans make mistakes
    even when the environment is a good deal more forgiving. But even data from the
    official APIs of, for example, large social media sites, is often flawed, with
    missing or incomplete fields, scar tissue from countless changes to the data schemas,
    deliberate mis-entry, and the like.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，为了提出一个适当的复杂挑战，我们的诺贝尔奖数据集是从维基百科上抓取的，这是一个手动编辑的网站，有相当非正式的指南。在这个意义上，数据肯定是脏的——即使在环境更宽容的情况下，人类也会犯错。但即使是来自例如大型社交媒体网站的官方API的数据，也经常存在缺失或不完整的字段，来自对数据模式无数更改的疤痕，故意的错误输入等等。
- en: So cleaning data is a fundamental part of the job of a data visualizer, stealing
    time from all the cool stuff you’d rather be doing—​which is an excellent reason
    to get really good at it and free up that drudge time for more meaningful pursuits.
    And a large part of getting good at cleaning data is choosing the right toolset,
    which is where pandas comes in. It’s a great way to slice and dice even fairly
    large datasets,^([1](ch09.xhtml#idm45607776705472)) and being comfortable with
    it could save you a lot of time. That is where this chapter comes in.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据清洗是数据可视化工作的基本组成部分，会占用你本来更愿意做的所有酷炫工作的时间——这是擅长它并释放出那些枯燥时间以进行更有意义追求的一个很好的理由。而擅长数据清洗的一个重要部分就是选择正确的工具集，而这正是pandas发挥作用的地方。即使是处理相当大的数据集也是切片和切块的一个很好的方法，^([1](ch09.xhtml#idm45607776705472))
    熟悉它可以节省大量时间。这就是本章的作用所在。
- en: 'To recap, scraping the Nobel data from Wikipedia using Python’s Scrapy library
    (see [Chapter 6](ch06.xhtml#chapter_heavy_scraping)) produced an array of JSON
    objects of the form:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，使用Python的Scrapy库（参见[第 6 章](ch06.xhtml#chapter_heavy_scraping)）从维基百科爬取诺贝尔奖数据产生了一个JSON对象数组：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The job of this chapter is to turn that array into as clean a data source as
    possible before we explore it with pandas in the next chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的任务是在我们在下一章用pandas探索之前，尽可能将该数组转换为尽可能干净的数据源。
- en: 'There are many forms of dirty data, most commonly:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多形式的脏数据，最常见的是：
- en: Duplicate entries/rows
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复条目/行
- en: Missing fields
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失字段
- en: Misaligned rows
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行不对齐
- en: Corrupted fields
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损坏的字段
- en: Mixed datatypes in a column
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列中混合的数据类型
- en: We’ll now probe our Nobel Prize data for these kinds of anomalies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探究我们的诺贝尔奖数据中是否存在这些异常。
- en: 'First, we need to load our JSON data into a DataFrame, as shown in the previous
    chapter (see [“Creating and Saving DataFrames”](ch08.xhtml#load_save_dataframe)).
    We can open the JSON data file directly:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将我们的JSON数据加载到DataFrame中，就像前一章中所示（参见[“创建和保存DataFrame”](ch08.xhtml#load_save_dataframe)）。我们可以直接打开JSON数据文件：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we’ve got our dirty scraped data into a DataFrame, let’s get a broad
    overview of what we have.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将脏数据加载到DataFrame中，让我们先来全面了解一下我们拥有的数据。
- en: Inspecting the Data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'The pandas DataFrame has a number of methods and properties that give a quick
    overview of the data contained within. The most general is `info`, which gives
    a neat summary of the number of data entries by column:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: pandas DataFrame有许多方法和属性，可以快速概述其中包含的数据。最通用的是`info`，它提供了每列的数据条目数量的整洁摘要：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see that some fields are missing entries. For example, although there
    are 1,052 rows in our DataFrame, there are only 1,040 gender attributes. Note
    also the handy `memory_usage`—pandas DataFrames are held in RAM, so as datasets
    increase in size, this number gives a nice indication of how close we are to our
    machine-specific memory limits.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到某些字段缺少条目。例如，虽然我们的DataFrame中有1,052行，但只有1,040个性别属性。还请注意方便的`memory_usage`——pandas
    DataFrames存储在RAM中，因此随着数据集的增大，此数字可以很好地指示我们离机器特定的内存限制有多近。
- en: 'DataFrame’s `describe` method gives a handy statistical summary of relevant
    columns:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame的`describe`方法提供了相关列的方便的统计摘要：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, by default only numerical columns are described. Already we
    can see an error in the data, the minimum year being 1809, which is impossible
    when the first Nobel was awarded in 1901.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，默认情况下只描述数值列。我们已经可以看到数据中的一个错误，最小年份为1809年，而第一个诺贝尔奖是在1901年颁发的，这是不可能的。
- en: '`describe` takes an `include` parameter that allows us to specify the column
    datatypes (`dtype`s) to be assessed. Other than year, the columns in our Nobel
    Prize dataset are all objects, which are pandas’s default, catchall `dtype`, capable
    of representing any numbers, strings, data times, and more. [Example 9-1](#describe_objects)
    shows how to get their stats.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe`接受一个`include`参数，允许我们指定要评估的列数据类型（`dtype`s）。除了年份之外，我们的诺贝尔奖数据集中的列都是对象，这是pandas的默认、万能的`dtype`，能够表示任何数字、字符串、日期时间等。[示例 9-1](#describe_objects)展示了如何获取它们的统计数据。'
- en: Example 9-1\. Describing the DataFrame
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-1\. 描述DataFrame
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO1-1)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO1-1)'
- en: The `include` argument is a list (or single item) of columnar `dtype`s to summarize.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`include`参数是要总结的列数据类型的列表（或单个项目）。'
- en: There’s quite a lot of useful information to be gleaned from the output of [Example 9-1](#describe_objects),
    such as that there are 59 unique nationalities with the United States, at 350,
    being the largest group.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从[示例 9-1](#describe_objects)的输出中可以得到相当多有用的信息，例如有59个独特的国籍，美国是最大的群体，有350个。
- en: One interesting tidbit is that of 1,044 recorded dates of birth, only 853 are
    unique, which could mean any number of things. Possibly some auspicious days saw
    the birth of more than one laureate or, wearing our data-cleaning hats, it’s more
    likely that there are some duplicated winners or that some dates are wrong or
    have only recorded the year. The duplicated winners hypothesis is confirmed by
    the observation that of 1,052 name counts, only 998 are unique. Now there have
    been a few multiple winners but not enough to account for 54 duplicates.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的一点是，在 1,044 个记录的出生日期中，只有 853 个是唯一的，这可能意味着很多事情。可能某些吉利的日子见证了多位获奖者的诞生，或者，在我们进行数据清理时，更有可能存在重复的获奖者，或者某些日期有误或者只记录了年份。重复的获奖者假设得到了证实，因为在
    1,052 个名称计数中，只有 998 个是唯一的。虽然有一些多次获奖者，但这些并不足以解释 54 个重复。
- en: DataFrame’s `head` and `tail` methods provide another easy way to get a quick
    feel for the data. By default, they display the top or bottom five rows, but we
    can set that number by passing an integer as the first argument. [Example 9-2](#dataframe_head)
    shows the result of using `head` with our Nobel DataFrame.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 的 `head` 和 `tail` 方法提供了另一种快速了解数据的方法。默认情况下，它们显示前五行或后五行，但我们可以通过将整数作为第一个参数传递来设置显示的行数。[示例 9-2](#dataframe_head)
    显示了在我们的诺贝尔 DataFrame 上使用 `head` 的结果。
- en: Example 9-2\. Sampling the first five DataFrame rows
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-2\. 对 DataFrame 的前五行进行采样
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO2-1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO2-1)'
- en: These rows have an entry for the `born_in` field and an asterisk by their name.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行在 `born_in` 字段中有条目，并且姓名旁边有星号标记。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO2-2)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO2-2)'
- en: The `date_of_death` field has a different time format than the other rows.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`date_of_death` 字段的时间格式与其他行不同。'
- en: The first five winners in [Example 9-2](#dataframe_head) show a couple of useful
    things. First, we see the names in rows 1 and 2 are marked by an asterisk and
    have an entry in the `born_in` field ![1](assets/1.png). Second, note that row
    2 has a different time format for `date_of_death` than the others, and that there
    are both month-day and day-month time formats in the `date_of_birth` field ![2](assets/2.png).
    This kind of inconsistency is a perennial problem for human-edited data, particularly
    dates and times. We’ll see how to fix it with pandas later.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 9-2](#dataframe_head) 中的前五位获奖者展示了一些有用的信息。首先，我们看到第 1 和第 2 行的姓名被星号标记，并在 `born_in`
    字段中有条目 ![1](assets/1.png)。其次，请注意第 2 行的 `date_of_death` 与其他行有不同的时间格式，而 `date_of_birth`
    字段中既有月-日格式，也有日-月格式 ![2](assets/2.png)。这种不一致性是人为编辑数据中的常见问题，特别是日期和时间。稍后我们将看到如何使用
    pandas 解决这个问题。'
- en: '[Example 9-1](#describe_objects) gives an object count of 1,052 for the `born_in`
    field, indicating no empty fields, but `head` shows only rows 1 and 2 have content.
    This suggests that the missing fields are an empty string or space, both of which
    count as data to pandas. Let’s change them to a noncounted `NaN`, which will make
    more sense of the numbers. But first we’re going to need a little primer in pandas
    data selection.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 9-1](#describe_objects) 给出了 `born_in` 字段的对象计数为 1,052，表明没有空字段，但 `head` 显示只有第
    1 和第 2 行有内容。这表明缺失字段是空字符串或空格，这两者在 pandas 中都算作数据。让我们将它们改为不计数的 `NaN`，这将更好地解释这些数字。但首先，我们需要对
    pandas 数据选择做一个简单的介绍。'
- en: Indices and pandas Data Selection
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引和 pandas 数据选择
- en: Before beginning to clean our data, let’s do a quick recap of basic pandas data
    selection, using the Nobel Prize dataset as an example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始清理数据之前，让我们快速回顾一下基本的 pandas 数据选择，以诺贝尔奖数据集为例。
- en: 'pandas indexes by rows and columns. Usually column indices are specified by
    the data file, SQL table, and so on, but, as shown in the last chapter, we can
    set or override these when the DataFrame is created by using the `names` argument
    to pass a list of column names. The columns index is accessible as a DataFrame
    property:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 按行和列进行索引。通常列索引由数据文件、SQL 表等指定，但正如上一章所示，我们可以通过使用 `names` 参数在创建 DataFrame
    时设置或覆盖这些列名。列索引可以作为 DataFrame 的属性访问：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By default, pandas specifies a zero-base integer index for the rows, but we
    can override this by passing a list in the `index` parameter on creation of the
    DataFrame or afterward by setting the `index` property directly. More often we
    want to use one or more^([2](ch09.xhtml#idm45607775813456)) of the DataFrame’s
    columns as an index. We can do this using the `set_index` method. If you want
    to return to the default index, you can use the `reset_index` method, as shown
    in [Example 9-3](#settingDFindex).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，pandas 为行指定零基整数索引，但我们可以通过在创建 DataFrame 时传递列表或直接设置`index`属性来覆盖这一点。我们更经常想要将一个或多个
    DataFrame 的列用作索引。我们可以使用`set_index`方法来做到这一点。如果要返回默认索引，可以使用`reset_index`方法，如[示例 9-3](#settingDFindex)所示。
- en: Example 9-3\. Setting the DataFrame’s index
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例9-3. 设置 DataFrame 的索引
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO3-1)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO3-1)'
- en: Sets the frame’s index to its name column. Set the result back to `df`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将框架的索引设置为其名称列。将结果返回给`df`。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO3-2)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO3-2)'
- en: The rows are now indexed by name.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在行已经按名称索引。
- en: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO3-3)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO3-3)'
- en: Resets the index to its integer. Note that we change it in place this time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将索引重置为整数。请注意，这次我们直接在原地更改它。
- en: '[![4](assets/4.png)](#co_cleaning_data_with_pandas_CO3-4)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_cleaning_data_with_pandas_CO3-4)'
- en: The index is now by integer position.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在索引是按整数位置。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are two ways to change a pandas DataFrame or Series: by altering the
    data in place or by assigning a copy. There is no guarantee that in place is faster,
    plus method-chaining requires that the operation return a changed object. Generally,
    I use the `df = df.foo(...)` form, but most mutating methods have an `inplace`
    argument `df.foo(..., inplace=True)`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以更改 pandas DataFrame 或 Series：通过直接更改数据或通过赋值副本。不能保证直接更改更快，而且方法链需要操作返回一个已更改的对象。通常情况下，我使用`df
    = df.foo(...)`形式，但大多数变异方法都有一个`inplace`参数`df.foo(..., inplace=True)`。
- en: Now that we understand the row-columnar indexing system, let’s start selecting
    slices of the DataFrame.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了行列索引系统，让我们开始选择 DataFrame 的切片。
- en: 'We can select a column of the DataFrame by dot notation (where no spaces or
    special characters are in the name) or square-bracket notation. Let’s take a look
    at that `born_in` column:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过点表示法（名称中没有空格或特殊字符）或方括号表示法选择 DataFrame 的列。让我们看看`born_in`列：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that the column selection returns a pandas Series, with the DataFrame indexing
    preserved.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意列选择返回一个 pandas Series，保留了 DataFrame 的索引。
- en: 'DataFrames and Series share the same methods for accessing rows/members. `iloc`
    selects by integer position, and `loc` selects by label. Let’s use `iloc` to grab
    the first row of our DataFrame:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames 和 Series 共享相同的方法来访问行/成员。`iloc`通过整数位置选择，`loc`通过标签选择。让我们使用`iloc`来获取我们
    DataFrame 的第一行：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Selecting Multiple Rows
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择多行
- en: 'Standard Python array slicing can be used with a DataFrame to select multiple
    rows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用标准的 Python 数组切片与 DataFrame 一起选择多行：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The standard way to select multiple rows based on a conditional expression
    (e.g., is the value of the column `value` greater than `x`) is to create a Boolean
    mask and use it in a selector. Let’s find all the Nobel Prize winners after the
    year 2000\. First, we create a mask by performing a Boolean expression on each
    of the rows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据条件表达式选择多行的标准方法（例如，列`value`的值是否大于`x`）是创建一个布尔掩码并在选择器中使用它。让我们找出所有2000年后的诺贝尔奖获得者。首先，我们通过对每一行执行布尔表达式来创建一个掩码：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO4-1)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO4-1)'
- en: '`True` for all rows where the `year` field is greater than 2000.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有`year`字段大于2000的行为`True`。
- en: 'The resulting Boolean mask shares our DataFrame’s index and can be used to
    select all `True` rows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果布尔掩码共享我们 DataFrame 的索引，可用于选择所有`True`行：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO5-1)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO5-1)'
- en: This will return a DataFrame containing only those rows where the Boolean `mask`
    array is `True`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个 DataFrame，其中仅包含布尔`mask`数组为`True`的行。
- en: 'Boolean masking is a very powerful technique capable of selecting any subset
    of the data you need. I recommend setting a few targets to practice constructing
    the right Boolean expressions. Generally, we dispense with the intermediate mask
    creation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 布尔遮罩是一种非常强大的技术，能够选择您需要的数据的任何子集。我建议设定一些目标来练习构建正确的布尔表达式。通常，我们会省略中间遮罩的创建。
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we can select individual and multiple rows by slicing or using a Boolean
    mask, in the next sections we’ll see how we can change our DataFrame, purging
    it of dirty data as we go.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过切片或使用布尔遮罩来选择单个和多个行，接下来的几节我们将看到如何改变我们的 DataFrame，在此过程中清除脏数据。
- en: Cleaning the Data
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理数据
- en: 'Now that we know how to access our data, let’s see how we can change it for
    the better, starting with what looks like empty `born_in` fields we saw in [Example 9-2](#dataframe_head).
    If we look at the count of the `born_in` columns, it doesn’t show any missing
    rows, which it would were any fields missing or `NaN` (not a number):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何访问我们的数据了，让我们看看如何改进它，从我们在 [示例 9-2](#dataframe_head) 中看到的看起来为空的 `born_in`
    字段开始。如果我们查看 `born_in` 列的计数，它不会显示任何缺失的行，如果有任何字段缺失或 `NaN`（非数字）的话，它会显示出来：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finding Mixed Types
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找混合类型
- en: 'Note that pandas stores all string-like data using the `dtype` object. A cursory
    inspection suggests that the column is a mixture of empty and country-name strings.
    We can quickly check that all the column members are strings by mapping the Python
    [`type` function](https://oreil.ly/jj9hY) to all members using the `apply` method
    and then making a set of the resulting list of column members by type:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，pandas 使用 `dtype` 对象存储所有类似字符串的数据。粗略检查表明，该列是空和国家名称字符串的混合。我们可以通过将 Python 的
    [`type` 函数](https://oreil.ly/jj9hY) 映射到所有成员，然后使用 `apply` 方法并将结果列表设置为列成员类型的集合来快速检查所有列成员是否都是字符串：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This shows that all of the `born_in` column members are strings of type `str`.
    Now let’s replace any empty strings with an empty field.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明 `born_in` 列的所有成员都是字符串类型 `str`。现在让我们用一个空字段来替换任何空字符串。
- en: Replacing Strings
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替换字符串
- en: 'We want to replace these empty strings with a `NaN`, to prevent them being
    counted.^([3](ch09.xhtml#idm45607774963264)) The pandas `replace` method is tailor-made
    for this and can be applied to the whole DataFrame or individual Series:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想用 `NaN` 替换这些空字符串，以防止它们被计数。[^3] pandas 的 `replace` 方法是专门为此而设计的，可以应用于整个 DataFrame
    或单个 Series：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO6-1)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO6-1)'
- en: Our empty `''` strings have been replaced with NumPy’s `NaN`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的空 `''` 字符串已被替换为 NumPy 的 `NaN`。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO6-2)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO6-2)'
- en: Unlike the empty strings, the `NaN` fields are discounted.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不像空字符串，`NaN` 字段是不计算在内的。
- en: After replacing the empty strings with `NaN`s, we get a true count of 142 for
    the `born_in` field.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在用 `NaN` 替换空字符串后，我们得到了 `born_in` 字段的真实计数为 142。
- en: 'Let’s replace all empty strings in our DataFrame with discounted `NaN`s:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用折扣的 `NaN` 替换我们 DataFrame 中的所有空字符串：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: pandas allows sophisticated replacement of strings (and other objects) in columns
    (e.g., allowing you to craft [regular expressions or regexes](https://oreil.ly/KK3b2),
    which are applied to whole Series, typically DataFrame columns). Let’s look at
    a little example, using the asterisk-marked names in our Nobel Prize DataFrame.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 允许对列中的字符串（以及其他对象）进行复杂的替换（例如，允许您编写 [正则表达式或 regexes](https://oreil.ly/KK3b2)，这些表达式适用于整个
    Series，通常是 DataFrame 的列）。让我们看一个小例子，使用我们的诺贝尔奖 DataFrame 中用星号标记的姓名。
- en: '[Example 9-2](#dataframe_head) showed that some of our Nobel Prize names are
    marked with an asterisk, denoting that these winners are recorded by country of
    birth, not country at the time of winning the prize:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 9-2](#dataframe_head) 显示，我们的一些诺贝尔奖获得者的名字被标记为星号，表示这些获奖者是按出生国家记录的，而不是获奖时的国家：'
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s set ourselves the task of cleaning up those names by removing the asterisks
    and stripping any remaining whitespace.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一个任务，通过去除星号并去除任何剩余的空白来清理这些名字。
- en: 'pandas Series have a handy `str` member, which provides a number of useful
    string methods to be performed on the array. Let’s use it to check how many asterisked
    names we have:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: pandas Series 有一个方便的 `str` 成员，提供了许多有用的字符串方法，可在数组上执行。让我们使用它来检查有多少带星号的名字：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO7-1)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO7-1)'
- en: We use `str`’s `contains` method on the `name` column. Note that we have to
    escape the asterisk (`'\*'`) as this is a regex string. The Boolean mask is then
    applied to our Nobel Prize DataFrame and the resulting names listed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`name`列上使用`str`的`contains`方法。请注意，我们必须转义星号（`'\*'`），因为这是一个正则表达式字符串。然后将布尔掩码应用于我们的诺贝尔奖
    DataFrame，并列出结果名称。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO7-2)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO7-2)'
- en: 142 of our 1,052 rows have a name containing `*`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 1,052 行中，有 142 行的名称中包含`*`。
- en: 'To clean up the names, let’s replace the asterisks with an empty string and
    strip any whitespace from the resulting names:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清理名称，让我们用空字符串替换星号，并从结果名称中去掉任何空格：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO8-1)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO8-1)'
- en: Removes all asterisks in the name fields and return the result to the DataFrame.
    Note that we have to explicitly set the `regex` flag to `True`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 删除名称字段中的所有星号，并将结果返回到DataFrame。请注意，我们必须显式地将`regex`标志设置为`True`。
- en: 'A quick check shows that the names are now clean:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 快速检查显示现在名称已经干净：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: pandas Series have an impressive number of string-handling functions, enabling
    you to search and adapt your string columns. You can find a full list of these
    [in the API docs](https://oreil.ly/2mCSZ).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: pandas Series 有大量的字符串处理函数，使您能够搜索和调整字符串列。您可以在[API 文档](https://oreil.ly/2mCSZ)中找到完整列表。
- en: Removing Rows
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除行
- en: To recap, the 142 winners with `born_in` fields are duplicates, having an entry
    in the Wikipedia biography page by both the country they were born in and their
    country when given the prize. Although the former could form the basis of an interesting
    visualization,^([4](ch09.xhtml#idm45607774530976)) for our visualization we want
    each individual prize represented once only and so need to remove these from our
    DataFrame.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，142位出生地字段为获奖者是重复的，在维基百科传记页上既有出生国家，又有获奖时的国家。尽管前者可能形成有趣的可视化基础，^([4](ch09.xhtml#idm45607774530976))
    但对于我们的可视化，我们希望每个个人奖只代表一次，因此需要从我们的DataFrame中删除这些内容。
- en: 'We want to create a new DataFrame using only those rows with a `NaN` `born_in`
    field. You might naively assume that a conditional expression comparing the `born_in`
    field to `NaN` would work here, but by definition^([5](ch09.xhtml#idm45607774527728))
    `NaN` Boolean comparisons always return `False`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想创建一个新的DataFrame，只包含那些具有`NaN`的`born_in`字段的行。您可能天真地认为将条件表达式与`born_in`字段与`NaN`进行比较会在这里起作用，但根据定义^([5](ch09.xhtml#idm45607774527728))，`NaN`布尔比较始终返回`False`：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As a result, pandas provides the dedicated `isnull` method to check for discounted
    (null) fields:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，pandas 提供了专门的`isnull`方法来检查折扣（空）字段：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO9-1)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO9-1)'
- en: '`isnull` produces a Boolean mask with `True` for all rows with an empty `born_in`
    field.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`isnull` 生成一个布尔掩码，对于所有具有空`born_in`字段的行返回`True`。'
- en: The `born_in` column is no longer of use, so let’s remove it for now:^([6](ch09.xhtml#idm45607774438432))
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`born_in`列现在不再有用，所以暂时删除它：^([6](ch09.xhtml#idm45607774438432))'
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO10-1)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO10-1)'
- en: '`drop` takes a single label or index (or list of same) as a first argument
    and an `axis` argument to indicate row (`0` and default) or column (`1`) index.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`drop` 接受一个单个标签或索引（或相同的列表）作为第一个参数，并且一个`axis`参数以指示行（`0`和默认）或列（`1`）索引。'
- en: Finding Duplicates
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找重复项
- en: Now, a quick internet search shows that 889 people and organizations have received
    the Nobel Prize up to 2015\. With 910 remaining rows, we still have a few duplicates
    or anomalies to account for.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，快速的互联网搜索显示截至 2015 年有 889 个人和组织获得了诺贝尔奖。还剩下 910 行，我们仍然有一些重复或异常需要处理。
- en: 'pandas has a handy `duplicated` method for finding matching rows. This matches
    by column name or list of column names. Let’s get the list of all duplicates by
    name:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 提供了一个方便的`duplicated`方法用于查找匹配的行。这可以根据列名或列名列表进行匹配。让我们获取所有名称重复的列表：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO11-1)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO11-1)'
- en: '`duplicated` returns a Boolean array with `True` for the first occurrence of
    any rows with the same `name` field.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`duplicated` 返回一个布尔数组，对于具有相同`name`字段的任何行的第一个出现，返回`True`。'
- en: Now, a few people have won the Nobel Prize more than once but not 46, which
    means 40-odd winners are duplicated. Given that the Wikipedia page we scraped
    listed prize winners by country, the best bet is winners being “claimed” by more
    than one country.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有些人曾多次获得诺贝尔奖，但不是 46 次，这意味着有 40 多个获奖者是重复的。考虑到我们抓取的维基百科页面按国家列出了获奖者，最有可能的情况是获奖者被多个国家“认领”。
- en: Let’s look at some of the ways we can find duplicates by name in our Nobel Prize
    DataFrame. Some of these are pretty inefficient, but it’s a nice way to demonstrate
    a few pandas functions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些在我们的诺贝尔奖 DataFrame 中通过名称查找重复项的方法。其中一些方法效率很低，但这是演示一些 pandas 函数的好方法。
- en: 'By default, `duplicated` indicates (Boolean `True`) all duplicates after the
    first occurrence, but setting the `keep` option to `*last*` sets the first occurrence
    of duplicated rows to `True`. By combining these two calls using a Boolean *or*
    (|), we can get the full list of duplicates:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`duplicated` 指示（布尔值 `True`）第一次出现后的所有重复项，但将 `keep` 选项设置为 `*last*` 将重复行的第一次出现设置为
    `True`。通过使用布尔 *or* (|) 结合这两个调用，我们可以得到完整的重复列表：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We could also get all the duplicates by testing whether our DataFrame rows
    have a name in the list of duplicate names. pandas has a handy `isin` method for
    this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过测试我们的 DataFrame 行中是否有一个名称在重复名称列表中来获取所有重复项。pandas 提供了一个方便的 `isin` 方法来实现这一点：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO12-1)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO12-1)'
- en: '`dupes_by_name.name` is a column Series containing all the duplicated names.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`dupes_by_name.name` 是一个包含所有重复名称的列 Series。'
- en: 'We can also find all duplicates by using pandas’s powerful `groupby` method,
    which groups our DataFrame’s rows by column or list of columns. It returns a list
    of key-value pairs with the column value(s) as key and list of rows as values:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 pandas 强大的 `groupby` 方法找到所有重复项，该方法通过列或列列表对我们的 DataFrame 行进行分组。它返回一系列键值对，键是列值，值是行的列表：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO13-1)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO13-1)'
- en: '`groupby` returns an iterator of (group name, group) tuples.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby` 返回一个 (组名，组) 元组的迭代器。'
- en: 'In order to get all duplicate rows, we merely need to check the length of the
    list of rows returned by key. Anything greater than one has name duplicates. Here
    we use pandas’s `concat` method, which takes a list of row lists and creates a
    DataFrame with all the duplicated rows. A Python list constructor is used to filter
    for groups with more than one row:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得所有重复的行，我们只需检查由 key 返回的行列表的长度。任何大于一的值都具有名称重复。在这里，我们使用 pandas 的 `concat` 方法，它接受一个行列表的列表，并创建一个包含所有重复行的
    DataFrame。使用 Python 列表构造函数来过滤具有多行的组：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO14-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO14-1)'
- en: Create a Python list by filtering the `name` row groups for those with more
    than one row (i.e., duplicated names).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过筛选具有多行的 `name` 行组（即重复的名称）来创建一个 Python 列表。
- en: Different Paths to the Same Goal
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 达到相同目标的不同途径
- en: With a large library like pandas, there are usually a number of ways to achieve
    the same thing. With small datasets like our Nobel Prize winners, any one will
    do, but for large datasets there could be significant performance implications.
    Just because pandas will do what you ask doesn’t mean it’s necessarily efficient.
    With a lot of complex data manipulation going on behind the scenes, it’s a good
    idea to be prepared to be flexible and alert to inefficient approaches.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 pandas 这样的大型库中，通常有多种方法可以实现相同的目标。对于像我们的诺贝尔奖获得者这样的小型数据集，任何一种方法都可以，但对于大型数据集来说，可能会有显著的性能影响。仅仅因为
    pandas 能够执行你要求的操作，并不意味着它一定是高效的。由于幕后进行了大量复杂的数据处理，因此最好做好准备，对低效的方法保持灵活和警惕。
- en: Sorting Data
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据排序
- en: Now that we have our `all_dupes` DataFrame, with all duplicated rows by name,
    let’s use it to demonstrate pandas’s `sort` method.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 `all_dupes` DataFrame，其中包含了所有重复的行，按名称排序，让我们用它来演示 pandas 的 `sort` 方法。
- en: 'pandas provides a sophisticated `sort` method for the DataFrame and Series
    classes, capable of sorting on multiple column names:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 为 DataFrame 和 Series 类提供了一个复杂的 `sort` 方法，能够在多个列名上进行排序：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO15-1)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO15-1)'
- en: Sorts the DataFrame first by name, then by score within those subgroups. Older
    pandas versions use `sort`, now deprecated.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先按名称排序 DataFrame，然后在这些子组内按分数排序。旧版本的 pandas 使用 `sort`，现在已不推荐使用。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO15-2)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO15-2)'
- en: Sorts the names in alphabetical ascending order; sorts scores from high to low.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将姓名按字母顺序升序排序；将分数从高到低排序。
- en: 'Let’s sort the DataFrame of `all_dupes` by name and then look at the name,
    country, and year columns:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按姓名对`all_dupes`的DataFrame进行排序，然后查看姓名、国家和年份列：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This output shows that, as expected, some winners have been attributed twice
    for the same year with different countries. It also reveals a few other anomalies.
    Although Marie Curie did win a Nobel Prize twice, she’s included here with both
    French and Polish nationalities.^([7](ch09.xhtml#idm45607773684544)) The fairest
    thing here is to split the spoils between Poland and France while settling on
    the single compound surname. We have also found our anomalous year of 1809 at
    row 960\. Sidney Altman is both duplicated and given the wrong year of 1990.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出显示，如预期的那样，一些获奖者因同一年份而分配了两次，但具有不同的国家。它还显示了一些其他异常情况。尽管玛丽·居里确实两次获得诺贝尔奖，但她在这里既有法国国籍又有波兰国籍。^([7](ch09.xhtml#idm45607773684544))在这里最公平的做法是将战利品分给波兰和法国，并最终选择单一的复合姓。我们还发现了我们960行处的1809年的异常年份。Sidney
    Altman既重复了，又错误地获得了1990年的年份。
- en: Removing Duplicates
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除重复项
- en: Let’s go about removing the duplicates we just identified and start compiling
    a little cleaning function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始移除我们刚刚识别的重复项，并开始编译一个小的清理函数。
- en: Tip
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Changing rows by numeric index is fine if you know your dataset is stable and
    don’t anticipate running any of your cleaning scripts again. But if, as in the
    case of our scraped Nobel Prize data, you may want to run the same cleaning script
    on an updated dataset, it’s much better to use stable indicators (i.e., grab the
    row with name Marie Curie and year 1911, not index 919).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您知道数据集是稳定的并且不预期再次运行任何清理脚本，则可以通过数字索引更改行。但是，就像我们抓取的诺贝尔奖数据一样，如果您希望在更新的数据集上运行相同的清理脚本，最好使用稳定的指示器（即，获取玛丽·居里和1911年的行，而不是索引919）。
- en: 'A more robust way of changing the country of a specific row is to use stable
    column values to select the row rather than its index. This means if the index
    value changes the cleaning script should still work. So to change Marie Curie’s
    1911 prize country to France, we can use a Boolean mask with the `loc` method
    to select a row and then set its country column to France. Note that we specify
    the Unicode for the Polish ł:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 更改特定行的国家的更健壮的方法是使用稳定的列值来选择行，而不是其索引。这意味着如果索引值更改，则清理脚本仍然可以工作。因此，要将玛丽·居里1911年的奖项国家更改为法国，我们可以使用带有`loc`方法的布尔掩码来选择一行，然后将其国家列设置为法国。请注意，我们为波兰的ł指定了Unicode：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As well as changing Marie Curie’s country, we want to remove or drop some rows
    from our DataFrame, based on column values. There are two ways we can do this,
    firstly by using the DataFrame’s `drop` method, which takes a list of index labels,
    or by creating a new DataFrame with a Boolean mask that filters the rows we want
    to drop. If we use `drop`, we can use the `inplace` argument to change the existing
    DataFrame.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更改玛丽·居里的国家外，我们还希望根据列值从DataFrame中删除或删除一些行。我们可以通过两种方法实现这一点，首先是使用DataFrame的`drop`方法，该方法接受一个索引标签列表，或者通过创建一个布尔掩码的新DataFrame来筛选我们想要删除的行。如果使用`drop`，我们可以使用`inplace`参数来更改现有的DataFrame。
- en: 'In the following code, we drop our duplicate Sidney Altman row by creating
    a DataFrame with the single row we want (remember, index labels are preserved)
    and passing that index to the `drop` method and changing the DataFrame in place:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们通过创建一个只包含我们想要的单行的DataFrame并将该索引传递给`drop`方法来删除我们的重复的Sidney Altman行，并在原地更改DataFrame：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Another way to remove the row is to use the same Boolean mask with a logical
    *not* (~) to create a new DataFrame with all rows except the one(s) we’re selecting:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 删除行的另一种方法是使用相同的布尔掩码与逻辑*非*（~）来创建一个新的DataFrame，该DataFrame除了我们选择的行之外的所有行：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s add this change and all current modifications to a `clean_data` method:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此更改和所有当前修改添加到`clean_data`方法中：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We now have a mix of valid duplicates (those few multiple Nobel Prize winners)
    and those with dual country. For the purposes of our visualization, we want each
    prize to count only once, so we have to discard half the dual-country prizes.
    The easiest way is to use the `duplicated` method, but because we collected the
    winners alphabetically by country, this would favor those nationalities with first
    letters earlier in the alphabet. Short of a fair amount of research and debate,
    the fairest way seems to pick one out at random and discard it. There are various
    ways to do this, but the simplest is to randomize the order of the rows before
    using `drop_duplicates`, a pandas method that drops all duplicated rows after
    the first encountered or, with the `take_last` argument set to `True`, all before
    the last.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了有效的重复项（那些少数多次获奖的诺贝尔奖获得者）和那些具有双重国家的重复项。为了我们可视化的目的，我们希望每个奖项只计数一次，因此我们必须丢弃一半的双重国家奖项。最简单的方法是使用
    `duplicated` 方法，但由于我们按国家字母顺序收集了获奖者，这将偏袒字母顺序较早的国家。除了进行大量研究和辩论之外，看起来最公平的方法似乎是随机选择一个并丢弃它。有各种方法可以做到这一点，但最简单的方法是在使用
    `drop_duplicates` 之前随机化行的顺序，这是一个pandas方法，它会在遇到第一个重复项之后删除所有重复的行，或者在设置 `take_last=True`
    参数时删除所有最后一个重复项之前的行。
- en: 'NumPy has a number of very useful methods in its `random` module, of which
    `permutation` is perfect for randomizing the row index. This method takes an array
    (or pandas index) of values and shuffles them. We can then use the DataFrame `reindex`
    method to apply the shuffled result. Note that we drop those rows sharing both
    name and year, which will preserve the legitimate double winners with different
    years for their prizes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 在其 `random` 模块中有许多非常有用的方法，其中 `permutation` 对于随机化行索引非常合适。该方法接受一个值数组（或pandas索引）并对其进行洗牌。然后我们可以使用DataFrame的
    `reindex` 方法应用洗牌后的结果。请注意，我们丢弃了那些共享名称和年份的行，这将保留具有不同年份的合法双重获奖者：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO17-1)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO17-1)'
- en: Create a shuffled version of `df`’s index and reindex `df` with it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `df` 索引的洗牌版本，并用它重新索引 `df`。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO17-2)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO17-2)'
- en: Drop all duplicates sharing name and year.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 删除所有共享名称和年份的重复项。
- en: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO17-3)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO17-3)'
- en: Return the index to sorted-by-integer position.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将索引返回到按整数位置排序。
- en: 'If our data wrangling has been successful, we should have only valid duplicates
    left, those vaunted double-prize winners. Let’s list the remaining duplicates
    to check:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据整理成功，应该只剩下有效的重复项，那些著名的双重获奖者。让我们列出剩余的重复项以进行检查：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO18-1)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO18-1)'
- en: We combine duplicates from the first with the last to get them all. If using
    an older version of pandas, you may need to use the argument `take_last=True`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将第一个和最后一个的重复项合并起来以获取它们所有的内容。如果使用较旧版本的pandas，可能需要使用参数 `take_last=True`。
- en: A quick internet check shows that we have the correct four double-prize winners.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 快速查阅互联网显示我们有正确的四位双重获奖者。
- en: Assuming we’ve caught the unwanted duplicates,^([10](ch09.xhtml#idm45607772960432))
    let’s move on to other “dirty” aspects of the data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经捕捉到了不需要的重复项^([10](ch09.xhtml#idm45607772960432))，让我们继续处理数据的其他“脏”方面。
- en: Dealing with Missing Fields
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理缺失字段
- en: 'Let’s see where we stand as far as *null* fields are concerned by counting
    our DataFrame:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们统计一下DataFrame中的*null*字段情况：
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO19-1)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO19-1)'
- en: A missing category field
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失了一个类别字段。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO19-2)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO19-2)'
- en: Eight missing gender fields
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失了八个性别字段。
- en: 'We appear to be missing a `category` field, which suggests a data entry mistake.
    If you remember, while scraping our Nobel Prize data we checked the category against
    a valid list (see [Example 6-3](ch06.xhtml#scrapy_process_li)). One of them appears
    to have failed this check. Let’s find out which one it is by grabbing the row
    where the category field is null and showing its name and text columns:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们缺少一个 `category` 字段，这表明存在数据输入错误。如果记得，在抓取我们的诺贝尔奖数据时，我们检查了类别是否与有效列表匹配（参见 [示例
    6-3](ch06.xhtml#scrapy_process_li)）。其中一个似乎未通过此检查。让我们找出是哪一个，方法是获取类别字段为空的行，并显示其名称和文本列：
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We saved the original link text for our winners and, as you can see, Alexis
    Carrel was listed as winning the Nobel prize for `Medicine`, when it should have
    been `Physiology or Medicine`. Let’s correct that now:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为获奖者保存了原始链接文本，您可以看到，亚历克西斯·卡雷尔被列为赢得`医学`诺贝尔奖，而实际上应该是`生理学或医学`。让我们现在更正一下：
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We are also missing `gender` for eight winners. Let’s list them:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还缺少8位获奖者的`gender`。让我们列出它们：
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'With the exception of Ragnar Granit, all these are genderless (missing person
    data) institutions. The focus of our visualization is on individual winners, so
    we’ll remove these while establishing Ragnar Granit’s gender^([11](ch09.xhtml#idm45607772538016)):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拉格纳·格拉尼特（Ragnar Granit）外，所有这些都是没有性别的（缺失人员数据）机构。我们的可视化重点是个人获奖者，因此我们将删除这些，同时确定拉格纳·格拉尼特的性别^([11](ch09.xhtml#idm45607772538016))：
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let’s see where those changes leave us by performing another count on our DataFrame:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这些变化把我们带到哪里，通过对我们的DataFrame执行另一个计数来查看：
- en: '[PRE43]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Having removed all the institutions, all entries should have at least a date
    of birth. Let’s find the missing entry and fix it:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 删除了所有机构后，所有条目至少应该有一个出生日期。让我们找到缺失的条目并修复它：
- en: '[PRE44]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Probably because Hiroshi Amano is a very recent (2014) winner, his date of
    birth was not available to be scraped. A quick web search establishes Amano’s
    date of birth, which we add to the DataFrame by hand:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是因为天野浩是最近（2014年）的获奖者，他的出生日期无法被抓取到。通过快速的网络搜索，我们确认了天野浩的出生日期，然后手动将其添加到DataFrame中：
- en: '[PRE45]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We now have 858 individual winners. Let’s do a final count to see where we
    stand:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有858位个人获奖者。让我们进行最后一次计数，看看我们的情况如何：
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The key fields of `category`, `date_of_birth`, `gender`, `country`, and `year`
    are all filled and there’s a healthy amount of data in the remaining stats. All
    in all, there’s enough clean data to form the basis for a rich visualization.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`category`、`date_of_birth`、`gender`、`country`和`year`的关键字段都填满了，剩余统计数据也有足够的数据。总的来说，有足够干净的数据来形成丰富的可视化基础。'
- en: Now let’s put on the finishing touches by making our temporal fields more usable.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过使我们的时间字段更可用来做出最后的修改。
- en: Dealing with Times and Dates
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理时间和日期
- en: 'Currently the `date_of_birth` and `date_of_death` fields are represented by
    strings. As we’ve seen, Wikipedia’s informal editing guidelines have led to a
    number of different time formats. Our original DataFrame shows an impressive variety
    of formats in the first 10 entries:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`date_of_birth`和`date_of_death`字段由字符串表示。正如我们所见，维基百科的非正式编辑指南导致了许多不同的时间格式。我们的原始DataFrame在前10个条目中显示了令人印象深刻的多种格式：
- en: '[PRE47]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In order to compare the date fields (for example, subtracting the prize *year*
    from *date of birth* to give the winners’ ages), we need to get them into a format
    that allows such operations. Unsurprisingly, pandas is good with parsing messy
    dates and times, converting them by default into the NumPy `datetime64` object,
    which has a slew of useful methods and operators.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较日期字段（例如，从生日减去奖项*年份*以得到获奖者的年龄），我们需要将它们转换为允许这样的操作的格式。毫不奇怪，pandas擅长解析混乱的日期和时间，将它们默认转换为NumPy的`datetime64`对象，该对象具有一系列有用的方法和运算符。
- en: 'Converting a time column to `datetime64`, we use pandas’s `to_datetime` method:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 将时间列转换为`datetime64`，我们使用pandas的`to_datetime`方法：
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO20-1)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)](#co_cleaning_data_with_pandas_CO20-1)'
- en: The `errors` default is `ignore`, but we want them flagged.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`errors`默认值是`ignore`，但是我们希望它们被标记。'
- en: By default `to_datetime` ignores errors, but here we want to know if pandas
    has been unable to parse a `date_of_birth`, giving us the opportunity to fix it
    manually. Thankfully, the conversion passes without error.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`to_datetime`会忽略错误，但是在这里，我们希望知道pandas是否无法解析`date_of_birth`，从而给我们手动修复的机会。幸运的是，转换顺利通过，没有错误。
- en: 'Let’s fix our DataFrame’s `date_of_birth` column before moving on:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在继续之前修复我们的DataFrame的`date_of_birth`列：
- en: '[PRE49]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Running `to_datetime` on the `date_of_birth` field raises a `ValueError` and
    an unhelpful one at that, giving no indication of the entry that triggered it:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在`date_of_birth`字段上运行`to_datetime`引发了一个`ValueError`，而且一个毫无帮助的错误，没有指示触发它的条目：
- en: '[PRE50]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'One naive way to find the bad dates would be to iterate through our rows of
    data, and catch and display any errors. pandas has a handy `iterrows` method that
    provides a row iterator. Combined with a Python `try-except` block, this successfully
    finds our problem date fields:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一种找到错误日期的天真方法是遍历我们的数据行，并捕获和显示任何错误。 pandas有一个方便的`iterrows`方法提供了一个行迭代器。结合Python的`try-except`块，这成功地找到了我们的问题日期字段：
- en: '[PRE51]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO21-1)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)](#co_cleaning_data_with_pandas_CO21-1)'
- en: Run `to_datetime` on the individual row and catch any errors.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在单独的行上运行`to_datetime`并捕获任何错误。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO21-2)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO21-2)'
- en: We left-justify the date of death in a text column of width 30 to make the output
    easier to read. pandas rows have a masking `Name` property, so we use string-key
    access with `['name']`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将死亡日期左对齐到30宽度的文本列，以使输出更易于阅读。pandas行具有遮罩`Name`属性，因此我们使用字符串键访问`['name']`。
- en: 'This lists the offending rows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这列出了有问题的行：
- en: '[PRE52]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: which is a good demonstration of the kind of data errors you get with collaborative
    editing.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是协作编辑时可能遇到的数据错误的良好示例。
- en: Although the last method works, whenever you find yourself iterating through
    rows of a pandas DataFrame, you should pause for a second and try to find a better
    way, one that exploits the multirow array handling that is a fundamental aspect
    of pandas’s efficiency.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最后一种方法有效，但是每当您发现自己在遍历pandas DataFrame的行时，您应该停顿一秒钟，并尝试找到更好的方法，利用pandas效率的多行数组处理是一个基本的方面。
- en: 'A better way to find the bad dates exploits the fact that pandas’s `to_datetime`
    method has a `coerce` argument, which, if `True`, converts any date exceptions
    to `NaT` (not a time), the temporal equivalent of `NaN`. We can then create a
    Boolean mask out of the resulting DataFrame based on the `NaT` date rows, producing
    [Figure 9-1](#clean_dodgy_dates):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 查找错误日期的更好方法利用了pandas的`to_datetime`方法的`coerce`参数，如果设为`True`，则会将任何日期异常转换为`NaT`（不是时间），这是`NaN`的时间等价物。然后我们可以基于结果DataFrame创建一个布尔掩码，根据`NaT`日期行生成[Figure 9-1](#clean_dodgy_dates)：
- en: '[PRE53]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO22-1)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO22-1)'
- en: Gets all rows with non-null date fields.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 获取所有具有非空日期字段的行。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO22-2)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO22-2)'
- en: Creates a Boolean mask for all bad dates in `with_death_dates` by checking against
    null (`NaT`) after coercing failed conversions to `NaT`. For older pandas versions,
    you may need to use `coerce=True`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查`with_death_dates`中的所有错误日期，创建一个布尔掩码，如果转换失败，则检查空值（`NaT`）。对于较旧的pandas版本，您可能需要使用`coerce=True`。
- en: '![dpj2 0901](assets/dpj2_0901.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0901](assets/dpj2_0901.png)'
- en: Figure 9-1\. The unparseable date fields
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 无法解析的日期字段
- en: 'Depending on how fastidious you want to be, these can be corrected by hand
    or coerced to NumPy’s time equivalent of `NaN`, `NaT`. We’ve got more than 500
    valid dates of death, which is enough to get some interesting time stats, so we’ll
    run `to_datetime` again and force errors to null:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 取决于您想要多么一丝不苟，这些可以手动更正或强制转换为NumPy的时间等效物`NaN`，`NaT`。我们有超过500个有效的死亡日期，足以获得一些有趣的时间统计数据，因此我们将再次运行`to_datetime`并强制错误为空：
- en: '[PRE54]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now that we have our time fields in a usable format, let’s add a field for
    the age of the winner on receiving his/her Nobel Prize. In order to get the year
    value of our new dates, we need to tell pandas that it’s dealing with a date column,
    using the `DatetimeIndex` method. Note that this gives a crude estimation of award
    age and may be off by a year. For the purposes of the next chapter’s dataviz exploration,
    this is more than adequate:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的时间字段已经以可用的格式存在，让我们添加一个字段，用于获得诺贝尔奖时获奖者的年龄。为了获取新日期的年份值，我们需要告诉pandas它正在处理一个日期列，使用`DatetimeIndex`方法。请注意，这给出了奖项年龄的粗略估计，可能会有一年的偏差。对于下一章的数据可视化探索目的，这已经足够了。
- en: '[PRE55]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO23-1)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO23-1)'
- en: Convert the column to a `DatetimeIndex`, an `ndarray` of `datetime64` data,
    and use the `year` property.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将列转换为`DatetimeIndex`，一个`datetime64`数据的`ndarray`，并使用`year`属性。
- en: 'Let’s use our new `award_age` field to see the youngest recipients of the Nobel
    Prize:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用新的`award_age`字段来查看诺贝尔奖最年轻的获奖者：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO24-1)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO24-1)'
- en: For activism for female education, I’d recommend reading more about [Malala’s
    inspirational story](https://oreil.ly/26szS).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于女性教育活动，我建议更多地了解[玛拉拉的励志故事](https://oreil.ly/26szS)。
- en: Now that we have our date fields in a manipulable form, let’s have a look at
    the full `clean_data` function, which summarizes this chapter’s cleaning efforts.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将我们的日期字段格式化，让我们来看看完整的`clean_data`函数，它总结了本章的清理工作。
- en: The Full clean_data Function
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整的`clean_data`函数
- en: For manually edited data like scraped Wikipedia datasets, it’s unlikely that
    you’ll catch all the errors on a first pass. So expect to pick up a few during
    the data exploration phase. Nevertheless, our Nobel Prize dataset is looking very
    usable. We’ll declare it clean enough and the job of this chapter done. [Example 9-4](#clean_data)
    shows the steps we used to achieve this cleaning feat.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像从维基百科抓取的数据集这样手动编辑的数据，第一次处理时可能无法捕捉到所有的错误。因此，在数据探索阶段期间可能会发现一些错误。尽管如此，我们的诺贝尔奖数据集看起来已经可以使用。我们将宣布它足够干净，这一章的工作完成了。[示例 9-4](#clean_data)
    展示了我们用来完成此清理工作的步骤。
- en: Example 9-4\. The full Nobel Prize dataset cleaning function
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-4\. 完整的诺贝尔奖数据集清理函数
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO25-1)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO25-1)'
- en: Makes a DataFrame containing the rows with `born_in` fields.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 创建包含具有 `born_in` 字段的行的 DataFrame。
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO25-2)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO25-2)'
- en: Removes duplicates from the DataFrame after randomizing the row order.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机化行顺序后，从 DataFrame 中删除重复项。
- en: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO25-3)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO25-3)'
- en: Converts the date columns to the practical `datetime64` datatype.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 将日期列转换为实用的 `datetime64` 数据类型。
- en: '[![4](assets/4.png)](#co_cleaning_data_with_pandas_CO25-4)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_cleaning_data_with_pandas_CO25-4)'
- en: We return a DataFrame with the deleted `born_in` field; this data will provide
    an interesting visualization in the next chapter.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个删除了 `born_in` 字段的 DataFrame；这些数据将在下一章节的可视化中提供有趣的展示。
- en: Adding the born_in column
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加 `born_in` 列
- en: 'While cleaning the winners DataFrame we removed the `born_in` column (see [“Removing
    Rows”](#removing_rows)). As we’ll see in the next chapter, this column has some
    interesting data that can be correlated with the winners’ country (of prize-winning
    origin) to tell an interesting story or two. The `clean_data` function returns
    the `born_in` data as a DataFrame. Let’s see how we can add this data to our freshly
    cleaned DataFrame. First, we’ll read in our original, dirty dataset and apply
    our data cleaning function:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在清理获奖者 DataFrame 时，我们移除了 `born_in` 列（请参阅 [“删除行”](#removing_rows)）。如我们将在下一章看到的那样，该列包含一些与获奖者国家（获奖来源国）相关的有趣数据，可以讲述一两个有趣的故事。`clean_data`
    函数返回 `born_in` 数据作为一个 DataFrame。让我们看看如何将这些数据添加到我们刚刚清理过的 DataFrame 中。首先，我们将读取我们的原始脏数据集，并应用我们的数据清理函数：
- en: '[PRE58]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now we’ll clean up the name field of the `df_born_in` DataFrame by removing
    the asterisks, stripping any whitespace and then removing any duplicate rows by
    name. Finally, we’ll set the index of the DataFrame to its name column:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将清理 `df_born_in` DataFrame 的名称字段，方法是删除星号，去除任何空白，然后通过名称删除任何重复行。最后，我们将 DataFrame
    的索引设置为其名称列：
- en: '[PRE59]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We now have a `df_born_in` DataFrame that we can query by name:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个可以通过名称查询的 `df_born_in` DataFrame：
- en: '[PRE60]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now we’ll write a little Python function to return the `born_in` field by name
    of our `df_born_in` DataFrame if it exists, otherwise returning a NumPy `nan`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写一个小的 Python 函数，如果存在于我们的 `df_born_in` DataFrame 的名称，将返回`born_in`字段，否则返回
    NumPy 的`nan`：
- en: '[PRE61]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We can now create a `born_in` column to our main DataFrame by applying this
    `get_born_in` function to each row, using the name field:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将 `get_born_in` 函数应用于每一行的名称字段，向我们的主 DataFrame 添加一个`born_in`列：
- en: '[PRE62]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Finally, let’s make sure we’ve successfully added a `born_in` column to our
    DataFrame:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保我们已成功向 DataFrame 添加了 `born_in` 列：
- en: '[PRE63]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Note that if there were no duplicate names among our Nobel winners, we could
    create the `born_in` column by simply setting the index of `df` and `df_born_in`
    to *name* and creating the column directly:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们的诺贝尔获奖者中没有重复的名称，我们可以通过简单地将 `df` 和 `df_born_in` 的索引设置为 *name* 并直接创建列来创建
    `born_in` 列：
- en: '[PRE64]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The use of `apply` can be inefficient for large datasets, but it provides a
    very flexible way of creating new columns based on the existing ones.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `apply` 处理大型数据集可能效率低下，但它提供了一种非常灵活的方式来基于现有列创建新列。
- en: Merging DataFrames
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并 DataFrame
- en: 'At this point, we can also create a merged database of our clean `winners`
    data and the image and biography dataset we scraped in [“Scraping Text and Images
    with a Pipeline”](ch06.xhtml#scraping_bio). This will provide a good opportunity
    to demonstrate pandas’s ability to merge DataFrames. The following code shows
    how to merge `df_clean` and the bio dataset:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们还可以创建一个合并后的数据库，将我们清理过的 `winners` 数据与我们在 [“使用管道抓取文本和图像”](ch06.xhtml#scraping_bio)
    中抓取的图像和传记数据集合并。这将展示 pandas 合并 DataFrame 的能力。以下代码展示了如何合并 `df_clean` 和生物数据集：
- en: '[PRE65]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO26-1)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO26-1)'
- en: '[pandas’s `merge`](https://oreil.ly/T3ujJ) takes two DataFrames and merges
    them based on shared column name(s) (`link`, in this case). The `how` argument
    specifies how to determine which keys are to be included in the resulting table
    and works in the same way as SQL joins. In this case, `outer` specifies a `FULL_OUTER_JOIN`.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[pandas的`merge`](https://oreil.ly/T3ujJ)接受两个DataFrame，并根据共享的列名（在本例中为`link`）将它们合并。`how`参数指定了如何确定哪些键应包含在结果表中，并且与SQL连接的方式相同。在本例中，`outer`指定了`FULL_OUTER_JOIN`。'
- en: 'Merging the two DataFrames results in redundancies in our merged dataset, with
    more than the 858 winning rows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 合并这两个DataFrame会导致我们合并数据集中的冗余数据，超过了858个获奖行：
- en: '[PRE66]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We can easily remove these by using `drop_duplicates` to remove any rows that
    share a `link` and `year` field after removing any rows without a `name` field:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地使用`drop_duplicates`来删除任何在删除没有`name`字段的行之后共享`link`和`year`字段的行：
- en: '[PRE67]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'A quick count shows that we now have the right number of winners with images
    for 770 and a `mini_bio` for all but one:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 快速计数显示，我们现在有770位获奖者的正确图像和一个`mini_bio`：
- en: '[PRE68]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'While we’re cleaning our dataset, let’s see which winner is missing a `mini_bio`
    field:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们清理数据集时，让我们看看哪个获奖者缺少`mini_bio`字段：
- en: '[PRE69]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: It turns out to be a Unicode error in creating the Wikipedia link for Lê Ðức
    Thọ, the Vietnamese Peace Prize winner. This can be corrected by hand.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 原来是在创建越南和平奖获得者Lê Ðức Thọ的维基百科链接时出现了Unicode错误。这可以手工修正。
- en: 'The `df_clean_bios` DataFrame includes an array of image URLs we scraped from
    Wikipedia. We won’t be using these, and they would have to be converted to JSON
    in order to save them to SQL. Let’s drop the `images_url` column to make our dataset
    as uncluttered as possible:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`df_clean_bios` DataFrame包含了我们从维基百科爬取的一系列图片URL。我们不会使用这些图片，并且它们必须转换为JSON格式才能保存到SQL中。让我们删除`images_url`列，以尽可能使我们的数据集清晰：'
- en: '[PRE70]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Now our dataset is cleaned and streamlined, let’s save it in a couple of handy
    formats.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据集已经清理和简化，让我们以几种方便的格式保存它。
- en: Saving the Cleaned Datasets
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存已清理的数据集
- en: Now we have the datasets required for our upcoming exploration with pandas,
    let’s save them in a couple of formats ubiquitous in data visualization, SQL and
    JSON.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了用于即将进行的pandas探索所需的数据集，让我们以几种数据可视化中常见的格式，即SQL和JSON格式，保存它们。
- en: 'First, we’ll save our cleaned DataFrame with `born_in` field and merged biographies
    as a JSON file using pandas’s handy `to_json` method:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用pandas方便的`to_json`方法将带有`born_in`字段和合并生物的清理DataFrame保存为JSON文件：
- en: '[PRE71]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO27-1)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO27-1)'
- en: We set the `orient` argument to `records` to store an array of row-objects and
    specify `'iso'` as the string encoding for our date format.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`orient`参数设置为`records`以存储一组行对象，并指定`'iso'`作为我们日期格式的字符串编码。
- en: 'Let’s save a copy of our clean DataFrame to an SQLite `nobel_prize` database
    in a local *data* directory. We’ll use this to demonstrate the Flask-based REST
    web API in [Chapter 13](ch13.xhtml#chapter_delivery_restful). Three lines of Python
    and the DataFrame’s `to_sql` method do the job succinctly (see [“SQL”](ch08.xhtml#sect_pandas_sql)
    for more details):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们干净的DataFrame副本保存到本地*data*目录中的SQLite `nobel_prize`数据库中。我们将使用这个数据库来演示第13章中基于Flask的REST
    Web API（详见[ch13.xhtml#chapter_delivery_restful](ch13.xhtml#chapter_delivery_restful)）。三行Python代码和DataFrame的`to_sql`方法可以简洁地完成这项工作（详见[“SQL”](ch08.xhtml#sect_pandas_sql)获取更多细节）：
- en: '[PRE72]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Let’s make sure we’ve successfully created the database by reading the contents
    back into a DataFrame:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将内容重新读入DataFrame来确保我们已成功创建了数据库：
- en: '[PRE73]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: With our cleaned data in the database, we’re ready to start exploring it in
    the next chapter.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们清理后的数据在数据库中，我们准备在下一章中开始探索它。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to clean a fairly messy dataset, producing
    data that will be much nicer to explore and generally work with. Along the way,
    a number of new pandas methods and techniques were introduced to extend the last
    chapter’s introduction to basic pandas.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您学习了如何清理一个相当混乱的数据集，产生了更容易探索和处理的数据。在此过程中，引入了许多新的pandas方法和技术，以扩展上一章对基本pandas的介绍。
- en: In the next chapter, we will use our newly minted dataset to start getting a
    feel for the Nobel Prize recipients, their country, gender, age, and any interesting
    correlations (or lack thereof) we can find.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用新创建的数据集来开始了解诺贝尔奖获得者，他们的国家、性别、年龄以及我们可以找到的任何有趣的相关性（或其缺乏）。
- en: ^([1](ch09.xhtml#idm45607776705472-marker)) *Large* is a very relative term,
    but pandas will take pretty much whatever will fit in your computer’s RAM memory,
    which is where DataFrames live.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.xhtml#idm45607776705472-marker)) *Large* 是一个非常相对的术语，但 pandas 将几乎任何能适合计算机
    RAM 内存的东西，这就是 DataFrame 所在的地方。
- en: ^([2](ch09.xhtml#idm45607775813456-marker)) pandas supports multiple indices
    using the `MultiIndex` object. This provides a very powerful way of refining higher-dimensional
    data. Check out the details [in the pandas documentation](https://oreil.ly/itwDR).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.xhtml#idm45607775813456-marker)) pandas 支持使用 `MultiIndex` 对象的多个索引。这提供了一种非常强大的方式来细化高维数据。在
    [pandas 文档](https://oreil.ly/itwDR) 中查看详情。
- en: ^([3](ch09.xhtml#idm45607774963264-marker)) By default, pandas uses NumPy’s
    `NaN` (not a number) float to designate missing values.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch09.xhtml#idm45607774963264-marker)) 默认情况下，pandas 使用 NumPy 的 `NaN`（不是一个数字）浮点数来表示缺失值。
- en: ^([4](ch09.xhtml#idm45607774530976-marker)) One interesting visualization might
    be charting the migration of Nobel Prize winners from their homelands.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch09.xhtml#idm45607774530976-marker)) 一个有趣的可视化可能是绘制诺贝尔奖获得者从祖国迁徙的图表。
- en: ^([5](ch09.xhtml#idm45607774527728-marker)) See IEEE 754 and [Wikipedia](https://oreil.ly/5H3q2).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch09.xhtml#idm45607774527728-marker)) 参见 IEEE 754 和 [维基百科](https://oreil.ly/5H3q2)。
- en: ^([6](ch09.xhtml#idm45607774438432-marker)) As you’ll see in the next chapter,
    the `born_in` fields contain some interesting information about the movements
    of Nobel Prize winners. We’ll see how to add this data to the cleaned dataset
    at the end of this chapter.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch09.xhtml#idm45607774438432-marker)) 正如你将在下一章中看到的那样，`born_in` 字段包含有关诺贝尔奖获得者移动的一些有趣信息。我们将在本章末尾看到如何将这些数据添加到清理后的数据集中。
- en: ^([7](ch09.xhtml#idm45607773684544-marker)) While France was Curie’s adopted
    country, she retained Polish citizenship and named her first discovered radioactive
    isotope *polonium* after her home country.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch09.xhtml#idm45607773684544-marker)) 尽管法国是居里夫人的养母国，但她保留了波兰国籍，并将她首次发现的放射性同位素命名为
    *钋*，以纪念她的祖国。
- en: ^([8](ch09.xhtml#idm45607773466112-marker)) Some users dismiss such warnings
    as nannying paranoia. See the discussion [on Stack Overflow](https://oreil.ly/b7G9r).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch09.xhtml#idm45607773466112-marker)) 一些用户将此类警告视为多管闲事的偏执狂。参见 [Stack Overflow
    上的讨论](https://oreil.ly/b7G9r)。
- en: '^([9](ch09.xhtml#idm45607773464480-marker)) They can be turned off with `pd.options.mode.chained_assignment
    = None # default=*warn*`.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch09.xhtml#idm45607773464480-marker)) 可以通过 `pd.options.mode.chained_assignment
    = None # default=*warn*` 来关闭它们。'
- en: ^([10](ch09.xhtml#idm45607772960432-marker)) Depending on the dataset, the cleaning
    phase is unlikely to catch all transgressors.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch09.xhtml#idm45607772960432-marker)) 根据数据集，清理阶段不太可能捕捉到所有违规者。
- en: ^([11](ch09.xhtml#idm45607772538016-marker)) Although Granit’s gender is not
    specified in the person data, his [Wikipedia biography](https://oreil.ly/PxUns)
    uses the male gender.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch09.xhtml#idm45607772538016-marker)) 虽然格兰尼特的性别在人物数据中未指定，但他的 [维基百科传记](https://oreil.ly/PxUns)
    使用了男性性别。
