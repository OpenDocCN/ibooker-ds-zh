- en: Chapter 9\. Cleaning Data with pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous two chapters introduced pandas and NumPy, the Numeric Python library
    it extends. Armed with basic pandas know-how, we’re ready to start the cleaning
    stage of our toolchain, aiming to find and eliminate the dirty data in our scraped
    dataset (see [Chapter 6](ch06.xhtml#chapter_heavy_scraping)). This chapter will
    also extend your pandas knowledge, introducing new methods in a working context.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 8](ch08.xhtml#chapter_intro_to_pandas), we covered the core components
    of pandas: the DataFrame, a programmatic spreadsheet capable of dealing with the
    many different datatypes found in the real world, and its building block, the
    Series, a heterogeneous extension of NumPy’s homogeneous `ndarray`. We also covered
    how to read from and write to different datastores, including JSON, CSV files,
    MongoDB, and SQL databases. Now we’ll start to put pandas through its paces, showing
    how it can be used to clean dirty data. I’ll introduce the key elements of data
    cleaning using our dirty Nobel Prize dataset as an example.'
  prefs: []
  type: TYPE_NORMAL
- en: I’ll take it slowly, introducing key pandas concepts in a working environment.
    Let’s first establish why cleaning data is such an important part of a data visualizer’s
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Coming Clean About Dirty Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think it’s fair to say that most people entering the field of data visualization
    underestimate, often by a fairly large factor, the amount of time they’re going
    to spend trying to make their data presentable. The fact is that getting clean
    datasets that are a pleasure to transform into cool visualizations could well
    take over half your time. Data in the wild is very rarely pristine, often bearing
    the sticky paw prints of mistaken manual data entry, missing whole fields due
    to oversight or parsing errors and/or mixed datetime formats.
  prefs: []
  type: TYPE_NORMAL
- en: For this book, and to pose a properly meaty challenge, our Nobel Prize dataset
    has been scraped from Wikipedia, a manually edited website with fairly informal
    guidelines. In this sense, the data is bound to be dirty—​humans make mistakes
    even when the environment is a good deal more forgiving. But even data from the
    official APIs of, for example, large social media sites, is often flawed, with
    missing or incomplete fields, scar tissue from countless changes to the data schemas,
    deliberate mis-entry, and the like.
  prefs: []
  type: TYPE_NORMAL
- en: So cleaning data is a fundamental part of the job of a data visualizer, stealing
    time from all the cool stuff you’d rather be doing—​which is an excellent reason
    to get really good at it and free up that drudge time for more meaningful pursuits.
    And a large part of getting good at cleaning data is choosing the right toolset,
    which is where pandas comes in. It’s a great way to slice and dice even fairly
    large datasets,^([1](ch09.xhtml#idm45607776705472)) and being comfortable with
    it could save you a lot of time. That is where this chapter comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, scraping the Nobel data from Wikipedia using Python’s Scrapy library
    (see [Chapter 6](ch06.xhtml#chapter_heavy_scraping)) produced an array of JSON
    objects of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The job of this chapter is to turn that array into as clean a data source as
    possible before we explore it with pandas in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many forms of dirty data, most commonly:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicate entries/rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misaligned rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corrupted fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed datatypes in a column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll now probe our Nobel Prize data for these kinds of anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to load our JSON data into a DataFrame, as shown in the previous
    chapter (see [“Creating and Saving DataFrames”](ch08.xhtml#load_save_dataframe)).
    We can open the JSON data file directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve got our dirty scraped data into a DataFrame, let’s get a broad
    overview of what we have.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pandas DataFrame has a number of methods and properties that give a quick
    overview of the data contained within. The most general is `info`, which gives
    a neat summary of the number of data entries by column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can see that some fields are missing entries. For example, although there
    are 1,052 rows in our DataFrame, there are only 1,040 gender attributes. Note
    also the handy `memory_usage`—pandas DataFrames are held in RAM, so as datasets
    increase in size, this number gives a nice indication of how close we are to our
    machine-specific memory limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'DataFrame’s `describe` method gives a handy statistical summary of relevant
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, by default only numerical columns are described. Already we
    can see an error in the data, the minimum year being 1809, which is impossible
    when the first Nobel was awarded in 1901.
  prefs: []
  type: TYPE_NORMAL
- en: '`describe` takes an `include` parameter that allows us to specify the column
    datatypes (`dtype`s) to be assessed. Other than year, the columns in our Nobel
    Prize dataset are all objects, which are pandas’s default, catchall `dtype`, capable
    of representing any numbers, strings, data times, and more. [Example 9-1](#describe_objects)
    shows how to get their stats.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. Describing the DataFrame
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `include` argument is a list (or single item) of columnar `dtype`s to summarize.
  prefs: []
  type: TYPE_NORMAL
- en: There’s quite a lot of useful information to be gleaned from the output of [Example 9-1](#describe_objects),
    such as that there are 59 unique nationalities with the United States, at 350,
    being the largest group.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting tidbit is that of 1,044 recorded dates of birth, only 853 are
    unique, which could mean any number of things. Possibly some auspicious days saw
    the birth of more than one laureate or, wearing our data-cleaning hats, it’s more
    likely that there are some duplicated winners or that some dates are wrong or
    have only recorded the year. The duplicated winners hypothesis is confirmed by
    the observation that of 1,052 name counts, only 998 are unique. Now there have
    been a few multiple winners but not enough to account for 54 duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame’s `head` and `tail` methods provide another easy way to get a quick
    feel for the data. By default, they display the top or bottom five rows, but we
    can set that number by passing an integer as the first argument. [Example 9-2](#dataframe_head)
    shows the result of using `head` with our Nobel DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. Sampling the first five DataFrame rows
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: These rows have an entry for the `born_in` field and an asterisk by their name.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `date_of_death` field has a different time format than the other rows.
  prefs: []
  type: TYPE_NORMAL
- en: The first five winners in [Example 9-2](#dataframe_head) show a couple of useful
    things. First, we see the names in rows 1 and 2 are marked by an asterisk and
    have an entry in the `born_in` field ![1](assets/1.png). Second, note that row
    2 has a different time format for `date_of_death` than the others, and that there
    are both month-day and day-month time formats in the `date_of_birth` field ![2](assets/2.png).
    This kind of inconsistency is a perennial problem for human-edited data, particularly
    dates and times. We’ll see how to fix it with pandas later.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-1](#describe_objects) gives an object count of 1,052 for the `born_in`
    field, indicating no empty fields, but `head` shows only rows 1 and 2 have content.
    This suggests that the missing fields are an empty string or space, both of which
    count as data to pandas. Let’s change them to a noncounted `NaN`, which will make
    more sense of the numbers. But first we’re going to need a little primer in pandas
    data selection.'
  prefs: []
  type: TYPE_NORMAL
- en: Indices and pandas Data Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before beginning to clean our data, let’s do a quick recap of basic pandas data
    selection, using the Nobel Prize dataset as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas indexes by rows and columns. Usually column indices are specified by
    the data file, SQL table, and so on, but, as shown in the last chapter, we can
    set or override these when the DataFrame is created by using the `names` argument
    to pass a list of column names. The columns index is accessible as a DataFrame
    property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: By default, pandas specifies a zero-base integer index for the rows, but we
    can override this by passing a list in the `index` parameter on creation of the
    DataFrame or afterward by setting the `index` property directly. More often we
    want to use one or more^([2](ch09.xhtml#idm45607775813456)) of the DataFrame’s
    columns as an index. We can do this using the `set_index` method. If you want
    to return to the default index, you can use the `reset_index` method, as shown
    in [Example 9-3](#settingDFindex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. Setting the DataFrame’s index
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Sets the frame’s index to its name column. Set the result back to `df`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The rows are now indexed by name.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Resets the index to its integer. Note that we change it in place this time.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_cleaning_data_with_pandas_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The index is now by integer position.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are two ways to change a pandas DataFrame or Series: by altering the
    data in place or by assigning a copy. There is no guarantee that in place is faster,
    plus method-chaining requires that the operation return a changed object. Generally,
    I use the `df = df.foo(...)` form, but most mutating methods have an `inplace`
    argument `df.foo(..., inplace=True)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the row-columnar indexing system, let’s start selecting
    slices of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can select a column of the DataFrame by dot notation (where no spaces or
    special characters are in the name) or square-bracket notation. Let’s take a look
    at that `born_in` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that the column selection returns a pandas Series, with the DataFrame indexing
    preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'DataFrames and Series share the same methods for accessing rows/members. `iloc`
    selects by integer position, and `loc` selects by label. Let’s use `iloc` to grab
    the first row of our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Selecting Multiple Rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Standard Python array slicing can be used with a DataFrame to select multiple
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The standard way to select multiple rows based on a conditional expression
    (e.g., is the value of the column `value` greater than `x`) is to create a Boolean
    mask and use it in a selector. Let’s find all the Nobel Prize winners after the
    year 2000\. First, we create a mask by performing a Boolean expression on each
    of the rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`True` for all rows where the `year` field is greater than 2000.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting Boolean mask shares our DataFrame’s index and can be used to
    select all `True` rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This will return a DataFrame containing only those rows where the Boolean `mask`
    array is `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boolean masking is a very powerful technique capable of selecting any subset
    of the data you need. I recommend setting a few targets to practice constructing
    the right Boolean expressions. Generally, we dispense with the intermediate mask
    creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we can select individual and multiple rows by slicing or using a Boolean
    mask, in the next sections we’ll see how we can change our DataFrame, purging
    it of dirty data as we go.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to access our data, let’s see how we can change it for
    the better, starting with what looks like empty `born_in` fields we saw in [Example 9-2](#dataframe_head).
    If we look at the count of the `born_in` columns, it doesn’t show any missing
    rows, which it would were any fields missing or `NaN` (not a number):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Finding Mixed Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Note that pandas stores all string-like data using the `dtype` object. A cursory
    inspection suggests that the column is a mixture of empty and country-name strings.
    We can quickly check that all the column members are strings by mapping the Python
    [`type` function](https://oreil.ly/jj9hY) to all members using the `apply` method
    and then making a set of the resulting list of column members by type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This shows that all of the `born_in` column members are strings of type `str`.
    Now let’s replace any empty strings with an empty field.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing Strings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want to replace these empty strings with a `NaN`, to prevent them being
    counted.^([3](ch09.xhtml#idm45607774963264)) The pandas `replace` method is tailor-made
    for this and can be applied to the whole DataFrame or individual Series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Our empty `''` strings have been replaced with NumPy’s `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the empty strings, the `NaN` fields are discounted.
  prefs: []
  type: TYPE_NORMAL
- en: After replacing the empty strings with `NaN`s, we get a true count of 142 for
    the `born_in` field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s replace all empty strings in our DataFrame with discounted `NaN`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: pandas allows sophisticated replacement of strings (and other objects) in columns
    (e.g., allowing you to craft [regular expressions or regexes](https://oreil.ly/KK3b2),
    which are applied to whole Series, typically DataFrame columns). Let’s look at
    a little example, using the asterisk-marked names in our Nobel Prize DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 9-2](#dataframe_head) showed that some of our Nobel Prize names are
    marked with an asterisk, denoting that these winners are recorded by country of
    birth, not country at the time of winning the prize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let’s set ourselves the task of cleaning up those names by removing the asterisks
    and stripping any remaining whitespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas Series have a handy `str` member, which provides a number of useful
    string methods to be performed on the array. Let’s use it to check how many asterisked
    names we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use `str`’s `contains` method on the `name` column. Note that we have to
    escape the asterisk (`'\*'`) as this is a regex string. The Boolean mask is then
    applied to our Nobel Prize DataFrame and the resulting names listed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 142 of our 1,052 rows have a name containing `*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clean up the names, let’s replace the asterisks with an empty string and
    strip any whitespace from the resulting names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Removes all asterisks in the name fields and return the result to the DataFrame.
    Note that we have to explicitly set the `regex` flag to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick check shows that the names are now clean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: pandas Series have an impressive number of string-handling functions, enabling
    you to search and adapt your string columns. You can find a full list of these
    [in the API docs](https://oreil.ly/2mCSZ).
  prefs: []
  type: TYPE_NORMAL
- en: Removing Rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To recap, the 142 winners with `born_in` fields are duplicates, having an entry
    in the Wikipedia biography page by both the country they were born in and their
    country when given the prize. Although the former could form the basis of an interesting
    visualization,^([4](ch09.xhtml#idm45607774530976)) for our visualization we want
    each individual prize represented once only and so need to remove these from our
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to create a new DataFrame using only those rows with a `NaN` `born_in`
    field. You might naively assume that a conditional expression comparing the `born_in`
    field to `NaN` would work here, but by definition^([5](ch09.xhtml#idm45607774527728))
    `NaN` Boolean comparisons always return `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, pandas provides the dedicated `isnull` method to check for discounted
    (null) fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`isnull` produces a Boolean mask with `True` for all rows with an empty `born_in`
    field.'
  prefs: []
  type: TYPE_NORMAL
- en: The `born_in` column is no longer of use, so let’s remove it for now:^([6](ch09.xhtml#idm45607774438432))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`drop` takes a single label or index (or list of same) as a first argument
    and an `axis` argument to indicate row (`0` and default) or column (`1`) index.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding Duplicates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, a quick internet search shows that 889 people and organizations have received
    the Nobel Prize up to 2015\. With 910 remaining rows, we still have a few duplicates
    or anomalies to account for.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas has a handy `duplicated` method for finding matching rows. This matches
    by column name or list of column names. Let’s get the list of all duplicates by
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`duplicated` returns a Boolean array with `True` for the first occurrence of
    any rows with the same `name` field.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, a few people have won the Nobel Prize more than once but not 46, which
    means 40-odd winners are duplicated. Given that the Wikipedia page we scraped
    listed prize winners by country, the best bet is winners being “claimed” by more
    than one country.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of the ways we can find duplicates by name in our Nobel Prize
    DataFrame. Some of these are pretty inefficient, but it’s a nice way to demonstrate
    a few pandas functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `duplicated` indicates (Boolean `True`) all duplicates after the
    first occurrence, but setting the `keep` option to `*last*` sets the first occurrence
    of duplicated rows to `True`. By combining these two calls using a Boolean *or*
    (|), we can get the full list of duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also get all the duplicates by testing whether our DataFrame rows
    have a name in the list of duplicate names. pandas has a handy `isin` method for
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`dupes_by_name.name` is a column Series containing all the duplicated names.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also find all duplicates by using pandas’s powerful `groupby` method,
    which groups our DataFrame’s rows by column or list of columns. It returns a list
    of key-value pairs with the column value(s) as key and list of rows as values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO13-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`groupby` returns an iterator of (group name, group) tuples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get all duplicate rows, we merely need to check the length of the
    list of rows returned by key. Anything greater than one has name duplicates. Here
    we use pandas’s `concat` method, which takes a list of row lists and creates a
    DataFrame with all the duplicated rows. A Python list constructor is used to filter
    for groups with more than one row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO14-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Python list by filtering the `name` row groups for those with more
    than one row (i.e., duplicated names).
  prefs: []
  type: TYPE_NORMAL
- en: Different Paths to the Same Goal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a large library like pandas, there are usually a number of ways to achieve
    the same thing. With small datasets like our Nobel Prize winners, any one will
    do, but for large datasets there could be significant performance implications.
    Just because pandas will do what you ask doesn’t mean it’s necessarily efficient.
    With a lot of complex data manipulation going on behind the scenes, it’s a good
    idea to be prepared to be flexible and alert to inefficient approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our `all_dupes` DataFrame, with all duplicated rows by name,
    let’s use it to demonstrate pandas’s `sort` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas provides a sophisticated `sort` method for the DataFrame and Series
    classes, capable of sorting on multiple column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO15-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Sorts the DataFrame first by name, then by score within those subgroups. Older
    pandas versions use `sort`, now deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO15-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Sorts the names in alphabetical ascending order; sorts scores from high to low.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s sort the DataFrame of `all_dupes` by name and then look at the name,
    country, and year columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This output shows that, as expected, some winners have been attributed twice
    for the same year with different countries. It also reveals a few other anomalies.
    Although Marie Curie did win a Nobel Prize twice, she’s included here with both
    French and Polish nationalities.^([7](ch09.xhtml#idm45607773684544)) The fairest
    thing here is to split the spoils between Poland and France while settling on
    the single compound surname. We have also found our anomalous year of 1809 at
    row 960\. Sidney Altman is both duplicated and given the wrong year of 1990.
  prefs: []
  type: TYPE_NORMAL
- en: Removing Duplicates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s go about removing the duplicates we just identified and start compiling
    a little cleaning function.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Changing rows by numeric index is fine if you know your dataset is stable and
    don’t anticipate running any of your cleaning scripts again. But if, as in the
    case of our scraped Nobel Prize data, you may want to run the same cleaning script
    on an updated dataset, it’s much better to use stable indicators (i.e., grab the
    row with name Marie Curie and year 1911, not index 919).
  prefs: []
  type: TYPE_NORMAL
- en: 'A more robust way of changing the country of a specific row is to use stable
    column values to select the row rather than its index. This means if the index
    value changes the cleaning script should still work. So to change Marie Curie’s
    1911 prize country to France, we can use a Boolean mask with the `loc` method
    to select a row and then set its country column to France. Note that we specify
    the Unicode for the Polish ł:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As well as changing Marie Curie’s country, we want to remove or drop some rows
    from our DataFrame, based on column values. There are two ways we can do this,
    firstly by using the DataFrame’s `drop` method, which takes a list of index labels,
    or by creating a new DataFrame with a Boolean mask that filters the rows we want
    to drop. If we use `drop`, we can use the `inplace` argument to change the existing
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we drop our duplicate Sidney Altman row by creating
    a DataFrame with the single row we want (remember, index labels are preserved)
    and passing that index to the `drop` method and changing the DataFrame in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way to remove the row is to use the same Boolean mask with a logical
    *not* (~) to create a new DataFrame with all rows except the one(s) we’re selecting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s add this change and all current modifications to a `clean_data` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We now have a mix of valid duplicates (those few multiple Nobel Prize winners)
    and those with dual country. For the purposes of our visualization, we want each
    prize to count only once, so we have to discard half the dual-country prizes.
    The easiest way is to use the `duplicated` method, but because we collected the
    winners alphabetically by country, this would favor those nationalities with first
    letters earlier in the alphabet. Short of a fair amount of research and debate,
    the fairest way seems to pick one out at random and discard it. There are various
    ways to do this, but the simplest is to randomize the order of the rows before
    using `drop_duplicates`, a pandas method that drops all duplicated rows after
    the first encountered or, with the `take_last` argument set to `True`, all before
    the last.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy has a number of very useful methods in its `random` module, of which
    `permutation` is perfect for randomizing the row index. This method takes an array
    (or pandas index) of values and shuffles them. We can then use the DataFrame `reindex`
    method to apply the shuffled result. Note that we drop those rows sharing both
    name and year, which will preserve the legitimate double winners with different
    years for their prizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO17-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a shuffled version of `df`’s index and reindex `df` with it.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO17-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Drop all duplicates sharing name and year.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO17-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Return the index to sorted-by-integer position.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our data wrangling has been successful, we should have only valid duplicates
    left, those vaunted double-prize winners. Let’s list the remaining duplicates
    to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO18-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We combine duplicates from the first with the last to get them all. If using
    an older version of pandas, you may need to use the argument `take_last=True`.
  prefs: []
  type: TYPE_NORMAL
- en: A quick internet check shows that we have the correct four double-prize winners.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming we’ve caught the unwanted duplicates,^([10](ch09.xhtml#idm45607772960432))
    let’s move on to other “dirty” aspects of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with Missing Fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see where we stand as far as *null* fields are concerned by counting
    our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO19-1)'
  prefs: []
  type: TYPE_NORMAL
- en: A missing category field
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO19-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Eight missing gender fields
  prefs: []
  type: TYPE_NORMAL
- en: 'We appear to be missing a `category` field, which suggests a data entry mistake.
    If you remember, while scraping our Nobel Prize data we checked the category against
    a valid list (see [Example 6-3](ch06.xhtml#scrapy_process_li)). One of them appears
    to have failed this check. Let’s find out which one it is by grabbing the row
    where the category field is null and showing its name and text columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We saved the original link text for our winners and, as you can see, Alexis
    Carrel was listed as winning the Nobel prize for `Medicine`, when it should have
    been `Physiology or Medicine`. Let’s correct that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also missing `gender` for eight winners. Let’s list them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'With the exception of Ragnar Granit, all these are genderless (missing person
    data) institutions. The focus of our visualization is on individual winners, so
    we’ll remove these while establishing Ragnar Granit’s gender^([11](ch09.xhtml#idm45607772538016)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see where those changes leave us by performing another count on our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Having removed all the institutions, all entries should have at least a date
    of birth. Let’s find the missing entry and fix it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Probably because Hiroshi Amano is a very recent (2014) winner, his date of
    birth was not available to be scraped. A quick web search establishes Amano’s
    date of birth, which we add to the DataFrame by hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have 858 individual winners. Let’s do a final count to see where we
    stand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The key fields of `category`, `date_of_birth`, `gender`, `country`, and `year`
    are all filled and there’s a healthy amount of data in the remaining stats. All
    in all, there’s enough clean data to form the basis for a rich visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s put on the finishing touches by making our temporal fields more usable.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with Times and Dates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Currently the `date_of_birth` and `date_of_death` fields are represented by
    strings. As we’ve seen, Wikipedia’s informal editing guidelines have led to a
    number of different time formats. Our original DataFrame shows an impressive variety
    of formats in the first 10 entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In order to compare the date fields (for example, subtracting the prize *year*
    from *date of birth* to give the winners’ ages), we need to get them into a format
    that allows such operations. Unsurprisingly, pandas is good with parsing messy
    dates and times, converting them by default into the NumPy `datetime64` object,
    which has a slew of useful methods and operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting a time column to `datetime64`, we use pandas’s `to_datetime` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO20-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `errors` default is `ignore`, but we want them flagged.
  prefs: []
  type: TYPE_NORMAL
- en: By default `to_datetime` ignores errors, but here we want to know if pandas
    has been unable to parse a `date_of_birth`, giving us the opportunity to fix it
    manually. Thankfully, the conversion passes without error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fix our DataFrame’s `date_of_birth` column before moving on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Running `to_datetime` on the `date_of_birth` field raises a `ValueError` and
    an unhelpful one at that, giving no indication of the entry that triggered it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'One naive way to find the bad dates would be to iterate through our rows of
    data, and catch and display any errors. pandas has a handy `iterrows` method that
    provides a row iterator. Combined with a Python `try-except` block, this successfully
    finds our problem date fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO21-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Run `to_datetime` on the individual row and catch any errors.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO21-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We left-justify the date of death in a text column of width 30 to make the output
    easier to read. pandas rows have a masking `Name` property, so we use string-key
    access with `['name']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This lists the offending rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: which is a good demonstration of the kind of data errors you get with collaborative
    editing.
  prefs: []
  type: TYPE_NORMAL
- en: Although the last method works, whenever you find yourself iterating through
    rows of a pandas DataFrame, you should pause for a second and try to find a better
    way, one that exploits the multirow array handling that is a fundamental aspect
    of pandas’s efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better way to find the bad dates exploits the fact that pandas’s `to_datetime`
    method has a `coerce` argument, which, if `True`, converts any date exceptions
    to `NaT` (not a time), the temporal equivalent of `NaN`. We can then create a
    Boolean mask out of the resulting DataFrame based on the `NaT` date rows, producing
    [Figure 9-1](#clean_dodgy_dates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO22-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Gets all rows with non-null date fields.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO22-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Creates a Boolean mask for all bad dates in `with_death_dates` by checking against
    null (`NaT`) after coercing failed conversions to `NaT`. For older pandas versions,
    you may need to use `coerce=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0901](assets/dpj2_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The unparseable date fields
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Depending on how fastidious you want to be, these can be corrected by hand
    or coerced to NumPy’s time equivalent of `NaN`, `NaT`. We’ve got more than 500
    valid dates of death, which is enough to get some interesting time stats, so we’ll
    run `to_datetime` again and force errors to null:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our time fields in a usable format, let’s add a field for
    the age of the winner on receiving his/her Nobel Prize. In order to get the year
    value of our new dates, we need to tell pandas that it’s dealing with a date column,
    using the `DatetimeIndex` method. Note that this gives a crude estimation of award
    age and may be off by a year. For the purposes of the next chapter’s dataviz exploration,
    this is more than adequate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO23-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the column to a `DatetimeIndex`, an `ndarray` of `datetime64` data,
    and use the `year` property.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use our new `award_age` field to see the youngest recipients of the Nobel
    Prize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO24-1)'
  prefs: []
  type: TYPE_NORMAL
- en: For activism for female education, I’d recommend reading more about [Malala’s
    inspirational story](https://oreil.ly/26szS).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our date fields in a manipulable form, let’s have a look at
    the full `clean_data` function, which summarizes this chapter’s cleaning efforts.
  prefs: []
  type: TYPE_NORMAL
- en: The Full clean_data Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For manually edited data like scraped Wikipedia datasets, it’s unlikely that
    you’ll catch all the errors on a first pass. So expect to pick up a few during
    the data exploration phase. Nevertheless, our Nobel Prize dataset is looking very
    usable. We’ll declare it clean enough and the job of this chapter done. [Example 9-4](#clean_data)
    shows the steps we used to achieve this cleaning feat.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-4\. The full Nobel Prize dataset cleaning function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO25-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Makes a DataFrame containing the rows with `born_in` fields.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning_data_with_pandas_CO25-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Removes duplicates from the DataFrame after randomizing the row order.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_cleaning_data_with_pandas_CO25-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Converts the date columns to the practical `datetime64` datatype.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_cleaning_data_with_pandas_CO25-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We return a DataFrame with the deleted `born_in` field; this data will provide
    an interesting visualization in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the born_in column
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While cleaning the winners DataFrame we removed the `born_in` column (see [“Removing
    Rows”](#removing_rows)). As we’ll see in the next chapter, this column has some
    interesting data that can be correlated with the winners’ country (of prize-winning
    origin) to tell an interesting story or two. The `clean_data` function returns
    the `born_in` data as a DataFrame. Let’s see how we can add this data to our freshly
    cleaned DataFrame. First, we’ll read in our original, dirty dataset and apply
    our data cleaning function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll clean up the name field of the `df_born_in` DataFrame by removing
    the asterisks, stripping any whitespace and then removing any duplicate rows by
    name. Finally, we’ll set the index of the DataFrame to its name column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a `df_born_in` DataFrame that we can query by name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll write a little Python function to return the `born_in` field by name
    of our `df_born_in` DataFrame if it exists, otherwise returning a NumPy `nan`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a `born_in` column to our main DataFrame by applying this
    `get_born_in` function to each row, using the name field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s make sure we’ve successfully added a `born_in` column to our
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if there were no duplicate names among our Nobel winners, we could
    create the `born_in` column by simply setting the index of `df` and `df_born_in`
    to *name* and creating the column directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The use of `apply` can be inefficient for large datasets, but it provides a
    very flexible way of creating new columns based on the existing ones.
  prefs: []
  type: TYPE_NORMAL
- en: Merging DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, we can also create a merged database of our clean `winners`
    data and the image and biography dataset we scraped in [“Scraping Text and Images
    with a Pipeline”](ch06.xhtml#scraping_bio). This will provide a good opportunity
    to demonstrate pandas’s ability to merge DataFrames. The following code shows
    how to merge `df_clean` and the bio dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO26-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[pandas’s `merge`](https://oreil.ly/T3ujJ) takes two DataFrames and merges
    them based on shared column name(s) (`link`, in this case). The `how` argument
    specifies how to determine which keys are to be included in the resulting table
    and works in the same way as SQL joins. In this case, `outer` specifies a `FULL_OUTER_JOIN`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Merging the two DataFrames results in redundancies in our merged dataset, with
    more than the 858 winning rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily remove these by using `drop_duplicates` to remove any rows that
    share a `link` and `year` field after removing any rows without a `name` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick count shows that we now have the right number of winners with images
    for 770 and a `mini_bio` for all but one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'While we’re cleaning our dataset, let’s see which winner is missing a `mini_bio`
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: It turns out to be a Unicode error in creating the Wikipedia link for Lê Ðức
    Thọ, the Vietnamese Peace Prize winner. This can be corrected by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `df_clean_bios` DataFrame includes an array of image URLs we scraped from
    Wikipedia. We won’t be using these, and they would have to be converted to JSON
    in order to save them to SQL. Let’s drop the `images_url` column to make our dataset
    as uncluttered as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Now our dataset is cleaned and streamlined, let’s save it in a couple of handy
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the Cleaned Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we have the datasets required for our upcoming exploration with pandas,
    let’s save them in a couple of formats ubiquitous in data visualization, SQL and
    JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll save our cleaned DataFrame with `born_in` field and merged biographies
    as a JSON file using pandas’s handy `to_json` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning_data_with_pandas_CO27-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We set the `orient` argument to `records` to store an array of row-objects and
    specify `'iso'` as the string encoding for our date format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s save a copy of our clean DataFrame to an SQLite `nobel_prize` database
    in a local *data* directory. We’ll use this to demonstrate the Flask-based REST
    web API in [Chapter 13](ch13.xhtml#chapter_delivery_restful). Three lines of Python
    and the DataFrame’s `to_sql` method do the job succinctly (see [“SQL”](ch08.xhtml#sect_pandas_sql)
    for more details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s make sure we’ve successfully created the database by reading the contents
    back into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: With our cleaned data in the database, we’re ready to start exploring it in
    the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to clean a fairly messy dataset, producing
    data that will be much nicer to explore and generally work with. Along the way,
    a number of new pandas methods and techniques were introduced to extend the last
    chapter’s introduction to basic pandas.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will use our newly minted dataset to start getting a
    feel for the Nobel Prize recipients, their country, gender, age, and any interesting
    correlations (or lack thereof) we can find.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.xhtml#idm45607776705472-marker)) *Large* is a very relative term,
    but pandas will take pretty much whatever will fit in your computer’s RAM memory,
    which is where DataFrames live.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.xhtml#idm45607775813456-marker)) pandas supports multiple indices
    using the `MultiIndex` object. This provides a very powerful way of refining higher-dimensional
    data. Check out the details [in the pandas documentation](https://oreil.ly/itwDR).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch09.xhtml#idm45607774963264-marker)) By default, pandas uses NumPy’s
    `NaN` (not a number) float to designate missing values.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch09.xhtml#idm45607774530976-marker)) One interesting visualization might
    be charting the migration of Nobel Prize winners from their homelands.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch09.xhtml#idm45607774527728-marker)) See IEEE 754 and [Wikipedia](https://oreil.ly/5H3q2).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch09.xhtml#idm45607774438432-marker)) As you’ll see in the next chapter,
    the `born_in` fields contain some interesting information about the movements
    of Nobel Prize winners. We’ll see how to add this data to the cleaned dataset
    at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch09.xhtml#idm45607773684544-marker)) While France was Curie’s adopted
    country, she retained Polish citizenship and named her first discovered radioactive
    isotope *polonium* after her home country.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch09.xhtml#idm45607773466112-marker)) Some users dismiss such warnings
    as nannying paranoia. See the discussion [on Stack Overflow](https://oreil.ly/b7G9r).
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch09.xhtml#idm45607773464480-marker)) They can be turned off with `pd.options.mode.chained_assignment
    = None # default=*warn*`.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch09.xhtml#idm45607772960432-marker)) Depending on the dataset, the cleaning
    phase is unlikely to catch all transgressors.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch09.xhtml#idm45607772538016-marker)) Although Granit’s gender is not
    specified in the person data, his [Wikipedia biography](https://oreil.ly/PxUns)
    uses the male gender.
  prefs: []
  type: TYPE_NORMAL
