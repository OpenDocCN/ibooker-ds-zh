- en: Chapter 3\. Text Matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 2](ch02.html#chapter_2), once our data is cleansed and
    consistently formatted, we can find matching entities by checking for exact matches
    between their data attributes. If the data is of high quality, and if the attribute
    values are nonrepetitive, then checking for equivalence is straightforward. However,
    this is rarely the case with real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: We can increase our likelihood of matching all relevant records by using *approximate*
    (often referred to as *fuzzy*) *matching techniques*. For numerical values, we
    can set a tolerance on how close the values need to be. For example, a date of
    birth might be matched if it’s within a few days or a location might be matched
    if its coordinates are within a certain distance apart. For textual data, we can
    look for similarities and differences between strings that could arise accidentally.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, by accepting nonexact matches as equivalent we open up the possibility
    of matching records incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce some frequently used text matching techniques
    and then apply them to our sample problem to see if this can improve our entity
    resolution performance.
  prefs: []
  type: TYPE_NORMAL
- en: Edit Distance Matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For matching text, one of the most useful approximate matching techniques is
    to measure the *edit distance* between two strings. The edit distance is the minimum
    number of operations to transform one string into the other. This metric can therefore
    be used to assess the likelihood that two strings do actually describe the same
    attribute, even if they were recorded differently.
  prefs: []
  type: TYPE_NORMAL
- en: The first, and most universally applicable, approximate matching technique we
    will consider is the Levenshtein distance.
  prefs: []
  type: TYPE_NORMAL
- en: Levenshtein Distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Levenshtein distance* is a well-known edit distance metric named after
    its creator, Soviet mathematician Vladimir Levenshtein.
  prefs: []
  type: TYPE_NORMAL
- en: The Levenshtein distance between two strings, a and b (of length |a| and |b|,
    respectively), is given by lev(a,b), where
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="l e v left-parenthesis a comma b right-parenthesis equals StartLayout
    Enlarged left-brace 1st Row  StartAbsoluteValue a EndAbsoluteValue if StartAbsoluteValue
    b EndAbsoluteValue equals 0 comma 2nd Row  StartAbsoluteValue b EndAbsoluteValue
    if StartAbsoluteValue a EndAbsoluteValue equals 0 comma 3rd Row  l e v left-parenthesis
    t a i l left-parenthesis a right-parenthesis comma t a i l left-parenthesis b
    right-parenthesis right-parenthesis if a left-bracket 0 right-bracket equals b
    left-bracket 0 right-bracket comma 4th Row  1 plus m i n StartLayout Enlarged
    left-brace 1st Row  l e v left-parenthesis t a i l left-parenthesis a right-parenthesis
    comma b right-parenthesis 2nd Row  l e v left-parenthesis a comma t a i l left-parenthesis
    b right-parenthesis right-parenthesis otherwise 3rd Row  l e v left-parenthesis
    t a i l left-parenthesis a right-parenthesis comma t a i l left-parenthesis b
    right-parenthesis right-parenthesis EndLayout EndLayout"><mrow><mi>l</mi> <mi>e</mi>
    <mi>v</mi> <mrow><mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mrow><mo>|</mo>
    <mi>a</mi> <mo>|</mo> <mtext>if</mtext> <mo>|</mo> <mi>b</mi> <mo>|</mo> <mo>=</mo>
    <mn>0</mn> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>|</mo>
    <mi>b</mi> <mo>|</mo> <mtext>if</mtext> <mo>|</mo> <mi>a</mi> <mo>|</mo> <mo>=</mo>
    <mn>0</mn> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>l</mi>
    <mi>e</mi> <mi>v</mi> <mo>(</mo> <mi>t</mi> <mi>a</mi> <mi>i</mi> <mi>l</mi> <mo>(</mo>
    <mi>a</mi> <mo>)</mo> <mo>,</mo> <mi>t</mi> <mi>a</mi> <mi>i</mi> <mi>l</mi> <mo>(</mo>
    <mi>b</mi> <mo>)</mo> <mo>)</mo> <mtext>if</mtext> <mi>a</mi> <mo>[</mo> <mn>0</mn>
    <mo>]</mo> <mo>=</mo> <mi>b</mi> <mo>[</mo> <mn>0</mn> <mo>]</mo> <mo>,</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mn>1</mn> <mo>+</mo> <mi>m</mi> <mi>i</mi>
    <mi>n</mi> <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mrow><mi>l</mi>
    <mi>e</mi> <mi>v</mi> <mo>(</mo> <mi>t</mi> <mi>a</mi> <mi>i</mi> <mi>l</mi> <mo>(</mo>
    <mi>a</mi> <mo>)</mo> <mo>,</mo> <mi>b</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mi>l</mi> <mi>e</mi> <mi>v</mi> <mo>(</mo> <mi>a</mi>
    <mo>,</mo> <mi>t</mi> <mi>a</mi> <mi>i</mi> <mi>l</mi> <mo>(</mo> <mi>b</mi> <mo>)</mo>
    <mo>)</mo> <mtext>otherwise</mtext></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>l</mi>
    <mi>e</mi> <mi>v</mi> <mo>(</mo> <mi>t</mi> <mi>a</mi> <mi>i</mi> <mi>l</mi> <mo>(</mo>
    <mi>a</mi> <mo>)</mo> <mo>,</mo> <mi>t</mi> <mi>a</mi> <mi>i</mi> <mi>l</mi> <mo>(</mo>
    <mi>b</mi> <mo>)</mo> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, the tail of some string x is a string of all but the first character of
    x, and x[n] is the nth character of the string x, counting from 0.
  prefs: []
  type: TYPE_NORMAL
- en: Opening the *Chapter3.ipynb* notebook, we can see how this works in practice.
    Fortunately, we don’t have to code the Levenshtein algorithm ourselves—the Jellyfish
    Python package has an implementation we can use. This library also contains a
    number of other fuzzy and phonetic string-matching functions.
  prefs: []
  type: TYPE_NORMAL
- en: Jellyfish
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Jellyfish](https://github.com/jamesturk/jellyfish) is a Python library for
    approximate and phonetic matching of strings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t have this package installed, you can use a Jupyter Notebook magic
    command `%pip` to install it before importing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Kernel Restart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After installing a new Python package, you may need to restart the kernel and
    rerun the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then calculate the edit distance metric to examine how a common misspelling
    error might be measured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Logically, the Levenshtein algorithm iterates character by character through
    the characters of the two strings, from first to last, incrementing the distance
    score if the characters do not match. In this case as the M, i, c, and h characters
    all match, the first time we increment the distance score is when we encounter
    the mismatch of letters a and e on the fifth character. At this point we then
    iterate through three variants of the remaining characters, selecting the minimum
    score between residual strings:'
  prefs: []
  type: TYPE_NORMAL
- en: “el” and “ael”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “ael” and “al”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “el” and “al”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three options also have a mismatch on the next character, incrementing the
    score again. Repeating the process on each option generates three more suboptions,
    the last of which is a simple match between the final “l” of each string, for
    a total minimum score of 2.
  prefs: []
  type: TYPE_NORMAL
- en: I leave it as an exercise for the reader to work through the remaining options,
    all of which produce the same score of 2.
  prefs: []
  type: TYPE_NORMAL
- en: Jaro Similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative method of assessing the similarity of strings was suggested by
    Matthew Jaro in 1989\. Wikipedia gives the formula as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The Jaro similarity *sim[j]* of two strings *s[1]* and *s[2]* is
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="s i m Subscript j Baseline equals StartLayout Enlarged left-brace
    1st Row  0 if m equals 0 2nd Row  one-third left-parenthesis StartFraction m Over
    StartAbsoluteValue s 1 EndAbsoluteValue EndFraction plus StartFraction m Over
    StartAbsoluteValue s 2 EndAbsoluteValue EndFraction plus StartFraction m minus
    t Over m EndFraction right-parenthesis otherwise EndLayout"><mrow><mi>s</mi> <mi>i</mi>
    <msub><mi>m</mi> <mi>j</mi></msub> <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd
    columnalign="left"><mrow><mn>0</mn> <mtext>if</mtext> <mi>m</mi> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mfrac><mn>1</mn> <mn>3</mn></mfrac> <mrow><mo>(</mo>
    <mfrac><mi>m</mi> <mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi> <mn>1</mn></msub>
    <mrow><mo>|</mo></mrow></mrow></mfrac> <mo>+</mo> <mfrac><mi>m</mi> <mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi>
    <mn>2</mn></msub> <mrow><mo>|</mo></mrow></mrow></mfrac> <mo>+</mo> <mfrac><mrow><mi>m</mi><mo>-</mo><mi>t</mi></mrow>
    <mi>m</mi></mfrac> <mo>)</mo></mrow> <mtext>otherwise</mtext></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '|*s[i]*| is the length of the string *s[i]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*m* is the number of matching characters (see below)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t* is the number of transpositions (see below)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaro similarity score is 0 if the strings do not match at all, and 1 if they
    are an exact match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first step, each character of *s[1]* is compared with all its matching
    characters in *s[2]*. Two characters from *s[1]* and *s[2]*, respectively, are
    considered matching only if they are the same and not farther than <math alttext="left
    floor StartFraction m a x left-parenthesis s 1 comma s 2 right-parenthesis Over
    2 EndFraction right floor minus 1"><mrow><mo>⌊</mo> <mfrac><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><msub><mi>s</mi>
    <mn>1</mn></msub> <mo>,</mo><msub><mi>s</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></mfrac> <mo>⌋</mo> <mo>-</mo> <mn>1</mn></mrow></math> characters apart.
    If no matching characters are found, then the strings are not similar and the
    algorithm terminates by returning Jaro similarity score 0\. If nonzero matching
    characters are found, the next step is to find the number of transpositions. Transposition
    is the number of matching characters that are not in the right order divided by
    two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again we can use the Jellyfish library to calculate this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the value is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartAbsoluteValue s 1 EndAbsoluteValue equals StartAbsoluteValue
    s 2 EndAbsoluteValue equals 7 left-parenthesis length of both strings right-parenthesis"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>s</mi> <mn>1</mn></msub> <mrow><mo>|</mo> <mo>=</mo> <mo>|</mo></mrow>
    <msub><mi>s</mi> <mn>2</mn></msub> <mrow><mo>|</mo> <mo>=</mo> <mn>7</mn> <mtext>(length</mtext>
    <mtext>of</mtext> <mtext>both</mtext> <mtext>strings)</mtext></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="m equals 7 left-parenthesis all characters match right-parenthesis"><mrow><mi>m</mi>
    <mo>=</mo> <mn>7</mn> <mtext>(all</mtext> <mtext>characters</mtext> <mtext>match)</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="t equals 1 left-parenthesis a and e transposition right-parenthesis"><mrow><mi>t</mi>
    <mo>=</mo> <mn>1</mn> <mtext>(a</mtext> <mtext>and</mtext> <mtext>e</mtext> <mtext>transposition)</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the Jaro similarity value is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals one-third left-parenthesis seven-sevenths plus seven-sevenths
    plus StartFraction left-parenthesis 7 minus 1 right-parenthesis Over 7 EndFraction
    right-parenthesis equals StartFraction 20 Over 21 EndFraction equals 0.9523809523809524
    period"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac> <mrow><mo>(</mo>
    <mfrac><mn>7</mn> <mn>7</mn></mfrac> <mo>+</mo> <mfrac><mn>7</mn> <mn>7</mn></mfrac>
    <mo>+</mo> <mfrac><mrow><mo>(</mo><mn>7</mn><mo>-</mo><mn>1</mn><mo>)</mo></mrow>
    <mn>7</mn></mfrac> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>20</mn> <mn>21</mn></mfrac>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>9523809523809524</mn> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In both the Levenshtein and Jaro methods all the characters in a string contribute
    equally to the score. Often, however, when matching names, the first few characters
    are more significant. Therefore, if they are the same, they are more likely to
    indicate equivalence. To recognize this, a modification of the Jaro similarity
    was proposed by William E. Winkler in 1990.
  prefs: []
  type: TYPE_NORMAL
- en: Jaro-Winkler Similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Jaro-Winkler similarity uses a prefix scale *p* that gives more favorable ratings
    to strings that match from the beginning for a set prefix length *l*. Given two
    strings *s[1]* and *s[2]* their Jaro-Winkler similarity *sim[w]* is *sim[w]* =
    *sim[j]* + *lp*(1 − *sim[j]*), where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sim[j]* is the Jaro similarity for *s*[1] and *s*[2].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*l* is the length of common prefix at the start of the string up to a maximum
    of four characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* is a constant scaling factor for how much the score is adjusted upward
    for having common prefixes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* should not exceed 0.25 (i.e., 1/4, with 4 being the maximum length of the
    prefix being considered); otherwise the similarity could become larger than 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard value for this constant in Winkler’s work is *p* = 0.1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using this metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'which is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals StartFraction 20 Over 21 EndFraction plus 4 times 0.1
    times left-parenthesis 1 minus StartFraction 20 Over 21 EndFraction right-parenthesis
    equals 0.9714285714285714"><mrow><mo>=</mo> <mfrac><mn>20</mn> <mn>21</mn></mfrac>
    <mo>+</mo> <mn>4</mn> <mo>×</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn> <mo>×</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mfrac><mn>20</mn> <mn>21</mn></mfrac> <mo>)</mo></mrow>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>9714285714285714</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sim[j]* = <math alttext="StartFraction 20 Over 21 EndFraction"><mfrac><mn>20</mn>
    <mn>21</mn></mfrac></math>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*l* = 4 (common prefix of “Mich”)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* = 0.1 (standard value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is worth noting that Jaro-Winkler similarity measures are case sensitive,
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A common practice is therefore to convert strings to lowercase before matching.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Phonetic Matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative to edit distance matching is to compare how similarly words are
    pronounced. Most of these phonetic algorithms are based on English pronunciation,
    of which two of the most popular are *Metaphone* and *Match Rating Approach* (*MRA*).
  prefs: []
  type: TYPE_NORMAL
- en: Metaphone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Metaphone algorithm encodes each word into a sequence of letters from the
    set of “0BFHJKLMNPRSTWXY” where 0 represents the “th” sound and X represents “sh”
    or “ch.” For example, using the Jellyfish package, we can see `'michael'` is reduced
    to `'MXL'` as is `'michel'`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This transformation produces a common key that can then be exact-matched to
    determine equivalence.
  prefs: []
  type: TYPE_NORMAL
- en: Match Rating Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MRA phonetic algorithm was developed in the late 1970s. Like Metaphone,
    it uses a set of rules to encode a word into a simplified phonetic representation.
    A set of comparison rules are then used to assess similarity, which is evaluated
    against a minimum threshold derived from their combined lengths to determine whether
    there is a match.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To compare the edit distance and phonetic similarity techniques, let’s examine
    how well they evaluate common misspellings and abbreviations of Michael:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This gives us the results shown in [Table 3-1](#table-3-1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Text matching comparison
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name1** | **Name2** | **Jaro** | **Jaro-Winkler** | **Levenshtein** | **Match
    rating** | **Metaphone** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Michael | Micheal | 0.952381 | 0.971429 | 2 | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| Michael | Michel | 0.952381 | 0.971429 | 1 | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| Michael | Mike | 0.726190 | 0.780952 | 4 | False | False |'
  prefs: []
  type: TYPE_TB
- en: '| Michael | Mick | 0.726190 | 0.808333 | 4 | True | False |'
  prefs: []
  type: TYPE_TB
- en: '| Micheal | Michel | 0.952381 | 0.971429 | 1 | True | True |'
  prefs: []
  type: TYPE_TB
- en: '| Micheal | Mike | 0.726190 | 0.780952 | 4 | False | False |'
  prefs: []
  type: TYPE_TB
- en: '| Micheal | Mick | 0.726190 | 0.780952 | 4 | True | False |'
  prefs: []
  type: TYPE_TB
- en: '| Michel | Mike | 0.750000 | 0.808333 | 3 | False | False |'
  prefs: []
  type: TYPE_TB
- en: '| Michel | Mick | 0.750000 | 0.825000 | 3 | True | False |'
  prefs: []
  type: TYPE_TB
- en: '| Mike | Mick | 0.833333 | 0.866667 | 2 | True | True |'
  prefs: []
  type: TYPE_TB
- en: As we can see just from this trivial example, there is a fair degree of consistency
    between the techniques but there is no single approach that is clearly superior
    in all cases. Many other string matching techniques have been developed that have
    their individuals strengths. For the purposes of this book, we will use the Jaro-Winkler
    algorithm because it performs well in matching names due to its bias toward the
    initial characters, which tend to be more significant. It is also widely supported
    in the data backends we will use.
  prefs: []
  type: TYPE_NORMAL
- en: Sample Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#chapter_2), we matched two lists of members of the
    UK House of Commons to explore a proposed correlation between social media presence
    and reelection. We used exact string matches to establish equivalence between
    the `Constituency`, `Firstname`, and `Lastname` attributes of the members.
  prefs: []
  type: TYPE_NORMAL
- en: We found 628 true positive matches. But differences between the names meant
    we didn’t find non-exact matches, giving us nine false negatives. Let’s see if
    by using a string similarity metric we can improve our performance. We start by
    loading the unmatched records that we saved in [Chapter 2](ch02.html#chapter_2),
    as shown in [Figure 3-1](#fig-3-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Unmatched population
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using the `apply` function, we can calculate the Jaro-Winkler similarity metric
    to compare the first names and last names between the two datasets. We use the
    Jaro-Winkler algorithm to take advantage of its better performance in matching
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can then apply a threshold of 0.8 on both `Firstname` and `Lastname` attributes,
    giving us six matches, as shown in [Figure 3-2](#fig-3-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Jaro-Winkler match population
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Not bad! We have now identified another six of the nine potential matches we
    previously missed. If we raise the threshold to 0.9 we would have found only two
    additional matches; if we lowered our threshold to 0.4, all would have matched
    all nine.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, in [Chapter 2](ch02.html#chapter_2), we used an exact match on
    constituency. Then, to identify the unmatched population, we selected those records
    where either the first name or last name did not match. This allowed us to differentiate
    between true negatives arising from by-elections and false negatives, where we
    needed a more flexible matching technique. However, we rarely have a high-cardinality
    categorical variable like constituency to help us, so we need to consider how
    we would match these entities on name only.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, we can no longer use the simple merge method on exact attribute
    matches to join our datasets. Instead, we need to manually construct a joint dataset
    with every possible combination of records and then apply our similarity function
    to each pair of first name and last name to see which are sufficiently similar.
    We can then discount those combinations with equivalence scores below a chosen
    threshold. Clearly, this method can result in a record from the first dataset
    matching with more than one record from the second.
  prefs: []
  type: TYPE_NORMAL
- en: Full Similarity Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Picking up the cleansed datasets from [Chapter 2](ch02.html#chapter_2), we
    can generate all the record combinations by using the cross-merge function. This
    generates a row for every name combination between the datasets, producing 650
    × 650 = 422,500 records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-3](#fig-3-3) shows the first few records in the cross-product dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Wikipedia, TheyWorkForYou cross-product
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can then calculate the Jaro-Winkler similarity metrics for both first name
    and last name on each row. Applying a threshold of 0.8 allows us to determine
    for each row whether these values approximately match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can then select records where both the `Firstname` and `Lastname` attributes
    are approximately equivalent to our prospective matches. We can check whether
    these are correct by using the `Constituency` attribute to verify our results.
    We know that when the constituency doesn’t match we aren’t referring to the same
    member of Parliament.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s see how many true positive matches we have now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'These true positives include the 628 exact matches from [Chapter 2](ch02.html#chapter_2)
    plus the 6 approximate matches we identified earlier. But let’s see how many false
    positives we have picked up where the name attributes are approximately equivalent
    but the `Constituency` doesn’t match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at a few of these 19 mismatching records in [Figure 3-4](#fig-3-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Full match false positives
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that there is a similarity between these names although they do not
    refer to the same individual. These mismatches are the price we pay for adopting
    a similarity match to maximize our number of true positives.
  prefs: []
  type: TYPE_NORMAL
- en: We can also examine the candidates we rejected by examining where the constituency
    matches but either the first name or last name doesn’t. We have to inspect these
    manually to determine whether they are true or false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-5](#fig-3-5) shows these 16 negative match records.'
  prefs: []
  type: TYPE_NORMAL
- en: Within the 16 negatives we can see the 13 by-election constituencies we declared
    as true negatives in [Chapter 2](ch02.html#chapter_2), plus 3 false negatives
    in the constituencies of Burton, South West Norfolk, and Newton Abbot, where the
    names are sufficiently different that their Jaro-Winkler match score falls below
    our 0.8 threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Full match true and false negatives
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Measuring Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let’s consider how our performance compares to the exact-only matching
    in [Chapter 2](ch02.html#chapter_2):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R e c a l l equals StartFraction upper T upper P Over left-parenthesis
    upper T upper P plus upper F upper N right-parenthesis EndFraction equals StartFraction
    634 Over left-parenthesis 634 plus 3 right-parenthesis EndFraction almost-equals
    99.2 percent-sign"><mrow><mi>R</mi> <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi>
    <mi>l</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>634</mn> <mrow><mo>(</mo><mn>634</mn><mo>+</mo><mn>3</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>99</mn> <mo>.</mo> <mn>2</mn> <mo>%</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P r e c i s i o n equals StartFraction upper T upper P
    Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction
    equals StartFraction 634 Over left-parenthesis 634 plus 19 right-parenthesis EndFraction
    almost-equals 97 percent-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>c</mi>
    <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>634</mn> <mrow><mo>(</mo><mn>634</mn><mo>+</mo><mn>19</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>97</mn> <mo>%</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A c c u r a c y equals StartFraction left-parenthesis upper
    T upper P plus upper T upper N right-parenthesis Over left-parenthesis upper T
    upper P plus upper T upper N plus upper F upper P plus upper F upper N right-parenthesis
    EndFraction equals StartFraction left-parenthesis 634 plus 13 right-parenthesis
    Over left-parenthesis 634 plus 13 plus 19 plus 3 right-parenthesis EndFraction
    almost-equals 96.7 percent-sign"><mrow><mi>A</mi> <mi>c</mi> <mi>c</mi> <mi>u</mi>
    <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>)</mo></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mo>(</mo><mn>634</mn><mo>+</mo><mn>13</mn><mo>)</mo></mrow>
    <mrow><mo>(</mo><mn>634</mn><mo>+</mo><mn>13</mn><mo>+</mo><mn>19</mn><mo>+</mo><mn>3</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>96</mn> <mo>.</mo> <mn>7</mn> <mo>%</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We can see in [Table 3-2](#table-3-2) that introducing a similarity threshold
    instead of demanding an exact match has improved our recall. In other words, we
    have missed fewer true matches, but at the expense of declaring a few incorrect
    matches, which reduces our precision and our overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-2\. Exact versus approximate matching performance
  prefs: []
  type: TYPE_NORMAL
- en: '|   | **Exact match** | **Approximate match** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | 100% | 97% |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | 98.6% | 99.2% |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 98.5% | 96.7% |'
  prefs: []
  type: TYPE_TB
- en: In this simple example, we set a threshold of 0.8 for both first name and last
    name, and we demanded that both attributes exceed this score for use to declare
    a match. This assigned the same importance to both attributes, but perhaps a match
    on first name isn’t as strong as a match on last name?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at how much repetition we see in both first names and last
    names in the Wikipedia dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this dataset, each `Firstname` is found, on average, 1.89 times versus 1.15
    times for each `Lastname`. Therefore we could say that a `Lastname` match is 64%
    (1.89/1.15) more differentiating than a `Firstname` match. In the next chapter,
    we will examine how we can use probabilistic techniques to weight the importance
    of each attribute and combine these to produce an overall match confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored how to use approximate matching techniques
    to assess the degree of equivalence between two attributes. We examined several
    algorithms for approximate text matching and set an equivalence threshold above
    which we declared a match.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how approximate matching can help us to find true positive matches we
    would otherwise miss but at the price of some false positives that we had to discount
    manually. We saw how the equivalence threshold we set affects this trade-off in
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we considered whether we should give equal weight to matching attributes
    that have different levels of uniqueness when we are evaluating whether two records
    refer to the same entity.
  prefs: []
  type: TYPE_NORMAL
