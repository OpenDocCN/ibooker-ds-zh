# 第八章：整理文件

在使用 Python 处理数据之前，了解存储数据源的文件是很有帮助的。您想要了解一些基本问题的答案：

+   您有多少数据？

+   源文件的格式是怎样的？

这些问题的答案可能非常有帮助。例如，如果您的文件太大或格式不符合您的期望，您可能无法正确加载它到数据框中。

虽然许多类型的结构都可以表示数据，在本书中，我们主要使用数据表，如 Pandas DataFrames 和 SQL 关系。（但请注意，第十三章研究了结构较少的文本数据，第十四章介绍了分层格式和二进制文件。）我们之所以专注于数据表，有几个原因。研究如何存储和操作数据表已经产生了稳定高效的工具来处理表格。此外，表格格式的数据与矩阵密切相关，矩阵是线性代数领域非常丰富的数学对象。当然，数据表非常常见。

在本章中，我们介绍了纯文本的典型文件格式和编码，描述了文件大小的度量，并使用 Python 工具检查源文件。在本章的后面部分，我们介绍了一种用于处理文件的替代方法：shell 解释器。Shell 命令为我们提供了一种在 Python 环境之外获取文件信息的程序化方式，而且对于大数据，shell 可能非常有用。最后，我们检查数据表的形状（行数和列数）和粒度（行代表什么）。这些简单的检查是清理和分析数据的起点。

我们首先简要描述了我们在本章中始终使用的示例数据集。

# 数据源示例

我们选择了两个示例来演示文件整理概念：一个关于药物滥用的政府调查，以及旧金山公共卫生部门有关餐馆检查的行政数据。在我们开始整理之前，我们先概述一下这些示例的数据范围（见第二章）。

## 药物滥用警戒网络（DAWN）调查

DAWN 是一个全国性的医疗保健调查，旨在监测药物滥用趋势。该调查旨在估计药物滥用对国家医疗保健系统的影响，并改善急诊科监测物质滥用危机的方式。DAWN 由[药物滥用和精神卫生服务管理局（SAMHSA）](https://www.samhsa.gov)于 1998 年至 2011 年每年进行一次。2018 年，由于阿片类药物流行，DAWN 调查得以重新启动。在这个例子中，我们查看了 2011 年的数据，这些数据已通过[SAMHSA 数据存档](https://oreil.ly/Y2SKG)提供。

目标人群包括美国所有药物相关急诊室就诊者。这些访问通过医院急诊室（及其记录）的框架进行访问。医院通过概率抽样进行调查选择（参见第三章），并且样本医院急诊室的所有药物相关访问都包括在调查中。所有类型的药物相关访问都包括在内，例如药物滥用、滥用、意外吞食、自杀企图、恶意中毒和不良反应。对于每次访问，记录可能包含最多 16 种不同的药物，包括非法药物、处方药物和非处方药物。

此数据集的源文件是一个需要外部文档（如代码书）来解释的固定宽度格式的示例。此外，由于它是一个相当大的文件，所以激发了如何找到文件大小的话题。而且，其粒度不同寻常，因为调查的主题是急诊访问，而不是个人。

旧金山餐厅文件具有其他特征，使它们成为本章的良好示例。

## 旧金山餐厅食品安全

[旧金山公共卫生部门](https://oreil.ly/kG1PN)定期对餐厅进行未经预先通知的访问，并检查其食品安全情况。检查员根据发现的违规行为计算评分，并提供违规行为的描述。这里的目标人群是旧金山所有餐厅。这些餐厅是通过 2013 年至 2016 年进行的餐厅检查框架来访问的。一些餐厅一年内进行多次检查，而不是所有 7000 多家餐厅每年都接受检查。

食品安全评分可通过城市的[开放数据计划](https://oreil.ly/kwh-F)获得，称为[DataSF](https://datasf.org)。 DataSF 是城市政府公开其数据的一个例子；DataSF 的使命是“在决策和服务交付中使用数据”，旨在改善居民、雇主、员工和访客的生活和工作质量。

旧金山要求餐厅公开展示其评分（参见图 8-1 作为示例标牌）。¹ 这些数据提供了不同结构、字段和粒度的多个文件的示例。一个数据集包含检查结果的摘要，另一个提供有关发现的违规行为的详细信息，第三个包含有关餐厅的一般信息。违规行为包括与食源性疾病传播有关的严重问题以及像未正确展示检查标牌这样的小问题。

![](img/leds_0801.png)

###### 图 8-1\. 展示在餐厅中的食品安全评分卡；分数范围在 0 到 100 之间。

DAWN 调查数据和旧金山餐厅检查数据都可以作为纯文本文件在线获取。然而，它们的格式有很大不同，在下一节中，我们将演示如何确定文件格式，以便将数据读入数据框架中。

# 文件格式

*文件格式*描述了数据如何存储在计算机的磁盘或其他存储设备上。了解文件格式帮助我们弄清楚如何将数据读入 Python，以便将其作为数据表进行处理。在本节中，我们介绍了几种用于存储数据表的流行格式。这些都是纯文本格式，意味着我们可以使用 VS Code、Sublime、Vim 或 Emacs 等文本编辑器轻松阅读它们。

###### 注意

文件格式和数据的*结构*是两个不同的事物。我们认为数据结构是数据的一种心理表示，告诉我们可以进行哪些操作。例如，表结构对应于按行和列排列的数据值。但是同一个表可以存储在许多不同类型的文件格式中。

我们描述的第一种格式是分隔文件格式。

## 分隔格式

分隔格式使用特定字符来分隔数据值。通常，这些分隔符可以是逗号（逗号分隔值，或简称 CSV），制表符（制表符分隔值，或 TSV），空格或冒号。这些格式适合存储具有表结构的数据。文件中的每一行表示一个记录，由换行符（`\n`或`\r\n`）分隔。而在一行内，记录的信息则由逗号字符（`,`）用于 CSV 或制表符字符（`\t`）用于 TSV 等分隔。这些文件的第一行通常包含表的列名/特征的名称。

旧金山餐厅评分存储在 CSV 格式的文件中。让我们显示*inspections.csv*文件的前几行。在 Python 中，内置的`pathlib`库具有一个有用的`Path`对象，用于指定跨平台的文件和文件夹路径。该文件位于*data*文件夹中，因此我们使用`Path()`来创建完整的文件路径名：

```py
`from` `pathlib` `import` `Path`

`# Create a Path pointing to our datafile`
`insp_path` `=` `Path``(``)` `/` `'``data``'` `/` `'``inspections.csv``'`

```

###### 注意

在处理不同操作系统（OSs）时，路径是棘手的。例如，Windows 中的典型路径可能看起来像*C:\files\data.csv*，而 Unix 或 macOS 中的路径可能看起来像*~/files/data.csv*。因此，适用于一个操作系统的代码可能无法在其他操作系统上运行。

`pathlib` Python 库的创建是为了避免特定于操作系统的路径问题。通过使用它，这里显示的代码更具*可移植性* —— 它可以在 Windows、macOS 和 Unix 上运行。

以下代码中的`Path`对象具有许多有用的方法，例如`read_text()`，它将整个文件内容作为字符串读取：

```py
`text` `=` `insp_path``.``read_text``(``)`
`# Print first five lines`
`print``(``'``\n``'``.``join``(``text``.``split``(``'``\n``'``)``[``:``5``]``)``)`

```

```py
"business_id","score","date","type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"
24,"98","20161005","routine"

```

请注意，字段名出现在文件的第一行；这些名称用逗号分隔并带引号。我们看到四个字段：业务标识符、餐厅得分、检查日期和检查类型。文件中的每一行对应一次检查，ID、分数、日期和类型的值用逗号分隔。除了识别文件格式外，我们还希望识别特征的格式。我们注意到两点：分数和日期都显示为字符串。我们希望将分数转换为数字，以便可以计算摘要统计信息并创建可视化图。我们将日期转换为日期时间格式，以便可以制作时间序列图。我们展示如何在第九章中执行这些转换。

显示文件的前几行是我们经常做的事情，因此我们创建一个函数作为快捷方式：

```py
`def` `head``(``filepath``,` `n``=``5``,` `width``=``-``1``)``:`
    `'''Prints the width characters of first n lines of filepath'''`
    `with` `filepath``.``open``(``)` `as` `f``:`
        `for` `_` `in` `range``(``n``)``:`
            `(``print``(``f``.``readline``(``)``,` `end``=``'``'``)` `if` `width` `<` `0`  
             `else` `print``(``f``.``readline``(``)``[``:``width``]``)``)`

```

###### 注意

人们经常将 CSV 和 TSV 文件与电子表格混淆。部分原因是大多数电子表格软件（如 Microsoft Excel）会自动将 CSV 文件显示为工作簿中的表格。在幕后，Excel 会像我们在本节中所做的那样查看文件格式和编码。然而，Excel 文件与 CSV 和 TSV 文件具有不同的格式，我们需要使用不同的`pandas`函数将这些格式读入 Python。

所有三个餐厅源文件都是 CSV 格式的。相比之下，DAWN 源文件采用固定宽度格式。我们接下来描述这种格式化方式。

## 固定宽度格式

固定宽度格式（FWF）不使用定界符来分隔数据值。相反，每行中特定字段的值出现在完全相同的位置。DAWN 源文件采用这种格式。文件中的每一行都非常长。为了显示目的，我们只展示文件中前五行的前几个字符：

```py
`dawn_path` `=` `Path``(``)` `/` `'``data``'` `/` `'``DAWN-Data.txt``'`
`head``(``dawn_path``,` `width``=``65``)`

```

```py
     1 2251082    .9426354082   3 4 1 2201141 2 865 105 1102005 1
     2 2291292   5.9920106887   911 1 3201134 12077  81  82 283-8
     3 7 7 251   4.7231718669   611 2 2201143 12313   1  12  -7-8
     410 8 292   4.0801470012   6 2 1 3201122 1 234 358  99 215 2
     5 122 942   5.1777093467  10 6 1 3201134 3 865 105 1102005 1

```

请注意，值如何从一行到下一行对齐。例如，每行的第 19 个字符处都有一个小数点。还要注意，一些值似乎被挤在一起，我们需要知道每行中每个信息片段的确切位置才能理解它。SAMHSA 提供了一个有 2000 页的[代码手册](https://oreil.ly/a4OFo)，其中包含所有这些信息，包括一些基本检查，以便我们可以确认我们已正确读取文件。例如，代码手册告诉我们年龄字段出现在 34-35 位置，并以 1 到 11 的间隔编码。前面代码中显示的前两条记录的年龄类别分别为 4 和 11；代码手册告诉我们，4 代表年龄段“6 到 11 岁”，而 11 代表“65 岁及以上”。

其他流行的纯文本格式包括分层格式和松散格式化文本（与直接支持表结构的格式形成对比）。这些在其他章节中有更详细的介绍，但为了完整起见，我们在这里简要描述它们。

###### 注意

一种广泛采用的约定是使用文件名扩展名来指示文件内容的格式，例如 *.csv*、*.tsv* 和 *.txt*。文件名以 *.csv* 结尾通常包含逗号分隔值，以 *.tsv* 结尾的文件通常包含制表符分隔值；*.txt* 通常表示没有指定格式的纯文本。但是，这些扩展名只是建议。即使文件的扩展名为 *.csv*，实际内容可能格式不正确！在加载到数据框之前检查文件内容是一个好习惯。如果文件不太大，可以使用纯文本编辑器打开和查看。否则，可以使用 `.readline()` 或 shell 命令查看几行。

## 分层格式

分层格式以嵌套形式存储数据。例如，JavaScript 对象表示法（JSON）通常用于 Web 服务器的通信，包括可以嵌套的键值对和数组，类似于 Python 字典。XML 和 HTML 是其他常见的用于在互联网上存储文档的格式。与 JSON 类似，这些文件具有分层的键值格式。我们在第十四章中更详细地介绍了这两种格式（JSON 和 XML）。

接下来，我们简要描述了其他不属于先前任何类别但仍具有一定结构以便于读取和提取信息的纯文本文件。

## 松散格式文本

网络日志、仪器读数和程序日志通常以纯文本形式提供数据。例如，这是网络日志的一行（我们已经将其分成多行以便阅读）。它包含日期、时间和对网站发出的请求类型等信息：

```py
169.237.46.168 - -
[26/Jan/2004:10:47:58 -0800]"GET /stat141/Winter04 HTTP/1.1" 301 328
"http://anson.ucdavis.edu/courses"
"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; .NET CLR 1.1.4322)"

```

存在组织模式，但不是简单的分隔格式。这就是我们所说的“松散格式”。我们看到日期和时间出现在方括号之间，并且请求类型（本例中为 `GET`）跟随日期时间信息，并以引号形式出现。在第十三章中，我们利用这些关于网络日志格式和字符串操作工具的观察，将感兴趣的值提取到数据表中。

作为另一个例子，这是从无线设备日志中获取的单条记录。设备报告时间戳、标识符、位置以及从其他设备接收到的信号强度。此信息使用了多种格式：键值对、分号分隔值和逗号分隔值：

```py
t=1139644637174;id=00:02:2D:21:0F:33;pos=2.0,0.0,0.0;degree=45.5;
00:14:bf:b1:97:8a=-33,2437000000,3;00:14:bf:b1:97:8a=-38,2437000000,3;

```

就像网络日志一样，我们可以利用字符串操作和记录中的模式将特征提取到表中。

我们主要介绍了用于存储和交换表格的纯文本数据格式。CSV 格式是最常见的，但其他格式，如制表符分隔和固定宽度格式，也很普遍。还有许多种存储数据的文件格式！

到目前为止，我们使用术语 *plain text* 来广泛覆盖可以在文本编辑器中查看的格式。然而，纯文本文件可能有不同的编码，如果我们没有正确指定编码，数据框中的值可能会包含无意义的内容。接下来我们概述文件编码。

# 文件编码

计算机将数据存储为 *比特* 的序列：0 和 1。像 ASCII 这样的 *字符编码* 告诉计算机如何在比特和文本之间进行转换。例如，在 ASCII 中，比特 `100 001` 表示字母 A，比特 `100 010` 表示 B。最基本的纯文本仅支持标准 ASCII 字符，包括大写和小写英文字母、数字、标点符号和空格。

ASCII 编码不包括许多特殊字符或其他语言的字符。其他更现代的字符编码有更多可以表示的字符。文档和网页的常见编码是 Latin-1（ISO-8859-1）和 UTF-8。UTF-8 具有超过一百万个字符，并且向后兼容 ASCII，这意味着它与英文字母、数字和标点的表示方式与 ASCII 相同。

当我们有一个文本文件时，通常需要弄清楚它的编码。如果我们选择错误的编码来读取文件，Python 要么读取错误的值，要么抛出错误。找到编码的最佳方法是检查数据的文档，通常文档会明确指出编码是什么。

当我们不知道编码时，必须猜测。`chardet` 包有一个名为 `detect()` 的函数，可以推断文件的编码。由于这些猜测并不完美，该函数还返回一个介于 0 和 1 之间的置信度。我们使用这个函数来查看我们示例中的文件：

```py
`import` `chardet`

`line` `=` `'``{:<25}`  `{:<10}`  `{}``'``.``format`

`# for each file, print its name, encoding & confidence in the encoding`
`print``(``line``(``'``File Name``'``,` `'``Encoding``'``,` `'``Confidence``'``)``)`

`for` `filepath` `in` `Path``(``'``data``'``)``.``glob``(``'``*``'``)``:`
    `result` `=` `chardet``.``detect``(``filepath``.``read_bytes``(``)``)`
    `print``(``line``(``str``(``filepath``)``,` `result``[``'``encoding``'``]``,` `result``[``'``confidence``'``]``)``)`

```

```py
File Name                 Encoding   Confidence
data/inspections.csv      ascii      1.0
data/co2_mm_mlo.txt       ascii      1.0
data/violations.csv       ascii      1.0
data/DAWN-Data.txt        ascii      1.0
data/legend.csv           ascii      1.0
data/businesses.csv       ISO-8859-1 0.73

```

检测函数非常确信除了一个文件外，所有文件都是 ASCII 编码的。例外是 *businesses.csv*，它似乎是 ISO-8859-1 编码的。如果我们忽略这种编码并尝试在不指定特殊编码的情况下将业务文件读入 `pandas` 中，我们将遇到麻烦：

```py
`# naively reads file without considering encoding`
`>>``>` `pd``.``read_csv``(``'``data/businesses.csv``'``)`
`[``.``.``.``stack` `trace` `omitted``.``.``.``]`
`UnicodeDecodeError``:` `'``utf-8``'` `codec` `can``'``t decode byte 0xd1 in`
`position` `8``:` `invalid` `continuation` `byte`

```

要成功读取数据，我们必须指定 ISO-8859-1 编码：

```py
`bus` `=` `pd``.``read_csv``(``'``data/businesses.csv``'``,` `encoding``=``'``ISO-8859-1``'``)`

```

|   | business_id | name | address | postal_code |
| --- | --- | --- | --- | --- |
| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | 94109 |
| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND FLOOR | 94104 |
| **2** | 31 | NORMAN’S ICE CREAM AND FREEZES | 2801 LEAVENWORTH ST | 94133 |
| **3** | 45 | CHARLIE’S DELI CAFE | 3202 FOLSOM ST | 94110 |

文件编码可能有点神秘，除非有明确给出编码的元数据，否则就要猜测。当编码没有完全确认时，最好寻找额外的文档。

另一个可能重要的源文件方面是其大小。如果文件很大，那么我们可能无法将其读入数据框架。在下一节中，我们将讨论如何确定源文件的大小。

# 文件大小

计算机资源是有限的。如果您的计算机因打开太多应用程序而变慢，您可能已经亲身经历了这些限制。我们希望确保在处理数据时不超出计算机的限制，并且可能会根据数据集的大小选择不同的文件查看方法。如果我们知道我们的数据集相对较小，那么使用文本编辑器或电子表格软件查看数据会很方便。另一方面，对于大型数据集，可能需要更多的程序化探索甚至分布式计算工具。

在许多情况下，我们分析从互联网下载的数据集。这些文件存储在计算机的*磁盘存储*上。为了使用 Python 探索和操作数据，我们需要将数据读入计算机的*内存*，也称为随机访问存储器（RAM）。无论代码有多短，所有 Python 代码都需要使用 RAM。计算机的 RAM 通常比磁盘存储小得多。例如，2018 年发布的某一款计算机型号的磁盘存储比 RAM 多 32 倍。不幸的是，这意味着数据文件通常比可读入内存的数据量要大得多。

磁盘存储和 RAM 容量都是以*字节*（八个 0 和 1）为单位测量的。粗略地说，文本文件中的每个字符增加一个字节的文件大小。为了简洁描述较大文件的大小，我们使用表 8-1 中描述的前缀；例如，包含 52,428,800 个字符的文件将占用<math><mn>5</mn> <mo>,</mo> <mn>242</mn> <mo>,</mo> <mn>8800</mn> <mrow><mo>/</mo></mrow> <mn>1</mn> <mo>,</mo> <msup><mn>024</mn> <mn>2</mn></msup> <mo>=</mo> <mn>50</mn>  <mrow><mtext>mebibytes</mtext></mrow></math> ，即 50 MiB 的磁盘空间。

表 8-1\. 常见文件大小的前缀

| Multiple | Notation | Number of bytes |
| --- | --- | --- |
| Kibibyte | KiB | 1,024 |
| Mebibyte | MiB | 1,024² |
| Gibibyte | GiB | 1,024³ |
| Tebibyte | TiB | 1,024⁴ |
| Pebibyte | PiB | 1,024⁵ |

###### 注意

为什么使用 1,024 的倍数而不是简单的 1,000 倍数来表示这些前缀？这是历史的结果，因为大多数计算机使用二进制数方案，其中 2 的幂更简单表示（<math><mn>1</mn> <mo>,</mo> <mn>024</mn> <mo>=</mo> <msup><mn>2</mn> <mrow><mn>10</mn></mrow></msup></math>）。您还会看到用于描述大小的典型 SI 前缀—例如，千字节、兆字节和千兆字节。不幸的是，这些前缀的使用不一致。有时，千字节指的是 1,000 字节；其他时候，千字节指的是 1,024 字节。为了避免混淆，我们坚持使用 kibi-、mebi-和 gibibytes 这些清楚表示 1,024 的倍数的前缀。

如果我们尝试用程序操作一个超出计算机内存容量的数据文件，那么在计算机上快乐存储的数据文件通常会溢出。因此，我们通常会通过使用内置的`os`库来确保文件的大小可管理：

```py
`from` `pathlib` `import` `Path`
`import` `os`

`kib` `=` `1024`
`line` `=` `'``{:<25}`  `{}``'``.``format`

`print``(``line``(``'``File``'``,` `'``Size (KiB)``'``)``)`
`for` `filepath` `in` `Path``(``'``data``'``)``.``glob``(``'``*``'``)``:`
    `size` `=` `os``.``path``.``getsize``(``filepath``)`
    `print``(``line``(``str``(``filepath``)``,` `np``.``round``(``size` `/` `kib``)``)``)`

```

```py
File                      Size (KiB)
data/inspections.csv      455.0
data/co2_mm_mlo.txt       50.0
data/violations.csv       3639.0
data/DAWN-Data.txt        273531.0
data/legend.csv           0.0
data/businesses.csv       645.0

```

我们看到*businesses.csv*文件在磁盘上占据了 645 KiB，远低于大多数系统的内存容量。虽然*violations.csv*文件占据了 3.6 MiB 的磁盘存储空间，但大多数机器也可以轻松将其读入`pandas`的`DataFrame`中。但包含 DAWN 调查数据的*DAWN-Data.txt*文件则要大得多。

DAWN 文件占用大约 270 MiB 的磁盘存储空间，尽管一些计算机可以在内存中处理此文件，但可能会减慢其他系统的速度。为了在 Python 中使此数据更易管理，我们可以选择仅加载部分列而不是全部列。

有时候我们对文件夹的总大小感兴趣，而不是单个文件的大小。例如，我们有三个餐厅文件，我们可能想看看是否可以将所有数据合并到一个单一的数据框架中。在以下代码中，我们计算*data*文件夹的大小，包括其中所有的文件：

```py
`mib` `=` `1024``*``*``2`

`total` `=` `0`
`for` `filepath` `in` `Path``(``'``data``'``)``.``glob``(``'``*``'``)``:`
    `total` `+``=` `os``.``path``.``getsize``(``filepath``)` `/` `mib`

`print``(``f``'``The data/ folder contains` `{``total``:``.2f``}` `MiB``'``)`

```

```py
The data/ folder contains 271.80 MiB

```

###### 注意

通常情况下，使用`pandas`读取文件需要至少五倍于文件大小的可用内存。例如，读取 1 GiB 文件通常需要至少 5 GiB 的可用内存。内存由计算机上运行的所有程序共享，包括操作系统、Web 浏览器和 Jupyter 笔记本本身。具有 4 GiB 总内存的计算机可能只有 1 GiB 可用内存。在只有 1 GiB 可用内存的情况下，`pandas`可能无法读取 1 GiB 文件。

有几种处理远远大于可加载到内存的数据的策略。接下来我们将介绍其中的一些。

流行的术语*大数据*通常指的是数据足够大，以至于即使顶级计算机也无法直接读取这些数据到内存中。这在科学领域如天文学中很常见，例如望远镜捕捉的空间图像可以达到 PB（ <math><msup><mn>2</mn> <mrow><mn>50</mn></mrow></msup></math>）级大小。虽然不及如此之大，社交媒体巨头、医疗保健提供者和其他公司也可能面临大量数据的挑战。

从这些数据集中提取见解是数据库工程和分布式计算领域的一个重要研究问题的核心动机。尽管本书不涵盖这些领域，我们提供了基本方法的简要概述：

对数据进行子集处理。

一种简单的方法是处理数据的部分。与加载整个源文件不同，我们可以选择其中的特定部分（例如一天的数据）或随机抽样数据集。由于其简单性，我们在本书中经常使用这种方法。其自然缺点是我们失去了分析大数据集时的许多优势，例如能够研究罕见事件。

使用数据库系统。

如在第七章中讨论的那样，关系数据库管理系统（RDBMSs）专门设计用于存储大型数据集。SQLite 是一个有用的系统，用于处理太大以至于无法完全放入内存但足够小以适合单台机器磁盘的数据集。对于太大以至于无法放入单台机器的数据集，可以使用更可扩展的数据库系统，如 MySQL 和 PostgreSQL。这些系统可以通过 SQL 查询操作无法完全放入内存的数据。由于其优势，RDBMSs 常用于研究和工业设置中的数据存储。其一个缺点是通常需要一个单独的服务器来存储数据，并需要其自己的配置。另一个缺点是 SQL 在计算能力上不如 Python 灵活，尤其在建模方面尤为明显。一种有用的混合方法是使用 SQL 来对数据进行子集化、聚合或抽样，将数据批处理成足够小的批次以便读入 Python。然后我们可以使用 Python 进行更复杂的分析。

使用分布式计算系统。

处理大数据集上复杂计算的另一种方法是使用 MapReduce、Spark 或 Ray 等分布式计算系统。这些系统在能够分解为许多较小部分的任务上效果最好，在这些任务中，它们将数据集分成较小的部分并同时在所有较小数据集上运行程序。这些系统具有很大的灵活性，并可在各种场景中使用。它们的主要缺点是通常需要大量工作来正确安装和配置，因为它们通常安装在需要彼此协调的许多计算机上。

使用 Python 确定文件格式、编码和大小可能很方便。另一个处理文件的强大工具是 shell；shell 广泛使用，其语法比 Python 更为简洁。在接下来的部分中，我们将介绍 shell 中可用的几个命令行工具，以执行在读取到数据帧之前查找文件信息的相同任务。

# Shell 和命令行工具

几乎所有计算机都提供对*shell 解释器*的访问，如`sh`、`bash`或`zsh`。这些解释器通常使用它们自己的语言、语法和内置命令在计算机上执行文件操作。

我们使用术语*命令行界面（CLI）工具*来指代 shell 解释器中可用的命令。虽然我们在这里只涵盖了一些 CLI 工具，但还有许多有用的 CLI 工具可以对文件执行各种操作。例如，在 `bash` shell 中，以下命令将列出本章 *figures/* 文件夹中的所有文件以及它们的文件大小：

```py
$ ls -l -h figures/

```

###### 注意

美元符号是 shell 提示符，显示用户在哪里输入。它不是命令本身的一部分。

shell 命令的基本语法是：

```py
 command -options arg1 arg2

```

CLI 工具通常需要一个或多个*参数*，类似于 Python 函数需要参数。在 shell 中，我们使用空格包裹参数，而不是使用括号或逗号。参数出现在命令行的末尾，它们通常是文件的名称或一些文本。在 `ls` 示例中，`ls` 的参数是 `figures/`。此外，CLI 工具支持*标志*，提供附加选项。这些标志紧跟在命令名称后面，使用破折号作为分隔符。在 `ls` 示例中，我们提供了 `-l`（提供有关每个文件的额外信息）和 `-h`（以更易读的格式提供文件大小）标志。许多命令具有默认参数和选项，`man` 工具会打印出任何命令的可接受选项、示例和默认值列表。例如，`man ls`描述了`ls`可用的约 30 个标志。

###### 注意

我们在本书中涵盖的所有 CLI 工具都是针对 `sh` shell 解释器的，这是当前 macOS 和 Linux 系统上 Jupyter 安装的默认解释器。Windows 系统有一个不同的解释器，书中显示的命令可能无法在 Windows 上运行，尽管 Windows 可通过其 Linux 子系统访问 `sh` 解释器。

本节中的命令可以在终端应用程序中运行，也可以通过 Jupyter 打开的终端运行。

我们从探索包含本章内容的文件系统开始，使用 `ls` 工具：

```py
$ ls

data                            wrangling_granularity.ipynb
figures                         wrangling_intro.ipynb                      
wrangling_command_line.ipynb    wrangling_structure.ipynb
wrangling_datasets.ipynb        wrangling_summary.ipynb
wrangling_formats.ipynb       

```

为了更深入地查看并列出 *data/* 目录中的文件，我们将目录名称作为 `ls` 的参数提供：

```py
$ ls -l -L -h data/

total 556664
-rw-r--r--  1 nolan  staff   267M Dec 10 14:03 DAWN-Data.txt
-rw-r--r--  1 nolan  staff   645K Dec 10 14:01 businesses.csv
-rw-r--r--  1 nolan  staff    50K Jan 22 13:09 co2_mm_mlo.txt
-rw-r--r--  1 nolan  staff   455K Dec 10 14:01 inspections.csv
-rw-r--r--  1 nolan  staff   120B Dec 10 14:01 legend.csv
-rw-r--r--  1 nolan  staff   3.6M Dec 10 14:01 violations.csv

```

我们在命令中添加了 `-l` 标志以获取有关每个文件的更多信息。文件大小显示在列表的第五列中，并且通过 `-h` 标志指定的方式更易读。当我们有多个简单选项标志（如 `-l`、`-h` 和 `-L`）时，我们可以将它们组合在一起作为简写：

```py
ls -lLh data/

```

###### 注意

在本书中处理数据集时，我们的代码通常会为 `ls` 和其他 CLI 工具使用额外的 `-L` 标志，如 `du`。我们这样做是因为我们在书中使用快捷方式（称为*符号链接*）设置了数据集。通常情况下，您的代码不需要 `-L` 标志，除非您也在使用符号链接。

用于检查文件大小的其他 CLI 工具是 `wc` 和 `du`。`wc` 命令（缩写为 *word count*）提供有关文件大小的有用信息，以行数、单词数和文件中的字符数表示：

```py
$ wc data/DAWN-Data.txt

  229211 22695570 280095842 data/DAWN-Data.txt

```

我们可以从输出中看到 *DAWN-Data.txt* 有 229,211 行和 280,095,842 个字符。（中间值是文件的单词数，对于包含句子和段落的文件有用，但对于包含数据（如 FWF 格式值）的文件并不十分有用。）

`ls` 工具不计算文件夹内容的累计大小。要正确计算文件夹的总大小（包括文件夹中的文件），我们使用 `du`（磁盘使用情况的缩写）。默认情况下，`du` 工具以称为 *blocks* 的单位显示大小：

```py
$ du -L data/

556664	data/

```

我们通常会在 `du` 命令中添加 `-s` 标志来显示文件和文件夹的大小，并添加 `-h` 标志以标准 KiB、MiB 或 GiB 格式显示数量。在下面的代码中，`data/*` 中的星号告诉 `du` 显示 _data_ 文件夹中每个项的大小：

```py
$ du -Lsh data/*

267M	data/DAWN-Data.txt
648K	data/businesses.csv
 52K	data/co2_mm_mlo.txt
456K	data/inspections.csv
4.0K	data/legend.csv
3.6M	data/violations.csv

```

要检查文件的格式，我们可以使用 `head` 命令查看前几行，或者使用 `tail` 命令查看后几行。这些 CLI 对于查看文件内容以确定其是否为 CSV、TSV 等格式非常有用。例如，让我们来看一下 *inspections.csv* 文件：

```py
$ head -4 data/inspections.csv

"business_id","score","date","type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"

```

默认情况下，`head` 显示文件的前 10 行。如果我们想显示四行，我们可以在命令中添加选项 `-n 4`（或者简写为 `-4`）。

我们可以使用 `cat` 命令打印文件的全部内容。但是，在使用此命令时需要小心，因为打印大文件可能会导致崩溃。*legend.csv* 文件很小，我们可以使用 `cat` 将其内容连接并打印出来：

```py
$ cat data/legend.csv

"Minimum_Score","Maximum_Score","Description"
0,70,"Poor"
71,85,"Needs Improvement"
86,90,"Adequate"
91,100,"Good"

```

在许多情况下，仅使用 `head` 或 `tail` 就足以让我们对文件结构有足够的了解，以便将其加载到数据框中进行进一步处理。

最后，`file` 命令可以帮助我们确定文件的编码：

```py
$ file -I data/*

data/DAWN-Data.txt:   text/plain; charset=us-ascii
data/businesses.csv:  application/csv; charset=iso-8859-1
data/co2_mm_mlo.txt:  text/plain; charset=us-ascii
data/inspections.csv: application/csv; charset=us-ascii
data/legend.csv:      application/csv; charset=us-ascii
data/violations.csv:  application/csv; charset=us-ascii

```

我们再次看到所有文件都是 ASCII 编码，除了 *businesses.csv* 使用 ISO-8859-1 编码。

###### 注意

通常，我们打开终端程序以启动 shell 解释器。但是，Jupyter 笔记本提供了一个方便的功能：如果 Python 代码单元格中的代码行以 `!` 字符开头，则该行将直接发送到系统的 shell 解释器。例如，在 Python 单元格中运行 `!ls` 将列出当前目录中的文件。

Shell 命令为我们提供了一种程序化处理文件的方式，而不是点-and-click 的“手动”方法。它们对以下情况非常有用：

文档

如果你需要记录你所做的事情。

减少错误

如果你想减少排版错误和其他简单但潜在有害的错误。

可重复性

如果你将来需要重复相同的过程，或者计划与他人分享你的过程。这样可以记录你的操作。

体积

如果你有许多重复操作要执行，你正在处理的文件很大，或者你需要快速完成任务。CLI 工具可以在所有这些情况下帮助你。

在数据加载到数据框之后，我们的下一个任务是弄清楚表格的形状和粒度。我们首先找出表格中的行数和列数（其形状）。然后我们需要理解一行代表什么，然后才能开始检查数据的质量。我们在下一节中讨论这些话题。

# 表格形状和粒度

正如前面所述，我们将数据集的*结构*称为数据的心理表示，特别是我们通过将值按行和列排列来表示具有*表*结构的数据。我们使用术语*粒度*来描述表中每一行代表的内容，术语*形状*量化了表的行和列。

现在我们已经确定了与餐厅相关的文件的格式，我们将它们加载到数据框中并检查它们的形状：

```py
`bus` `=` `pd``.``read_csv``(``'``data/businesses.csv``'``,` `encoding``=``'``ISO-8859-1``'``)`
`insp` `=` `pd``.``read_csv``(``"``data/inspections.csv``"``)`
`viol` `=` `pd``.``read_csv``(``"``data/violations.csv``"``)`

```

```py
`print``(``"` `Businesses:``"``,` `bus``.``shape``,` `"``\t` `Inspections:``"``,` `insp``.``shape``,` 
     `"``\t` `Violations:``"``,` `viol``.``shape``)`

```

```py
 Businesses: (6406, 9) 	 Inspections: (14222, 4) 	 Violations: (39042, 3)

```

我们发现餐厅信息表（商业表）有 6,406 行和 9 列。现在让我们来弄清楚这张表的粒度。首先，我们可以看一下前两行：

|   | business_id | 名称 | 地址 | 城市 | ... | 邮政编码 | 纬度 | 经度 | 电话号码 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | 旧金山 | ... | 94109 | 37.79 | -122.42 | +14157763262 |
| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND FLOOR | 旧金山 | ... | 94104 | 37.79 | -122.40 | +14156779494 |

```py
2 rows × 9 columns
```

这两行给我们的印象是每个记录代表一个特定的餐厅。但是，我们无法仅凭两个记录就知道这是否正确。名为`business_id`的字段暗示它是餐厅的唯一标识符。我们可以通过检查数据框中的记录数是否与字段`business_id`中的唯一值数目匹配来确认这一点：

```py
`print``(``"``Number of records:``"``,` `len``(``bus``)``)`
`print``(``"``Number of unique business ids:``"``,` `len``(``bus``[``'``business_id``'``]``.``unique``(``)``)``)`

```

```py
Number of records: 6406
Number of unique business ids: 6406

```

唯一的`business_id`数目与表中的行数相匹配，因此可以安全地假设每一行代表一个餐厅。由于`business_id`在数据框中唯一标识每条记录，我们将`business_id`视为该表的*主键*。我们可以使用主键来连接表（参见第六章）。有时主键由两个（或更多）特征组成。这是其他两个餐厅文件的情况。让我们继续检查检查和违规数据框，并找出它们的粒度。

## 餐厅检查和违规的粒度

我们刚刚看到，检查表中的行比商业表中的行要多得多。让我们仔细看一下前几次检查：

|   | business_id | 分数 | 日期 | 类型 |
| --- | --- | --- | --- | --- |
| **0** | 19 | 94 | 20160513 | 常规 |
| **1** | 19 | 94 | 20171211 | 常规 |
| **2** | 24 | 98 | 20171101 | 常规 |
| **3** | 24 | 98 | 20161005 | 常规 |

```py
`(``insp`
 `.``groupby``(``[``'``business_id``'``,` `'``date``'``]``)`
 `.``size``(``)`
 `.``sort_values``(``ascending``=``False``)`
 `.``head``(``5``)`
`)`

```

```py
business_id  date    
64859        20150924    2
87440        20160801    2
77427        20170706    2
19           20160513    1
71416        20171213    1
dtype: int64

```

餐馆 ID 和检查日期的组合在这张表中唯一标识每条记录，除了三家餐馆的 ID-日期组合有两条记录。让我们检查餐馆`64859`的行：

```py
`insp``.``query``(``'``business_id == 64859 and date == 20150924``'``)`

```

|   | business_id | score | date | type |
| --- | --- | --- | --- | --- |
| **7742** | 64859 | 96 | 20150924 | 常规 |
| **7744** | 64859 | 91 | 20150924 | 常规 |

这家餐馆在同一天得到了两个不同的检查分数！这怎么可能发生？可能是餐馆在一天内接受了两次检查，或者可能是一个错误。我们在考虑第九章中的数据质量时会解决这类问题。由于这种双重检查只有三次，我们可以在清理数据之前忽略这个问题。因此，如果从表中删除同一天的检查，主键将是餐馆 ID 和检查日期的组合。

请注意，检查表中的`business_id`字段充当对业务表主键的引用。因此，在`insp`中的`business_id`是一个*外键*，因为它将检查表中的每条记录链接到业务表中的一条记录。这意味着我们可以很容易地将这两个表连接在一起。

接下来，我们来检查第三个表的粒度，即包含违规的表：

|   | business_id | date | description |
| --- | --- | --- | --- |
| **0** | 19 | 20171211 | 食品安全知识不足或没有... |
| **1** | 19 | 20171211 | 未经批准或未维护的设备或器具 |
| **2** | 19 | 20160513 | 未经批准或未维护的设备或器具... |
| **...** | ... | ... | ... |
| **39039** | 94231 | 20171214 | 高危害害虫侵扰... |
| **39040** | 94231 | 20171214 | 中度风险食品保持温度... |
| **39041** | 94231 | 20171214 | 擦拭布不干净或未正确存放... |

```py
39042 rows × 3 columns
```

查看此表中的前几条记录，我们发现每次检查都有多个条目。粒度似乎是在检查中发现的违规水平。阅读描述，我们看到如果得到纠正，描述中会列出方括号中的日期。

```py
`viol``.``loc``[``39039``,` `'``description``'``]`

```

```py
'High risk vermin infestation  [ date violation corrected: 12/15/2017 ]'

```

简而言之，我们发现这三个食品安全表格具有不同的粒度。因为我们已经为它们确定了主键和外键，所以我们可以潜在地将这些表格连接起来。如果我们有兴趣研究检查，我们可以使用商业 ID 和检查日期将违规和检查一起连接起来。这将使我们能够将检查中发现的违规数量与检查分数联系起来。

通过选择每个餐厅最近的一次检查，我们还可以将检查表缩减为每个餐厅一个。这种精简的数据表基本上以餐厅为粒度，可能对基于餐厅的分析有用。在第九章中，我们涵盖了这些重塑数据表、转换列并创建新列的操作。

我们通过查看 DAWN 调查数据的形状和粒度来结束本节。

## DAWN 调查的形状和粒度

正如本章前面提到的，DAWN 文件采用固定宽度格式，我们需要依靠代码簿查找字段的位置。例如，代码簿中的一个片段在图 8-2 中告诉我们，年龄出现在行的第 34 和 35 位置，并被分为 11 个年龄组：1 表示 5 岁及以下，2 表示 6 至 11 岁，……，11 表示 65 岁及以上。此外，-8 表示缺失值。

![](img/leds_0802.png)

###### 图 8-2\. DAWN 年龄编码部分的屏幕截图

我们早些时候确定这个文件包含 200,000 行和超过 2.8 亿个字符，因此平均每行约有 1,200 个字符。这可能是他们使用固定宽度而不是 CSV 格式的原因。想象一下，如果每个字段之间都有逗号，文件会变得多么庞大！

鉴于每行都包含大量信息，我们只需将几个特征读入数据框中。我们可以使用`pandas.read_fwf`方法来完成这个任务。我们指定要提取的字段的确切位置，并为这些字段及其他有关标头和索引的信息提供名称：

```py
`colspecs` `=` `[``(``0``,``6``)``,` `(``14``,``29``)``,` `(``33``,``35``)``,` `(``35``,` `37``)``,` `(``37``,` `39``)``,` `(``1213``,` `1214``)``]`
`varNames` `=` `[``"``id``"``,` `"``wt``"``,` `"``age``"``,` `"``sex``"``,` `"``race``"``,``"``type``"``]`
`dawn` `=` `pd``.``read_fwf``(``'``data/DAWN-Data.txt``'``,` `colspecs``=``colspecs``,` 
                   `header``=``None``,` `index_col``=``0``,` `names``=``varNames``)`

```

|   | wt | age | sex | race | type |
| --- | --- | --- | --- | --- | --- |
| id |   |   |   |   |   |
| --- | --- | --- | --- | --- | --- |
| **1** | 0.94 | 4 | 1 | 2 | 8 |
| **2** | 5.99 | 11 | 1 | 3 | 4 |
| **3** | 4.72 | 11 | 2 | 2 | 4 |
| **4** | 4.08 | 2 | 1 | 3 | 4 |
| **5** | 5.18 | 6 | 1 | 3 | 8 |

我们可以将表中的行与文件中的行数进行比较：

```py
`dawn``.``shape`

```

```py
(229211, 5)

```

数据框中的行数与文件中的行数相匹配。这很好。由于调查设计的复杂性，数据框的粒度有点复杂。请记住，这些数据是大型科学研究的一部分，具有复杂的抽样方案。一行代表一个急诊室就诊，因此粒度是在急诊室就诊级别。然而，为了反映抽样方案并代表一年内所有与药物相关的急诊室访问的人群，提供了权重。在计算汇总统计数据、构建直方图和拟合模型时，我们必须将权重应用于每个记录。（`wt`字段包含这些值。）

权重考虑到这种类型的急诊室就诊出现在样本中的几率。所谓“这种类型的”是指具有类似特征的就诊，如访客年龄、种族、就诊地点和时段。让我们来检查`wt`中的不同值：

```py
`dawn``[``'``wt``'``]``.``value_counts``(``)`

```

```py
wt
0.94     1719
84.26    1617
1.72     1435
         ... 
1.51        1
3.31        1
3.33        1
Name: count, Length: 3500, dtype: int64

```

在您的分析中包括调查权重非常关键，以获取代表大多数人口的数据。例如，我们可以比较包含和不包含权重计算的急诊女性比例：

```py
`print``(``f``'``Unweighted percent female:` `{``np``.``average``(``dawn``[``"``sex``"``]` `==` `2``)``:``.1%``}``'``)`
`print``(``f``'` `Weighted percent female:``'``,`
      `f``'``{``np``.``average``(``dawn``[``"``sex``"``]` `==` `2``,` `weights``=``dawn``[``"``wt``"``]``)``:``.1%``}``'``)`

```

```py
Unweighted percent female: 48.0%
  Weighted percent female: 52.3%

```

这些数字相差超过 4 个百分点。加权版本是女性在整个与药物相关的急诊访问人口中比例的更准确估计。

有时，像我们在检查数据中看到的那样，粒度可能很难确定。而其他时候，我们需要考虑抽样权重，比如 DAWN 数据。这些示例表明，在进行分析之前花时间审查数据描述是非常重要的。

# 总结

数据清洗是数据分析的重要组成部分。没有它，我们可能会忽略数据中可能对未来分析产生重大影响的问题。本章介绍了数据清洗的重要第一步：从纯文本源文件中读取数据到 Python 数据框架并确定其粒度。我们介绍了不同类型的文件格式和编码，并编写了可以从这些格式读取数据的代码。我们检查了源文件的大小，并考虑了用于处理大型数据集的替代工具。

我们还介绍了命令行工具作为检查文件格式、编码和大小的 Python 替代方案。由于其简单的语法，这些 CLI 工具在面向文件系统任务时尤为方便。我们只是触及了 CLI 工具的表面。在实践中，shell 能够进行复杂的数据处理，是值得学习的工具。

理解表的形状和粒度使我们能够洞察数据表中的一行代表什么。这有助于我们确定粒度是否混合，是否需要聚合或是否需要权重。在查看数据集的粒度后，您应该能回答以下问题：

记录代表什么？

弄清这一点将帮助您正确分析数据并陈述您的发现。

表中的所有记录是否以相同的粒度捕获？

有时，表中包含其他摘要行，其粒度不同，您希望仅使用那些具有正确细节级别的行。

如果数据已经聚合，聚合是如何执行的？

汇总和平均值是常见的聚合类型。对于平均化的数据，通常可以减少测量中的变异性，并且关系通常看起来更强。

您可能对数据执行哪些类型的聚合？

聚合可能对将一个数据表与另一个数据表合并非常有用或必要。

确定您的表的粒度是清理数据的第一步，也指导您如何分析数据。例如，我们看到 DAWN 调查的粒度是急诊就诊。这自然引导我们思考病人人口统计数据与整个美国的比较。

本章的数据整理技术帮助我们将数据从源文件导入数据框架，并了解其结构。一旦我们有了数据框架，就需要进一步整理数据，评估和提高数据质量，并为分析准备数据。我们将在下一章中涵盖这些主题。

¹ 2020 年，该市开始向餐馆提供彩色编码的牌子，指示餐馆是否通过（绿色）、有条件通过（黄色）或未通过（红色）检查。这些新的牌子不再显示数字检查得分。然而，餐馆的得分和违规仍然可以在 DataSF 上查看。
