# 第三章 理解数据质量

数据无处不在。它由我们的移动设备自动生成，我们的购物活动和身体活动。它由我们的电表、公共交通系统和通信基础设施捕获。它用于估计我们的健康结果、收入潜力和信用价值^([1](ch03.html#idm45143427340352))。经济学家甚至宣称数据是“新的石油”^([2](ch03.html#idm45143427337760))，因其有可能转变人类生活的许多方面。

尽管数据可能非常丰富，但事实是*好的*数据却是稀缺的。所谓的“数据革命”声称，有了足够的数据，我们可以更好地理解现在，改进甚至预测未来。然而，要实现任何这些可能性，这些洞察背后的数据必须是高质量的。没有好质量的数据，我们所有的努力去整理、分析、可视化和传播数据，充其量只会让我们对世界的了解毫无进展。虽然这将是一种不幸的努力浪费，但没有意识到我们拥有质量低劣的数据的后果更为严重，因为这可能导致我们形成一个看似合理但危险扭曲的现实观。更重要的是，因为数据驱动的系统被用来大规模做决策，即使有少量不好的数据也可能造成重大危害。当然，可以用来“训练”机器学习模型的数据可能涉及数百甚至数千人的数据。但是，如果这些数据不代表模型将应用到的人群，那么系统可能造成的后果将影响到原始数据集中人数的数百甚至数千倍。由于风险如此之高，确保数据质量是数据整理的一个重要部分。但是，数据“高质量”意味着什么呢？在我看来，只有数据既*适合*特定目的，又具有高度的内部*完整性*，才能称得上是高质量的数据。

那么，每一个术语实际上意味着什么呢？这正是我们将在本章深入探讨的内容。我们将从讨论数据*适合性*的概念开始，这与数据在特定上下文中的适用性或用于回答特定问题相关。然后，我们将详细分析数据*完整性*的许多方面：影响数据适合性及我们可以负责使用的分析类型的数据集特征。最后，我们将讨论一些工具和策略，帮助您找到和处理数据，以提升其整体质量，从而增强您与其相关工作的信心和可信度。

如果你开始觉得这些内容有点乏味，让我从[《什么是“数据整理”？》](ch01.html#describing_data_wrangling)中再次劝告你：试图“跳过”评估数据质量的工作只会削弱你在数据整理方面的努力。充其量，你在分享工作时会遇到关于你的流程的问题，而你却无法回答。最糟糕的情况下，你可能会推广一些既错误又有害的“见解”。在这个过程中，你也会欺骗自己，因为解决数据质量问题是你扩展编程知识的最佳途径。如果你真的想在数据整理工作中表现出色，评估数据质量必须成为你实践的一部分。

# 评估数据的适应性

也许关于数据整理最常见的误解之一是它是一个主要*定量*的过程，也就是说，数据整理主要是处理数字、公式和代码。事实上，无论你处理的是什么类型的数据——从温度读数到社交媒体帖子——数据整理的核心工作都涉及做出判断：从你的数据是否准确地反映了你正在调查的现象，到如何处理缺失的数据点，以及是否有足够的数据来生成任何真正的见解。第一个概念——一个给定数据集如何准确地反映你正在调查的现象——大体上就是我所说的它的*适配性*，评估你的数据集对于特定目的的*适配*远不只是应用数学公式那么简单。其原因很简单：这个世界是一个混乱的地方，即使是最简单的关于它的数据也总是通过某种人类视角来过滤。以测量你工作空间一周内温度为例。理论上，你只需要一个温度计，把它放在空间里，每天记录一下读数就行了。完成了，对吧？

或者呢？让我们从你的设备开始说起。你用的是数字温度计还是水银温度计？你把它放在哪里？是在靠近门、窗户、还是加热或冷却源的地方？你每天都在同一时间测量吗？温度计是否曾直接暴露在阳光下？典型的湿度水平是多少？

你可能认为我在这里引入了一种人为的复杂性水平，但如果你曾经生活在共享空间（比如公寓大楼）中，你可能经历过感觉像比某个温度计所说的温度更高或更低的体验。同样，如果你曾经照顾过生病的孩子，你很可能对不同类型温度计或甚至同一温度计不同时间读数的体验感到非常熟悉。

换句话说，导致你记录的两位数或三位数温度的因素有很多，而这个数字本身并不提供任何信息。这就是为什么当你开始尝试用数据回答问题时，仅仅知道数据集中包含什么是不够的；你需要了解收集数据时使用的过程和机制。然后，根据你对数据收集方式的了解，你需要确定它是否真的可以用来有意义地回答你的具体问题。

当然，这个问题既不新鲜也不独特；这是所有科学领域在努力探索世界新信息时面临的挑战。如果每个研究人员都必须亲自进行每项研究，癌症研究几乎无法取得进展；如果没有利用他人工作的能力，科学和技术进步将停滞不前（如果不是完全偏离轨道）。因此，科学界随着时间的推移已经发展出了三个用于确定数据集是否适合回答给定问题的关键指标：*有效性*、*可靠性*和*代表性*。

## **有效性**

在其最基本的层面上，*有效性*描述了某物测量其应测量的程度。在我们的室温例子中，这意味着确保你选择的温度计确实能够测量空气温度，而不是其他什么东西。例如，传统的液体玻璃温度计可能很好地捕捉空气温度，而红外线温度计则倾向于捕捉其指向的任何表面的温度。因此，即使是看似基础的室温问题，你也需要理解用于收集数据读数的工具和方法，以确保它们在与你的问题相关的*有效性*方面的适用性。

毫不奇怪，当我们不是收集关于常见物理现象的数据时，事情会变得更加复杂。*建构有效性*描述了你的数据测量有效地捕捉了（通常是抽象的）*建构*或思想，你试图理解的内容。例如，假设你想知道你所在地区哪些学校是“最好的”。哪些数据可以帮助你回答这个问题？首先，我们必须认识到术语*最好*是不精确的。最好是指什么？你关心哪所学校拥有最高的毕业率？标准化考试成绩？学校分配的成绩？教师评价？学生满意度？课外活动参与度？

为了利用数据开始回答这个问题，您首先需要阐明两件事。首先，“最佳” *适合谁*？您是在为自己的孩子回答这个问题吗？朋友的孩子？回答了这个问题后，您将更能够完成第二项任务，即*操作化* 您对“最佳”具体想法的理解。例如，如果您朋友的孩子喜欢运动，课外活动可能比学术更重要。

在数据分析中，选择测量的过程被称为*操作化构建*，它无可避免地需要在您尝试理解的概念或想法中选择并平衡代理。这些代理 —— 如毕业率、考试成绩、课外活动等 —— 是您可以*收集* *数据*的事物，您选择使用它们来代表无法直接测量的抽象概念（“最佳”学校）。良好的数据至少必须在您的问题方面具有良好的*结构有效性*，否则您的数据处理结果将毫无意义。

数据拟合中另一种重要的有效性类型是*内容有效性*。这种有效性类型涉及到给定代理测量的数据完整性。在“最佳”学校的例子中，假设您已确定成绩对于确定最佳学校很重要，但您只有历史和体育课程的成绩数据可用。尽管对于许多人来说，成绩数据原则上可能具有*结构有效性*以确定最佳学校，但仅有两种类型课程的成绩数据是不足以满足*内容有效性*的要求的 —— 而对于高质量的数据，您需要*两者兼顾*。

## 可靠性

在数据集内部，给定测量的*可靠性*描述了其*准确性*和*稳定性*。这两者帮助我们评估在相同情况下两次进行相同测量是否会得到相同或非常相似的结果。回顾我们的体温例子：用口温度计测量儿童体温可能不太*可靠*，因为这个过程需要孩子闭口相对较长时间（根据我的经验，他们在这方面表现不佳）。相比之下，腋下测量可能更可靠 —— 因为您可以拥抱他们以保持温度计的位置，但可能不能像其他方法那样提供孩子真实的体内温度读数的*准确性*。这就是为什么大多数医疗建议根据使用哪种方法测量体温为儿童设置不同的发热温度阈值的原因。

对于抽象概念和现实世界的数据，确定数据测量的可靠性尤为棘手，因为真的无法再次收集数据——无论是因为成本过高、情况无法复制，还是两者兼而有之。在这些情况下，我们通常通过将一个类似群体与另一个类似群体进行比较来估计可靠性，使用之前或新收集的数据。因此，即使学校在一年内的标准化考试成绩出现显著波动表明这些分数可能不是学校质量的*可靠*衡量标准，这种不一致本身只是故事的一部分。毕竟，这些考试成绩可能反映了教学质量，但它们也可能反映了考试被实施的变化、评分方式或其他对学习或考试环境的干扰。为了确定标准化考试数据是否足够*可靠*以成为你的“最佳”学校评估的一部分，你需要查看其他年份或其他学校的比较数据，同时了解可能导致波动的更广泛情况。最终，你可能会得出结论，大多数考试成绩信息足够可靠以纳入，但应删除一些特定数据点，或者你可能会得出结论，数据过于不可靠，不能成为高质量数据流程的一部分。

## *代表性*

数据驱动系统的关键价值主张是它们允许我们生成关于人和现象的见解——甚至预测——这些人或现象对于人类来说过于庞大或过于复杂，无法有效推理。通过整理和分析数据，逻辑是，我们可以做出比人类更快速、更公平的决策。鉴于即使是个人——更不用说公司了——如今都可以访问的强大计算工具，毫无疑问，数据驱动系统可以比人类更快地生成“决策”。然而，这些见解是否是特定人群或情况的准确描述，直接取决于所使用数据的*代表性*。

数据集是否足够代表性取决于几个因素，其中最重要的回到了我们在[“有效性”](#validity)中讨论过的“为谁？”问题。如果你试图为特定小学设计新的课程表，你可能能够收集关于整个学生群体的数据。如果数据适应性的所有其他标准都已满足，那么你已经知道你的数据是具有代表性的，因为你已经直接从或关于将适用的整个群体收集了数据。

但是，如果你试图为整个城市的学校完成相同的任务呢？几乎不可能收集到每个学校每个学生的数据，这意味着当你尝试设计新的课程表时，你将仅依赖于部分学生的输入。

无论何时，当你以这种方式处理子集或*样本*时，确保它*代表*你计划应用发现的更广泛人群至关重要。虽然适当的抽样方法学超出了本书的范围，^([4](ch03.html#idm45143427230496)) 基本思想是，为了你的见解能准确地推广到特定社群，你使用的数据样本必须按比例反映该社群的构成。这意味着在你甚至能知道你的样本是否代表性之前，你需要投入时间和资源来全面了解该社群的一些事情*作为一个整体*。

此时你可能会想：等等，如果我们已经可以获得整个人群的信息，我们一开始就不需要样本了！这是真的——在某种程度上是真的。在许多情况下，我们可以获得一些关于整个社群的信息——只是不是我们需要的精确信息。例如，在我们的学校排课场景中，我们理想情况下会获取有关学生每天如何以及多长时间通勤的信息，以及其监护人的日程安排的一些了解。但是，我们可能*已经*有关于整个学校人口的信息（如果我们与学校系统合作），可能仅包括家庭地址、学校地址，也许还有[交通支持类型](https://schools.nyc.gov/school-life/transportation/bus-eligibility)。利用这些信息，可能还结合一些额外的行政信息，我们可以开始为特定类型学生通勤者在*整个*人群中所占比例创建估计，然后试图在调查结果中选择*代表性样本*以复制这些比例。只有在这一点上，我们才准备好进入数据整理过程的下一步。

正如你所见，确保代表性要求我们仔细考虑人群的哪些特征与我们的数据整理问题相关，并寻找足够的额外信息，以确保我们的数据集比例地代表这些特征。也许并不奇怪的是，这是许多数据驱动产品和服务在数据适应性测试中屡屡失败的原因。虽然公司和研究人员可能吹嘘他们用于在系统中开发的数据的*数量*，但事实是，大多数对公司和研究人员来说容易获取的数据集往往不代表例如美国或全球人口。例如，关于搜索引擎趋势、社交媒体活动、公共交通使用或智能手机拥有情况的数据，例如，都极不可能代表更广泛的人口，因为它们不可避免地受到诸如互联网访问和收入水平之类的影响。这意味着一些社区在这些数据集中*过度*代表，而其他一些社区（有时严重地）*不足*代表。其结果是系统不*泛化*，比如面部识别系统[无法“看到”黑人面孔](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)。

面对不具代表性的数据，你会怎么做呢？至少，你需要修订（并清楚地传达）你的“为谁？”评估，以反映它实际上代表的人群；这是你的数据整理洞察力有效的唯一社区。还要记住，代表性只能确保你的数据整理努力的结果准确反映现实；这并不是对是否应该“持续”这种现实的价值判断。如果你的数据整理工作的结果将用于对系统或组织进行更改，那么你的数据整理过程的结束实际上只是开始考虑如何处理你的洞察力，特别是对复杂问题如[公平性](https://youtube.com/watch?v=jIXIuYdnyyk)。

即使你拥有整个人口的数据，这也是真实的。例如，如果你想知道哪些组织获得了某种类型的资助，那么你感兴趣的“人口”或社区只是那些资助接收者。同时，检查你的数据是否代表整体人口仍然有价值：如果一个或多个社区在你的资助接收者群体中过度或不足代表，这可能暗示着影响谁获得这笔资金的隐藏因素。

# 评估数据完整性

数据“适合”实质上是关于你是否拥有正确的数据来回答你的数据整理问题。另一方面，“完整性”主要是关于你拥有的数据是否能支持你需要执行的分析来回答这个问题。正如你将在本节中看到的，执行完整性评估时需要考虑数据的许多不同方面。但是，一个给定的数据集是否需要拥有所有这些特征才能具备高完整性和高质量？不一定。虽然有些是必不可少的，但其他特征的重要性取决于你具体的问题和你需要回答它的方法。而且除了少数例外，许多特征是你将作为数据整理过程的一部分增强和开发的数据特征。

换句话说，确保数据“适合”的情况是不可选的，但你的数据整理项目需要的“完整性”类型和程度会有所不同。当然，你的数据满足的要求越多，它对你和他人都越有用。

一般而言，一个高完整性的数据集将在某种程度上包括以下特征：^([5](ch03.html#idm45143427185920))

必要但不足的

+   已知来源的

+   良好注释的

重要的

+   及时的

+   完整的

+   大容量的

+   多变量的

+   原子的

可实现的

+   一致的

+   清晰的

+   维度结构化的

正如我之前提到的，然而，并非所有这些数据完整性特征同等重要。其中一些是必不可少的，而另一些几乎总是数据整理过程中某些步骤的结果，而不是其先导条件。而在你开始分析之前，希望你的数据具备尽可能多的这些特征之间，总是需要在更详细地描述你的数据和及时完成你的工作之间取得平衡。

在这个意义上，评估数据完整性可以成为优先考虑数据整理工作的一种有用方式。例如，如果一个数据集缺少“必要但不足的”特征中的任何一个，你可能需要完全放弃它。如果它在“重要的”特征中缺少一两个，可能仍然有可能通过与其他数据的合并或限制分析和声明的范围来挽救你正在处理的数据。与此同时，开发“可实现的”特征通常是你的数据整理过程中“清理”步骤的目标，而不是你第一次遇到它时就可以期待大多数现实世界数据具备的内容。

最终，你的数据集能够体现以下特征的程度，取决于你投入数据整理项目的时间，但如果缺乏其中大部分特征，你的洞察力将受限。正如将在[第6章](ch06.html#chapter6)详细展示的那样，这种变异性正是数据整理和数据质量如此紧密交织的原因。

## 必要，但不足以

大多数情况下，我们正在处理的数据是由其他人编制的——或者是一群我们无法直接接触的人和流程。与此同时，我们需要能够支持我们的数据处理过程以及从中得出的任何见解。这意味着有几个数据特征对于成功的数据处理过程非常*重要*。

### 来源已知

如[“什么是“数据质量”？”](ch01.html#what_is_data_quality)中所讨论的，数据是关于衡量什么以及如何衡量的人类决策的产物。这意味着使用他人收集的数据集需要对他们进行相当程度的信任，尤其是因为很少可能独立验证*每一个数据点*；如果可以的话，你可能会选择自己收集数据。这就是为什么了解数据集的*来源*如此重要：如果你不知道谁编制了数据、他们使用的方法和/或他们收集数据的目的，你将很难判断它是否适合你的数据处理目的，或者如何正确解释它。

当然，这并不意味着你需要知道每个帮助构建给定数据集的人的生日和最喜欢的颜色。但是，你*应该*尽力了解足够的关于他们的专业背景、收集数据的动机（例如，它是否被法律要求）、以及他们使用的方法，这样你就能大致了解哪些测量值需要核实，哪些可能可以直接采用。理想情况下，关于数据作者的信息和关于这些过程的充分文档应该很容易获得，以便你可以迅速回答关于数据*来源*的所有这些问题。然而，如果他们难以找到，你可能会考虑放弃；因为你需要这些信息来评估数据*适合性*，你的时间可能花在寻找其他数据集上，甚至是自己收集数据。

### 良好注释

一个良好注释的数据集有足够的周围信息，或者*元数据*，使得解释成为可能。这将包括从数据收集方法的高层次解释到描述每个数据测量及其单位的“数据字典”等一切。尽管这可能看起来很简单，但并不总是有关于如何提供这些注释信息的公认标准：有时它直接出现在数据文件或数据条目本身中，有时信息可能包含在完全与检索数据位置不同的单独文件或文档中。

无论数据的结构或提供方式如何，健壮的数据注释文档都是高完整性数据的重要组成部分，因为没有这些文档，就不可能应用任何分析或从中得出任何推断。例如，试想一下，试图解释一个预算而不知道提供的数字是美元、千美元还是百万美元，显然是不可能的。或者以一种非常美国化的表达来说明数据字典等注释文档的重要性，有时可能需要[起诉](https://trac.syr.edu/foia/ice/20210805)才能获得。

## 重要

尽管在数据整理中，除非有关于数据集的充分溯源和元数据信息，否则你将无法进展，但是除非你有足够、来自正确时间段且具有适当详细级别的数据，否则你仍然无法走得太远。话虽如此，以下数据特征是我们可能经常评估的，并且有时可以通过我们自己的数据整理工作来改进。因此，尽管处理起来已经拥有这些特征的数据集肯定会更容易，但即使在一些方面表现不佳的数据集，仍然值得探索。

### 及时

你使用的数据有多新？除非你正在研究一个历史时期，确保你的数据足够新以有意义地描述当前世界的状态是重要的——尽管“太旧”是多老将取决于你正在探索的现象以及收集和发布关于它的数据的频率。

例如，如果你对社区人口统计数据感兴趣，但你最近的数据已经几年前的，那么很可能自那时以来情况已经发生了很大变化。对于失业数据来说，超过一个月的数据将不再及时，而对于股市数据来说，只需几秒钟信息就可能被认为已经太旧，无法用于交易。除非涉及到你已经熟悉的领域，否则评估你的数据是否及时可能需要一些专家研究以及数据发布者的数据。

### 完整

数据集是否包含了它应该包含的所有数据值？例如，在[“USDA 数据追踪”](#usda_data)中的 USDA 苹果数据中，只有少数行包含了外观描述，而大多数行都是空白的。即使数据的部分内容如此不完整，我们仍然能够生成有用的数据洞察吗？回答这个问题首先意味着要回答另外两个问题。首先，为什么数据丢失了？其次，你是否需要这些数据测量结果来执行你的数据整理过程中所需的特定分析？

例如，你的数据可能不完整，因为个别数值缺失，或者数据报告不规律——也许数据通常每月记录，但却存在半年间隔的数据空白。数据集也可能被截断，当大数据集在电子表格程序中打开时，这是一个常见问题。无论什么原因，找出数据缺失的*原因*是至关重要的，以便知道如何继续进行。在[“USDA 数据调查”](#usda_data)中，如果我们主要关心不同苹果品种的定价方式，我们可以考虑忽略“外观”类别，或者我们可以再次联系市场终端，澄清外观列中的空白值是否实际上对应于他们未明确指出的某个默认值。即使是截断的数据，如果我们可用的数据涵盖了我们的目的所需的足够时间段，也可能不成问题，但了解数据的真实记录数和日期范围以进行上下文分析仍然很有用。虽然有时可以使用统计技巧使数据收集中的间隙变得不那么问题，但了解记录间隙是由于某些破坏性事件造成的可能会改变您进行分析的类型，甚至是您完全追求的问题。

换句话说，虽然拥有*完整*的数据始终是可取的，一旦您知道数据缺失的*原因*，您可能可以继续进行数据整理过程。但是，您应该始终了解并确保记录所学到的内容！

### 高容量

“足够”数据点有多少才够？至少，数据集需要有足够的记录来支持回答您特定问题所需的分析类型。如果您需要的是*计数*——例如，某一年中涉及噪音投诉的311呼叫数量——那么拥有“足够”数据意味着拥有那一年中所有311呼叫的记录。

然而，如果您的问题涉及一般或可普遍化的模式或趋势，那么“足够”的定义就不那么清晰了。例如，如果您想确定哪个Citi Bike站点最“繁忙”，您应该考虑多长时间段？一周？一个月？一年？应该只考虑工作日，还是所有天？正确的答案部分取决于更详细地说明您的问题。您是对通勤者或访客的体验感兴趣吗？您是在生成驱动交通规划、零售布局或服务质量的见解吗？另外，您是否*真的*关心哪个站点最“繁忙”，还是更关心特定周转率？如常见的情况一样，正确的答案主要取决于正确说明问题——而这通常需要非常具体。

然而，评估数据“完整性”的其中一个棘手部分是，在不太了解主题领域的情况下，很难考虑可能影响你正在调查的趋势或模式的因素。例如，尽管我们可能很容易预期Citi Bike的使用量可能随季节变化而变化，但公共交通服务的减少呢？票价的增加呢？这些变化*可能*对我们的分析有影响，但我们在刚刚开始时如何知道呢？

答案——就像往常一样——是（人类）专家。也许票价上涨暂时增加了共享单车的骑行人数，但这种增长只持续了几个月。也许自行车通勤者已经做好了应对恶劣天气的准备，即使下雪也会坚持他们的习惯。在无限的时间和数据条件下，我们可能能够自己回答这些问题；但与*人类*交流要快得多，信息量也要丰富得多。信不信由你，几乎每个领域都有专家存在。例如，快速搜索[Google Scholar](https://oreil.ly/eGJzD)上关于“季节性共享单车骑行量”的内容，从博客文章到同行评审的研究都应有尽有。

这对数据的完整性意味着什么？现有的研究，或者更好的是与学科专家的实时交流（参见[“学科专家”](app03.html#smes)了解更多），将帮助你决定在分析中包含哪些因素，从而决定你选择的分析需要多少数据才能使其完整。

### 多变量

调整现实世界的数据不可避免地意味着遇到真实世界的复杂性，各种因素可能影响我们试图调查的现象。我们“最繁忙”的Citi Bike站点就是一个例子：除了季节性和交通服务外，[周围的地形](https://www.sciencedirect.com/science/article/abs/pii/S136192091731057X)或站点的密度也可能起到作用。如果我们的Citi Bike数据仅包含关于特定站点启程和结束的骑行次数的信息，那么很难创建出超出“该站点在特定时间段内有最多自行车被取出和归还”的分析。

当数据是多变量的时候，意味着每条记录都有多个*属性*或*特征*与之关联。例如，对于[历史Citi Bike数据](https://citibikenyc.com/system-data)，我们知道在[“利用Citi Bike数据上路”](ch02.html#hitting_the_road_intro)中经过整理之后，我们拥有以下所有信息：

```py
['tripduration', 'starttime', 'stoptime', 'start station id', 'start station
 name', 'start station latitude', 'start station longitude', 'end station id',
 'end station name', 'end station latitude', 'end station longitude', 'bikeid',
 'usertype', 'birth year', 'gender']
```

每次记录的骑行都涉及15种不同的特征，其中任意数量可能能够利用起来更有意义或更细致地理解哪个Citi Bike站点最“繁忙”。

### 原子的

原子数据非常细粒度；它既精确测量，又没有汇总为摘要统计或指标。一般来说，摘要统计量如比率和平均数并不是进一步分析的好选择，因为已经丢失了大量底层数据的细节。例如，以下两组数字的*算术平均数*或*均值*都是 30：20、25、30、45 和 15、20、40、45。当进行不同数据集的比较时，摘要统计量通常很有帮助，但它们对数据底层结构的洞察力太少，以支持进一步分析。

## 可实现性

无论数据集一开始看起来如何，事实是，真正干净、组织良好且无误的数据几乎是不可能的——部分原因是，随着时间的推移，某些事物（如金额）简单地*变得*不一致。因此，作为数据处理者，有些数据质量特征我们应该始终期待需要审查和改进。幸运的是，这正是 Python 的灵活性和可重用性真正发光的地方——这意味着随着我们积累编程技能，这些任务会变得更容易、更快速。

### 一致性

高完整性数据需要在多种不同方式上保持一致。数据集中最明显的一致性类型通常与随时间收集的数据频率有关。个体记录之间的时间间隔*一致*吗？是否存在间隔？数据记录之间的不规则间隔是重要的调查对象（如果不解决），部分原因是因为它们可能是数据收集过程中的干扰或不一致的第一个指标。另一个普遍出现*不*一致数据的来源通常是每当涉及文本的数据字段时：包含名称或描述符的字段几乎不可避免地会包含相同术语的多种拼写。甚至可能看起来很容易标准化的字段也可能包含不同程度的细节。例如，“邮政编码”字段可能包含五位邮政编码条目和更精确的“Zip+4”值。

其他类型的一致性可能不那么明显，但同样重要。例如，度量单位需要在整个数据集中保持一致。虽然这可能看起来很明显，但实际上比你想象的更容易出现*不*一致的情况。比如说，你想看一下十年间苹果的价格。当然，你的数据可能记录了所有价格的美元，但通货膨胀会不断改变一个美元的*价值*。虽然大多数人都知道摄氏度和华氏度的“度”大小不同，但问题并不仅限于此：英制品脉约为568毫升，而美制品脉约为473毫升。事实上，甚至拿拿破仑·波拿巴（Napoleon Bonaparte）身高矮小的描述，可能是19世纪法国英寸（约2.71厘米）和今天版本（约2.54厘米）之间的[不一致](https://britannica.com/story/was-napoleon-short)的结果。

这类不一致性的解决方案是在进行任何比较或分析之前*标准化*您的数据。在大多数情况下，这只是简单算术的问题：您只需选择将所有其他单位转换为其何种解释（文档化的另一重要时刻！）。即使是货币，分析师们通常也会开始将其转换为“真实”（即通货膨胀控制的）美元，使用某一年作为基准。例如，如果我们想比较2009年（上次调整时）美国联邦最低工资的实际美元价值与2021年的差异，我们可以使用由劳工统计局（BLS）维护的通货膨胀计算器[（如此类的）](https://bls.gov/data/inflation_calculator.htm)，看到2009年每小时7.25美元的实际美元价值相当于2021年每小时5.72美元。

### 清晰

就像我们的Python代码一样，我们的数据及其标签理想上应该易于阅读和解释。现实情况是，字段（甚至数据集）描述有时可能只是需要不断与数据字典或其他资源交叉参考的晦涩代码。这也是为什么这种数据完整性几乎总是*数据整理*的结果，而不是其*前提*的一部分。

例如，美国人口调查局的美国社区调查人口和住房估计的表代码是[`DP05`](https://oreil.ly/kD48L)。也许有一些逻辑在其中。但对于人口普查数据的偶尔使用者来说，这显然并不明显，就像列标签`DP05_0001E`一样可能不会^([6](ch03.html#idm45143427035984))。虽然下载这个人口普查表会包含多个文件，帮助你理解文件名和列标题的含义，但要开发出清晰、高完整性的数据集很可能需要相当多的重命名、重新格式化和重新索引，特别是涉及政府生产的数据。然而，无论如何，随着进行的信息来源记录和重命名过程至关重要。

### 结构化维度

结构化的维度数据包含已分组或额外标记的字段，例如地理区域、年份或语言，这些特征通常为数据增强（我们将在[“数据增强”](#data_augmentation)中讨论）和数据分析提供快速入口，因为它们减少了我们自己必须进行的相关性和聚合工作。这些特征还可以作为数据创建者认为重要记录的指示器。

例如，我们的 Citi Bike 数据的维度包括自行车是否被分配给“订阅者”或“顾客”的账户，以及账户持有人的出生年份和性别—表明数据的设计者认为这些特征可能会提供关于 Citi Bike 行程和使用的有用见解。然而，正如我们将在[第7章](ch07.html#chapter7)中看到的那样，他们并*没有*选择包括任何“工作日/假日”指示器—这意味着这是我们如果需要的话需要自己推导的数据*维度*。

# 提高数据质量

正如前面所述，数据质量的许多方面是数据整理过程的产物，无论是调和和标准化单位、澄清不明确数据标签的含义还是寻找关于数据集代表性的背景信息。这部分说明的是，在现实世界中，确保数据*质量*至少部分上是多次迭代的数据整理过程的结果。虽然数据整理过程中这些阶段的术语各不相同，我通常将它们描述为数据*清理*和数据*增强*。

## 数据清理

实际上，“数据清理”不是数据整理过程中的独立步骤，而是伴随每一个*其他*步骤而进行的持续活动，这是因为大多数数据在我们接触时都*不*是干净的，而且数据集（或其部分）需要“清理”的方式通常在工作中逐步显现。从高层次来看，干净的数据可能被概括为没有错误或拼写错误，如不匹配的单位、同一单词或术语的多种拼写以及未分隔良好的字段和缺失或不可能的值。虽然这些问题中许多至少在某种程度上是容易识别的（尽管并非总是容易纠正），但更深层次的数据问题仍然可能存在。测量变化、计算错误以及其他疏忽，尤其是在系统生成的数据中，通常在进行了一定程度的分析并与具有重要专业知识和/或第一手经验的人员进行了“现实检查”后才会显现出来。

数据清洗的迭代性质说明了数据整理是一个循环过程，而不是线性的步骤序列：随着你的工作揭示了关于数据的更多信息，你对其与世界关系的理解加深，你可能发现需要重新审视之前完成的工作，并重复或调整数据的某些方面。当然，这也是为什么记录你的数据整理工作如此关键的另一个原因：你可以利用你的文档快速识别何处以及如何进行了任何更改或更新，并与现在的知识相调和。如果没有健全的文档来指导你，你可能很快就会发现自己需要从头开始。

## 数据增强

*增强*数据集是通过扩展或详细说明来实现的，通常是通过与其他数据集连接——这实际上是21世纪“大数据”的本质。通过使用在数据集之间共享的特征，可以将来自多个来源的数据汇集在一起，以更完整地描述发生在世界中的情况，填补空白，提供协同措施或添加帮助我们更好评估数据适应性的上下文数据。在我们的“最佳”学校示例中，这可能意味着使用学校代码将不同实体收集的几种数据，如州级标准化考试成绩和本地建筑信息，汇总到一起，如[“数据解码”](#data_decoding)中提到的。通过有效的研究和数据整理的结合，数据增强可以帮助我们建立数据质量，并回答通过任何单一数据集无法解决的细微问题。

与数据清洗类似，数据增强的机会可能在数据整理过程的几乎任何时候出现。同时，我们引入的每个新数据集都将产生其自身的数据整理过程。这意味着，与数据清洗不同，数据增强没有明确的“结束状态”——总会有另一个可以添加的数据集。这也是为什么明确和简明地在数据整理问题前提上述明是如此重要的另一个原因：如果没有明确表达关于你试图调查的内容的声明，你很容易在数据整理工作中耗尽时间或其他资源。好消息是，如果你一直在保持你的数据日记，你将永远不会失去对未来项目中有用的有前途的数据集的追踪。

# 结论

由于到目前为止我们的实际数据操作有限，本章讨论的许多概念可能现在看起来有点抽象。别担心！接下来的章节中，我们将开始处理来自不同格式和不同来源的数据，深入了解数据质量的各种特征是如何影响我们在访问、评估、清理、分析和呈现数据时做出决策的。到本卷结束时，您将能够创建有意义、准确和引人入胜的数据分析和可视化，与世界分享您的见解！

在下一章中，我们将通过处理各种格式的数据，将其结构化，以便进行所需的清理、增强和分析。让我们开始吧！

^([1](ch03.html#idm45143427340352-marker)) 例如，参见埃曼纽尔·马丁内斯和劳伦·克尔施纳在[*The Markup*](https://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval-algorithms)上的文章“抵押贷款批准算法中隐藏的秘密偏见”。

^([2](ch03.html#idm45143427337760-marker)) “世界上最有价值的资源不再是石油，而是数据”，见[*经济学人*](https://economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data)。

^([3](ch03.html#idm45143427320640-marker)) 例如，参见亚德里安·拉弗兰斯在[*大西洋*](https://theatlantic.com/technology/archive/2015/10/raiders-of-the-lost-web/409210)上的文章“失落网络的突袭”。

^([4](ch03.html#idm45143427230496-marker)) 要了解简明易懂的概述，请参见阿米塔夫·巴纳吉和苏普拉卡什·乔杜里的[“无泪统计”](https://ncbi.nlm.nih.gov/pmc/articles/PMC3105563)。

^([5](ch03.html#idm45143427185920-marker)) 此列表改编自斯蒂芬·尤的优秀著作*《Now You See It: Simple Visualization Techniques for Quantitative Analysis》*（Analytics Press），并根据我的数据处理经验进行了调整。

^([6](ch03.html#idm45143427035984-marker)) 顺便提一下，它表示总人口估计。

^([7](ch03.html#idm45143426993168-marker)) 详细讨论大数据，请参阅丹娜·博伊德和凯特·克劳福德的[“大数据的关键问题”](https://tandfonline.com/doi/full/10.1080/1369118X.2012.678878)。
