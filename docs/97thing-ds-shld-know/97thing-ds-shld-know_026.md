# 第二十四章：对于正确是否错误？

# Marty Ellingsworth

![](img/marty_ellingsworth.png)

Celent 高级分析师

更好的数据、更好的模型和更好的决策引擎带来更好的结果。无论是手动训练模型，使用自动化机器学习系统，还是采用任何最新的“x”学习网络进行预训练，只要模型表现出色，为什么它能够表现得如此出色并不重要呢？最精确的模型不应该在所有情况下都能够使用吗？这就是进步，对吧？应该允许您的模型自行学习以进一步推动此进展并进行更好的个性化吗？实际上，如果没有伤害，那么就没有犯规，对吧？

学术上，许多人认为这一切都关乎对数据的提升和修正。在职业上，关怀标准仍在逐步形成中。有数十个实践领域，其中更好的模型和更好的数据似乎会神奇地、定期地和规模化地出现，以获得更好的结果——无需质疑——从而使用户、发明家和投资者感到满意。隐私权倡导者发出警告。

在法律上，我们正处在科技飞速发展的阶段，远远超出了现有法律和合规法规的范畴。今天许多控制措施是在上世纪 70 年代和 80 年代关于就业公平、贷款和住房方面出现的，并且在上世纪 90 年代和 21 世纪初关于欺诈、盗窃、勾结、洗钱和恐怖主义问题方面出现的。已知的法律条文通常定义得很明确，但是这些最接近数据的人通常对其并不了解。往往存在复合定义，将某事物定义为禁止或受限（谁、什么、何时、何地、为何、如何、多少、以何方式、在什么使用案例中、适合什么年龄群体以及在哪个司法管辖区）。这可能导致更多的情况未被界定而非被预言。如果不违法，那还可以吗？

从伦理角度看，“什么是可以接受的”往往是一个不断变化的目标。业务影响推动了对更好结果的需求，公众也可能善变，并且为特定信息可能会被说服甚至是支付价值。在数据科学中，谁是伦理门户，何时他们参与，以及在情况变化时何时重新参与？这是一个新兴的问题。

实际上，道路上的橡胶通常会显示出多种时间上的打滑痕迹。当优秀的预测对子群体不起作用时，或者当特定受保护的子群体被预测时，会发生重大打滑。何时预测，何时不预测可能是一个比“商业道德”更广泛的伦理问题。

许多老板会打击报信者。大多数伦理部门都源于某种失误。许多小公司甚至没有伦理部门。非美国公司可能具有不同的伦理标准，同行业内服务相同客户和相同地理位置的不同公司也可能如此。哪种伦理框架应当成为标准尚不明确。在那之前，更常见的情况将是对受保护类别的不同影响或违法变量的侵害。这可能是由于基础数据引入的偏见，特定类型数据的管理不善，或者可嵌入微小细节的区分特征（特别是在考虑文本、图像、扫描、诊断、声音、视频、时间、交易、生物、人口统计、医疗、牙科、地理位置、传感器、社交网络、知识图谱关联、过去历史、当前活动、行为、在线活动、购买、个人互动和位置数据时）造成的。

数据、分析和决策支持系统，特别是那些可以事后学习的系统，都存在风险。它们可能学习到不良行为，过度代表不平衡的人群统计数据，“作弊”通过模拟处理器在流程中的行为，无意间利用非法和受保护信息，或者在数据样本中找到在生产环境中可能漂移或消失的特征。

自动化机器学习和需要仅进行组件装配的人工智能正在将更多的分析模型构建能力交给业务人员和其他“公民数据科学家”。但是，简化模型构建并不能解决选择好问题、以伦理为核心解决问题的艺术、技艺和伦理问题。

在分析实践中，很多时候，问题定义的艺术和在生产中使用数据之前了解其背景的技艺与良好模型的表现喜悦及近期交付日期的紧迫性发生冲突——更不用说似乎在初创公司领域的独角兽群体上空似乎如雨般的数百万和数十亿美元。

目的是否能够证明手段正当？这是每个数据科学家以及他们的雇主、董事会、审计员、监管者和投资者都需要考虑的问题。
