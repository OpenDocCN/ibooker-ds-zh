# 第十二章：不偏不等于公平：对于数据科学而言，不能只关注数学问题。

# Doug Hague

![](img/Doug_Hague.png)

北卡罗来纳大学夏洛特分校数据科学学院执行主任

当我深思数据科学中的伦理影响时，有一件事对我变得非常明显：数据科学家喜欢数学！这并不奇怪。但是当我们建立模型并做出伟大的预测时，我们往往将关于伦理的讨论简化为数学术语。我的对白人美国人的预测和对非洲裔美国人的预测相同吗？女性的预测是否等同于男性的预测？我们制定混淆矩阵并测量预测的准确性。或者也许灵敏度（真正例率）或特异性（真负例率）很重要，所以我们为各个子群体进行平衡。不幸的是，数学家们已经表明，虽然我们可以在真实数据集上平衡准确性、特异性或其他偏见度量，但我们不能平衡所有这些并创建完全无偏见的模型。因此，我们在所提供的框架内尽力而为，并宣称我们的模型是公平的。

在研究了相关问题和应用之后，我断言平衡偏见的模型并不公平。公平真正关注的不是数学，而是个体观点、社会文化规范和道德。换句话说，公平由社会系统和哲学定义。

例如，在刑事司法中，累犯模型预测被捕人员在获得保释后是否会再次犯罪。作为被起诉的个人，您认为误判率应尽可能低，这样您就不会在不应该被关押时被关押。然而，普通市民希望尽可能降低误放人员再犯的可能性，因此希望将假阴性率尽可能降低。平衡这两者是一个权衡，双方都会说这不公平。而且我们甚至还没有开始讨论数据和系统中导致非裔美国人被不成比例地关押的偏见问题。

当人们考虑数据科学的伦理影响时，很快就会讨论模型被部署到的社会的文化和道德规范。当数据科学团队部署模型时，必须考虑这些文化规范。功利主义及其衍生物在西方社会中很常见；在这里，整体利益的角色被辩论，个体利益和共同利益之间的平衡被讨论。在其他文化和地理区域，不同的哲学构想得到青睐。了解模型将触及哪些文化以及它将如何触及它们，对于实现部署模型的公平性至关重要。

理解模型部署所处的系统同样重要。随着模型的部署，它们进入一个运行系统。根据具体情况，经常在模型预测之后做出决策。通常数据科学家根据数学预测开发和衡量模型的准确性。然而，测量整个系统和模型预测之后发生的决策同样重要。此外，人在回路模型通常被认为更准确；然而，它们是否也更少偏见和更公平？在人为因素参与的情况下，偏见可能重新蔓延到决策中。此外，如果有多个决策者，不同的人会带来不同水平的信息以及文化差异。这些差异每一个都可能导致系统偏见和公平问题，即使模型经过调优和准备，力求尽可能公平。应该为模型结果以及系统结果制定运作框架并进行性能衡量。我认为许多关于公平和歧视的诉讼之所以发生，是因为双方对情况的框架不同。在各自的框架内，每一方都是“正确”的，但陪审团会认定哪个框架更公平？

作为负责任的数据科学家，我们应该将我们的道德考虑扩展到模型的数学偏差之外，包括文化和社会对公平的定义，而我们的模型部署应考虑到系统的结果框架，而不仅仅是模型预测。
