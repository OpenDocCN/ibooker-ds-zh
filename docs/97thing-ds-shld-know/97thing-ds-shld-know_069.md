# 第六十四章：模型可解释性的伦理困境

# Grant Fleming

![](img/Grant_Fleming.png)

数据科学家，Elder Research Inc.

数据科学的进展在很大程度上是由越来越复杂的“黑盒”模型的预测性能不断改进推动的。然而，这些预测性能的提升是以失去解释模型预测器和目标之间关系的能力为代价的，这导致了误用和公众争议。这些缺点表明*可解释性实际上是一个伦理问题*；数据科学家应努力实施额外的可解释性方法，以维持预测性能（模型复杂性），同时最小化其危害。

任何对“AI”或“数据科学”的学术或流行文献的检视都明显显示出对最大化预测性能的深刻重视。毕竟，最近在模型设计方面的突破以及由此带来的预测性能改进已经使模型超过了医生在检测多种医学问题方面的表现，并且超过了人类的阅读理解能力。这些突破得益于从线性模型向深度神经网络（DNN）和梯度提升树（例如 XGBoost）等黑盒模型的过渡。这些黑盒模型不再使用特征的线性变换来生成预测，而是采用复杂的非线性特征变换来产生更高保真度的预测结果。

由于其背后复杂的数学原理，这些黑盒模型扮演了神谕的角色，能够在不提供可解释的人类解释的情况下产生预测结果。尽管这些预测通常比线性模型更准确，但远离线性模型内置的可解释性可能会带来挑战。例如，无法解释模型的决策规则可能会使得用户、客户和监管机构难以建立信任，即使这些模型在其他方面设计良好且有效。

放弃模型可解释性也为科学界带来了伦理困境。通过提高我们预测世界状态的能力，黑盒模型牺牲了一部分帮助我们*理解*这些预测背后推理的能力。经济学、医学和心理学的整个子领域依赖于成功将线性模型解释转化为政策建议。对于这些任务来说，预测性能通常次于探索模型在其预测器和目标之间创建的关系。仅关注预测性能可能会削弱我们在这些领域的理解，甚至可能阻止本应从更透明的模型中获得的未来发现。

在公共政策和科学领域之外，放弃模型可解释性带来了更直接的挑战。在[健康保健](https://oreil.ly/OLcSU)、[法律系统](https://oreil.ly/Mi7MW)和企业[招聘流程](https://oreil.ly/HFLet)中，错误应用的黑盒模型无意中伤害了它们本应服务的人群和组织。在这些案例中，黑盒的预测显然是不准确的；然而，鉴于模型的本质，难以在部署前调试和检测潜在问题。这些情况理所当然地引发了公众对数据科学伦理的争议，同时也引发了对算法数据收集、透明度和公平性的[更强监管要求](https://oreil.ly/OzS46)。

平衡复杂性和模型可解释性显然是一个挑战。幸运的是，有几种*可解释性方法*允许数据科学家在一定程度上理解复杂黑盒模型的内部运作，否则这些是无法知晓的。应用这些方法可以在保持任意黑盒模型改进的预测性能的同时，恢复部分线性模型移开时失去的可解释性。

个体解释性方法可以服务于各种功能。例如，全局解释性方法如部分依赖图（PDPs）可以提供对特征对预测影响的诊断可视化。这些图表描述了黑盒模型输入和输出特征之间的数量关系，并允许类似于线性模型系数如何使用的人类解释。像 Shapley 值这样的局部方法可以为个体预测的特定特征值的影响提供解释，通过展示模型依赖于特定特征来增加用户信任。这些方法提供的增强的洞察力也使模型调试工作变得更加简单，表明即使已经表现良好的黑盒模型也存在提升性能的机会。

道德数据科学当然不仅仅是能够解释模型的内部功能和输出。然而，为何模型可解释性应成为道德最佳实践的一部分的论据是有说服力的。将解释性方法整合到黑盒模型中的数据科学家正在提升其工作的道德尽职调查；这是保持模型可解释性的同时利用黑盒模型巨大潜力的方法。
