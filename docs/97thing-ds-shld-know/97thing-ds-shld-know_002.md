# 第一章：真相关于 AI 偏见

# Cassie Kozyrkov

![](img/cassie_kozyrkov.png)

谷歌云的首席决策科学家

没有技术是免于其创造者的。尽管我们对科幻小说充满期望，但并不存在真正分离和自主的 AI 系统……因为它们始于*我们*。尽管其影响可能在你按下按钮后久久不散，所有技术都是其建造者愿望的回声。

## 数据和数学并不等同于客观性

如果你将 AI 视为从人类缺陷中拯救你的救世主，要小心行事。当然，数据和数学可以增加你在决策中使用的信息量，和/或者避免你在当下热情时的愚蠢，但是如何使用它们仍然取决于你。

听着，我知道科幻小说卖座。说“AI 学会了自己完成这项任务”比告诉真相更加耀眼：“人们使用一个名字酷炫的工具来帮助他们编写代码。他们输入他们认为合适的例子，发现其中的模式，并将这些模式转化为指令。然后，他们检查这些指令对他们的影响是否令他们满意。”

真相充满了人类的主观性——看看沿途所有那些留给项目负责人自行决定的小选择吧。*我们应该将 AI 应用于什么？这值得吗？在什么情况下？我们如何定义成功？它需要多好地运行？* [问题层出不穷](https://oreil.ly/FkHtg)。

令人哀笑的是，将数据添加到混合物中掩盖了永存的人类因素，创造了客观性的幻象。在核心周围包裹一层华丽的数学外衣，并不会使其变得不那么“软软的”。

技术始终来源于并由人设计，这意味着它不比我们更客观。

## 什么是算法偏见？

算法偏见指的是计算机系统反映其创建者的隐含价值观的情况。按照这一定义，即使是最良性的计算机系统也是有偏见的；当我们将数学应用于某一目的时，该目的受我们这个时代的感性影响。AI 豁免吗？一点也不。停止将 AI 视为一个实体，看清它真正的面貌：写代码的极佳工具。

AI 的整体目的是让你用例子（数据！）而不是指令向计算机解释你的意愿。哪些例子？这取决于你试图教会你的系统做什么。把你的数据集想象成你要求机器学生学习的教科书。

## 数据集由人类作者编写

当我说“AI 偏见不是来自 AI 算法，而是来自人”，一些人写信告诉我我错了，因为偏见来自数据。好吧，我们都可以是赢家……因为数据是由人创造的。就像教科书一样，数据集反映了其作者的偏见。

请考虑以下图片。

![图片](img/aeds_01in01.png)

你的第一个想法是“香蕉”吗？为什么你没有提到塑料袋卷或香蕉的颜色？这个例子来自谷歌的 AI 公平培训课程，它表明，尽管这三个答案在技术上都是正确的，但你有偏好倾向于更喜欢其中一个。并非所有人都会分享这种偏好；我们的感知和反应受到我们的规范的影响。如果你生活在一个所有香蕉都是蓝色的星球上，你可能会在这里回答“黄色的香蕉”。如果你以前从未见过香蕉，你可能会说“架子上放着黄色的东西”。这两个答案也是正确的。

你为系统创造的数据将受到你如何看待世界的偏见的影响。

## 这并不是做一个混蛋的借口

哲学上的论证无效化真正无偏和客观技术的存在并不意味着任何人可以以此为借口成为一个混蛋。如果有什么，你不能把伦理责任推给机器的事实更多地把责任放在你的肩膀上，而不是减少。

当然，我们的观念是由我们所处的时代塑造的。关于美德、正义、善良、公平和荣誉的社会观念与几千年前居住在此地的人们的观念并不相同，它们可能会不断演变。这并不意味着这些观念不重要；这只意味着我们不能把它们外包给一堆电线。它们是我们所有人共同的责任。

## 公平性在人工智能中

一旦你意识到*你*要负责如何使用你的工具以及你将它们指向何处，就努力让自己意识到你的选择如何影响其他人类。例如，决定追求哪个应用程序是一种影响其他人的选择。要仔细考虑。

你另一个选择是使用哪些数据来进行 AI。你应该期望在与你的系统学习相似的示例上表现更好。如果你选择不使用像我这样的人群的数据，当我作为你的用户出现时，你的系统更有可能出错。当这种情况发生时，你有责任考虑可能造成的痛苦。

至少，我希望你有常识去检查你的用户群体的分布是否与你的数据分布相匹配。例如，如果你的所有训练示例都来自一个国家的居民，但你的目标用户是全球的...预料到会出现问题。

## 公平与意识

我在这里写了很多字，当我本可以告诉你，在 AI 偏见和公平性的研究中，主要关注的是确保你的系统对某些用户群体相对于其他群体没有不成比例的影响。AI 伦理的主要焦点是分布检查和类似的分析。

我写这么多的原因是希望你能做得更好。自动化分布检查只能做到这么多。没有人比系统的创造者更了解这个系统，所以如果你正在构建一个系统，请花时间考虑你的行动将影响谁以及如何影响，并尽力让那些受影响的人有发言权，以引导你解决盲点。
