# 第九章：机器学习算法时代的公平性

# 安娜·雅各布森

![](img/Anna_Jacobson.png)

加州大学伯克利分校数据科学硕士候选人

在数据科学领域进行的所有激动人心的工作中，机器学习算法（MLA）是吸引最广泛关注的进步之一，对许多人来说，它是数据科学领域未来最有前途的领域。然而，就像所有强大的技术一样，机器学习算法也存在成为世界破坏性力量的风险。

早期机器学习算法的应用包括电子邮件垃圾邮件过滤、图像识别以及娱乐推荐系统。在这些低风险的环境中，任何错误的代价都很低，通常最多只是轻微的不便。然而，随着它们开始应用于人类，如在预测性警务中，机器学习算法的错误成本显著增加。尽管机器学习算法训练过程表面上看起来是客观的，但有时会导致从人类角度看来偏见和不公正的算法输出。在高风险环境中，产生不公平结果的机器学习算法可能会造成巨大的伤害。

在机器学习的实践中，算法的质量是根据其准确率（正确结果的百分比）、精确度（不将负样本误标为正的能力）或召回率（找到所有正样本的能力）来判断的，公平性是一个难以捉摸的概念。决定这三种度量中哪一种最能代表公平性并不总是直截了当的，而且在一个度量指标上的改进可能会导致其他指标的下降。

从根本上说，机器学习算法的公平性取决于数据本身。如果底层数据在任何方面都存在偏见，那么这些结构性的不平等可能不仅会被复制，甚至可能在算法中被放大。机器学习工程师必须意识到自己的盲点；他们对训练数据所做的所有小决定可能对其工程技术一样有重大影响。然而，更为问题的是，社会问题如歧视和排斥深深扎根于我们周围的世界之中，因此它们也存在于我们从世界中提取的数据之中。

实现算法公平性似乎与实现人类主导系统的公平性一样困难。人类系统在所有算法系统存在偏见的方式中都存在偏见，而人类还以机器无法做到的方式存在偏见。然而，算法系统可能既不那么显眼也不那么透明：通常，人们不知道正在使用算法来做出影响他们的决定—即使他们意识到了，算法也呈现为一个复杂、不可知的“黑箱”，根本无法看到，更别说理解了。

为了提高算法的公平性，必须采取三个明确的步骤：

1.  首先，我们必须做得更好，以确保用于训练算法的数据质量。例如，所有受试者应有平等的机会在数据中被代表，这意味着可能需要额外的努力来获取传统上未被充分代表的群体的数据。此外，模型还必须定期用新数据重新训练，以开始消除历史偏见，尽管这会增加额外的费用。

1.  其次，在机器学习领域内，必须在整个行业标准化流程，以尽可能消除工程过程中的偏见。这应包括一系列方法，包括为工程师提供类似情报分析员定期接受的无意识偏见培训；类似于科学研究严格同行评审的工程协议；以及独立的后实施审计，评估算法公平性的质量，评估算法的标准不仅限于标准工程指标，还包括其对受其影响最脆弱的人群的影响。

1.  第三，MLA 必须在我们社会中被揭示，以便我们都知道它们在影响我们生活的方式上的使用；一个信息充足的公民群体对于保持创建和使用这些算法的团体的责任性至关重要，以确保它们的公平性。我们在宪法上有权获得正当程序和平等保护；我们应将这些权利解释为包括了解用于输入的数据以及在使用 MLA 时生成的任何输出的权利。

采取这些步骤将需要社会各界的深刻变革，涵盖多个利益相关者和多个领域。在一个法律和传统从未预见到 MLA 力量的世界中，确保机器学习系统公平性的责任属于所有在其中工作或与之合作的人。随着 MLA 在我们社会中的普及，人类在处理这一问题上的角色变得愈发关键，以确保这项技术兑现其造福社会的承诺，而不是潜在的危害。

## 参考资料

+   [维亚切斯拉夫·波隆斯基](https://oreil.ly/TIKHr)，“缓解预测正义中的算法偏见：AI 公平性的四个设计原则”，Towards Data Science，2018 年 11 月 23 日，[*https://oreil.ly/TIKHr*](https://oreil.ly/TIKHr)。

+   [加尔·约纳](https://oreil.ly/NbVOD)，“算法公平性讨论的初步介绍”，Towards Data Science，2017 年 10 月 5 日，[*https://oreil.ly/NbVOD*](https://oreil.ly/NbVOD)。

+   [莫里茨·哈特](https://oreil.ly/kNrtx)，“大数据的不公平性”，Medium，2014 年 9 月 26 日，[*https://oreil.ly/kNrtx*](https://oreil.ly/kNrtx)。

+   [朱莉娅·安格温](https://oreil.ly/b41AW)，[杰夫·拉尔森](https://oreil.ly/b41AW)，[苏里亚·马图](https://oreil.ly/b41AW)和[劳伦·柯克纳](https://oreil.ly/b41AW)，“机器偏见”，ProPublica，2016 年 5 月 23 日，[*https://oreil.ly/b41AW*](https://oreil.ly/b41AW)。

+   Hugo-Bowne Anderson，“数学毁灭的武器（与凯西·奥尼尔）”，2018 年 11 月 26 日，在*DataFramed*，播客，55:53，[*https://oreil.ly/5ScpO*](https://oreil.ly/5ScpO)。

+   Kate Crawford 和 Jason Schultz，“大数据与正当程序：走向补偿预测性隐私伤害的框架”，*波士顿学院法律评论*，第 55 卷，第 1 期（2014 年 1 月 29 日），[*https://oreil.ly/X_W8h*](https://oreil.ly/X_W8h)。

+   “关于预测性警务的关注声明由 ACLU 和 16 个民权、隐私、种族正义和技术组织共同发表”，美国公民自由联盟，2016 年 8 月 31 日，[*https://oreil.ly/_hZHO*](https://oreil.ly/_hZHO)。

+   Sam Corbett-Davies, Emma Pierson, Avi Feller, and Sharad Goel，“一个计算机程序用于保释和判决决策，被标记为对黑人有偏见。事实上并不那么清楚”，*华盛顿邮报*，2016 年 10 月 17 日，[*https://oreil.ly/cQMJz*](https://oreil.ly/cQMJz)。

+   Mark Puente，“洛杉矶时报在批评后放弃一些犯罪数据程序”，*洛杉矶时报*，2019 年 4 月 5 日，[*https://oreil.ly/JI7NA*](https://oreil.ly/JI7NA)。
