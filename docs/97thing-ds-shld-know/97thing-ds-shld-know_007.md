# 第六章：规则与理性

# 克里斯多夫·沃尔夫·布伦纳

![](img/christof_wolf-brenner.png)

咨询师，Know-Center GmbH

在艾萨克·阿西莫夫著名的科幻故事中，一套层次分明的法律作为核心，确保人工道德代理的伦理行为。这些机器人——既是计算机也是机器——能够高效处理复杂任务，而这些任务原本需要人类水平的智慧来完成。

阿西莫夫认为，他的规则集是理性人类与适应并灵活选择自己行动路线的机器人之间互动的唯一合适基础。今天，几乎 80 年过去了，自从 1942 年第一次制定这些法律以来，死忠粉丝仍然认为阿西莫夫的法律足以指导道德决策。然而，看看阿西莫夫在 1985 年最终确定的规则集，就会清楚地发现，单独应用这些法律可能无法产生我们所称的“好决策”：

零法则

一台机器人不得伤害人类，或者通过不作为，允许人类受到伤害。

第一法则

一台机器人不得伤害任何人类，或者通过不作为，允许任何人类受到伤害。

第二法则

一台机器人必须遵循人类给它的命令，除非这些命令与第一法则相冲突。

第三法则

机器人必须保护自己的存在，只要这种保护不与第一法则或第二法则相冲突。

阿西莫夫的自治伦理代理能够根据它们处理的世界信息和上述嵌入其人工大脑中的法律组合来评估情况并采取相应行动。然而，随着不同法律之间的冲突出现，机器人也能够反思、推理并得出合理的结论。这一微小且常被忽视的细节提供了一个初步的迹象，说明静态规则集可能无法单独充分支持道德决策制定，而艾萨克·阿西莫夫很可能已经意识到这一点。至少，即使他强烈推广法律的唯一适用性，他的情节通常围绕无法作出明确决定的边缘案例展开，因此需要进一步的推理。请考虑，如果你愿意，涉及自动驾驶车辆的事故，设置类似著名的电车问题：

> 一辆完全自动驾驶的汽车——在阿西莫夫术语中是一个机器人——正将一个人类（A）运送到目的地。突然，命运弄人，一些生物（B）出现在道路上。控制车辆的人工智能（即计算机）必须在一秒钟的分秒之间做出决定：采取规避行动还是继续前进。如果它尝试躲避 B，车辆打滑撞到树上，A 死亡，B 幸存。如果不躲避，A 生存，B 死亡。为了简化起见，我们假设附带损害是可以忽略不计的，或者在两种情况下是相同的。

基于这种边缘情景，我们可以推断出阿西莫夫法则存在两个主要问题。首先，如果机器人汽车必须在伤害人类和伤害非人类之间做出决定，非人类总是处于劣势。这导致了种族主义倾向的机器人——即，机器人因为某种生物的物种而对其偏爱或偏见。如果 B 是一组动物或某种动物的最后一批，我们肯定至少应该考虑撞死它们或它的后果。

其次，这些法则并不适合在可能发生对人类造成不同程度伤害的情况下支持决策。如果一个情景的所有潜在结果都涉及对人类的伤害，法则集将无法指导我们做出决定：如果所有替代方案都符合法则，它们是同等好的。如果结果可能导致一个人失去一只胳膊或另一个人失去两只胳膊，也没有优先选择。相反，阿西莫夫的法则纯粹设计用来优先考虑人类或整个人类群体。即使我们很容易基于理性辩护一个行动方案优于另一个，如果机器人只是遵循法则，它将无法这样做。

如果阿西莫夫的法则被视为机器人进行道德决策的基础，它们还需要能够在边缘案例中理性地辩护以争取更好的结果或反对更坏的结果。理性必须成为法则之间的粘合剂。但那么为什么首先使用静态法则？使用我们自己的理性来决定哪些行为是好的不是更容易吗？
