# 第十七章：首先，不要伤害

# 艾瑞克·施密特

![](img/Eric_Schmidt.png)

可口可乐全球数据与分析总监

现在是 2019 年 10 月，我正在亚特兰大历史医学学院参加一个数据科学会议。在男性洗手间的一个角落里，不起眼地摆放着希波克拉底的半身像，这位职业伦理学之父的半身像旁边还有一个写着希波克拉底誓言的牌匾。希波克拉底誓言的基本精神是*primum non nocere*，即“首先，不要伤害”。这是一个非常好的概括，代表了一个数千年来一直存在的伦理框架。作为一个新兴的专业领域，数据科学正处于定义我们伦理框架的初期阶段。

当我凝视着希波克拉底的半身像时，我想问他：谁来定义伦理？科技公司的领导者一直在这个讨论的中心，但我想知道他们是否在利用已有的伦理哲学来解决这个问题。与其发明新的框架，拉里·佩奇、谢尔盖·布林、马克·扎克伯格、史蒂夫·乔布斯、拉里·埃里森、比尔·盖茨、杰夫·贝索斯或者马云能从苏格拉底、孔子、霍布斯、洛克、康德和尼采那里学到关于数据隐私或其他现代伦理困境的什么呢？例如，霍布斯和洛克可能会因社会契约理论的差异而试图说服扎克伯格朝着更或者更少绝对政府管制的方向发展。如果我们将数据科学的伦理问题留给技术官员，并允许公司按照它们的天性行事，他们会像霍布斯所说的那样驱使社会走向无情吗？当然，我们可以在企业行为中找到这种无情的证据。

在大多数职业中，从业者定义了伦理。比如“律师与客户的保密权”，“保护与服务”，“寻求真相”，“不泄露你的消息来源”，“为人民服务”：这些口号都概括了一个职业的伦理准则。虽然这些准则可能在历史伦理哲学的基础上有些依据，但职业是在从社会中得到反馈的过程中发展出他们的伦理，随着标准的演变而更新。最终的伦理准则只有在从业者对其进行解释时才有效。此外，只有专业人士对其进行实施，历史表明这并不总是发生的。如果我们让数据科学从业者为该领域定义伦理，他们能够达到并自我实施这些准则吗？

或许数据科学领域甚至不需要伦理准则。毕竟，数学怎么可能不道德？2 + 2 = 4，不是吗？这个等式不在乎它得到了什么样的输入；它客观地将两个数相加而没有偏见。然而，机器学习更加复杂，一个论点是，如果数据科学家使用不完整或者不具代表性的训练数据，他们可能会无意中创建有偏见的模型。在我看来，这是一个效能问题，而不是伦理问题。

如果机器学习或人工智能本质上既不是善也不是恶，那么我们应该考虑数据科学的应用。例如，我们可以考虑使用数据科学来识别患有癌前病变的人，以便推销人寿保险或推荐医疗检查。哪种用例是符合伦理的？只要我们能拯救患者的生命，这两种情况都可以接受吗？如果伦理真的关乎应用，那么从业者们可以简单地在应用的地方采纳伦理。这个领域可能不需要自己的框架。

或许伦理问题在于谁应该有权访问数据科学方法。在 20 世纪 90 年代，技术竞赛主要集中在计算能力上。美国政府出于国家安全原因限制了高性能计算机的出口，根据 1979 年出口管理法案。苹果计算机在 1999 年发布了 Power Mac G4，“第一台桌面超级计算机”，充分利用了这种情况。这个前提，可能更多是营销而非现实，是 G4 的出口应受限制，以防止流氓国家使用计算机开发核武器或其他先进武器。基于这个前提，应该限制先进的数据科学方法，以防止流氓行为者、组织或国家造成伤害吗？这似乎与数据科学的开源文化背道而驰，但正如洛克可能会主张的那样，为了确保其他更重要的自由，可能需要自愿放弃一些自由。伦理很难，而且可能会有一些成本。

最后一个想法：我对希波克拉底雕像的问题仍然存在。谁应该为数据科学定义伦理——企业领导、应用领域、从业者，还是可能是一个开源伦理人工智能？考虑到当今世界的复杂性，正确理解伦理是一个巨大的挑战。一个好的起点可能是像希波克拉底几千年前为医学界确立的简单黄金法则：首先，不要伤害他人。
