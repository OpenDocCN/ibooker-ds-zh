# 第四十五章：在模型表现出结果之前不要进行概括

# 迈克尔·亨德

![](img/Michael_Hind.png)

IBM 研究 AI 卓越研究员

机器学习的惊人进展源于其能够在（通常是大规模的）训练数据集中找到模式的能力。这种能力可以导致预测与甚至超过人类在同一任务上的预测。然而，这些系统有时会被一些不会迷惑人类的预测任务所欺骗。一个例子是一个机器学习系统可以正确识别街道标志，如停车标志，但[会错误地预测一些贴了几个黑白贴纸的停车标志实际上是速限标志](https://arxiv.org/pdf/1707.08945.pdf)。

这种令人惊讶的能力不足的原因是，机器学习系统与人类的预测方式不同。它们寻找各种结果组的区别模式，比如哪些贷款申请者应该获批或被拒绝。然而，人类则应用模式识别和推理的结合。机器学习系统中缺乏这一推理步骤可能会导致意想不到的结果，就像停车标志的例子一样。

公众对机器学习（AI）的印象如下：

+   AI 有时“思考”的能力比人类更好。

+   AI 很容易被欺骗，因此不值得信任。

结果是一个超人类技术，不值得信任。请在此处插入您喜欢的电影剧本。

我们能做些什么呢？嗯，作为数据科学家，我们无法控制媒体如何描绘我们的工作，特别是考虑到技术工作需要为非技术人员总结。然而，我认为在这种沟通管道的开始阶段，我们对增加对 AI 系统的信任还不够。我们需要更明确地说明我们的系统实际上是如何运作的。

让我们考虑一个例子。假设我们开发了一个模型，用于预测纽约布鲁克林的贷款申请者的信用价值。该模型考虑薪水、债务、房屋和车辆所有权等信息，以预测申请者是否会偿还他们的贷款。该模型在布鲁克林进行了测试和部署，并显示出 95%的准确率。基于这一成功，公司正在考虑将模型部署到北达科他州俾斯麦。我们是否应该期望在俾斯麦也能获得同样的准确率，那里的其他因素，如房屋和车辆所有权，可能与布鲁克林有很大的不同？我认为我们如何描述布鲁克林的经验将极大地影响对俾斯麦部署的期望。考虑以下两个说法：

+   该模型能以 95%的准确率预测信用价值。

+   该模型能以 95%的准确率预测纽约布鲁克林的申请者的信用价值。

尽管两个声明只在“适用于纽约布鲁克林的申请人”这些词语上有所不同，影响却可能很大。第二个声明准确描述了模型的特征，而第一个声明暗示着模型以 95%的准确率在一般情况下工作。第二个声明对来自其他地方甚至纽约其他地区的申请人没有明确或暗示性的声明。它鼓励有兴趣的一方去问：比斯马克的贷款申请人是否和布鲁克林的贷款申请人相似？

由于两个城市之间的多个因素可能平均上会有很大差异，人们需要在对比斯马克的模型进行广泛测试之后才能对其有效性有信心。

*泛化* 是指模型是否真正适用于测试数据集（布鲁克林）之外的一般输入（比如俾斯麦或其他地方）。因此，数据科学家们，请准确描述模型的结果，不要在你知道模型是否泛化之前对模型的能力进行泛化的声明！
