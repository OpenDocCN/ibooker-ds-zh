# 第五十六章：公然歧视性的算法

# 埃里克·西格尔

![](img/Eric_Siegel.png)

创始人，预测分析世界

想象一下，你坐在一个正在接受工作、贷款或假释评估的人的对面。当他们问到决策过程是如何运作的，你告诉他们：“其中一件事是，我们的算法因为你是黑人而将你的分数降低了七分。”

我们正朝着这个方向前进。杰出的专家们现在正在为执法和其他领域的歧视性算法进行宣传。他们认为，计算机应该被授权直接基于种族和其他受保护群体做出改变生活的决策。这将意味着计算机可以明确因为黑人而惩罚黑人的被告。

在大多数情况下，数据科学家故意设计算法，使其对受保护群体保持盲目。这是通过禁止预测模型输入这些因素来实现的。这样做并不会消除*机器偏见*，这种众所周知的现象，其中模型通过“替代”变量（在我在本书的第七部分中讨论的文章，“*在预测执法中抗击偏见，正义不能是色盲*”）错误地标记一个群体多于另一个群体）。但抑制此类模型输入是一个基本的第一步，若没有它，模型就是*歧视性的*。

我使用“歧视性”一词是指部分基于受保护群体的决策，例如通过种族或宗教进行剖析以确定警察搜查的情况。当决策旨在使受保护群体受益时，例如为了平权行动，或确定某人是否符合指定少数群体的补助资格时，则存在例外。

歧视性算法恰恰符合不平等的定义。例如，在通知预审释放、假释和判决决策时，模型计算未来犯罪定罪的概率。如果数据将种族与定罪联系起来——显示黑人被告的定罪次数多于白人被告——那么生成的模型会因为黑人的肤色而惩罚每个黑人的分数。这不可能有比将黑色犯罪化更公然的案例了。

支持歧视性政策和决策为歧视性算法铺平了道路。三十六％的美国人支持基于宗教的政策，禁止穆斯林进入美国。美国禁止跨性别人士服役于军队。四分之三的美国人支持基于种族部分增加机场安全检查，25％的美国人支持警方使用种族 profiling。拥有“白人听起来的名字”的简历收到的回复比拥有“非裔美国人听起来的名字”的简历多 50％。¹

支持歧视性算法的专家表示了一种新兴威胁。一篇与斯坦福大学助理教授 Sharad Goel 共同撰写的论文批评了算法不应该歧视的标准。该论文建议在“受保护的特征增加预测价值时”采用歧视性模型。在一堂讲座中，这位教授说：“我们可以假装我们没有这些信息，但事实上我们有... 把种族包含在你的算法中实际上是件好事。”

宾夕法尼亚大学犯罪学教授 Richard Berk——曾被假释部门委托构建预测模型——还呼吁基于种族进行预测，在一篇关于应用机器学习预测哪些罪犯会杀人或被杀的论文中。Berk 写道：“可以采用最佳模型，这些数据恰好包括种族作为预测因子。这是最技术上合理的立场。”

数据促使这些专家支持歧视性算法。对他们来说，这是一种偏见的理由。就像数据在说：“要种族歧视一样。”

但是，“服从”数据并生成歧视性算法违反了公平和公民权利的最基本概念。即使我的群体确实犯更多的罪，也会侵犯我的权利，让我为他人负责，让我的分类对我不利。

歧视性计算机造成的破坏比实施歧视政策的人类更为严重。一旦作为算法晶化，歧视过程将自动执行，冷酷地在更广泛的范围内影响更多的人群。形式化和机械化地部署，它具有具体和被接受的地位。它成为了系统的一部分。比起任何人类，计算机更像是“那个人”。

所以，需要更多数据。就像我们人类决策者在考虑职位候选人或犯罪嫌疑人时会努力尽可能超越种族因素一样，进行类似的努力——在更大的范围内扩展数据库，将使我们的计算机也能超越歧视。抵制投资这一努力将表明愿意妥协本国的自由，这些自由正是我们通过执法和移民政策来保护的。

¹ 本文中许多具体内容的参考资料可以在 Eric Siegel 的《当机器和数据促进公然歧视时》一文中找到，*旧金山纪事报*，2018 年 9 月 21 日，[*https://oreil.ly/8YzGp*](https://oreil.ly/8YzGp)。
