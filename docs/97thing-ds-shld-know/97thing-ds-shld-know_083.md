# 第七十七章：实施 AI 伦理治理与控制

# 史蒂夫·斯通

![](img/STEVE_STONE.png)

创始人，NSU 技术公司 & 前首席信息官，

低 e 和有限品牌

电影*战争游戏*的发布与我在技术领域的职业生涯的开始同时。对许多人来说，*战争游戏*介绍了人工智能的概念及其可能对我们生活的影响。

快进 36 年，我们看到智能算法在从购买产品到国防的各个方面扮演着重要角色。计算能力和数据存储的主要进步，加上曾经模拟的过程日益增加的数字化，推动了计算智能解决方案的前所未有的增长。

尽管大多数人认为这些进步极大地造福了社会，但许多人对由机器驱动的决策所带来的道德影响感到担忧。就像我们在*战争游戏*中看到的那样，机器会执行它们被训练来做的事情，即使这种训练对社会的大部分人是有害的。确保计算智能解决方案的安全和道德运行对于使用这些解决方案的公司以及整个社会来说都是一个重大关注点。我想把这次讨论重点放在为 AI 解决方案开发必要的治理和控制环境上，以确保所有利益相关者的安全和道德状态。

与任何形式的软件开发一样，AI 项目的结果受到开发生态系统、迁移到生产状态所需的流程以及最终解决方案的持续审计的影响。然而，确保 AI 解决方案的道德状态需要在解决方案生命周期的各个步骤中增加额外的控制。

## 采用 AI 道德行为准则

为 AI 解决方案维护适当的开发生态系统始于 AI 道德行为准则。这一行为准则应概述所有 AI 开发者必须遵循的步骤，以消除偏见、促进透明度，并承担社会责任。AI 道德行为准则应包含指导开发者的标准和实践，涵盖审计可能性、可访问性、数据管理、权利委派以及道德/道德责任等主题。这一行为准则将通过为所有开发者提供强制性培训来加强，以确保他们理解组织的道德责任。

## 强调在招聘和招聘中的多样性

除了采用 AI 道德行为准则外，组织还应专注于招聘和雇佣多样化的开发者，以帮助消除“群体思维”，并在开发生态系统中强化包容性思维的文化。

最后，在 AI 工作的结果可能影响到社会的大部分人群的情况下，组织应雇佣伦理学家。伦理学家是专家，他们教育并与开发者一起致力于道德开发实践。

## 确保符合道德审查委员会的要求

在适当的开发生态系统到位后，下一个关注领域是将 AI 解决方案迁移到生产环境的过程。在 IT 领域，质量审查委员会（QRB）或架构审查委员会（ARB）的概念很普遍。对于 AI 解决方案，需要一个新的治理机构，伦理审查委员会（ERB）。在确保在开发和使用 AI 中遵守伦理实践的治理框架的同时，ERB 还充当新 AI 解决方案进入生产状态的守门人。未能通过 ERB 审查的新解决方案将不被允许投入生产。

## 建立审计和反馈回路

一旦 AI 应用投入生产，就必须持续进行审核以确保合规性。这些审核将不仅检查算法，还将检查为算法提供数据的数据。由于 AI 算法通过迭代学习，数据中的偏见会导致算法的有偏向的“学习”。

尽管审计和持续测试以理解意外结果至关重要，但这并不足够。此外，还应向那些操作超出系统 AI 控制范围的用户提供反馈回路。可以通过应用程序内置反馈回路或使用调查工具来实现反馈回路。

简而言之，建立一个具有适当独立性和透明度的操作性人工智能生态系统对于那些构建和运营对社会有影响的智能解决方案的组织来说是必不可少的。

AI 伦理控制并不性感或令人兴奋。但让我们面对现实：如果这些控制措施存在，*战争游戏* 就会是一部更加无聊的电影。
