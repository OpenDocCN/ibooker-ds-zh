# 第十九章：构建多视角 AI

# Hassan Masum 和

Sébastien Paquet

![](img/Hassan_Masum.png)![](img/Sebastien_Paquet.png)

Prodigy Education 的高级分析总监

Element AI 的应用研究科学家

数据科学和人工智能的部署是由它们的制造者和所有者决定的，但其他利益相关者有时会受到影响甚至受到伤害。

如果你关心避免伤害，部署数据科学和人工智能时必须考虑多个视角。你必须站在别人的角度，问自己：如果我是那个人，这个系统会如何影响我的生活？

新技术的影响可能是意外的和复杂的，即使是对连接人们这样看似良性目标也是如此。创造者或任何单一行为者不能看到整体画面。但每个行为者深刻理解自己的情况，因此可以评估特定技术如何服务或伤害他们。

从利益相关者的意见和生活经验中学习长期以来一直是负责任技术讨论的一部分（例如“实施研究”和“创新扩散”中的理念）。在数据科学时代，这些积累的经验仍然具有重要意义。

考虑到这一点，如何在数据科学和人工智能的发展中融入更多的观点？以下是应考虑融入的人们的视角及其关心的问题：

用户们思考：“这个 AI 能否减轻我的问题？对我而言安全且公平吗？它会帮助我成为怎样的人？”

正如[英国政府的设计原则](https://oreil.ly/osClX)和[数字服务标准](https://oreil.ly/Ig9pY)所强调的，从用户需求出发。热心的用户可能特别坦率地提供反馈；渴望更好解决方案的绝望或沮丧的用户也是如此。投入时间和同理心，深入了解用户更深层次的问题和愿望。

关心他人的人们思考：“这个 AI 如何帮助或伤害我的朋友、同事或亲人？”

那些深知一个人的人往往能评估什么对他们有益或有害。例如，要了解一个孩子如何受到 AI 的影响，你可以与孩子的父母、兄弟姐妹、朋友和老师交流。

受影响者思考：“这个 AI 会如何影响我？即使我从未请求，它是否正在伤害我？”

即使是不使用 AI 的人也可能受到其影响，比如因算法而被剥夺福利资格或假释资格的公民。这些公民可以从《欧盟一般数据保护条例》第 22 条（[链接](https://oreil.ly/sJAc4)）这样的政策中获益，该条例规定人们有权表达对影响其自动化决策的观点，对此类决策提出异议，并获得人工干预。在大规模实施数据科学之前，要绘制谁会受到影响以及这种影响可能如何进一步影响其他人的地图。

怀疑者们认为：“为什么这个 AI 不能改进或者放弃？谁是那些建造它的白痴？”

寻找建设性的怀疑者：那些最具洞察力但 *不* 支持你愿景的人。即使怀疑者看起来无建设性，你仍然可以试图理解他们的情绪和世界观。

监管机构和民间社会认为：“这个人工智能操作是否合法？它安全且公平吗？如何能够为公众利益服务并平衡不同人的利益？”

支持有效的监管和民间监督，以符合公众利益。帮助监督工作更有效，例如通过分享你的人工智能的影响信息，或者共同开发标准和算法问责机制。（你需要决定支持哪些国家的监管机构和伦理标准。）

数据科学家和人工智能开发者思考：“我是否考虑了我作为创造者的责任、我的偏见和限制，以及他人的视角？这将是合法的、安全的和可信赖的，并且能够改善世界吗？”

反思 *你自己* 的视角。你为什么要创建这个新技术？什么因素影响了你的动机，包括你的工作、财务、文化和个性？哪些义务阻碍了你满足用户和受影响者的需求？你能提高对他人的同理心和理解力吗？

在日常责任的冲击下，做到这一切并不容易。但是花时间更广泛地审视你所构建的内容，有助于降低风险，让客户满意，并让世界变得更美好。

听取多样化的利益相关者意见可能会很艰难，但也会很有回报。同样，以各种方式参与他们的意见，比如通过调查、访谈和民族志研究。如果你能做到这一点，并构建真正多视角的人工智能，那么你将有更好的机会推出能够产生长期用户和社会价值的创新。
