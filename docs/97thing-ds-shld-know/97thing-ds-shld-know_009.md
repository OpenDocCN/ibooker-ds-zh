# 第八章：小心“心之决策”

# 休·沃森

![](img/Hugh_Watson.png)

乔治亚大学特里商学院 MIS 教授

如今，公司和政府组织越来越多地使用深度学习等先进分析技术来部分或完全自动化决策制定。分析技术被用来进行贷款决策、建议缓刑或监狱判决、筛选职位申请者等。虽然这些算法可以加快、降低成本、提高效率，甚至使决策更公平，但也不是没有风险。凯西·奥尼尔在她有影响力的著作《*数学毁灭之武器*》（Crown）中以及其他人的论述中指出，算法可能增加不平等，拒绝服务和机会，甚至威胁到民主。

在 L·弗兰克·鲍姆的《绿野仙踪》中，锡人和稻草人之间的对话提供了一个有趣的视角，说明了在自动化决策时需要包括“心”和“脑”（即算法）：

> “*我不知道足够多，*”稻草人愉快地回答道。“*你知道，我的头是塞满稻草的，这就是我去奥兹要求他给我一些大脑的原因。*”“*哦，我明白了，*”锡人说。“*但是，毕竟，大脑并不是世界上最好的东西。*”“*你有吗？*”稻草人问。“*没有，我的头是完全空的，*”锡人回答道，“*但我曾经有过大脑，还有一颗心；所以，既然两者都试过了，我更愿意拥有一颗心。*”

当决策可能严重影响到人们生活时，决策过程应该包括“心”和“脑”。开发的应用程序应当没有偏见，不会不公平地歧视某些人群；应当遵守日益复杂的法律法规；不会损害公司品牌；并且允许个人选择退出和/或获取决策原因的解释，并寻求补救措施。

在构建模型时，要注意不要引入意外的错误和偏见，这可能是由于模型训练/测试数据选择不当造成的。例如，在使用过去只录取学生数据的学院入学模型时可能会出现预筛选偏见。您必须知道如何处理在重要类别中只有少量观察值的分类数据。此外，您还应确保持续监控模型的准确性及其对不同人群的影响。

欧盟的 GDPR 于 2018 年 5 月生效，而加利福尼亚州消费者隐私法（CCPA）于 2020 年 1 月生效，两者都对个人数据的使用和共享设置了限制。GDPR 要求必须通过选择授权来收集任何个人数据；任何使用个人数据的请求必须具体明确；个人数据的收集和使用必须是为了特定和清晰理解的业务目的；并且公民有权利让他们的个人数据被删除（所谓的被遗忘权）。GDPR 的第 22 条规定：“[个人]应有权利不受仅基于自动处理的决定的影响。”CCPA 与 GDPR 有相似之处，但侧重于消费者隐私权和公司向客户披露所需信息。例如，公司必须在其网站上有一个标题为“不出售我的个人信息”的链接。

某些算法的使用虽然合法，但对商业有害。一个经常被引用的例子是 Target 使用预测建模来识别可能怀孕的女性，然后发送与怀孕相关的优惠券。当一名 16 岁的女孩收到这样的优惠券并且她的父亲抱怨这些优惠券促进了未成年怀孕时，问题就出现了（后来他发现她确实怀孕了）。这个故事被《纽约时报》、《财富》和其他广为人知的出版物报道，并且给 Target 的品牌带来了污点。

人们应该能够要求并收到关于为什么做出决定的解释。美国计算机协会（ACM）的公共政策委员会和 ACM 欧洲政策委员会分别和共同工作，制定了确保个人数据和算法公平使用的七项原则。第四项指导原则是解释的需要——即当被问及时，能够用人类术语传达算法的逻辑。个人还应该能够质疑自动化决策和/或了解可以采取什么措施加以补救。由于一些最强大的预测模型（例如深度学习）的“黑箱”特性，这一要求可能具有挑战性，并可能导致使用在人类术语中更可解释的、预测力略低的模型（例如决策树）。

为了满足对算法合法和道德使用的需求，特别是涉及到涉及心理决策的算法，数据科学家需要采用更广泛的视角来审视他们的责任，并且公司需要扩展他们的治理（例如人员、委员会和流程），包括他们的法律人员和与客户接触的业务人员。
