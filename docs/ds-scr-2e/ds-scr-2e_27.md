# 第二十六章：数据伦理

> 先吃饭，然后考虑伦理。
> 
> 贝托尔特·布莱希特

# 什么是数据伦理？

随着数据的使用，数据的滥用也随之而来。这几乎一直如此，但最近这个想法已经被具体化为“数据伦理”，并在新闻中占据了一定的位置。

例如，在 2016 年的选举中，一家名为剑桥分析公司的公司[不当获取了 Facebook 数据](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)，并将其用于政治广告定向投放。

在 2018 年，由 Uber 测试的自动驾驶汽车[撞死了一名行人](https://www.nytimes.com/2018/05/24/technology/uber-autonomous-car-ntsb-investigation.html)（汽车上有一名“安全驾驶员”，但显然她当时没有注意）。

算法被用来[预测罪犯再犯的风险](https://www.themarshallproject.org/2015/08/04/the-new-science-of-sentencing)并据此判刑。这比允许法官做出相同判断更公平吗？

一些航空公司[给家庭分配单独的座位](https://twitter.com/ShelkeGaneshB/status/1066161967105216512)，强迫他们额外付费才能坐在一起。一个数据科学家应该介入阻止这种情况吗？（链接线程中的许多数据科学家似乎认为应该如此。）

“数据伦理”自称提供了对这些问题的答案，或者至少提供了一个处理这些问题的框架。我并不傲慢到告诉你如何考虑这些事情（而且“这些事情”正在迅速变化），所以在本章中，我们将快速浏览一些最相关的问题，并（希望）激发你进一步思考这些问题。 （遗憾的是，我不是一个足够好的哲学家，无法从零开始进行伦理思考。）

# 不，真的，什么是数据伦理？

好吧，让我们从“什么是伦理学？”开始。如果你总结每一个你能找到的定义的平均值，你最终会得到类似于*伦理学*是一个思考“正确”和“错误”行为的框架。*数据*伦理，因此，是一个思考涉及数据的正确和错误行为的框架。

有些人谈论“数据伦理”似乎是（也许是隐含地）关于你可以做什么和不可以做什么的一套戒律。有些人正在努力创建宣言，其他人正在制定希望你发誓遵守的强制性承诺。还有一些人正在努力让数据伦理成为数据科学课程的强制组成部分——因此本章，作为一种在他们成功的情况下敲定我的赌注的方式。

###### 注

令人好奇的是，[没有太多数据表明伦理课程会导致道德行为](https://www.washingtonpost.com/news/on-leadership/wp/2014/01/13/can-you-teach-businessmen-to-be-ethical)，在这种情况下，也许这场运动本身就是数据不道德的表现！

其他人（例如，诚挚地）认为，合理的人经常会在对错的微妙问题上意见分歧，并且数据伦理的重要部分是承诺*考虑*你的行为的伦理后果。这需要*理解*许多“数据伦理”倡导者不赞同的事情，但不一定需要同意他们的反对意见。

# 我应该关心数据伦理吗？

无论你的工作是什么，你都应该关注伦理问题。如果你的工作涉及数据，你可以自由地将你的关心称为“数据伦理”，但你也应该同样关心工作中与数据无关的伦理问题。

或许技术工作不同之处在于技术*扩展*，个人在解决技术问题时（无论是与数据相关还是其他）做出的决策可能具有潜在的广泛影响。

改动一点点新闻发现算法可能会导致成百上千的人阅读一篇文章，或者没有人阅读它。

一个单一有缺陷的假释算法在全国范围内使用，系统性地影响数百万人，而一个自身存在缺陷的假释委员会只影响到前来面见它的人。

因此，是的，总体而言，你应该关心你的工作对世界的影响。而你的工作影响越广泛，你就越需要担心这些事情。

不幸的是，围绕数据伦理的一些讨论涉及到人们试图把他们的伦理结论强加给你。你是否应该关心他们关心的事情，这确实取决于你自己。

# 构建糟糕的数据产品

一些“数据伦理”问题源于构建*糟糕的产品*。

例如，微软[发布了一个名为 Tay 的聊天机器人](https://en.wikipedia.org/wiki/Tay_(bot))，它会复述对它发推特的内容，互联网很快发现这使得他们能让 Tay 发表各种冒犯性的言论。看起来微软没有讨论发布这个“种族主义”机器人的伦理性；很可能他们只是简单地制作了一个机器人，但未能深思其可能被滥用的后果。这可能是一个低门槛，但让我们一致认为你应该考虑你所构建的东西可能如何被滥用。

另一个例子是，Google Photos 曾经使用一个图像识别算法，有时会将黑人的照片分类为“大猩猩”。同样，几乎没有人认为谷歌有*明确决定*发布这一功能（更不用说在“伦理”方面苦苦挣扎了）。在这里，问题很可能是训练数据的问题，模型的不准确性，以及这个错误的极其冒犯性（如果模型偶尔将邮箱分类为消防车，可能没有人会在意）。

在这种情况下，解决方案不太明显：你如何确保你训练的模型不会做出在某种程度上冒犯性的预测？当然，你应该在各种输入上训练（和测试）你的模型，但你能确保你的模型永远不会出现某种让你感到尴尬的输入吗？这是一个难题。（谷歌似乎通过简单地拒绝预测“大猩猩”来“解决”了这个问题。）

# 平衡准确性与公平性

想象一下，你正在建立一个模型，预测人们采取某些行动的可能性。你做得相当不错（表格 26-1）。

表格 26-1\. 做得相当不错

| 预测 | 人们 | 行动 | % |
| --- | --- | --- | --- |
| 不可能 | 125 | 25 | 20% |
| 可能 | 125 | 75 | 60% |

你预测的人中，有 20%的人不太可能采取行动。而你预测的人中，有 60%的人采取了行动。看起来不太糟糕。

现在想象一下，人们可以分为两组：A 和 B。你的一些同事担心你的模型对其中一组是*不公平*的。虽然模型不考虑组别成员资格，但它确实考虑了与组别成员资格相关的各种以复杂方式相关的其他因素。

实际上，当你按组别分解预测时，你会发现一些令人惊讶的统计数据（表格 26-2）。

表格 26-2\. 令人惊讶的统计数据

| 组别 | 预测 | 人们 | 行动 | % |
| --- | --- | --- | --- | --- |
| A | 不可能 | 100 | 20 | 20% |
| A | 可能 | 25 | 15 | 60% |
| B | 不可能 | 25 | 5 | 20% |
| B | 可能 | 100 | 60 | 60% |

你的模型不公平吗？你团队的数据科学家提出了各种论点：

Argument 1

你的模型将 80%的 A 组分类为“不可能”，但将 80%的 B 组分类为“可能”。这位数据科学家抱怨说，模型在某种程度上不公平地对待了两组，因为它在两组之间生成了截然不同的预测。

Argument 2

无论组别成员资格如何，如果我们预测“不可能”，你有 20%的行动机会，如果我们预测“可能”，你有 60%的行动机会。这位数据科学家坚持认为，模型在某种意义上是“准确”的，因为它的预测似乎无论你属于哪个组，都*意味着*相同的事情。

Argument 3

B 组的 40/125 = 32%被错误标记为“可能”，而 A 组的 10/125 = 8%被错误标记为“可能”。这位数据科学家（认为“可能”预测是一件坏事）坚持认为模型不公平地污名化了 B 组。

Argument 4

20/125 = 16%的 A 组被错误标记为“不可能”，而只有 5/125 = 4%的 B 组被错误标记为“不可能”。这位数据科学家（认为“不可能”预测是一件坏事）坚持认为模型不公平地污名化了 A 组。

这些数据科学家中哪些是正确的？有没有正确的？也许这取决于情境。

可能当两组是“男性”和“女性”时，您的感觉会有所不同；当两组是“R 用户”和“Python 用户”时，您的感觉又会有所不同。或者，如果 Python 用户偏向男性而 R 用户偏向女性，可能也不会有不同的感觉？

如果模型用于预测 DataSciencester 用户是否将通过 DataSciencester 求职板申请工作，您可能会有一种感觉；如果模型用于预测用户是否将*通过*这样的面试，您可能会有另一种感觉。

可能您的意见取决于模型本身，它考虑了哪些特征以及它训练的数据。

无论如何，我的观点是要向您强调“准确性”和“公平性”之间可能存在权衡（当然，这取决于您如何定义它们），而这些权衡并不总是有明显的“正确”解决方案。

# 合作

一个压制（按您的标准）国家的政府官员最终决定允许公民加入 DataSciencester。然而，他们坚持要求来自他们国家的用户不得讨论深度学习。此外，他们希望您向他们报告任何试图寻找深度学习信息的用户的姓名，即使他们只是*尝试*寻找。

这个国家的数据科学家是否更适合访问您将被允许提供的主题限制（并受到监视的）DataSciencester？还是建议的限制如此可怕，以至于他们干脆不访问？

# 可解释性

DataSciencester HR 部门要求您开发一个模型，预测哪些员工最有可能离开公司，以便他们可以进行干预并试图让他们更快乐。 （离职率是您的 CEO 渴望出现在“10 个最幸福工作场所”杂志特写中的重要组成部分。）

您收集了一系列历史数据，正在考虑三种模型：

+   决策树

+   一个神经网络

+   一个高价“留存专家”

您的一个数据科学家坚持认为您应该使用表现最好的模型。

第二个坚持您不要使用神经网络模型，因为只有其他两个模型能解释它们的预测，而只有预测的解释才能帮助 HR 实施广泛的变革（而不是一次性的干预）。

第三个说，虽然这位“专家”可以对她的预测提供*一个*解释，但没有理由相信她的解释描述了她预测的*真正*原因。

和我们的其他例子一样，在这里没有绝对的最佳选择。在某些情况下（可能是出于法律原因或者如果您的预测对生活有重大影响），您可能更喜欢一个性能较差但可以解释其预测的模型。在其他情况下，您可能只想要预测最好的模型。在另一些情况下，也许没有一个可解释的模型表现良好。

# 推荐

正如我们在第二十三章中讨论的那样，一个常见的数据科学应用涉及向人们推荐事物。当有人观看 YouTube 视频时，YouTube 会推荐他们接下来应该观看的视频。

YouTube 通过广告赚钱，（据推测）希望推荐您更有可能观看的视频，以便它们可以向您展示更多广告。然而，事实证明，人们喜欢观看关于阴谋论的视频，这些视频往往出现在推荐中。

###### 注

在我写这一章时，如果您在 YouTube 上搜索“saturn”，第三个结果是“Something Is Happening On Saturn… Are THEY Hiding It?”这也许可以让您感受到我所说的那些视频的类型。

YouTube 是否有义务不推荐阴谋视频？即使这是许多人似乎想观看的内容？

另一个例子是，如果您转到 google.com（或 bing.com）并开始输入搜索内容，搜索引擎会提供自动完成您搜索的建议。这些建议基于其他人的搜索（至少部分地）；特别是，如果其他人正在搜索不良内容，这可能会反映在您的建议中。

搜索引擎是否应该积极过滤掉它不喜欢的建议？谷歌（出于某种原因）似乎坚决不提供与人们宗教有关的建议。例如，如果在 Bing 中键入“mitt romney m”，第一个建议是“mitt romney mormon”（这是我预料中的），而谷歌拒绝提供这样的建议。

实际上，谷歌明确过滤掉它认为是[“冒犯性或贬低性”的](https://blog.google/products/search/google-search-autocomplete/)自动建议。（它如何决定什么是冒犯性或贬低性是模糊的。）但有时候真相就是冒犯的。保护人们免受这些建议的影响是道德行为吗？还是不道德的行为？或者根本不是道德问题？

# 偏见数据

在“词向量”中，我们使用了一组文档语料库来学习单词的向量嵌入。这些向量被设计为展示*分布式相似性*。也就是说，出现在相似上下文中的词语应该具有相似的向量。特别是，训练数据中存在的任何偏见都将反映在词向量本身中。

例如，如果我们的文档都是关于 R 用户是道德败类，Python 用户是美德典范，那么模型很可能会学习到“Python”和“R”的这种关联。

更常见的情况是，词向量基于一些组合：Google 新闻文章、维基百科、书籍和爬取的网页。这意味着它们将学习到这些来源中存在的任何分布模式。

例如，如果关于软件工程师的大多数新闻文章都是关于*男性*软件工程师，那么“软件”的学习向量可能更接近于其他“男性”词语的向量，而不是“女性”词语的向量。

在那一点上，您使用这些向量构建的任何下游应用程序可能也会表现出这种紧密性。根据应用程序的不同，这可能是个问题，也可能不是。在这种情况下，您可以尝试各种技术来“消除”特定的偏见，尽管您可能永远无法消除所有偏见。但这是您应该注意的问题。

同样，就像在“构建糟糕的数据产品”中的“照片”示例一样，如果您在非代表性数据上训练模型，那么它很可能会在真实世界中表现不佳，可能会以冒犯或令人尴尬的方式表现出来。

另一方面，您的算法可能也会使实际世界中存在的实际偏见被编码。例如，您的假释模型可能完美地预测哪些释放的罪犯会再次被逮捕，但如果这些再次逮捕本身是有偏见的现实世界过程的结果，那么您的模型可能会延续这种偏见。

# 数据保护

您了解 DataSciencester 用户的很多信息。您知道他们喜欢什么技术，他们的数据科学家朋友是谁，他们在哪工作，他们赚多少钱，他们在网站上花费多少时间，他们点击哪些职位发布等等。

赚钱副总裁想将这些数据卖给广告商，他们渴望向您的用户营销各种“大数据”解决方案。首席科学家想将这些数据与学术研究人员分享，他们热衷于发表关于如何成为数据科学家的论文。竞选副总裁计划将这些数据提供给政治竞选活动，他们中的大多数人渴望招募自己的数据科学组织。政府事务副总裁希望使用这些数据来回答执法部门的问题。

由于一位有远见的合同副总裁，您的用户同意了服务条款，几乎允许您对他们的数据做任何想做的事情。

然而（正如您现在预料到的），您团队中的各个数据科学家对这些各种用途提出了各种异议。有人认为将数据交给广告商是错误的；另一些人担心学术界不能信任地负责保护数据。第三个人认为公司应该远离政治，而最后一个人坚持认为警方不可信任，与执法部门合作将伤害无辜人群。

这些数据科学家中有人有道理吗？

# 总结一下

这些都是很多需要担心的事情！而且我们还没有提到的无数其他问题，还会有更多未来会出现但今天我们无法想象的问题。

# 进一步探索

+   在谈论数据伦理的重要思想的人并不少。在 Twitter（或者你最喜欢的新闻网站）上搜索可能是了解当前最新数据伦理争议的最佳方式。

+   如果你想要更实际一点的东西，Mike Loukides、Hilary Mason 和 DJ Patil 编写了一本短篇电子书，[*数据科学与伦理*](https://www.oreilly.com/library/view/ethics-and-data/9781492043898/)，讲述了如何将数据伦理付诸实践，因为 Mike 在 2014 年同意出版《*从零开始的数据科学*》，我觉得有义务推荐这本书。（练习：这样做对吗？）
