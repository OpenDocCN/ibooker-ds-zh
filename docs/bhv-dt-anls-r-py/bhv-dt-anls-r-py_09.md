# 第6章。处理缺失数据

在数据分析中，缺失数据是常见的。在大数据时代，许多作者甚至更多的从业者将其视为一个次要的困扰，几乎没有太多思考：只需过滤掉缺失数据的行——如果你从1200万行减少到1100万行，那有什么大不了的呢？这仍然为您提供了足够的数据来运行您的分析。

不幸的是，过滤掉含有缺失数据的行可能会在你的分析中引入显著的偏差。比如说，老年客户更有可能存在缺失数据，因为他们较少设置自动付款；如果过滤这些客户，你的分析就会偏向于年轻客户，在过滤数据中会过度代表他们。其他处理缺失数据的常见方法，比如用该变量的平均值替换，也会引入它们自己的偏差。

统计学家和方法学家已经开发出了方法，这些方法的偏差要小得多，甚至没有偏差。然而，这些方法尚未被广泛采纳，但希望本章可以帮助您走在前列！

缺失值理论根植于统计学，可能会变得非常数学化。为了使本章节的学习更具体化，我们将通过一个模拟数据集来探讨AirCnC。在业务背景下，市场部门为了更好地了解客户特征和动机，向三个州的2,000名客户发送了一封调查问卷，并收集了以下信息：

+   人口统计特征

    +   年龄

    +   性别

    +   州（只选择三个州的客户，为了方便起见我们将它们称为A、B和C）

+   个性特质

    +   开放性

    +   外向性

    +   神经质

+   预订金额

为了简化问题，我们假设人口统计变量都是预订金额的原因，并且彼此无关（见[图 6-1](#the_demographic_variables_cause_booking)）。

![人口统计变量导致预订金额](Images/BEDA_0601.png)

###### 图6-1。人口统计变量导致预订金额

###### 注意

正如我们在[第2章](ch02.xhtml#understanding_behavioral_data)中讨论的，当我说人口统计变量如*性别*和*外向性*是*预订金额*的原因时，我指的是两件事情：首先，它们是外生变量（即在我们研究中是主要原因），其次，它们由于社会现象的因果效应而成为*预订金额*的预测因素。

例如，*性别*的影响可能通过个人的收入、职业和家庭状况等多种因素进行中介。从这个意义上讲，更准确地说*性别*是*预订金额*的原因的原因。然而，重要的是要注意，这种效应并没有混淆，因此它是真正的因果关系。

本章的流程将遵循您在面对新数据集时所采取的步骤：首先，我们将可视化缺失数据，以了解大致情况。然后，我们将学习如何诊断缺失数据，并看到由统计学家唐纳德·鲁宾开发的分类，这是参考资料。最后三节将展示如何处理该分类中的每一类。

对于Python用户来说，不幸的是，我们将要使用的出色R包没有直接的Python对应包。我将尽力向您展示Python中的替代方法和解决方法，但代码将会显著更长，不那么优雅。抱歉！

# 数据和包

使用模拟数据的一个好处是我们知道缺失数据的真实值。[本章的GitHub文件夹](https://oreil.ly/BehavioralDataAnalysisCh6) 包含三个数据集（[表 6-1](#variables_in_our_dat)）：

+   我们四个变量的完整数据

+   “可用”数据中某些变量的一些值缺失

+   辅助变量的辅助数据集，我们将使用它来补充我们的分析

表6-1\. 我们数据中的变量

|  | 变量描述 | chap6-complete_data.csv | chap6-available_data.csv | chap6-available_data_supp.csv |
| --- | --- | --- | --- | --- |
| *年龄* | 顾客年龄 | 完整 | 完整 |  |
| *开放度* | 开放度心理特征，0-10 | 完整 | 完整 |  |
| *额外* | 外向性心理特征，0-10 | 完整 | 部分 |  |
| *神经* | 神经质心理特征，0-10 | 完整 | 部分 |  |
| *性别* | 顾客性别的分类变量，F/M | 完整 | 完整 |  |
| *州* | 顾客居住州的分类变量，A/B/C | 完整 | 部分 |  |
| *预订金额* | 顾客预订金额 | 完整 | 部分 |  |
| *保险* | 顾客购买的旅行保险金额 |  |  | 完整 |
| *活跃度* | 顾客预订活跃程度的数值测量 |  |  | 完整 |

在本章中，除了常用的包外，我们还将使用以下包：

```py
## R
library(mice) # For multiple imputation
library(reshape) #For function melt()
library(psych) #For function logistic()

```

```py
## Python
from statsmodels.imputation import mice # For multiple imputation
import statsmodels.api as sm # For OLS call in Mice
```

# 可视化缺失数据

根据定义，缺失数据很难可视化。单变量方法（即一次处理一个变量）只能带我们走那么远，所以我们大多数时候会依赖双变量方法，将两个变量相互绘制以挖掘一些见解。与因果图结合使用，双变量图将使我们能够可视化关系，否则这些关系将非常复杂。

我们的第一步是了解“数据缺失”的情况。R中的mice包有一个非常方便的函数`md.pattern()`来可视化缺失数据：

```py
## R
> md.pattern(available_data)
    age open gender bkg_amt state extra neuro     
368   1    1      1       1     1     1     1    0
358   1    1      1       1     1     1     0    1
249   1    1      1       1     1     0     1    1
228   1    1      1       1     1     0     0    2
163   1    1      1       1     0     1     1    1
214   1    1      1       1     0     1     0    2
125   1    1      1       1     0     0     1    2
120   1    1      1       1     0     0     0    3
33    1    1      1       0     1     1     1    1
23    1    1      1       0     1     1     0    2
15    1    1      1       0     1     0     1    2
15    1    1      1       0     1     0     0    3
24    1    1      1       0     0     1     1    2
24    1    1      1       0     0     1     0    3
23    1    1      1       0     0     0     1    3
18    1    1      1       0     0     0     0    4
      0    0      0     175   711   793  1000 2679
```

`md.pattern()` 函数返回一张表，其中每一行代表数据可用性的模式。第一行每个变量都有“1”，因此表示完整的记录。表的左侧数字表示具有该模式的行数，右侧数字表示该模式中缺失的字段数。我们的数据中有368行完整记录。第二行只有*神经质*变量为“0”，因此表示只有*神经质*缺失的记录；我们的数据中有358行这样的记录。表的底部数字表示对应变量的缺失值数量，并且变量按缺失值数量递增排序。*神经质*变量位于表的最右侧，这意味着它有最高数量的缺失值，为1,000。此函数还方便地返回表的可视化表示（[图 6-2](#patterns_of_missing_data)）。

![缺失数据的模式](Images/BEDA_0602.png)

###### 图6-2\. 缺失数据的模式

正如我们在[图 6-2](#patterns_of_missing_data)中看到的，变量*年龄*、*开放性*和*性别*没有任何缺失数据，但其他所有变量都有。我们可以用我写的一个特定函数在Python中获得相同的结果，尽管格式不够易读：

```py
## Python 
def md_pattern_fun(dat_df):
    # Getting all column names
    all_cols = dat_df.columns.tolist()
    # Getting the names of columns with some missing values
    miss_cols = [col for col in all_cols if dat_df[col].isnull().sum()]
    if miss_cols == all_cols: dat_df['index'] = dat_df.index
    # Removing columns with no missing values
    dat_df = dat_df.loc[:,miss_cols]
    #Showing total number of missing values per variable
    print(dat_df.isnull().sum()) 
    # Adding count value
    dat_df['count'] = 1
    # Showing count for missingness combinations
    print(dat_df.isnull().groupby(miss_cols).count())
md_pattern_fun(available_data_df)

extra       793
neuro      1000
state       711
bkg_amt     175
dtype: int64
                           count
extra neuro state bkg_amt       
False False False False      368
                  True        33
            True  False      163
                  True        24
      True  False False      358
                  True        23
            True  False      214
                  True        24
True  False False False      249
                  True        15
            True  False      125
                  True        23
      True  False False      228
                  True        15
            True  False      120
                  True        18
```

输出由两张表组成：

+   第一张表显示了我们数据中每个变量的缺失值总数，如[图 6-2](#patterns_of_missing_data)的底部所示。*外向性*有793个缺失值，依此类推。

+   第二张表显示每种缺失数据模式的详细信息。左侧的逻辑值上方的变量（即*外向性、神经质、状态、预订金额*）是数据中有一些缺失值的变量。表的每一行指示具有特定缺失数据模式的行数。第一行由四个`False`组成，即没有任何变量缺失数据的模式，我们的数据中有368行，正如您在[图 6-2](#patterns_of_missing_data)的第一行中看到的那样。第二行只将最后一个`False`改为`True`，为了易读性省略了前三个`False`（即任何空白逻辑值应往上读取）。这种模式 `False``/``False``/``False``/``True` 出现在仅*预订金额*有缺失值的情况下，发生在我们的数据中的33行，依此类推。

即使是这样一个小数据集，这种可视化也非常丰富，很难知道要寻找什么。我们将探讨两个方面：

缺失数据量

我们的数据有多少是缺失的？哪些变量有最高百分比的缺失数据？我们可以简单地丢弃有缺失数据的行吗？

缺失相关性

缺失数据是在个体级别还是变量级别？

## 缺失数据量

首要任务是确定我们数据的缺失情况以及哪些变量的缺失比例最高。我们可以在[图 6-2](#patterns_of_missing_data)的底部找到所需的值，其中包括每个变量的缺失值数量，按照缺失程度递增排序，或在Python输出的底部找到。如果缺失数据量非常有限，例如您有一个1000万行数据集，其中没有变量有超过10个缺失值，那么通过多重插补来妥善处理它们将是过度的，我们稍后将看到。只需删除所有具有缺失数据的行，问题就解决了。这里的理由是，即使缺失值极其偏倚，它们的数量太少，不会以任何方式实质性地影响您分析的结果。

在我们的示例中，缺失值最多的变量是*神经质*，有1,000个缺失值。这多吗？限制在哪里？是10、100、1,000行还是更多？这取决于上下文。您可以使用一个快速且简单的策略：

1.  选择缺失值最多的变量，并创建两个新数据集：一个将该变量的所有缺失值替换为该变量的最小值，另一个将其替换为该变量的最大值。

1.  对您现在拥有的三个数据集中的每一个，运行关于该变量与您感兴趣效果的最重要关系的回归。例如，如果该变量是您感兴趣效果的预测变量，则运行该回归。

1.  如果在三次回归中，回归系数没有实质性差异，即基于不同数值得出相同的业务影响或采取相同行动，那么您的数据缺失率在可接受范围内，可以放弃缺失数据。简而言之：这些数字对您的业务伙伴来说意味着相同的事情吗？如果是这样，您可以放弃缺失数据。

###### 注意

这一经验法则很容易适用于数值变量，但是对于二元或分类变量呢？

对于二元变量，最小值将为0，最大值将为1，这是可以接受的。您创建的两个数据集将转化为最佳情况和最坏情况。

对于分类变量，最小和最大规则必须稍作调整：将所有缺失值替换为最少出现或最常出现的类别。

让我们以*神经质*为例，做到这一点。*神经质*是我们感兴趣效果*预订金额*的一个预测变量，所以我们将使用前面指出的关系：

```py
## R (output not shown)
min_data <- available_data %>%
  mutate(neuro = ifelse(!is.na(neuro), neuro, min(neuro, na.rm = TRUE)))
max_data <- available_data %>%
  mutate(neuro = ifelse(!is.na(neuro), neuro, max(neuro, na.rm = TRUE)))
summary(lm(bkg_amt~neuro, data=available_data))
summary(lm(bkg_amt~neuro, data=min_data))
summary(lm(bkg_amt~neuro, data=max_data))

```

```py
## Python (output not shown) 
min_data_df = available_data_df.copy()
min_data_df.neuro = np.where(min_data_df.neuro.isna(), min_data_df.neuro.min(), 
                             min_data_df.neuro)

max_data_df = available_data_df.copy()
max_data_df.neuro = np.where(max_data_df.neuro.isna(), max_data_df.neuro.max(), 
                             max_data_df.neuro)

print(ols("bkg_amt~neuro", data=available_data_df).fit().summary())
print(ols("bkg_amt~neuro", data=min_data_df).fit().summary())
print(ols("bkg_amt~neuro", data=max_data_df).fit().summary())
```

结果如下：

+   基于可用数据的系数为−5.9。

+   基于用*神经质*的最小值替换缺失值的系数为−8.0。

+   基于用*神经质*的最大值替换缺失值的系数为2.7。

这些值彼此非常不同，甚至具有不同的符号，因此我们绝对超过了材料显著性的阈值。我们不能简单地删除那些对*神经质*有缺失数据的行。对其他变量采用同样的方法也会显示出我们不能忽视它们的缺失值，需要适当处理。

## 缺失的相关性

一旦确定了需要处理的变量，我们就想知道它们的缺失程度有多相关。如果有变量的缺失高度相关，这表明一个变量的缺失导致了其他变量的缺失（例如，如果某人在调查中途停止回答问题，则之后的所有答案都将缺失）。或者，它们的缺失可能有共同的原因（例如，某些受试者更不愿意透露有关自己的信息）。在这两种情况下，识别缺失的相关性将帮助您建立更精确的CD，节省时间并使分析更有效。

让我们通过一个简单的例子来看一下：想象一下我们有两个办公室的面试数据：Tampa和Tacoma。在两个办公室中，候选人必须通过相同的三个强制性面试部分，但在Tampa，第一位面试官负责记录候选人的所有分数，而在Tacoma，每位面试官都要记录其部分的分数。面试官是人类，有时会忘记将数据交给HR。在Tampa，如果面试官忘记提交数据，我们对于候选人除了系统中的ID外就没有任何数据（[Figure 6-3](#highly_correlated_missingness_in_tampa)只显示了Tampa的数据）。

![Tampa数据中高度相关的缺失](Images/BEDA_0603.png)

###### 图6-3。Tampa数据中高度相关的缺失。

高缺失相关性的标志是一行具有大量浅色方块（这里是3），代表了高数量的案例（这里是总行数2,000中的400）。此外，图中没有只有一个或两个浅色方块的行。

在这种情况下，逐个变量分析我们的缺失数据是毫无意义的。如果我们发现当Murphy是第一位面试官时，第一部分的数据非常可能丢失，那么对于其他部分也是如此。（Murphy，你只有一个任务！）

相反，在Tacoma，不同部分的缺失完全不相关（[Figure 6-4](#uncorrelated_missingness_in_tacomaapost)）。

这种模式与Tampa相反：

+   我们有大量只有少数缺失变量的行（请看图右边的所有1和2）。

+   这些行代表了我们数据的大部分（左边的数据表明只有17行有3个缺失变量）。

+   图中底部有大量浅色方块的线代表非常少的案例（相同的17个个体），因为它们是独立随机性的结果。

![Tacoma数据中的不相关缺失](Images/BEDA_0604.png)

###### 图6-4\. Tacoma数据中的不相关缺失

最后一个要点的论证可以通过更广泛地观察我们可以称之为“俄罗斯套娃”序列来扩展，这些序列呈递增的缺失，其中每个模式都在上一个模式上添加一个缺失变量，例如（I3）→（I3，I2）→（I3，I2，I1）。相应的案例数量是262 → 55 → 17\. 这些数字形成了一个递减序列，这是合乎逻辑的，因为如果变量的缺失完全不相关，我们有：

*Prob*（*I*3 *缺失 & I*2 *缺失*）= *Prob*（*I*3 *缺失*）* *Prob*（*I*2 *缺失*）

*Prob*（*I*3 *缺失 & I*2 *缺失 & I*1 *缺失*）= *Prob*（*I*3 *缺失*）* *Prob*（*I*2 *缺失*）** Prob*（*I*1 *缺失*）

在样本较小和/或缺失程度非常高的情况下，这些方程可能在我们的数据中并不完全成立，但是如果任何变量的缺失量不到50％，我们通常应该有：

*Prob*（*I*3 *缺失* & *I*2 *缺失* & *I*1 *缺失*）< *Prob*（*I*3 *缺失* & *I*2 *缺失*）< *Prob*（*I*3 *缺失*）

在实际情况下，自己测试所有这些不等式可能会相当麻烦，尽管您可以编写一个函数以进行大规模测试。相反，我建议查看任何重要的异常值的可视化（即，一些相同变量的值远大于相同变量的某些值）。

更广泛地说，这种可视化在只有几个变量时很容易使用。一旦您有大量变量，您将不得不构建和可视化缺失的相关矩阵：

```py
## R (output not shown)
# Building the correlation matrices
tampa_miss <- tampa %>%
  select(-ID) %>%
  mutate(across(everything(),is.na))
tampa_cor <- cor(tampa_miss) %>%
  melt()

tacoma_miss <- tacoma %>%
  select(-ID) %>%
  mutate(across(everything(),is.na))
tacoma_cor <- cor(tacoma_miss) %>%
  melt()

```

```py
## Python (output not shown)
# Building the correlation matrices
tampa_miss_df = tampa_df.copy().drop(['ID'], axis=1).isna()
tacoma_miss_df = tacoma_df.copy().drop(['ID'], axis=1).isna()

tampa_cor = tampa_miss_df.corr()
tacoma_cor = tacoma_miss_df.corr()
```

[Figure 6-5](#correlation_matrices_for_completely_cor)显示了生成的相关矩阵。在左侧的矩阵中，对于Tampa，所有值都等于1：如果一个变量缺失，那么其他两个变量也是如此。在右侧的相关矩阵中，对于Tacoma，主对角线上的值等于1，但在其他地方都等于0：知道一个变量缺失并不能告诉您其他变量的缺失情况。

![完全相关缺失的相关矩阵（左）和完全不相关缺失的相关矩阵（右）](Images/BEDA_0605.png)

###### 图6-5\. 完全相关缺失（左）和完全不相关缺失（右）的相关矩阵

让我们回到我们的AirCnC数据集，并看看它在我们的理论访谈示例中概述的两个极端之间的位置。[Figure 6-6](#patterns_of_missing_data_left_parenthes)重复了[Figure 6-2](#patterns_of_missing_data)以便更易于访问。

[图 6-6](#patterns_of_missing_data_left_parenthes) 位于中间位置：所有可能的缺失模式都相当代表，这表明我们没有强烈聚集的缺失源。

![缺失数据的模式（重复）](Images/BEDA_0606.png)

###### 图 6-6\. 缺失数据的模式（重复 [图 6-2](#patterns_of_missing_data)）

[图 6-7](#correlation_matrix_of_missingness_in_ou) 显示了我们的 AirCnC 数据缺失的相关性矩阵。正如你所看到的，我们变量的缺失几乎完全不相关，完全在随机波动范围内。如果你想更加熟悉缺失中的相关性模式，本章的一个[GitHub上的练习](https://oreil.ly/BehavioralDataAnalysisCh6)要求你识别其中一些。作为提醒，查看相关性模式本身并不是必要的，但通常可以启发思考并节省时间。

![我们 AirCnC 数据中缺失的相关性矩阵](Images/BEDA_0607.png)

###### 图 6-7\. 我们 AirCnC 数据中缺失的相关性矩阵

# 诊断缺失数据

现在我们已经可视化了我们的缺失数据，是时候了解是什么导致了它。这就是因果图的作用所在，因为我们将使用它们来表示缺失数据的因果机制。

让我们从[第 1 章](ch01.xhtml#the_causal_behavioral_framework_for_da)中的一个非常简单的例子开始。在介绍因果图时，我提到未观察到的变量，比如顾客对香草冰淇淋的喜好，用一个较深的阴影矩形来表示（[图 6-8](#unobserved_variables_are_represented_in)）。

![未观察到的变量以椭圆形表示](Images/BEDA_0608.png)

###### 图 6-8\. 未观察到的变量以较深的阴影矩形表示

未观察到的变量，在某些学科中有时被称为“潜变量”，指的是我们实际上没有的信息，尽管理论上可能是可以访问的或不可访问的。在目前的情况下，假设我们强迫客户在购买前透露他们对香草的口味。这将在我们的系统中创建相应的数据，然后我们将用它们进行数据分析（[图 6-9](#collecting_previously_unobserved_inform)）。

![收集先前未观察到的信息](Images/BEDA_0609.png)

###### 图 6-9\. 收集先前未观察到的信息

但是，试图强迫客户透露他们不想透露的信息通常是不良的商业行为，并且通常是可选的。更一般地说，对一些客户收集了大量数据，而对其他客户则没有。我们将通过在 CD 中用虚线绘制相应的框来表示该情况（[图 6-10](#representing_partially_observed_variabl)）。

![用虚线框表示部分观察到的变量](Images/BEDA_0610.png)

###### 图 6-10\. 用虚线框表示部分观察到的变量

例如，对于三名顾客，我们可能有以下数据，其中一名顾客拒绝透露他们对香草冰淇淋的口味（[表 6-2](#the_data_underlying_our_cd)）。

表 6-2\. 我们CD的基础数据

| 顾客姓名 | 对香草的喜好 | 声明的口味 | 在摊位购买冰淇淋（Y/N） |
| --- | --- | --- | --- |
| 安 | 低 | 低 | N |
| 鲍勃 | 高 | 高 | Y |
| 卡罗琳 | 高 | N/A | Y |

在本章中，我们感兴趣的是理解一个变量的缺失原因，而不仅仅是理解一个变量的值的原因。因此，我们将创建一个变量来跟踪声明口味变量何时缺失 ([表 6-3](#adding_a_missingness_variable))。

表 6-3\. 添加一个缺失变量

| 顾客姓名 | 对香草的喜好 | 声明的香草口味 | 声明的口味缺失（Y/N） | 在摊位购买冰淇淋 |
| --- | --- | --- | --- | --- |
| 安 | 低 | 低 | N | N |
| 鲍勃 | 高 | 高 | N | Y |
| 卡罗琳 | 高 | N/A | Y | Y |

让我们在我们的CD中添加该变量 ([图 6-11](#adding_missingness_to_our_causal_diagra))。

![在我们的因果图中添加缺失](Images/BEDA_0611.png)

###### 图 6-11\. 在我们的因果图中添加缺失

我们通常将缺失作为相应部分观察变量的原因。直觉是信息完全存在于未观察变量中，并且部分观察变量等于未观察变量，除非信息被缺失变量“隐藏”。这种约定将使我们的生活更加轻松，因为它允许我们在CD中表达和讨论缺失的原因，这些原因代表了我们感兴趣的关系，而不必单独考虑缺失。

现在缺失已成为我们CD的一部分，下一个自然的步骤是问自己：“是什么原因导致了它？”

## 缺失的原因：鲁宾的分类

对于引起变量缺失的基本且互斥的三种可能性，统计学家唐纳德·鲁宾已进行了分类。

首先，如果一个变量的缺失仅取决于我们数据之外的变量，例如纯随机因素，那么该变量被称为*完全随机缺失*（MCAR） ([图 6-12](#stated_taste_is_missing_completely_at_r))。

![声明的口味是完全随机缺失的](Images/BEDA_0612.png)

###### 图 6-12\. 声明的口味是完全随机缺失的

接着，如果我们的数据中的任何一个变量影响其缺失状态，那么一个变量从MCAR（完全随机缺失）变为*随机缺失*（MAR）。数据之外的变量和随机因素也可能起作用，但变量的值可能不会影响其自身的缺失状态。例如，如果*购买*导致了*声明的香草口味*的缺失，例如因为我们只采访购买的顾客而不是路人 ([图 6-13](#stated_taste_is_missing_at_random))。

![声明的口味是随机缺失的](Images/BEDA_0613.png)

###### 图6-13\. 所述口味是随机缺失

最后，任何值影响其自身缺失的变量都被认为是 *缺失不是随机的*（MNAR），即使数据内外的其他变量也会影响缺失。我们的数据内外可能还有其他变量起作用，但变量一旦影响其自身缺失，就从MCAR或MAR变为MNAR。在我们的例子中，这意味着 *香草口味* 导致 *陈述香草口味* 的缺失（[图 6-14](#stated_taste_is_missing_not_at_randomdo)）。

![所述口味缺失不是随机](Images/BEDA_0614.png)

###### 图6-14\. 所述口味缺失不是随机发生的

###### 注意

我们通过从未观察到的变量而不是部分观察到的变量引入箭头来表示变量的值影响其缺失的想法。这样，我们可以做出有意义的陈述，如“实际上低于某个阈值的所有值在我们的数据中都是缺失的”。如果箭头来自部分可观察到的变量，我们将被困在无信息的陈述中，例如“缺失的值导致它们自己缺失”。

在理想的世界里，本节其余部分将包括识别每个缺失类别的方法。不幸的是，缺失数据分析仍然是一个尚未完全探索的开放领域。特别是，缺失和因果关系如何相互作用尚不清楚。因此，处理缺失数据仍然更像是一门艺术而不是科学。试图创建系统化的方法将需要处理大量的异常情况，以及引入循环论证，如“模式X表明变量1是MAR，除非变量2是MNAR；模式Y表明变量2是MNAR，除非变量1是MAR”。我已尽力在有限的数据集中涵盖尽可能多的情况，但在现实世界中，您可能会遇到“有点这个，有点那个”的情况，您需要判断如何继续。

然而，有些好消息是，除了我会指出的几个例外外，谨慎行事会花费更多时间但不会引入偏见。当您不确定一个变量是MCAR、MAR还是MNAR时，只需假设可能的最糟情形，您的分析将尽可能是无偏的。

在这种情况下，请回顾我们的AirCnC数据，看看如何在一个实际数据集中诊断缺失情况。作为一个快速的提醒，我们的数据集包含以下变量：

+   人口统计特征

    +   年龄

    +   性别

    +   状态（A、B和C）

+   个性特征

    +   开放性

    +   外向性

    +   神经质

+   预订金额

## 诊断MCAR变量

MCAR 变量是最简单的情况。传感器故障了，一个 bug 阻止了数据从客户的移动应用程序传输，或者客户只是错过了输入他们对香草冰淇淋口味的字段。总之，缺失以直觉上的“随机”方式发生。我们默认诊断 MCAR 变量：如果变量似乎不是 MAR，则将其视为 MCAR。换句话说，在缺乏证据的情况下，你可以将 MCAR 视为我们的零假设。

我们将用来诊断缺失性的主要工具是逻辑回归，即一个变量是否缺失，取决于我们数据集中的所有其他变量。让我们以 *外向性* 变量为例：

```py
## Python (output not shown)
available_data_df['md_extra'] = available_data_df['extra'].isnull().astype(float)
md_extra_mod =smf.logit('md_extra~age+open+neuro+gender+state+bkg_amt',
                      data=available_data_df)
md_extra_mod.fit().summary()

```

```py
## R
> md_extra_mod <- glm(is.na(extra)~.,
                        family = binomial(link = "logit"), 
                        data=available_data)
> summary(md_extra_mod)

...
Coefficients:
              Estimate Std. Error z value Pr(>|z|)
(Intercept) -0.7234738  0.7048598  -1.026    0.305
age         -0.0016082  0.0090084  -0.179    0.858
open         0.0557508  0.0425013   1.312    0.190
neuro        0.0501370  0.0705626   0.711    0.477
genderF     -0.0236904  0.1659661  -0.143    0.886
stateB      -0.0780339  0.2000428  -0.390    0.696
stateC      -0.0556228  0.2048822  -0.271    0.786
bkg_amt     -0.0007701  0.0011301  -0.681    0.496
...
```

没有任何变量具有大且强烈统计显著的系数。在没有任何其他证据的情况下，这表明 *外向性* 的缺失性纯粹是随机的，我们将把我们的 *外向性* 变量视为 MCAR。

你可以将 MCAR 数据视为掷骰子或抛硬币。从我们的角度来看，这两种行为都是“随机”的，但它们仍然遵守物理法则。理论上，如果我们有足够的信息和计算能力，结果将是完全可预测的。这里也可能发生同样的情况。当我们说 *外向性* 是 MCAR 时，我们并不是在说“*外向性* 的缺失性基本上是随机且不可预测的”，我们只是说“我们目前分析中包含的变量没有一个与 *外向性* 的缺失性相关联。”但可能——甚至很可能——其他变量（责任感？信任？对技术的熟悉度？）会相关联。我们的目标不是对 *外向性* 发表哲学性声明，而是确定其缺失是否可能会使我们的分析产生偏见，考虑到当前可用的数据。

## 诊断 MAR 变量

MAR 变量是指其缺失性取决于数据集中其他变量的值。如果数据集中的其他变量能够预测某个变量的缺失性，则 MAR 将成为我们该变量的默认假设，除非我们有足够强的证据表明其为 MNAR。让我们看看 *状态* 变量的情况：

```py
## R (output not shown)
md_state_mod <- glm(is.na(state)~.,
                    family = binomial(link = "logit"), 
                    data=available_data)
summary(md_state_mod)

```

```py
## Python
available_data_df['md_state'] = available_data_df['state'].isnull()\
    .astype(float)
md_state_mod =smf.logit('md_state~age+open+extra+neuro+gender+bkg_amt',
                      data=available_data_df)
md_state_mod.fit(disp=0).summary()
...
              coef   std err       z         P>|z|   [0.025  0.975]
Intercept   -0.2410   0.809     -0.298       0.766   -1.826  1.344
gender[T.F] -0.1742   0.192     -0.907       0.364   -0.551  0.202
age          0.0206   0.010      2.035       0.042    0.001  0.040
open         0.0362   0.050      0.727       0.467   -0.061  0.134
extra        0.0078   0.048      0.162       0.871   -0.087  0.102
neuro       -0.1462   0.087     -1.687       0.092   -0.316  0.024
bkg_amt     -0.0019   0.001     -1.445       0.149   -0.005  0.001
...
```

*年龄* 略有显著性，具有正系数。换句话说，年长客户似乎更不可能提供他们的州。相应的因果图表示在 [图 6-15](#gender_missing_at_random) 中。

![性别随机缺失](Images/BEDA_0615.png)

###### 图 6-15\. 性别随机缺失

我们可以通过绘制观察到的 *年龄* 按 *状态* 缺失情况的密度图（[图 6-16](#density_of_missing_and_observed_state)）来确认这种相关性。相对于年轻客户，*状态* 在年长客户中的观察值更多，或者反过来，对于年长客户，*状态* 的缺失值更多。

![缺失和观察到的年龄密度状态数据](Images/BEDA_0616.png)

###### 图 6-16\. 缺失和观察到的状态数据密度，按观察到的年龄分

这个密度图的一个局限性是它并不显示X变量（这里是*Age*）也缺失的行。当该变量也有缺失值时，这可能会产生问题或误导。一个可能的技巧是将X变量的缺失值替换为一个不合理的值，比如−10。*Age*没有任何缺失值，所以我们将使用*Xtraversion*作为我们的X变量，该变量有缺失值。让我们按*Extraversion*的值绘制观察到的和缺失的*State*数据的密度（[图6-17](#density_of_missing_and_observed_state_d)）。

![按Extraversion水平绘制的缺失和观察到的State数据密度，包括缺失的Extraversion](Images/BEDA_0617.png)

###### 图6-17\. 按Extraversion水平绘制的缺失和观察到的State数据密度，包括缺失的Extraversion

[图6-17](#density_of_missing_and_observed_state_d)显示，在没有*Extraversion*数据的个体中，我们观察到的*State*要比*State*缺失的个体更多。总体上，我们看到了强有力的证据表明*State*不是MCAR，而是MAR，因为其缺失似乎与我们数据集中其他可用变量相关。

###### 注意

你可能注意到我在早些时候谈论*Age*（或*Extraversion*）与*State*缺失之间关系时使用了“相关性”这个词。事实上，到目前为止，我们只展示了相关性，完全有可能*Age*并不导致*State*的缺失，而是它们两者都由第三个未观察到的变量导致。幸运的是，在谈论缺失时，相关性的因果性质（或其缺乏）不会影响我们的分析。松散地将两者等同起来不会引入偏差，因为我们实际上永远不会处理那种关系的系数。

## 诊断MNAR变量

MNAR变量是其缺失性取决于其自身值的变量：较高的值缺失的可能性比较低的值更高，反之亦然。这种情况对数据分析是最具问题的，并且诊断起来最为棘手。它很难诊断，因为根据定义，我们不知道缺失的值。因此，我们需要做更多的调查工作。

让我们来看一下*Neuroticism*变量，并且像之前一样，首先对数据中其缺失情况进行回归分析：

```py
## Python (output not shown)
available_data_df['md_neuro'] = available_data_df['neuro'].isnull()\
    .astype(float)
md_neuro_mod =smf.logit('md_neuro~age+open+extra+state+gender+bkg_amt',
                      data=available_data_df)
md_neuro_mod.fit(disp=0).summary()

```

```py
## R
md_neuro_mod <- glm(is.na(neuro)~.,
                    family = binomial(link = "logit"), 
                    data=available_data)
summary(md_neuro_mod)

...
Coefficients:
             Estimate Std. Error z value Pr(>|z|)   
(Intercept) -0.162896   0.457919  -0.356  0.72204   
age         -0.012610   0.008126  -1.552  0.12071   
open         0.052419   0.038502   1.361  0.17337   
extra       -0.084991   0.040617  -2.092  0.03639 * 
genderF     -0.093537   0.151376  -0.618  0.53663   
stateB       0.047106   0.181932   0.259  0.79570   
stateC      -0.128346   0.187978  -0.683  0.49475   
bkg_amt      0.003216   0.001065   3.020  0.00253 **
...
```

我们可以看到*BookingAmount*具有显著的系数。表面上看，这表明*神经质*对*BookingAmount*的MAR。然而，这是一个关键线索，*BookingAmount*在我们的CD中是*神经质*的子节点。从行为角度来看，*神经质*对*BookingAmount*的MNAR更为可能（即，缺失是由个性特征而非客户消费金额驱动的）。

确认我们怀疑的一种方法是识别另一个具有缺失数据的变量的子类，理想情况下与其尽可能相关，与第一个子类的相关性尽可能小。在我们的次要数据集中，我们有关于客户终生在公司购买的旅行保险总金额的数据。每次旅行的费用取决于只与预订金额略微相关的行程特征，因此在这方面我们做得很好。在我们的数据集中添加*保险*后，我们发现它强烈预测了*神经质*的缺失，并且观察到和缺失的*神经质*之间的*保险*金额分布彼此截然不同（[图 6-18](#density_of_missing_and_observed_neuroti)）。

![观察到的保险金额下缺失和观察到的神经质数据密度](Images/BEDA_0618.png)

###### 图6-18\. 观察到的保险金额下缺失和观察到的神经质数据密度

我们找到与*神经质*的缺失相关的更多子变量，我们的案例就越强烈，表明这个变量是MNAR。正如我们稍后将看到的，处理MNAR变量的方法是在我们的填充模型中添加辅助变量，我们的子变量是理想的候选人，因此找到它们不是浪费时间而是下一步的先机。

从技术上讲，我们永远无法完全证明一个变量是MNAR而不仅仅是MAR其子类中的一个，但这不是问题：辅助变量不会使缺失数据修补产生偏差，如果原始变量确实是MAR而不是MNAR。

## 缺失作为一个谱系

鲁宾的分类依赖于二元测试。例如，只要一个变量更可能对较高值缺失而不是对较低值缺失（反之亦然），它就是MNAR，不考虑任何其他因素。然而，该关系在值和缺失之间的形状对实际目的很重要：如果一个变量的*所有*值在某个阈值以上或以下都缺失，我们将需要与默认方法不同的方式处理这个变量。这种情况也可能发生在MAR变量中，因此值得退后一步，更广泛地考虑缺失形状。

我们可以将变量的缺失看作是在完全概率性和完全确定性之间的一个谱系。在谱系的“概率性”端，变量是MCAR，所有值都可能缺失。在谱系的“确定性”端，存在一个阈值：所有在阈值一侧的个体的值都是缺失的，而在阈值另一侧的个体的值是可用的。这通常是由于业务规则的应用。例如，在招聘背景下，如果只有GPA超过3.0的候选人才能接受面试，那么对于低于该阈值的候选人将没有任何面试分数。这会使*面试分数*在*GPA*上成为MAR（[Figure 6-19](#interview_score_being_mar_on_gpa)）。

![Interview score being MAR on GPA](Images/BEDA_0619.png)

###### 图6-19. *面试分数*在*GPA*上成为MAR

###### 注意

精研的MCAR/MAR/MNAR分类仅基于缺失原因，而不考虑该因果关系是否显示出随机性。在这里，令人感到反直觉的是，*InterviewScore*的缺失基于*GPA*的确定性关系，使得*InterviewScore*在*GPA*上成为MAR，尽管其中并没有涉及随机性。

这也可能发生在MNAR类型的变量上，仅记录超过或低于某一阈值的值。例如，只有在文件中保存了正常范围之外的值，或者只有超过或低于某一阈值的人才会注册（例如，出于税务目的）。

在完全随机和完全确定之间的这两个极端情况（无论是MAR还是MNAR类型），存在着根据缺失原因的值连续增加或减少缺失概率的情况。

[Figure 6-20](#missingness_spectrumcomma_from_mcar_lef)显示了两个变量X和Y的最简单情况下的情况，其中X具有缺失值。为了便于阅读，可用值显示为实心方块，而“缺失”值显示为叉号。[Figure 6-20](#missingness_spectrumcomma_from_mcar_lef)的第一行显示了Y相对于X的散点图，第二行显示了X和缺失概率之间关系的线图：

+   最左列显示X是MCAR。缺失的概率在0.5处恒定且与X无关。方块和叉号在图中分布类似。

+   中间列显示X是具有逐渐增强概率MNAR的概率性MNAR。方块在图的左侧更为普遍，而叉号在右侧更为普遍，但左侧仍然存在叉号，右侧仍然存在方块。

+   最右侧的列显示X是确定性MNAR。所有低于5的X值是可用的（方块），而所有高于5的值都是“缺失”的（叉号）。

![从 MCAR（最左侧）到确定性 MNAR（最右侧），穿过概率性 MNAR（中心）的缺失程度谱](Images/BEDA_0620.png)

###### 图 6-20\. 缺失程度谱，从 MCAR（最左侧）到确定性 MNAR（最右侧），穿过概率性 MNAR（中心）

这种缺失程度的谱很少在缺失数据的统计处理中讨论，因为仅凭数学方法很难确认。但这是一本关于行为分析的书籍，因此我们可以和应该使用常识和业务知识。在 GPA 的例子中，数据的阈值是由你应该了解的业务规则应用而来的。在大多数情况下，您期望一个变量落在某些值的特定范围内，并且您应该知道可能出现某个可能值不在您的数据中的概率有多高。

在我们的 AirCnC 调查数据中，我们有三个人格特质：*开放性*、*外倾性*和*神经质*。在现实生活中，这些变量将是对几个问题回答的汇总，并且在已知区间内有钟形分布（参见 Funder (2016) 了解人格心理学的良好介绍）。让我们假设在我们的数据中，相关区间是从 0 到 10，并且让我们来看看我们变量的分布（[图 6-21](#histograms_of_personality_traits_in_our)）。

![我们数据中的人格特征直方图](Images/BEDA_0621.png)

###### 图 6-21\. 我们数据中的人格特征直方图

显然，*神经质*有些问题。基于人格特征的构建方式，我们预期三个变量的曲线类型相同，而不会出现大量数值为5的客户，也不会一个数值为4的客户都没有。这极大地暗示了一个确定性的缺失非随机变量，我们必须相应地处理。

现在，您应该能够合理判断数据集中缺失情况的模式。有多少缺失值？它们的缺失是否与变量本身的值（MNAR）、另一个变量（MAR）或者都不相关（MCAR）有关？这些缺失关系是概率性的还是确定性的？

[图 6-22](#decision_tree_to_diagnose_missing_data) 展示了一个决策树，总结了我们诊断缺失数据的逻辑。

![诊断缺失数据的决策树](Images/BEDA_0622.png)

###### 图 6-22\. 诊断缺失数据的决策树

在接下来的部分，我们将看到如何在每种情况下处理缺失数据。

# 处理缺失数据

当我们进入本章的操作部分时，首先要记住的是我们并不是为了处理缺失数据而处理缺失数据：我们的目标是在我们的数据中获得无偏差和准确的因果关系估计。只有当缺失数据干扰了这一目标时，缺失数据才会成为问题。

这一点需要强调，因为你的第一反应可能是，成功处理缺失数据的结果是一个没有缺失数据的数据集，但事实并非如此。我们将使用的方法，多重插补（MI），会为您的数据创建多个副本，每个副本都有其自己的插补值。通俗地说，我们永远不会说“Bob 缺失的年龄的正确替代值是 42 岁”，而是“Bob 可能是 42 岁、38 岁、44 岁、42 岁或者 38 岁”。没有单一的最佳猜测，而是一系列可能性。另一个最佳实践方法，最大似然估计，甚至不会对个别值进行任何猜测，只处理高阶系数，例如均值和协方差。

在下一小节中，我将为您提供 MI 方法的高级概述。之后，我们将深入研究模型的更详细算法规格：

1.  首先，预测均值匹配算法

1.  接下来，正常算法

1.  最后，如何将辅助变量添加到算法中

不幸的是，鲁宾分类中缺失类型与适当的算法规格之间并不存在一对一的关系，因为可用的信息量也很重要（参见 [表 6-4](#optimal_mi_parameters_based_on_type_of)）。

表 6-4. 根据缺失类型和可用信息的最佳 MI 参数

| 缺失类型 | 无信息 | 变量分布为正态 | 缺失分布为确定性 |
| --- | --- | --- | --- |
| **MCAR** | 均值匹配 | 正态分布 | （不可能） |
| **MAR** | 均值匹配 | 正态分布 | 正态分布 + 辅助变量 |
| **MNAR** | 均值匹配 + 辅助变量 | 正态分布 + 辅助变量 | 正态分布 + 辅助变量 |

## 多重插补（MI）介绍

要理解多重插补的工作原理，有助于将其与传统的处理缺失数据的方法进行对比。除了简单地删除所有具有缺失值的行之外，传统方法都依赖于用特定值替换缺失值。替换值可以是变量的整体平均值，或者基于其他可用于该客户的变量预测的值。无论用于替换值的规则如何，这些方法都存在根本性缺陷，因为它们忽略了由缺失数据引入的额外不确定性，并且可能会在分析中引入偏差。

解决这个问题的 MI 解决方案，正如其名称所示，是构建多个数据集，其中缺失值被不同的值替换，然后用每个数据集运行我们感兴趣的分析，并最终聚合生成的系数。

在 R 和 Python 中，整个过程都是在后台管理的，如果您想保持简单，只需指定要运行的数据和分析即可。

首先让我们看看 R 代码：

```py
## R
> MI_data <- mice(available_data, print = FALSE)
> MI_summ <- MI_data  %>%
    with(lm(bkg_amt~age+open+extra+neuro+gender+state)) %>%
    pool() %>%
    summary()
> print(MI_summ)
         term   estimate  std.error statistic        df      p.value
1 (Intercept) 240.990671 15.9971117 15.064636  22.51173 3.033129e-13
2         age  -1.051678  0.2267569 -4.637912  11.61047 6.238993e-04
3        open   3.131074  0.8811587  3.553360 140.26375 5.186727e-04
4       extra  11.621288  1.2787856  9.087753  10.58035 2.531137e-06
5       neuro  -6.799830  1.9339658 -3.516003  15.73106 2.929145e-03
6     genderF -11.409747  4.2044368 -2.713740  20.73345 1.310002e-02
7      stateB  -9.063281  4.0018260 -2.264786 432.54286 2.401986e-02
8      stateC  -5.334055  4.7478347 -1.123471  42.72826 2.675102e-01
```

`mice`包（多重插补通过链式方程）具有`mice()`函数，用于生成多个数据集。然后，我们使用`with()`关键字将感兴趣的回归应用于每一个数据集。最后，从`mice`中使用`pool()`函数以我们可以用传统的`summary()`函数读取的格式汇总结果。

Python代码几乎相同，因为它实现了相同的方法：

```py
## Python 
MI_data_df = mice.MICEData(available_data_df)                                 
fit = mice.MICE(model_formula='bkg_amt~age+open+extra+neuro+gender+state', 
                model_class=sm.OLS, data=MI_data_df)                
MI_summ = fit.fit().summary()                                                       
print(MI_summ)

                          Results: MICE
===================================================================
Method:                  MICE         Sample size:          2000   
Model:                   OLS          Scale                 5017.30
Dependent variable:      bkg_amt      Num. imputations      20     
-------------------------------------------------------------------
           Coef.   Std.Err.    t    P>|t|   [0.025   0.975]   FMI  
-------------------------------------------------------------------
Intercept 120.3570   8.8662 13.5748 0.0000 102.9795 137.7344 0.4712
age        -1.1318   0.1726 -6.5555 0.0000  -1.4702  -0.7934 0.2689
open        3.1316   0.8923  3.5098 0.0004   1.3828   4.8804 0.1723
extra      11.1265   1.0238 10.8680 0.0000   9.1200  13.1331 0.3855
neuro      -4.5894   1.7968 -2.5542 0.0106  -8.1111  -1.0677 0.4219
gender_M   65.9603   4.8191 13.6873 0.0000  56.5151  75.4055 0.4397
gender_F   54.3966   4.6824 11.6171 0.0000  45.2192  63.5741 0.4154
state_A    40.9352   3.9080 10.4748 0.0000  33.2757  48.5946 0.3921
state_B    37.3490   4.0727  9.1706 0.0000  29.3666  45.3313 0.2904
state_C    42.0728   3.8643 10.8875 0.0000  34.4989  49.6468 0.2298
===================================================================
```

在这里，`mice`算法从`statsmodels.imputation`包中导入。`MICEData()`函数生成多个数据集。然后，通过`MICE()`函数指定模型公式、回归类型（这里是`statsmodels.OLS`）和要使用的数据。我们使用`.fit()`和`.summary()`方法拟合我们的模型，然后打印结果。

###### 注意

Python实现的`mice`方法的一个复杂之处在于它不支持分类变量作为预测因子。如果您确实想使用Python，您首先需要对分类变量进行独热编码。以下代码展示了如何处理*性别*变量：

```py
## Python
gender_dummies = pd.get_dummies(available_data_df.\
                                gender, 
                                prefix='gender')
available_data_df =  pd.concat([available_data_df, 
                                gender_dummies], 
                               axis=1)
available_data_df.gender_F = \
np.where(available_data_df.gender.isna(), 
         float('NaN'), available_data_df.gender_F)
available_data_df.gender_M = \
np.where(available_data_df.gender.isna(), 
         float('NaN'), available_data_df.gender_M)
available_data_df =  available_data_df.\
drop(['gender'], axis=1)
```

首先，我们使用pandas的`get_dummies()`函数创建变量`gender_F`和`gender_M`。在将这些列添加到我们的数据帧之后，我们指示缺失值的位置（默认情况下，独热编码函数在分类变量值缺失时将所有二进制变量设置为0）。最后，我们从数据中删除原始分类变量，并使用包含新变量的模型进行拟合。

然而，独热编码通过删除变量之间的逻辑连接来破坏数据的内部结构，因此效果可能因人而异（例如，您可以看到因为结构不同，R和Python中的分类变量系数不同），如果分类变量在您的数据中起重要作用，我建议您使用R而不是Python。

就是这样！如果您现在停止阅读本章，您将得到一个处理缺失数据的解决方案，这比传统方法要好得多。然而，通过花时间揭示内部机制并更好地理解插补算法，我们可以做得更好。

## 默认插补方法：预测均值匹配

在前一小节中，我们未指定插补方法，依赖于`mice`的默认设置。在Python中，唯一可用的插补方法是预测均值匹配，因此在这里没有其他操作。让我们来看看R中默认的插补方法，通过请求插补过程的摘要：

```py
## R
> summary(MI_data)
Class: mids
Number of multiple imputations:  5 
Imputation methods:
     age     open    extra    neuro   gender    state  bkg_amt 
      ""       ""    "pmm"    "pmm"       "" "logreg"    "pmm" 
PredictorMatrix:
       age open extra neuro gender state bkg_amt
age      0    1     1     1      1     1       1
open     1    0     1     1      1     1       1
extra    1    1     0     1      1     1       1
neuro    1    1     1     0      1     1       1
gender   1    1     1     1      0     1       1
state    1    1     1     1      1     0       1
```

这是很多信息。现在，让我们只看看`Imputation methods`一行。没有缺失数据的变量有一个空字段`""`，这是有道理的，因为它们不会被填充。分类变量具有方法`logreg`，即逻辑回归。最后，数值变量具有方法`pmm`，即预测均值匹配（PMM）。`pmm`方法的工作原理是选择具有缺失值的个体的最近邻居，并用一个邻居的值替换缺失值。例如，假设数据集只有两个变量：*Age* 和 *ZipCode*。如果您有一个来自邮编60612且年龄缺失的客户，算法将随机选择同一邮编中另一个客户的年龄，或者尽可能接近的客户的年龄。

由于过程中存在一些随机性，每个填充的数据集最终会得到略有不同的值，我们可以通过`mice`包中方便的`densityplot()`函数可视化来查看：

```py
## R 
> densityplot(MI_data, thicker = 3, lty = c(1,rep(2,5)))
```

[图 6-23](#distributions_of_imputed_values_for_num)显示了原始可用数据（粗线）和填充数据集（细虚线）中数值变量的分布。如您所见，分布与原始数据非常接近；唯一的例外是*BookingAmount*，在填充数据集中，它的分布总体上更集中在均值周围（即“更高的峰值”）。

![我们数据中数值变量的填充值分布](Images/BEDA_0623.png)

###### 图 6-23\. 我们数据中数值变量的填充值分布

PMM 具有一些重要的特性，这些特性可能是需要的，也可能不需要，这取决于上下文。最重要的特性是它基本上是一种插值方法。因此，您可以将 PMM 想象为创建介于现有值之间的值。通过这样做，它最小化了创建荒谬情况的风险，比如怀孕的父亲或负数金额。这种方法在变量具有奇怪分布时也能很好地工作，比如我们数据中的*Age*，它有两个峰值，因为它对整体分布的形状没有假设；它只是选择一个邻居。

然而，PMM 有几个缺点：它速度较慢，并且在大数据集上不易扩展，因为它必须不断重新计算个体之间的距离。此外，当您有许多变量或许多缺失值时，最近的邻居可能“很远”，填充的质量会降低。这就是为什么当我们拥有分布信息时，PMM 不会是我们首选的选项，正如我们将在下一小节中看到的那样。

## 从 PMM 到正态填充（仅限 R）

虽然PMM是一个不错的起点，但我们经常有关于数值变量分布的信息，我们可以利用这些信息来加快和改进R中的插补模型。特别是，行为和自然科学通常假设数值变量服从正态分布，因为这是非常常见的。在这种情况下，我们可以对变量拟合正态分布，然后从该分布中提取插补值，而不是使用PMM。这通过创建一个插补方法的向量来完成，对于我们将假设正态性的变量，向量中的值设为`"norm.nob"`，然后将该向量传递给`mice()`函数的`parameter`方法。

```py
## R
> imp_meth_dist <- c("pmm", rep("norm.nob",3), "", "logreg", "norm.nob")
> MI_data_dist <- mice(available_data, print = FALSE, method = imp_meth_dist)
```

正如您所见，语法非常简单。唯一的问题是确定我们要为哪些数值变量使用正常插补。让我们来看看我们可用数据中的数值变量（[图 6-24](#distribution_of_numeric_variables_in_ou)）。

![我们数据中的数值变量分布](Images/BEDA_0624.png)

###### 图 6-24\. 我们数据中的数值变量分布

*年龄*显然不是正态的，因为它有两个峰，但所有其他变量只有一个峰。*开放性*、*外向性*和*预订金额*看起来也相对对称（在技术上来说，它们不倾斜）。统计模拟表明，只要一个变量是单峰的，并且在一个方向上没有“粗尾”，假设正态性不会引入偏差。因此，我们可以假设*开放性*、*外向性*和*预订金额*是正态分布的。

正如我们在前一节中看到的，*神经质*呈现出不寻常的非对称模式：尽管我们使用的心理学尺度从0到10，但其值被限制在[5,10]之间，这表明*神经质*可能是“确定性地”MNAR，即*神经质*的所有值低于某个阈值都会缺失。在这种情况下使用PMM进行插补是有问题的：在大部分值范围内插补时几乎没有或根本没有邻居可用。极端情况下，X的所有缺失值将被插补为5，即阈值的值。这是一种普通方法更能恢复真实缺失值的情况。我们可以通过比较两种方法插补的值来看出这一点。[图 6-25](#pmm_imputation_left_parenthesistopright)显示了*神经质*的可用值（方块）以及用PMM方法插补的值（十字，顶部面板）和用普通方法插补的值（十字，底部面板）。

![PMM插补（顶部）和普通插补（底部）对确定性MNAR变量的影响](Images/BEDA_0625.png)

###### 图 6-25\. PMM插补（顶部）和普通插补（底部）对确定性MNAR变量的影响

正如您所见，PMM 方法不对*神经质*低于 5 的任何值进行插补，而普通方法则会这样做。此外，PMM 方法对接近 10 的值进行了过多的插补，而普通方法更好地捕捉了分布的整体形状。然而，普通方法远未恢复真实分布（该分布一直延伸到零）。这是确定性地为 MAR 或 MNAR 变量的常见问题。我们可以通过使用辅助变量进一步改进普通插补，正如我们将在下一小节中看到的那样。

## 添加辅助变量

很多时候，我们会有与我们的某个具有缺失数据的变量相关的变量（例如，该变量的原因或影响），但不属于我们的回归模型。这是 `mice` 算法特别出色的情况，因为我们可以将这些变量添加到我们的插补模型中，以提高其准确性。然后它们被称为我们插补模型的“辅助变量”。

对于我们的 AirCnC 示例，可用的补充数据集包含两个变量，*保险*和*活跃*。前者指示客户购买的旅行保险金额，与*神经质*强相关；而后者则测量客户选择活跃度假（例如攀岩）的程度，并与*外向性*强相关。我们将使用它们来帮助插补这两个人格变量。

向插补模型添加辅助变量非常简单：我们只需在插补阶段之前将它们添加到我们的数据集中即可。

```py
## R
augmented_data <- cbind(available_data, available_data_supp)
MI_data_aux <- mice(augmented_data, print = FALSE)

```

```py
## Python
augmented_data_df = pd.concat([available_data_df, available_data_supp_df], 
                              axis=1)
MI_data_aux_df = mice.MICEData(augmented_data_df)
```

我们可以像以前一样运行所有分析。在添加辅助变量时，通常有意义使用普通方法来处理与我们的辅助变量相关的变量（这里是*神经质*和*外向性*），特别是当这些变量被截断或 MNAR 时。

除了计算约束外，我们可以包含的辅助变量数量没有限制。然而，潜在风险是我们的一些辅助变量可能仅仅因为纯粹的随机性而似乎与原始数据集中的某个变量相关，例如，*保险*看似与*外向性*相关，尽管实际上并非如此。这样的“假阳性”相关性将会被插补模型错误地加强。

解决这个潜在问题的方法是仅限制辅助变量用于某些变量的插补。不幸的是，这种解决方案只在 R 中可用。这就是 `mice()` 函数的预测矩阵发挥作用的地方。该矩阵出现在打印插补阶段摘要时，并且还可以直接从我们的 `MIDS` 对象中提取：

```py
## R
> pred_mat <- MI_data_aux$predictorMatrix
> pred_mat
          age open extra neuro gender state bkg_amt insurance active
age         0    1     1     1      1     1       1         1      1
open        1    0     1     1      1     1       1         1      1
extra       1    1     0     1      1     1       1         1      1
neuro       1    1     1     0      1     1       1         1      1
gender      1    1     1     1      0     1       1         1      1
state       1    1     1     1      1     0       1         1      1
bkg_amt     1    1     1     1      1     1       0         1      1
insurance   1    1     1     1      1     1       1         0      1
active      1    1     1     1      1     1       1         1      0
```

该矩阵指示用于填充哪些变量。默认情况下，除了自身外，所有变量都用于填充所有变量。矩阵中的“1”表示“列”变量用于填充“行”变量。因此，我们需要修改最后两列，用于仅填充*保险*和*活跃度*到*神经质*和*外向性*：

```py
## R
> pred_mat[,"insurance"] <- 0
> pred_mat[,"active"] <- 0
> pred_mat["neuro","insurance"] <- 1
> pred_mat["extra","active"] <- 1
> pred_mat
          age open extra neuro gender state bkg_amt insurance active
age         0    1     1     1      1     1       1         0      0
open        1    0     1     1      1     1       1         0      0
extra       1    1     0     1      1     1       1         0      1
neuro       1    1     1     0      1     1       1         1      0
gender      1    1     1     1      0     1       1         0      0
state       1    1     1     1      1     0       1         0      0
bkg_amt     1    1     1     1      1     1       0         0      0
insurance   1    1     1     1      1     1       1         0      0
active      1    1     1     1      1     1       1         0      0
```

通过这种修改，我们将减少意外地将偶然相关性纳入我们的填充模型的风险。

## 扩展缺失数据集数量

`mice`算法在R中的默认生成的填充数据集数量为5，在Python中为10。这些对于探索性分析来说是很好的默认值。

在最终运行时，如果你只关心回归系数的估计值，应使用20（通过将`m=20`作为`mice()`函数的参数设置传递）。如果你需要更精确的信息，例如置信区间或变量之间的交互作用，可能需要目标设定为50到100。主要的限制因素是计算机的速度和内存——如果你的数据集大小为100 Mb甚至1 Gb，你是否有足够的RAM来创建100份拷贝？——以及你的耐心。

更改填充数据集数量的语法很简单。在R中，它作为参数传递给`mice()`函数，而在Python中，它作为`MICE`对象的`.fit()`方法的参数传递：

```py
## R
MI_data <- mice(available_data, print = FALSE, m=20)

```

```py
## Python
fit = mice.MICE(model_formula='bkg_amt~age+open+extra+neuro+gender+state', 
                model_class=sm.OLS, data=MI_data_df) 
MI_summ = fit.fit(n_imputations=20).summary()
```

# 结论

缺失数据在行为数据分析中可能会带来真正的问题，但这并非必然。至少，使用R或Python中的`mice`包及其默认参数将优于删除所有具有缺失值的行。通过基于Rubin的分类正确诊断缺失性，并利用所有可用信息，通常可以取得更好的效果。总结决策规则，[图6-26](#decision_tree_to_diagnose_missing_datad)展示了用于诊断缺失数据的决策树，[表6-5](#optimal_imputation_method_based_on_type)展示了基于缺失类型和可用信息的最佳MI参数。

![诊断缺失数据的决策树](Images/BEDA_0626.png)

###### 图6-26\. 诊断缺失数据的决策树

表6-5\. 根据缺失类型和可用信息的最佳MI参数

| 缺失类型 | 无信息 | 变量分布正常 | 缺失分布确定性 |
| --- | --- | --- | --- |
| **MCAR** | PMM | `norm.nob` |   |
| **MAR** | PMM | `norm.nob` | `norm.nob` + aux. var. |
| **MNAR** | PMM + aux. var. | `norm.nob` + aux. var. | `norm.nob` + aux. var. |
